[["0",{"pageContent":"import { BaseLanguageModel } from \"../base_language/index.js\";\nimport { CallbackManager, Callbacks } from \"../callbacks/manager.js\";\nimport { LLMChain } from \"../chains/llm_chain.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport {\nAgentAction,\nAgentFinish,\nAgentStep,\nBaseChatMessage,\nChainValues,\n} from \"../schema/index.js\";\nimport { Tool } from \"../tools/base.js\";\nimport {\nAgentActionOutputParser,\nAgentInput,\nSerializedAgent,\nStoppingMethod,\n} from \"./types.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type OutputParserArgs = Record<string, any>;\n\nclass ParseError extends Error {\noutput: string;\n\nconstructor(msg: string, output: string) {\nsuper(msg);\nthis.output = output;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":1,"to":30}}}}],["1",{"pageContent":"abstract class BaseAgent {\nabstract get inputKeys(): string[];\n\nget returnValues(): string[] {\nreturn [\"output\"];\n}\n\nget allowedTools(): string[] | undefined {\nreturn undefined;\n}\n\n/**\n* Return the string type key uniquely identifying this class of agent.\n*/\n_agentType(): string {\nthrow new Error(\"Not implemented\");\n}\n\n/**\n* Return the string type key uniquely identifying multi or single action agents.\n*/\nabstract _agentActionType(): string;\n\n/**\n* Return response when agent has been stopped due to max iterations\n*/\nreturnStoppedResponse(\nearlyStoppingMethod: StoppingMethod,\n_steps: AgentStep[],\n_inputs: ChainValues,\n_callbackManager?: CallbackManager\n): Promise<AgentFinish> {\nif (earlyStoppingMethod === \"force\") {\nreturn Promise.resolve({\nreturnValues: { output: \"Agent stopped due to max iterations.\" },\nlog: \"\",\n});\n}\n\nthrow new Error(`Invalid stopping method: ${earlyStoppingMethod}`);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":401,"to":441}}}}],["2",{"pageContent":"/**\n* Prepare the agent for output, if needed\n*/\nasync prepareForOutput(\n_returnValues: AgentFinish[\"returnValues\"],\n_steps: AgentStep[]\n): Promise<AgentFinish[\"returnValues\"]> {\nreturn {};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":802,"to":811}}}}],["3",{"pageContent":"abstract class BaseSingleActionAgent extends BaseAgent {\n_agentActionType(): string {\nreturn \"single\" as const;\n}\n\n/**\n* Decide what to do, given some input.\n*\n* @param steps - Steps the LLM has taken so far, along with observations from each.\n* @param inputs - User inputs.\n* @param callbackManager - Callback manager.\n*\n* @returns Action specifying what tool to use.\n*/\nabstract plan(\nsteps: AgentStep[],\ninputs: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<AgentAction | AgentFinish>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":1205,"to":1224}}}}],["4",{"pageContent":"abstract class BaseMultiActionAgent extends BaseAgent {\n_agentActionType(): string {\nreturn \"multi\" as const;\n}\n\n/**\n* Decide what to do, given some input.\n*\n* @param steps - Steps the LLM has taken so far, along with observations from each.\n* @param inputs - User inputs.\n* @param callbackManager - Callback manager.\n*\n* @returns Actions specifying what tools to use.\n*/\nabstract plan(\nsteps: AgentStep[],\ninputs: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<AgentAction[] | AgentFinish>;\n}\n\nexport interface LLMSingleActionAgentInput {\nllmChain: LLMChain;\noutputParser: AgentActionOutputParser;\nstop?: string[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":1604,"to":1629}}}}],["5",{"pageContent":"class LLMSingleActionAgent extends BaseSingleActionAgent {\nllmChain: LLMChain;\n\noutputParser: AgentActionOutputParser;\n\nstop?: string[];\n\nconstructor(input: LLMSingleActionAgentInput) {\nsuper();\nthis.stop = input.stop;\nthis.llmChain = input.llmChain;\nthis.outputParser = input.outputParser;\n}\n\nget inputKeys(): string[] {\nreturn this.llmChain.inputKeys;\n}\n\n/**\n* Decide what to do given some input.\n*\n* @param steps - Steps the LLM has taken so far, along with observations from each.\n* @param inputs - User inputs.\n* @param callbackManager - Callback manager.\n*\n* @returns Action specifying what tool to use.\n*/\nasync plan(\nsteps: AgentStep[],\ninputs: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<AgentAction | AgentFinish> {\nconst output = await this.llmChain.call(\n{\nintermediate_steps: steps,\nstop: this.stop,\n...inputs,\n},\ncallbackManager\n);\nreturn this.outputParser.parse(\noutput[this.llmChain.outputKey],\ncallbackManager\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":2004,"to":2049}}}}],["6",{"pageContent":"interface AgentArgs {\noutputParser?: AgentActionOutputParser;\n\ncallbacks?: Callbacks;\n\n/**\n* @deprecated Use `callbacks` instead.\n*/\ncallbackManager?: CallbackManager;\n}\n\n/**\n* Class responsible for calling a language model and deciding an action.\n*\n* @remarks This is driven by an LLMChain. The prompt in the LLMChain *must*\n* include a variable called \"agent_scratchpad\" where the agent can put its\n* intermediary work.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":2408,"to":2425}}}}],["7",{"pageContent":"abstract class Agent extends BaseSingleActionAgent {\nllmChain: LLMChain;\n\noutputParser: AgentActionOutputParser;\n\nprivate _allowedTools?: string[] = undefined;\n\nget allowedTools(): string[] | undefined {\nreturn this._allowedTools;\n}\n\nget inputKeys(): string[] {\nreturn this.llmChain.inputKeys.filter((k) => k !== \"agent_scratchpad\");\n}\n\nconstructor(input: AgentInput) {\nsuper();\nthis.llmChain = input.llmChain;\nthis._allowedTools = input.allowedTools;\nthis.outputParser = input.outputParser;\n}\n\n/**\n* Prefix to append the observation with.\n*/\nabstract observationPrefix(): string;\n\n/**\n* Prefix to append the LLM call with.\n*/\nabstract llmPrefix(): string;\n\n/**\n* Return the string type key uniquely identifying this class of agent.\n*/\nabstract _agentType(): string;\n\n/**\n* Get the default output parser for this agent.\n*/\nstatic getDefaultOutputParser(\n_fields?: OutputParserArgs\n): AgentActionOutputParser {\nthrow new Error(\"Not implemented\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":2809,"to":2853}}}}],["8",{"pageContent":"/**\n* Create a prompt for this class\n*\n* @param _tools - List of tools the agent will have access to, used to format the prompt.\n* @param _fields - Additional fields used to format the prompt.\n*\n* @returns A PromptTemplate assembled from the given tools and fields.\n* */\nstatic createPrompt(\n_tools: Tool[],\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_fields?: Record<string, any>\n): BasePromptTemplate {\nthrow new Error(\"Not implemented\");\n}\n\n/** Construct an agent from an LLM and a list of tools */\nstatic fromLLMAndTools(\n_llm: BaseLanguageModel,\n_tools: Tool[],\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_args?: AgentArgs\n): Agent {\nthrow new Error(\"Not implemented\");\n}\n\n/**\n* Validate that appropriate tools are passed in\n*/\nstatic validateTools(_tools: Tool[]): void {}\n\n_stop(): string[] {\nreturn [`\\n${this.observationPrefix()}`];\n}\n\n/**\n* Name of tool to use to terminate the chain.\n*/\nfinishToolName(): string {\nreturn \"Final Answer\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":3212,"to":3252}}}}],["9",{"pageContent":"/**\n* Construct a scratchpad to let the agent continue its thought process\n*/\nasync constructScratchPad(\nsteps: AgentStep[]\n): Promise<string | BaseChatMessage[]> {\nreturn steps.reduce(\n(thoughts, { action, observation }) =>\nthoughts +\n[\naction.log,\n`${this.observationPrefix()}${observation}`,\nthis.llmPrefix(),\n].join(\"\\n\"),\n\"\"\n);\n}\n\nprivate async _plan(\nsteps: AgentStep[],\ninputs: ChainValues,\nsuffix?: string,\ncallbackManager?: CallbackManager\n): Promise<AgentAction | AgentFinish> {\nconst thoughts = await this.constructScratchPad(steps);\nconst newInputs: ChainValues = {\n...inputs,\nagent_scratchpad: suffix ? `${thoughts}${suffix}` : thoughts,\n};\n\nif (this._stop().length !== 0) {\nnewInputs.stop = this._stop();\n}\n\nconst output = await this.llmChain.predict(newInputs, callbackManager);\nreturn this.outputParser.parse(output, callbackManager);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":3606,"to":3642}}}}],["10",{"pageContent":"/**\n* Decide what to do given some input.\n*\n* @param steps - Steps the LLM has taken so far, along with observations from each.\n* @param inputs - User inputs.\n* @param callbackManager - Callback manager to use for this call.\n*\n* @returns Action specifying what tool to use.\n*/\nplan(\nsteps: AgentStep[],\ninputs: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<AgentAction | AgentFinish> {\nreturn this._plan(steps, inputs, undefined, callbackManager);\n}\n\n/**\n* Return response when agent has been stopped due to max iterations\n*/\nasync returnStoppedResponse(\nearlyStoppingMethod: StoppingMethod,\nsteps: AgentStep[],\ninputs: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<AgentFinish> {\nif (earlyStoppingMethod === \"force\") {\nreturn {\nreturnValues: { output: \"Agent stopped due to max iterations.\" },\nlog: \"\",\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":4005,"to":4036}}}}],["11",{"pageContent":"if (earlyStoppingMethod === \"generate\") {\ntry {\nconst action = await this._plan(\nsteps,\ninputs,\n\"\\n\\nI now need to return a final answer based on the previous steps:\",\ncallbackManager\n);\nif (\"returnValues\" in action) {\nreturn action;\n}\n\nreturn { returnValues: { output: action.log }, log: action.log };\n} catch (err) {\n// fine to use instanceof because we're in the same module\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (!(err instanceof ParseError)) {\nthrow err;\n}\nreturn { returnValues: { output: err.output }, log: err.output };\n}\n}\n\nthrow new Error(`Invalid stopping method: ${earlyStoppingMethod}`);\n}\n\n/**\n* Load an agent from a json-like object describing it.\n*/\nstatic async deserialize(\ndata: SerializedAgent & { llm?: BaseLanguageModel; tools?: Tool[] }\n): Promise<Agent> {\nswitch (data._type) {\ncase \"zero-shot-react-description\": {\nconst { ZeroShotAgent } = await import(\"./mrkl/index.js\");\nreturn ZeroShotAgent.deserialize(data);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":4401,"to":4437}}}}],["12",{"pageContent":":\nthrow new Error(\"Unknown agent type\");\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":4795,"to":4799}}}}],["13",{"pageContent":"import { Tool } from \"../../tools/base.js\";\n\nexport abstract class Toolkit {\nabstract tools: Tool[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/base.ts","loc":{"lines":{"from":1,"to":5}}}}],["14",{"pageContent":"export { JsonToolkit, createJsonAgent } from \"./json/json.js\";\nexport { SqlToolkit, createSqlAgent, SqlCreatePromptArgs } from \"./sql/sql.js\";\nexport {\nRequestsToolkit,\nOpenApiToolkit,\ncreateOpenApiAgent,\n} from \"./openapi/openapi.js\";\nexport {\nVectorStoreInfo,\nVectorStoreToolkit,\nVectorStoreRouterToolkit,\ncreateVectorStoreAgent,\ncreateVectorStoreRouterAgent,\n} from \"./vectorstore/vectorstore.js\";\nexport { ZapierToolKit } from \"./zapier/zapier.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/index.ts","loc":{"lines":{"from":1,"to":15}}}}],["15",{"pageContent":"import { BaseLanguageModel } from \"../../../base_language/index.js\";\nimport { Tool } from \"../../../tools/base.js\";\nimport {\nJsonGetValueTool,\nJsonListKeysTool,\nJsonSpec,\n} from \"../../../tools/json.js\";\nimport { JSON_PREFIX, JSON_SUFFIX } from \"./prompt.js\";\nimport { LLMChain } from \"../../../chains/llm_chain.js\";\nimport { ZeroShotCreatePromptArgs, ZeroShotAgent } from \"../../mrkl/index.js\";\nimport { Toolkit } from \"../base.js\";\nimport { AgentExecutor } from \"../../executor.js\";\n\nexport class JsonToolkit extends Toolkit {\ntools: Tool[];\n\nconstructor(public jsonSpec: JsonSpec) {\nsuper();\nthis.tools = [\nnew JsonListKeysTool(jsonSpec),\nnew JsonGetValueTool(jsonSpec),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/json.ts","loc":{"lines":{"from":1,"to":24}}}}],["16",{"pageContent":"function createJsonAgent(\nllm: BaseLanguageModel,\ntoolkit: JsonToolkit,\nargs?: ZeroShotCreatePromptArgs\n) {\nconst {\nprefix = JSON_PREFIX,\nsuffix = JSON_SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\n} = args ?? {};\nconst { tools } = toolkit;\nconst prompt = ZeroShotAgent.createPrompt(tools, {\nprefix,\nsuffix,\ninputVariables,\n});\nconst chain = new LLMChain({ prompt, llm });\nconst agent = new ZeroShotAgent({\nllmChain: chain,\nallowedTools: tools.map((t) => t.name),\n});\nreturn AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/json.ts","loc":{"lines":{"from":55,"to":81}}}}],["17",{"pageContent":"export const JSON_PREFIX = `You are an agent designed to interact with JSON.\nYour goal is to return a final answer by interacting with the JSON.\nYou have access to the following tools which help you learn more about the JSON you are interacting with.\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\nDo not make up any information that is not contained in the JSON.\nYour input to the tools should be in the form of in json pointer syntax (e.g. /key1/0/key2).\nYou must escape a slash in a key with a ~1, and escape a tilde with a ~0.\nFor example, to access the key /foo, you would use /~1foo\nYou should only use keys that you know for a fact exist. You must validate that a key exists by seeing it previously when calling 'json_list_keys'.\nIf you have not seen a key in one of those responses, you cannot use it.\nYou should only add one key at a time to the path. You cannot add multiple keys at once.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/prompt.ts","loc":{"lines":{"from":1,"to":11}}}}],["18",{"pageContent":"If you encounter a null or undefined value, go back to the previous key, look at the available keys, and try again.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/prompt.ts","loc":{"lines":{"from":12,"to":12}}}}],["19",{"pageContent":"If the question does not seem to be related to the JSON, just return \"I don't know\" as the answer.\nAlways begin your interaction with the 'json_list_keys' with an empty string as the input to see what keys exist in the JSON.\n\nNote that sometimes the value at a given path is large. In this case, you will get an error \"Value is a large dictionary, should explore its keys directly\".\nIn this case, you should ALWAYS follow up by using the 'json_list_keys' tool to see what keys exist at that path.\nDo not simply refer the user to the JSON or a section of the JSON, as this is not a valid answer. Keep digging until you find the answer and explicitly return it.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/prompt.ts","loc":{"lines":{"from":14,"to":19}}}}],["20",{"pageContent":"const JSON_SUFFIX = `Begin!\"\n\nQuestion: {input}\nThought: I should look at the keys that exist to see what I can query. I should use the 'json_list_keys' tool with an empty string as the input.\n{agent_scratchpad}`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/prompt.ts","loc":{"lines":{"from":21,"to":25}}}}],["21",{"pageContent":"import { BaseLanguageModel } from \"../../../base_language/index.js\";\nimport { Tool } from \"../../../tools/base.js\";\nimport { DynamicTool } from \"../../../tools/dynamic.js\";\nimport { JsonSpec } from \"../../../tools/json.js\";\nimport { AgentExecutor } from \"../../executor.js\";\nimport {\nOPENAPI_PREFIX,\nOPENAPI_SUFFIX,\nJSON_EXPLORER_DESCRIPTION,\n} from \"./prompt.js\";\nimport { LLMChain } from \"../../../chains/llm_chain.js\";\nimport { ZeroShotCreatePromptArgs, ZeroShotAgent } from \"../../mrkl/index.js\";\nimport { Toolkit } from \"../base.js\";\nimport {\nHeaders,\nRequestsGetTool,\nRequestsPostTool,\n} from \"../../../tools/requests.js\";\nimport { createJsonAgent, JsonToolkit } from \"../json/json.js\";\n\nexport class RequestsToolkit extends Toolkit {\ntools: Tool[];\n\nconstructor(headers?: Headers) {\nsuper();\nthis.tools = [new RequestsGetTool(headers), new RequestsPostTool(headers)];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/openapi.ts","loc":{"lines":{"from":1,"to":28}}}}],["22",{"pageContent":"class OpenApiToolkit extends RequestsToolkit {\nconstructor(jsonSpec: JsonSpec, llm: BaseLanguageModel, headers?: Headers) {\nsuper(headers);\nconst jsonAgent = createJsonAgent(llm, new JsonToolkit(jsonSpec));\nthis.tools = [\n...this.tools,\nnew DynamicTool({\nname: \"json_explorer\",\nfunc: async (input: string) => {\nconst result = await jsonAgent.call({ input });\nreturn result.output as string;\n},\ndescription: JSON_EXPLORER_DESCRIPTION,\n}),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/openapi.ts","loc":{"lines":{"from":78,"to":94}}}}],["23",{"pageContent":"function createOpenApiAgent(\nllm: BaseLanguageModel,\nopenApiToolkit: OpenApiToolkit,\nargs?: ZeroShotCreatePromptArgs\n) {\nconst {\nprefix = OPENAPI_PREFIX,\nsuffix = OPENAPI_SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\n} = args ?? {};\n\nconst { tools } = openApiToolkit;\nconst prompt = ZeroShotAgent.createPrompt(tools, {\nprefix,\nsuffix,\ninputVariables,\n});\nconst chain = new LLMChain({\nprompt,\nllm,\n});\nconst toolNames = tools.map((tool) => tool.name);\nconst agent = new ZeroShotAgent({ llmChain: chain, allowedTools: toolNames });\nreturn AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/openapi.ts","loc":{"lines":{"from":158,"to":186}}}}],["24",{"pageContent":"export const OPENAPI_PREFIX = `You are an agent designed to answer questions by making web requests to an API given the OpenAPI spec.\n\nIf the question does not seem related to the API, return I don't know. Do not make up an answer.\nOnly use information provided by the tools to construct your response.\n\nTo find information in the OpenAPI spec, use the 'json_explorer' tool. The input to this tool is a question about the API.\n\nTake the following steps:\nFirst, find the base URL needed to make the request.\n\nSecond, find the relevant paths needed to answer the question. Take note that, sometimes, you might need to make more than one request to more than one path to answer the question.\n\nThird, find the required parameters needed to make the request. For GET requests, these are usually URL parameters and for POST requests, these are request body parameters.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/prompt.ts","loc":{"lines":{"from":1,"to":13}}}}],["25",{"pageContent":"Fourth, make the requests needed to answer the question. Ensure that you are sending the correct parameters to the request by checking which parameters are required. For parameters with a fixed set of values, please use the spec to look at which values are allowed.\n\nUse the exact parameter names as listed in the spec, do not make up any names or abbreviate the names of parameters.\nIf you get a not found error, ensure that you are using a path that actually exists in the spec.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/prompt.ts","loc":{"lines":{"from":15,"to":18}}}}],["26",{"pageContent":"const OPENAPI_SUFFIX = `Begin!\"\n\nQuestion: {input}\nThought: I should explore the spec to find the base url for the API.\n{agent_scratchpad}`;\nexport const JSON_EXPLORER_DESCRIPTION = `\nCan be used to answer questions about the openapi spec for the API. Always use this tool before trying to make a request. \nExample inputs to this tool: \n'What are the required query parameters for a GET request to the /bar endpoint?'\n'What are the required parameters in the request body for a POST request to the /foo endpoint?'\nAlways give this tool a specific question.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/prompt.ts","loc":{"lines":{"from":30,"to":40}}}}],["27",{"pageContent":"export const SQL_PREFIX = `You are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results using the LIMIT clause.\nYou can order the results by a relevant column to return the most interesting examples in the database.\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\nYou have access to tools for interacting with the database.\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\nYou MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/sql/prompt.ts","loc":{"lines":{"from":1,"to":10}}}}],["28",{"pageContent":"If the question does not seem related to the database, just return \"I don't know\" as the answer.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/sql/prompt.ts","loc":{"lines":{"from":12,"to":12}}}}],["29",{"pageContent":"const SQL_SUFFIX = `Begin!\n\nQuestion: {input}\nThought: I should look at the tables in the database to see what I can query.\n{agent_scratchpad}`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/sql/prompt.ts","loc":{"lines":{"from":14,"to":18}}}}],["30",{"pageContent":"import { Tool } from \"../../../tools/base.js\";\nimport {\nInfoSqlTool,\nListTablesSqlTool,\nQueryCheckerTool,\nQuerySqlTool,\n} from \"../../../tools/sql.js\";\nimport { Toolkit } from \"../base.js\";\nimport { BaseLanguageModel } from \"../../../base_language/index.js\";\nimport { SQL_PREFIX, SQL_SUFFIX } from \"./prompt.js\";\nimport { renderTemplate } from \"../../../prompts/template.js\";\nimport { LLMChain } from \"../../../chains/llm_chain.js\";\nimport { ZeroShotAgent, ZeroShotCreatePromptArgs } from \"../../mrkl/index.js\";\nimport { AgentExecutor } from \"../../executor.js\";\nimport { SqlDatabase } from \"../../../sql_db.js\";\n\nexport interface SqlCreatePromptArgs extends ZeroShotCreatePromptArgs {\n/** Number of results to return. */\ntopK?: number;\n}\n\nexport class SqlToolkit extends Toolkit {\ntools: Tool[];\n\ndb: SqlDatabase;\n\ndialect = \"sqlite\";\n\nconstructor(db: SqlDatabase) {\nsuper();\nthis.db = db;\nthis.tools = [\nnew QuerySqlTool(db),\nnew InfoSqlTool(db),\nnew ListTablesSqlTool(db),\nnew QueryCheckerTool(),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/sql/sql.ts","loc":{"lines":{"from":1,"to":39}}}}],["31",{"pageContent":"function createSqlAgent(\nllm: BaseLanguageModel,\ntoolkit: SqlToolkit,\nargs?: SqlCreatePromptArgs\n) {\nconst {\nprefix = SQL_PREFIX,\nsuffix = SQL_SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\ntopK = 10,\n} = args ?? {};\nconst { tools } = toolkit;\nconst formattedPrefix = renderTemplate(prefix, \"f-string\", {\ndialect: toolkit.dialect,\ntop_k: topK,\n});\n\nconst prompt = ZeroShotAgent.createPrompt(tools, {\nprefix: formattedPrefix,\nsuffix,\ninputVariables,\n});\nconst chain = new LLMChain({ prompt, llm });\nconst agent = new ZeroShotAgent({\nllmChain: chain,\nallowedTools: tools.map((t) => t.name),\n});\nreturn AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/sql/sql.ts","loc":{"lines":{"from":77,"to":109}}}}],["32",{"pageContent":"export const VECTOR_PREFIX = `You are an agent designed to answer questions about sets of documents.\nYou have access to tools for interacting with the documents, and the inputs to the tools are questions.\nSometimes, you will be asked to provide sources for your questions, in which case you should use the appropriate tool to do so.\nIf the question does not seem relevant to any of the tools provided, just return \"I don't know\" as the answer.`;\n\nexport const VECTOR_ROUTER_PREFIX = `You are an agent designed to answer questions.\nYou have access to tools for interacting with different sources, and the inputs to the tools are questions.\nYour main task is to decide which of the tools is relevant for answering question at hand.\nFor complex questions, you can break the question down into sub questions and use tools to answers the sub questions.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/prompt.ts","loc":{"lines":{"from":1,"to":9}}}}],["33",{"pageContent":"import { Tool } from \"../../../tools/base.js\";\nimport { VectorStoreQATool } from \"../../../tools/vectorstore.js\";\nimport { VectorStore } from \"../../../vectorstores/base.js\";\nimport { Toolkit } from \"../base.js\";\nimport { BaseLanguageModel } from \"../../../base_language/index.js\";\nimport { ZeroShotCreatePromptArgs, ZeroShotAgent } from \"../../mrkl/index.js\";\nimport { VECTOR_PREFIX, VECTOR_ROUTER_PREFIX } from \"./prompt.js\";\nimport { SUFFIX } from \"../../mrkl/prompt.js\";\nimport { LLMChain } from \"../../../chains/llm_chain.js\";\nimport { AgentExecutor } from \"../../executor.js\";\n\nexport interface VectorStoreInfo {\nvectorStore: VectorStore;\nname: string;\ndescription: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/vectorstore.ts","loc":{"lines":{"from":1,"to":16}}}}],["34",{"pageContent":"class VectorStoreToolkit extends Toolkit {\ntools: Tool[];\n\nllm: BaseLanguageModel;\n\nconstructor(vectorStoreInfo: VectorStoreInfo, llm: BaseLanguageModel) {\nsuper();\nconst description = VectorStoreQATool.getDescription(\nvectorStoreInfo.name,\nvectorStoreInfo.description\n);\nthis.llm = llm;\nthis.tools = [\nnew VectorStoreQATool(vectorStoreInfo.name, description, {\nvectorStore: vectorStoreInfo.vectorStore,\nllm: this.llm,\n}),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/vectorstore.ts","loc":{"lines":{"from":118,"to":137}}}}],["35",{"pageContent":"class VectorStoreRouterToolkit extends Toolkit {\ntools: Tool[];\n\nvectorStoreInfos: VectorStoreInfo[];\n\nllm: BaseLanguageModel;\n\nconstructor(vectorStoreInfos: VectorStoreInfo[], llm: BaseLanguageModel) {\nsuper();\nthis.llm = llm;\nthis.vectorStoreInfos = vectorStoreInfos;\nthis.tools = vectorStoreInfos.map((vectorStoreInfo) => {\nconst description = VectorStoreQATool.getDescription(\nvectorStoreInfo.name,\nvectorStoreInfo.description\n);\nreturn new VectorStoreQATool(vectorStoreInfo.name, description, {\nvectorStore: vectorStoreInfo.vectorStore,\nllm: this.llm,\n});\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/vectorstore.ts","loc":{"lines":{"from":246,"to":268}}}}],["36",{"pageContent":"function createVectorStoreAgent(\nllm: BaseLanguageModel,\ntoolkit: VectorStoreToolkit,\nargs?: ZeroShotCreatePromptArgs\n) {\nconst {\nprefix = VECTOR_PREFIX,\nsuffix = SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\n} = args ?? {};\nconst { tools } = toolkit;\nconst prompt = ZeroShotAgent.createPrompt(tools, {\nprefix,\nsuffix,\ninputVariables,\n});\nconst chain = new LLMChain({ prompt, llm });\nconst agent = new ZeroShotAgent({\nllmChain: chain,\nallowedTools: tools.map((t) => t.name),\n});\nreturn AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/vectorstore.ts","loc":{"lines":{"from":375,"to":401}}}}],["37",{"pageContent":"function createVectorStoreRouterAgent(\nllm: BaseLanguageModel,\ntoolkit: VectorStoreRouterToolkit,\nargs?: ZeroShotCreatePromptArgs\n) {\nconst {\nprefix = VECTOR_ROUTER_PREFIX,\nsuffix = SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\n} = args ?? {};\nconst { tools } = toolkit;\nconst prompt = ZeroShotAgent.createPrompt(tools, {\nprefix,\nsuffix,\ninputVariables,\n});\nconst chain = new LLMChain({ prompt, llm });\nconst agent = new ZeroShotAgent({\nllmChain: chain,\nallowedTools: tools.map((t) => t.name),\n});\nreturn AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/vectorstore.ts","loc":{"lines":{"from":508,"to":534}}}}],["38",{"pageContent":"import { Toolkit } from \"../base.js\";\nimport { Tool } from \"../../../tools/base.js\";\nimport { ZapierNLARunAction, ZapierNLAWrapper } from \"../../../tools/zapier.js\";\n\nexport class ZapierToolKit extends Toolkit {\ntools: Tool[] = [];\n\nstatic async fromZapierNLAWrapper(\nzapierNLAWrapper: ZapierNLAWrapper\n): Promise<ZapierToolKit> {\nconst toolkit = new ZapierToolKit();\nconst actions = await zapierNLAWrapper.listActions();\nfor (const action of actions) {\nconst tool = new ZapierNLARunAction(\nzapierNLAWrapper,\naction.id,\naction.description,\naction.params\n);\ntoolkit.tools.push(tool);\n}\nreturn toolkit;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/zapier/zapier.ts","loc":{"lines":{"from":1,"to":24}}}}],["39",{"pageContent":"import { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { LLMChain } from \"../../chains/llm_chain.js\";\nimport {\nChatPromptTemplate,\nHumanMessagePromptTemplate,\nSystemMessagePromptTemplate,\n} from \"../../prompts/chat.js\";\nimport { AgentStep } from \"../../schema/index.js\";\nimport { Tool } from \"../../tools/base.js\";\nimport { Optional } from \"../../types/type-utils.js\";\nimport { Agent, AgentArgs, OutputParserArgs } from \"../agent.js\";\nimport { AgentInput } from \"../types.js\";\nimport { ChatAgentOutputParser } from \"./outputParser.js\";\nimport { FORMAT_INSTRUCTIONS, PREFIX, SUFFIX } from \"./prompt.js\";\n\nexport interface ChatCreatePromptArgs {\n/** String to put after the list of tools. */\nsuffix?: string;\n/** String to put before the list of tools. */\nprefix?: string;\n/** List of input variables the final prompt will expect. */\ninputVariables?: string[];\n}\n\nexport type ChatAgentInput = Optional<AgentInput, \"outputParser\">;\n\n/**\n* Agent for the MRKL chain.\n* @augments Agent\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/index.ts","loc":{"lines":{"from":1,"to":30}}}}],["40",{"pageContent":"class ChatAgent extends Agent {\nconstructor(input: ChatAgentInput) {\nconst outputParser =\ninput?.outputParser ?? ChatAgent.getDefaultOutputParser();\nsuper({ ...input, outputParser });\n}\n\n_agentType() {\nreturn \"chat-zero-shot-react-description\" as const;\n}\n\nobservationPrefix() {\nreturn \"Observation: \";\n}\n\nllmPrefix() {\nreturn \"Thought:\";\n}\n\n_stop(): string[] {\nreturn [\"Observation:\"];\n}\n\nstatic validateTools(tools: Tool[]) {\nconst invalidTool = tools.find((tool) => !tool.description);\nif (invalidTool) {\nconst msg =\n`Got a tool ${invalidTool.name} without a description.` +\n` This agent requires descriptions for all tools.`;\nthrow new Error(msg);\n}\n}\n\nstatic getDefaultOutputParser(_fields?: OutputParserArgs) {\nreturn new ChatAgentOutputParser();\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/index.ts","loc":{"lines":{"from":121,"to":156}}}}],["41",{"pageContent":"async constructScratchPad(steps: AgentStep[]): Promise<string> {\nconst agentScratchpad = await super.constructScratchPad(steps);\nif (agentScratchpad) {\nreturn `This was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\n${agentScratchpad}`;\n}\nreturn agentScratchpad;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/index.ts","loc":{"lines":{"from":257,"to":263}}}}],["42",{"pageContent":"/**\n* Create prompt in the style of the zero shot agent.\n*\n* @param tools - List of tools the agent will have access to, used to format the prompt.\n* @param args - Arguments to create the prompt with.\n* @param args.suffix - String to put after the list of tools.\n* @param args.prefix - String to put before the list of tools.\n*/\nstatic createPrompt(tools: Tool[], args?: ChatCreatePromptArgs) {\nconst { prefix = PREFIX, suffix = SUFFIX } = args ?? {};\nconst toolStrings = tools\n.map((tool) => `${tool.name}: ${tool.description}`)\n.join(\"\\n\");\nconst template = [prefix, toolStrings, FORMAT_INSTRUCTIONS, suffix].join(\n\"\\n\\n\"\n);\nconst messages = [\nSystemMessagePromptTemplate.fromTemplate(template),\nHumanMessagePromptTemplate.fromTemplate(\"{input}\\n\\n{agent_scratchpad}\"),\n];\nreturn ChatPromptTemplate.fromPromptMessages(messages);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/index.ts","loc":{"lines":{"from":374,"to":395}}}}],["43",{"pageContent":"static fromLLMAndTools(\nllm: BaseLanguageModel,\ntools: Tool[],\nargs?: ChatCreatePromptArgs & AgentArgs\n) {\nChatAgent.validateTools(tools);\nconst prompt = ChatAgent.createPrompt(tools, args);\nconst chain = new LLMChain({\nprompt,\nllm,\ncallbacks: args?.callbacks ?? args?.callbackManager,\n});\nconst outputParser =\nargs?.outputParser ?? ChatAgent.getDefaultOutputParser();\n\nreturn new ChatAgent({\nllmChain: chain,\noutputParser,\nallowedTools: tools.map((t) => t.name),\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/index.ts","loc":{"lines":{"from":494,"to":515}}}}],["44",{"pageContent":"import { AgentActionOutputParser } from \"../types.js\";\nimport { AgentFinish } from \"../../schema/index.js\";\nimport { FORMAT_INSTRUCTIONS } from \"./prompt.js\";\n\nexport const FINAL_ANSWER_ACTION = \"Final Answer:\";\nexport class ChatAgentOutputParser extends AgentActionOutputParser {\nasync parse(text: string) {\nif (text.includes(FINAL_ANSWER_ACTION) || !text.includes(`\"action\":`)) {\nconst parts = text.split(FINAL_ANSWER_ACTION);\nconst output = parts[parts.length - 1].trim();\nreturn { returnValues: { output }, log: text } satisfies AgentFinish;\n}\n\nconst action = text.includes(\"```\")\n? text.trim().split(/```(?:json)?/)[1]\n: text.trim();\ntry {\nconst response = JSON.parse(action.trim());\nreturn {\ntool: response.action,\ntoolInput: response.action_input,\nlog: text,\n};\n} catch {\nthrow new Error(\n`Unable to parse JSON response from chat agent.\\n\\n${text}`\n);\n}\n}\n\ngetFormatInstructions(): string {\nreturn FORMAT_INSTRUCTIONS;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/outputParser.ts","loc":{"lines":{"from":1,"to":34}}}}],["45",{"pageContent":"export const PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;\nexport const FORMAT_INSTRUCTIONS = `The way you use the tools is by specifying a json blob, denoted below by $JSON_BLOB\nSpecifically, this $JSON_BLOB should have a \"action\" key (with the name of the tool to use) and a \"action_input\" key (with the input to the tool going here). \nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n\n\\`\\`\\`\n{{\n\"action\": \"calculator\",\n\"action_input\": \"1 + 2\"\n}}\n\\`\\`\\`\n\nALWAYS use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: \n\\`\\`\\`\n$JSON_BLOB\n\\`\\`\\`\nObservation: the result of the action\n... (this Thought/Action/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/prompt.ts","loc":{"lines":{"from":1,"to":24}}}}],["46",{"pageContent":"const SUFFIX = `Begin! Reminder to always use the exact characters \\`Final Answer\\` when responding.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/prompt.ts","loc":{"lines":{"from":25,"to":25}}}}],["47",{"pageContent":"import { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { LLMChain } from \"../../chains/llm_chain.js\";\nimport {\nChatPromptTemplate,\nHumanMessagePromptTemplate,\nMessagesPlaceholder,\nSystemMessagePromptTemplate,\n} from \"../../prompts/chat.js\";\nimport { renderTemplate } from \"../../prompts/template.js\";\nimport {\nAIChatMessage,\nAgentStep,\nBaseChatMessage,\nHumanChatMessage,\n} from \"../../schema/index.js\";\nimport { Tool } from \"../../tools/base.js\";\nimport { Optional } from \"../../types/type-utils.js\";\nimport { Agent, AgentArgs, OutputParserArgs } from \"../agent.js\";\nimport { AgentActionOutputParser, AgentInput } from \"../types.js\";\nimport { ChatConversationalAgentOutputParser } from \"./outputParser.js\";\nimport {\nPREFIX_END,\nDEFAULT_PREFIX,\nDEFAULT_SUFFIX,\nTEMPLATE_TOOL_RESPONSE,\n} from \"./prompt.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":1,"to":26}}}}],["48",{"pageContent":"interface ChatConversationalCreatePromptArgs {\n/** String to put after the list of tools. */\nsystemMessage?: string;\n/** String to put before the list of tools. */\nhumanMessage?: string;\n/** List of input variables the final prompt will expect. */\ninputVariables?: string[];\n/** Output parser to use for formatting. */\noutputParser?: AgentActionOutputParser;\n}\n\nexport type ChatConversationalAgentInput = Optional<AgentInput, \"outputParser\">;\n\n/**\n* Agent for the MRKL chain.\n* @augments Agent\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":157,"to":173}}}}],["49",{"pageContent":"class ChatConversationalAgent extends Agent {\nconstructor(input: ChatConversationalAgentInput) {\nconst outputParser =\ninput.outputParser ?? ChatConversationalAgent.getDefaultOutputParser();\nsuper({ ...input, outputParser });\n}\n\n_agentType() {\nreturn \"chat-conversational-react-description\" as const;\n}\n\nobservationPrefix() {\nreturn \"Observation: \";\n}\n\nllmPrefix() {\nreturn \"Thought:\";\n}\n\n_stop(): string[] {\nreturn [\"Observation:\"];\n}\n\nstatic validateTools(tools: Tool[]) {\nconst invalidTool = tools.find((tool) => !tool.description);\nif (invalidTool) {\nconst msg =\n`Got a tool ${invalidTool.name} without a description.` +\n` This agent requires descriptions for all tools.`;\nthrow new Error(msg);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":312,"to":343}}}}],["50",{"pageContent":"async constructScratchPad(steps: AgentStep[]): Promise<BaseChatMessage[]> {\nconst thoughts: BaseChatMessage[] = [];\nfor (const step of steps) {\nthoughts.push(new AIChatMessage(step.action.log));\nthoughts.push(\nnew HumanChatMessage(\nrenderTemplate(TEMPLATE_TOOL_RESPONSE, \"f-string\", {\nobservation: step.observation,\n})\n)\n);\n}\nreturn thoughts;\n}\n\nstatic getDefaultOutputParser(\n_fields?: OutputParserArgs\n): AgentActionOutputParser {\nreturn new ChatConversationalAgentOutputParser();\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":479,"to":498}}}}],["51",{"pageContent":"/**\n* Create prompt in the style of the ChatConversationAgent.\n*\n* @param tools - List of tools the agent will have access to, used to format the prompt.\n* @param args - Arguments to create the prompt with.\n* @param args.systemMessage - String to put before the list of tools.\n* @param args.humanMessage - String to put after the list of tools.\n*/\nstatic createPrompt(\ntools: Tool[],\nargs?: ChatConversationalCreatePromptArgs\n) {\nconst systemMessage = (args?.systemMessage ?? DEFAULT_PREFIX) + PREFIX_END;\nconst humanMessage = args?.humanMessage ?? DEFAULT_SUFFIX;\nconst outputParser =\nargs?.outputParser ?? new ChatConversationalAgentOutputParser();\nconst toolStrings = tools\n.map((tool) => `${tool.name}: ${tool.description}`)\n.join(\"\\n\");\nconst formatInstructions = renderTemplate(humanMessage, \"f-string\", {\nformat_instructions: outputParser.getFormatInstructions(),\n});\nconst toolNames = tools.map((tool) => tool.name).join(\"\\n\");\nconst finalPrompt = renderTemplate(formatInstructions, \"f-string\", {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":637,"to":660}}}}],["52",{"pageContent":"tools: toolStrings,\ntool_names: toolNames,\n});\nconst messages = [\nSystemMessagePromptTemplate.fromTemplate(systemMessage),\nnew MessagesPlaceholder(\"chat_history\"),\nHumanMessagePromptTemplate.fromTemplate(finalPrompt),\nnew MessagesPlaceholder(\"agent_scratchpad\"),\n];\nreturn ChatPromptTemplate.fromPromptMessages(messages);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":785,"to":795}}}}],["53",{"pageContent":"static fromLLMAndTools(\nllm: BaseLanguageModel,\ntools: Tool[],\nargs?: ChatConversationalCreatePromptArgs & AgentArgs\n) {\nChatConversationalAgent.validateTools(tools);\nconst prompt = ChatConversationalAgent.createPrompt(tools, args);\nconst chain = new LLMChain({\nprompt,\nllm,\ncallbacks: args?.callbacks ?? args?.callbackManager,\n});\nconst outputParser =\nargs?.outputParser ?? ChatConversationalAgent.getDefaultOutputParser();\n\nreturn new ChatConversationalAgent({\nllmChain: chain,\noutputParser,\nallowedTools: tools.map((t) => t.name),\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":942,"to":963}}}}],["54",{"pageContent":"import { AgentActionOutputParser } from \"../types.js\";\nimport { FORMAT_INSTRUCTIONS } from \"./prompt.js\";\n\nexport class ChatConversationalAgentOutputParser extends AgentActionOutputParser {\nasync parse(text: string) {\nlet jsonOutput = text.trim();\nif (jsonOutput.includes(\"```json\")) {\njsonOutput = jsonOutput.split(\"```json\")[1].trimStart();\n} else if (jsonOutput.includes(\"```\")) {\nconst firstIndex = jsonOutput.indexOf(\"```\");\njsonOutput = jsonOutput.slice(firstIndex + 3).trimStart();\n}\nconst lastIndex = jsonOutput.lastIndexOf(\"```\");\nif (lastIndex !== -1) {\njsonOutput = jsonOutput.slice(0, lastIndex).trimEnd();\n}\n\nconst response = JSON.parse(jsonOutput);\n\nconst { action, action_input } = response;\n\nif (action === \"Final Answer\") {\nreturn { returnValues: { output: action_input }, log: text };\n}\nreturn { tool: action, toolInput: action_input, log: text };\n}\n\ngetFormatInstructions(): string {\nreturn FORMAT_INSTRUCTIONS;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/outputParser.ts","loc":{"lines":{"from":1,"to":31}}}}],["55",{"pageContent":"export const DEFAULT_PREFIX = `Assistant is a large language model trained by OpenAI.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/prompt.ts","loc":{"lines":{"from":1,"to":5}}}}],["56",{"pageContent":"Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/prompt.ts","loc":{"lines":{"from":7,"to":7}}}}],["57",{"pageContent":"const PREFIX_END = ` However, above all else, all responses must adhere to the format of RESPONSE FORMAT INSTRUCTIONS.`;\n\nexport const FORMAT_INSTRUCTIONS = `RESPONSE FORMAT INSTRUCTIONS\n----------------------------\n\nWhen responding to me please, please output a response in one of two formats:\n\n**Option 1:**\nUse this if you want the human to use a tool.\nMarkdown code snippet formatted in the following schema:\n\n\\`\\`\\`json\n{{{{\n\"action\": string \\\\ The action to take. Must be one of {tool_names}\n\"action_input\": string \\\\ The input to the action\n}}}}\n\\`\\`\\`\n\n**Option #2:**\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n\n\\`\\`\\`json\n{{{{\n\"action\": \"Final Answer\",\n\"action_input\": string \\\\ You should put what you want to return to use here and make sure to use valid json newline characters.\n}}}}\n\\`\\`\\``;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/prompt.ts","loc":{"lines":{"from":58,"to":84}}}}],["58",{"pageContent":"const DEFAULT_SUFFIX = `TOOLS\n------\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n\n{{tools}}\n\n{format_instructions}\n\nUSER'S INPUT\n--------------------\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n\n{{{{input}}}}`;\n\nexport const TEMPLATE_TOOL_RESPONSE = `TOOL RESPONSE:\n---------------------\n{observation}\n\nUSER'S INPUT\n--------------------\n\nOkay, so what is the response to my original question? If using information from tools, you must say it explicitly - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/prompt.ts","loc":{"lines":{"from":116,"to":137}}}}],["59",{"pageContent":"import { BaseChain, ChainInputs } from \"../chains/base.js\";\nimport { BaseMultiActionAgent, BaseSingleActionAgent } from \"./agent.js\";\nimport { Tool } from \"../tools/base.js\";\nimport { StoppingMethod } from \"./types.js\";\nimport { SerializedLLMChain } from \"../chains/serde.js\";\nimport {\nAgentAction,\nAgentFinish,\nAgentStep,\nChainValues,\n} from \"../schema/index.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\nexport interface AgentExecutorInput extends ChainInputs {\nagent: BaseSingleActionAgent | BaseMultiActionAgent;\ntools: Tool[];\nreturnIntermediateSteps?: boolean;\nmaxIterations?: number;\nearlyStoppingMethod?: StoppingMethod;\n}\n\n/**\n* A chain managing an agent using tools.\n* @augments BaseChain\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/executor.ts","loc":{"lines":{"from":1,"to":25}}}}],["60",{"pageContent":"class AgentExecutor extends BaseChain {\nagent: BaseSingleActionAgent | BaseMultiActionAgent;\n\ntools: Tool[];\n\nreturnIntermediateSteps = false;\n\nmaxIterations?: number = 15;\n\nearlyStoppingMethod: StoppingMethod = \"force\";\n\nget inputKeys() {\nreturn this.agent.inputKeys;\n}\n\nget outputKeys() {\nreturn this.agent.returnValues;\n}\n\nconstructor(input: AgentExecutorInput) {\nsuper(\ninput.memory,\ninput.verbose,\ninput.callbacks ?? input.callbackManager\n);\nthis.agent = input.agent;\nthis.tools = input.tools;\nif (this.agent._agentActionType() === \"multi\") {\nfor (const tool of this.tools) {\nif (tool.returnDirect) {\nthrow new Error(\n`Tool with return direct ${tool.name} not supported for multi-action agent.`\n);\n}\n}\n}\nthis.returnIntermediateSteps =\ninput.returnIntermediateSteps ?? this.returnIntermediateSteps;\nthis.maxIterations = input.maxIterations ?? this.maxIterations;\nthis.earlyStoppingMethod =\ninput.earlyStoppingMethod ?? this.earlyStoppingMethod;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/executor.ts","loc":{"lines":{"from":163,"to":204}}}}],["61",{"pageContent":"/** Create from agent and a list of tools. */\nstatic fromAgentAndTools(fields: AgentExecutorInput): AgentExecutor {\nreturn new AgentExecutor(fields);\n}\n\nprivate shouldContinue(iterations: number): boolean {\nreturn this.maxIterations === undefined || iterations < this.maxIterations;\n}\n\n/** @ignore */\nasync _call(\ninputs: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nconst toolsByName = Object.fromEntries(\nthis.tools.map((t) => [t.name.toLowerCase(), t])\n);\nconst steps: AgentStep[] = [];\nlet iterations = 0;\n\nconst getOutput = async (finishStep: AgentFinish) => {\nconst { returnValues } = finishStep;\nconst additional = await this.agent.prepareForOutput(returnValues, steps);\n\nif (this.returnIntermediateSteps) {\nreturn { ...returnValues, intermediateSteps: steps, ...additional };\n}\nawait runManager?.handleAgentEnd(finishStep);\nreturn { ...returnValues, ...additional };\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/executor.ts","loc":{"lines":{"from":331,"to":360}}}}],["62",{"pageContent":"while (this.shouldContinue(iterations)) {\nconst output = await this.agent.plan(\nsteps,\ninputs,\nrunManager?.getChild()\n);\n// Check if the agent has finished\nif (\"returnValues\" in output) {\nreturn getOutput(output);\n}\n\nlet actions: AgentAction[];\nif (Array.isArray(output)) {\nactions = output as AgentAction[];\n} else {\nactions = [output as AgentAction];\n}\n\nconst newSteps = await Promise.all(\nactions.map(async (action) => {\nawait runManager?.handleAgentAction(action);\n\nconst tool = toolsByName[action.tool?.toLowerCase()];\nconst observation = tool\n? await tool.call(action.toolInput, runManager?.getChild())\n: `${action.tool} is not a valid tool, try another one.`;\n\nreturn { action, observation };\n})\n);\n\nsteps.push(...newSteps);\n\nconst lastStep = steps[steps.length - 1];\nconst lastTool = toolsByName[lastStep.action.tool?.toLowerCase()];\n\nif (lastTool?.returnDirect) {\nreturn getOutput({\nreturnValues: { [this.agent.returnValues[0]]: lastStep.observation },\nlog: \"\",\n});\n}\n\niterations += 1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/executor.ts","loc":{"lines":{"from":489,"to":533}}}}],["63",{"pageContent":"const finish = await this.agent.returnStoppedResponse(\nthis.earlyStoppingMethod,\nsteps,\ninputs\n);\n\nreturn getOutput(finish);\n}\n\n_chainType() {\nreturn \"agent_executor\" as const;\n}\n\nserialize(): SerializedLLMChain {\nthrow new Error(\"Cannot serialize an AgentExecutor\");\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/executor.ts","loc":{"lines":{"from":658,"to":674}}}}],["64",{"pageContent":"import type { SerializedAgentT, AgentInput } from \"./types.js\";\nimport { Tool } from \"../tools/base.js\";\nimport { LLMChain } from \"../chains/llm_chain.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/helpers.ts","loc":{"lines":{"from":1,"to":4}}}}],["65",{"pageContent":"const deserializeHelper = async <\nT extends string,\nU extends Record<string, unknown>,\nV extends AgentInput,\nZ\n>(\nllm: BaseLanguageModel | undefined,\ntools: Tool[] | undefined,\ndata: SerializedAgentT<T, U, V>,\nfromLLMAndTools: (llm: BaseLanguageModel, tools: Tool[], args: U) => Z,\nfromConstructor: (args: V) => Z\n): Promise<Z> => {\nif (data.load_from_llm_and_tools) {\nif (!llm) {\nthrow new Error(\"Loading from llm and tools, llm must be provided.\");\n}\n\nif (!tools) {\nthrow new Error(\"Loading from llm and tools, tools must be provided.\");\n}\n\nreturn fromLLMAndTools(llm, tools, data);\n}\nif (!data.llm_chain) {\nthrow new Error(\"Loading from constructor, llm_chain must be provided.\");\n}\n\nconst llmChain = await LLMChain.deserialize(data.llm_chain);\nreturn fromConstructor({ ...data, llmChain });\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/helpers.ts","loc":{"lines":{"from":35,"to":64}}}}],["66",{"pageContent":"export {\nAgent,\nAgentArgs,\nBaseSingleActionAgent,\nLLMSingleActionAgent,\nLLMSingleActionAgentInput,\nOutputParserArgs,\n} from \"./agent.js\";\nexport {\nJsonToolkit,\nOpenApiToolkit,\nRequestsToolkit,\nSqlToolkit,\nVectorStoreInfo,\nVectorStoreRouterToolkit,\nVectorStoreToolkit,\nZapierToolKit,\ncreateJsonAgent,\ncreateOpenApiAgent,\ncreateSqlAgent,\nSqlCreatePromptArgs,\ncreateVectorStoreAgent,\n} from \"./agent_toolkits/index.js\";\nexport { Toolkit } from \"./agent_toolkits/base.js\";\nexport {\nChatAgent,\nChatAgentInput,\nChatCreatePromptArgs,\n} from \"./chat/index.js\";\nexport { ChatAgentOutputParser } from \"./chat/outputParser.js\";\nexport {\nChatConversationalAgent,\nChatConversationalAgentInput,\nChatConversationalCreatePromptArgs,\n} from \"./chat_convo/index.js\";\nexport { ChatConversationalAgentOutputParser } from \"./chat_convo/outputParser.js\";\nexport { AgentExecutor, AgentExecutorInput } from \"./executor.js\";\nexport {\ninitializeAgentExecutor,\ninitializeAgentExecutorWithOptions,\nInitializeAgentExecutorOptions,\n} from \"./initialize.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/index.ts","loc":{"lines":{"from":1,"to":42}}}}],["67",{"pageContent":"{\nZeroShotAgent,\nZeroShotAgentInput,\nZeroShotCreatePromptArgs,\n} from \"./mrkl/index.js\";\nexport { ZeroShotAgentOutputParser } from \"./mrkl/outputParser.js\";\nexport {\nAgentActionOutputParser,\nAgentInput,\nSerializedAgent,\nSerializedAgentT,\nSerializedZeroShotAgent,\nStoppingMethod,\n} from \"./types.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/index.ts","loc":{"lines":{"from":57,"to":70}}}}],["68",{"pageContent":"import { BaseLanguageModel } from \"../base_language/index.js\";\nimport { CallbackManager } from \"../callbacks/manager.js\";\nimport { BufferMemory } from \"../memory/buffer_memory.js\";\nimport { Tool } from \"../tools/base.js\";\nimport { ChatAgent } from \"./chat/index.js\";\nimport { ChatConversationalAgent } from \"./chat_convo/index.js\";\nimport { AgentExecutor, AgentExecutorInput } from \"./executor.js\";\nimport { ZeroShotAgent } from \"./mrkl/index.js\";\n\ntype AgentType =\n| \"zero-shot-react-description\"\n| \"chat-zero-shot-react-description\"\n| \"chat-conversational-react-description\";\n\n/**\n* @deprecated use initializeAgentExecutorWithOptions instead\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":1,"to":17}}}}],["69",{"pageContent":"const initializeAgentExecutor = async (\ntools: Tool[],\nllm: BaseLanguageModel,\n_agentType?: AgentType,\n_verbose?: boolean,\n_callbackManager?: CallbackManager\n): Promise<AgentExecutor> => {\nconst agentType = _agentType ?? \"zero-shot-react-description\";\nconst verbose = _verbose;\nconst callbackManager = _callbackManager;\nswitch (agentType) {\ncase \"zero-shot-react-description\":\nreturn AgentExecutor.fromAgentAndTools({\nagent: ZeroShotAgent.fromLLMAndTools(llm, tools),\ntools,\nreturnIntermediateSteps: true,\nverbose,\ncallbackManager,\n});\ncase \"chat-zero-shot-react-description\":\nreturn AgentExecutor.fromAgentAndTools({\nagent: ChatAgent.fromLLMAndTools(llm, tools),\ntools,\nreturnIntermediateSteps: true,\nverbose,\ncallbackManager,\n});\ncase \"chat-conversational-react-description\":\nreturn AgentExecutor.fromAgentAndTools({\nagent: ChatConversationalAgent.fromLLMAndTools(llm, tools),\ntools,\nverbose,\ncallbackManager,\n});\ndefault:\nthrow new Error(\"Unknown agent type\");\n}\n};\n\n/**\n* @interface\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":131,"to":172}}}}],["70",{"pageContent":"type InitializeAgentExecutorOptions =\n| ({\nagentType: \"zero-shot-react-description\";\nagentArgs?: Parameters<typeof ZeroShotAgent.fromLLMAndTools>[2];\nmemory?: never;\n} & Omit<AgentExecutorInput, \"agent\" | \"tools\">)\n| ({\nagentType: \"chat-zero-shot-react-description\";\nagentArgs?: Parameters<typeof ChatAgent.fromLLMAndTools>[2];\nmemory?: never;\n} & Omit<AgentExecutorInput, \"agent\" | \"tools\">)\n| ({\nagentType: \"chat-conversational-react-description\";\nagentArgs?: Parameters<typeof ChatConversationalAgent.fromLLMAndTools>[2];\n} & Omit<AgentExecutorInput, \"agent\" | \"tools\">);\n\n/**\n* Initialize an agent executor with options\n* @param tools Array of tools to use in the agent\n* @param llm LLM or ChatModel to use in the agent\n* @param options Options for the agent, including agentType, agentArgs, and other options for AgentExecutor.fromAgentAndTools\n* @returns AgentExecutor\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":275,"to":297}}}}],["71",{"pageContent":"const initializeAgentExecutorWithOptions = async (\ntools: Tool[],\nllm: BaseLanguageModel,\noptions: InitializeAgentExecutorOptions = {\nagentType:\nllm._modelType() === \"base_chat_model\"\n? \"chat-zero-shot-react-description\"\n: \"zero-shot-react-description\",\n}\n): Promise<AgentExecutor> => {\nswitch (options.agentType) {\ncase \"zero-shot-react-description\": {\nconst { agentArgs, ...rest } = options;\nreturn AgentExecutor.fromAgentAndTools({\nagent: ZeroShotAgent.fromLLMAndTools(llm, tools, agentArgs),\ntools,\n...rest,\n});\n}\ncase \"chat-zero-shot-react-description\": {\nconst { agentArgs, ...rest } = options;\nreturn AgentExecutor.fromAgentAndTools({\nagent: ChatAgent.fromLLMAndTools(llm, tools, agentArgs),\ntools,\n...rest,\n});\n}\ncase \"chat-conversational-react-description\": {\nconst { agentArgs, memory, ...rest } = options;\nconst executor = AgentExecutor.fromAgentAndTools({\nagent: ChatConversationalAgent.fromLLMAndTools(llm, tools, agentArgs),\ntools,\nmemory:\nmemory ??\nnew BufferMemory({\nreturnMessages: true,\nmemoryKey: \"chat_history\",","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":402,"to":438}}}}],["72",{"pageContent":"inputKey: \"input\",\n}),\n...rest,\n});\nreturn executor;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":539,"to":544}}}}],["73",{"pageContent":": {\nthrow new Error(\"Unknown agent type\");\n}\n}\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":673,"to":677}}}}],["74",{"pageContent":"import { Agent } from \"./agent.js\";\nimport { Tool } from \"../tools/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { loadFromHub } from \"../util/hub.js\";\nimport { FileLoader, loadFromFile } from \"../util/load.js\";\nimport { parseFileConfig } from \"../util/parse.js\";\n\nconst loadAgentFromFile: FileLoader<Agent> = async (\nfile: string,\npath: string,\nllmAndTools?: { llm?: BaseLanguageModel; tools?: Tool[] }\n) => {\nconst serialized = parseFileConfig(file, path);\nreturn Agent.deserialize({ ...serialized, ...llmAndTools });\n};\n\nexport const loadAgent = async (\nuri: string,\nllmAndTools?: { llm?: BaseLanguageModel; tools?: Tool[] }\n): Promise<Agent> => {\nconst hubResult = await loadFromHub(\nuri,\nloadAgentFromFile,\n\"agents\",\nnew Set([\"json\", \"yaml\"]),\nllmAndTools\n);\nif (hubResult) {\nreturn hubResult;\n}\n\nreturn loadFromFile(uri, loadAgentFromFile, llmAndTools);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/load.ts","loc":{"lines":{"from":1,"to":33}}}}],["75",{"pageContent":"import { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { LLMChain } from \"../../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\nimport { renderTemplate } from \"../../prompts/template.js\";\nimport { Tool } from \"../../tools/base.js\";\nimport { Optional } from \"../../types/type-utils.js\";\nimport { Agent, AgentArgs, OutputParserArgs } from \"../agent.js\";\nimport { deserializeHelper } from \"../helpers.js\";\nimport {\nAgentInput,\nSerializedFromLLMAndTools,\nSerializedZeroShotAgent,\n} from \"../types.js\";\nimport { ZeroShotAgentOutputParser } from \"./outputParser.js\";\nimport { FORMAT_INSTRUCTIONS, PREFIX, SUFFIX } from \"./prompt.js\";\n\nexport interface ZeroShotCreatePromptArgs {\n/** String to put after the list of tools. */\nsuffix?: string;\n/** String to put before the list of tools. */\nprefix?: string;\n/** List of input variables the final prompt will expect. */\ninputVariables?: string[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/index.ts","loc":{"lines":{"from":1,"to":24}}}}],["76",{"pageContent":"type ZeroShotAgentInput = Optional<AgentInput, \"outputParser\">;\n\n/**\n* Agent for the MRKL chain.\n* @augments Agent\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/index.ts","loc":{"lines":{"from":144,"to":149}}}}],["77",{"pageContent":"class ZeroShotAgent extends Agent {\nconstructor(input: ZeroShotAgentInput) {\nconst outputParser =\ninput?.outputParser ?? ZeroShotAgent.getDefaultOutputParser();\nsuper({ ...input, outputParser });\n}\n\n_agentType() {\nreturn \"zero-shot-react-description\" as const;\n}\n\nobservationPrefix() {\nreturn \"Observation: \";\n}\n\nllmPrefix() {\nreturn \"Thought:\";\n}\n\nstatic getDefaultOutputParser(fields?: OutputParserArgs) {\nreturn new ZeroShotAgentOutputParser(fields);\n}\n\nstatic validateTools(tools: Tool[]) {\nconst invalidTool = tools.find((tool) => !tool.description);\nif (invalidTool) {\nconst msg =\n`Got a tool ${invalidTool.name} without a description.` +\n` This agent requires descriptions for all tools.`;\nthrow new Error(msg);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/index.ts","loc":{"lines":{"from":290,"to":321}}}}],["78",{"pageContent":"/**\n* Create prompt in the style of the zero shot agent.\n*\n* @param tools - List of tools the agent will have access to, used to format the prompt.\n* @param args - Arguments to create the prompt with.\n* @param args.suffix - String to put after the list of tools.\n* @param args.prefix - String to put before the list of tools.\n* @param args.inputVariables - List of input variables the final prompt will expect.\n*/\nstatic createPrompt(tools: Tool[], args?: ZeroShotCreatePromptArgs) {\nconst {\nprefix = PREFIX,\nsuffix = SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\n} = args ?? {};\nconst toolStrings = tools\n.map((tool) => `${tool.name}: ${tool.description}`)\n.join(\"\\n\");\n\nconst toolNames = tools.map((tool) => tool.name);\n\nconst formatInstructions = renderTemplate(FORMAT_INSTRUCTIONS, \"f-string\", {\ntool_names: toolNames,\n});\n\nconst template = [prefix, toolStrings, formatInstructions, suffix].join(\n\"\\n\\n\"\n);\n\nreturn new PromptTemplate({\ntemplate,\ninputVariables,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/index.ts","loc":{"lines":{"from":447,"to":480}}}}],["79",{"pageContent":"static fromLLMAndTools(\nllm: BaseLanguageModel,\ntools: Tool[],\nargs?: ZeroShotCreatePromptArgs & AgentArgs\n) {\nZeroShotAgent.validateTools(tools);\nconst prompt = ZeroShotAgent.createPrompt(tools, args);\nconst outputParser =\nargs?.outputParser ?? ZeroShotAgent.getDefaultOutputParser();\nconst chain = new LLMChain({\nprompt,\nllm,\ncallbacks: args?.callbacks ?? args?.callbackManager,\n});\n\nreturn new ZeroShotAgent({\nllmChain: chain,\nallowedTools: tools.map((t) => t.name),\noutputParser,\n});\n}\n\nstatic async deserialize(\ndata: SerializedZeroShotAgent & { llm?: BaseLanguageModel; tools?: Tool[] }\n): Promise<ZeroShotAgent> {\nconst { llm, tools, ...rest } = data;\nreturn deserializeHelper(\nllm,\ntools,\nrest,\n(\nllm: BaseLanguageModel,\ntools: Tool[],\nargs: SerializedFromLLMAndTools\n) =>\nZeroShotAgent.fromLLMAndTools(llm, tools, {\nprefix: args.prefix,\nsuffix: args.suffix,\ninputVariables: args.input_variables,\n}),\n(args) => new ZeroShotAgent(args)\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/index.ts","loc":{"lines":{"from":597,"to":640}}}}],["80",{"pageContent":"import { OutputParserArgs } from \"../agent.js\";\nimport { AgentActionOutputParser } from \"../types.js\";\n\nimport { FORMAT_INSTRUCTIONS } from \"./prompt.js\";\n\nexport const FINAL_ANSWER_ACTION = \"Final Answer:\";\nexport class ZeroShotAgentOutputParser extends AgentActionOutputParser {\nfinishToolName: string;\n\nconstructor(fields?: OutputParserArgs) {\nsuper();\nthis.finishToolName = fields?.finishToolName || FINAL_ANSWER_ACTION;\n}\n\nasync parse(text: string) {\nif (text.includes(this.finishToolName)) {\nconst parts = text.split(this.finishToolName);\nconst output = parts[parts.length - 1].trim();\nreturn {\nreturnValues: { output },\nlog: text,\n};\n}\n\nconst match = /Action: (.*)\\nAction Input: (.*)/s.exec(text);\nif (!match) {\nthrow new Error(`Could not parse LLM output: ${text}`);\n}\n\nreturn {\ntool: match[1].trim(),\ntoolInput: match[2].trim().replace(/^\"+|\"+$/g, \"\") ?? \"\",\nlog: text,\n};\n}\n\ngetFormatInstructions(): string {\nreturn FORMAT_INSTRUCTIONS;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/outputParser.ts","loc":{"lines":{"from":1,"to":40}}}}],["81",{"pageContent":"export const PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;\nexport const FORMAT_INSTRUCTIONS = `Use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question`;\nexport const SUFFIX = `Begin!\n\nQuestion: {input}\nThought:{agent_scratchpad}`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/prompt.ts","loc":{"lines":{"from":1,"to":15}}}}],["82",{"pageContent":"/* eslint-disable no-process-env */\nimport { expect, test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { loadAgent } from \"../load.js\";\nimport { AgentExecutor } from \"../index.js\";\nimport { SerpAPI } from \"../../tools/serpapi.js\";\nimport { Calculator } from \"../../tools/calculator.js\";\nimport { initializeAgentExecutorWithOptions } from \"../initialize.js\";\nimport { WebBrowser } from \"../../tools/webbrowser.js\";\nimport { Tool } from \"../../tools/base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/agent.int.test.ts","loc":{"lines":{"from":1,"to":11}}}}],["83",{"pageContent":"test(\"Run agent from hub\", async () => {\nconst model = new OpenAI({ temperature: 0, modelName: \"text-babbage-001\" });\nconst tools: Tool[] = [\nnew SerpAPI(undefined, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n];\nconst agent = await loadAgent(\n\"lc://agents/zero-shot-react-description/agent.json\",\n{ llm: model, tools }\n);\nconst executor = AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\nconst res = await executor.call({\ninput:\n\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\",\n});\nconsole.log(res);\n});\n\ntest(\"Run agent locally\", async () => {\nconst model = new OpenAI({ temperature: 0, modelName: \"text-babbage-001\" });\nconst tools = [\nnew SerpAPI(undefined, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/agent.int.test.ts","loc":{"lines":{"from":124,"to":159}}}}],["84",{"pageContent":"const executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);\n});\n\ntest(\"Run agent with incorrect api key should throw error\", async () => {\nconst model = new OpenAI({\ntemperature: 0,\nmodelName: \"text-babbage-001\",\nopenAIApiKey: \"invalid\",\n});\nconst tools = [\nnew SerpAPI(undefined, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/agent.int.test.ts","loc":{"lines":{"from":260,"to":293}}}}],["85",{"pageContent":"// Test that the model throws an error\nawait expect(() => model.call(input)).rejects.toThrowError(\n\"Request failed with status code 401\"\n);\n\n// Test that the agent throws the same error\nawait expect(() => executor.call({ input })).rejects.toThrowError(\n\"Request failed with status code 401\"\n);\n}, 10000);\n\ntest(\"Run tool web-browser\", async () => {\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\nnew SerpAPI(process.env.SERPAPI_API_KEY, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\nnew WebBrowser({ model, embeddings: new OpenAIEmbeddings() }),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"zero-shot-react-description\",\nreturnIntermediateSteps: true,\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `What is the word of the day on merriam webster`;\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/agent.int.test.ts","loc":{"lines":{"from":392,"to":426}}}}],["86",{"pageContent":"expect(result.intermediateSteps.length).toBeGreaterThanOrEqual(1);\nexpect(result.intermediateSteps[0].action.tool).toEqual(\"web-browser\");\nexpect(result.output).not.toEqual(\"\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/agent.int.test.ts","loc":{"lines":{"from":523,"to":526}}}}],["87",{"pageContent":"import { test, jest, expect } from \"@jest/globals\";\nimport LambdaClient from \"@aws-sdk/client-lambda\";\n\nimport { AWSLambda } from \"../../tools/aws_lambda.js\";\n\njest.mock(\"@aws-sdk/client-lambda\", () => ({\nLambdaClient: jest.fn().mockImplementation(() => ({\nsend: jest.fn().mockImplementation(() =>\nPromise.resolve({\nPayload: new TextEncoder().encode(\nJSON.stringify({ body: \"email sent.\" })\n),\n})\n),\n})),\nInvokeCommand: jest.fn().mockImplementation(() => ({})),\n}));\n\ntest(\"AWSLambda invokes the correct lambda function and returns the response.body contents\", async () => {\nif (!LambdaClient) {\n// this is to avoid a linting error. S3Client is mocked above.\n}\n\nconst lambda = new AWSLambda({\nname: \"email-sender\",\ndescription:\n\"Sends an email with the specified content to holtkam2@gmail.com\",\nregion: \"us-east-1\",\naccessKeyId: \"abc123\",\nsecretAccessKey: \"xyz456/1T+PzUZ2fd\",","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/aws_lambda.test.ts","loc":{"lines":{"from":1,"to":30}}}}],["88",{"pageContent":"Name: \"testFunction1\",\n});\n\nconst result = await lambda.call(\"Hello world! This is an email.\");\n\nexpect(result).toBe(\"email sent.\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/aws_lambda.test.ts","loc":{"lines":{"from":39,"to":45}}}}],["89",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { Calculator } from \"../../tools/calculator.js\";\n\ntest(\"Calculator tool, sum\", async () => {\nconst calculator = new Calculator();\nconst result = await calculator.call(\"1 + 1\");\nexpect(result).toBe(\"2\");\n});\n\ntest(\"Calculator tool, product\", async () => {\nconst calculator = new Calculator();\nconst result = await calculator.call(\"2 * 3\");\nexpect(result).toBe(\"6\");\n});\n\ntest(\"Calculator tool, division\", async () => {\nconst calculator = new Calculator();\nconst result = await calculator.call(\"7 /2\");\nexpect(result).toBe(\"3.5\");\n});\n\ntest(\"Calculator tool, exponentiation\", async () => {\nconst calculator = new Calculator();\nconst result = await calculator.call(\"2 ^ 8\");\nexpect(result).toBe(\"256\");\n});\n\ntest(\"Calculator tool, complicated expression\", async () => {\nconst calculator = new Calculator();\nconst result = await calculator.call(\"((2 + 3) * 4) / 2\");\nexpect(result).toBe(\"10\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/calculator.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["90",{"pageContent":"/* eslint-disable no-process-env */\nimport { expect, test } from \"@jest/globals\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\nimport { SerpAPI } from \"../../tools/serpapi.js\";\nimport { Calculator } from \"../../tools/calculator.js\";\nimport { initializeAgentExecutorWithOptions } from \"../initialize.js\";\nimport { HumanChatMessage } from \"../../schema/index.js\";\nimport { RequestsGetTool, RequestsPostTool } from \"../../tools/requests.js\";\nimport { AIPluginTool } from \"../../tools/aiplugin.js\";\n\ntest(\"Run agent locally\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst tools = [\nnew SerpAPI(undefined, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"chat-zero-shot-react-description\",\n});\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\nconsole.log(`Executing with input \"${input}\"...`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_agent.int.test.ts","loc":{"lines":{"from":1,"to":27}}}}],["91",{"pageContent":"const result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);\n});\n\ntest(\"Run agent with klarna and requests tools\", async () => {\nconst tools = [\nnew RequestsGetTool(),\nnew RequestsPostTool(),\nawait AIPluginTool.fromPluginUrl(\n\"https://www.klarna.com/.well-known/ai-plugin.json\"\n),\n];\nconst agent = await initializeAgentExecutorWithOptions(\ntools,\nnew ChatOpenAI({ temperature: 0 }),\n{ agentType: \"chat-zero-shot-react-description\", verbose: true }\n);\n\nconst result = await agent.call({\ninput: \"what t shirts are available in klarna?\",\n});\n\nconsole.log({ result });\n});\n\ntest(\"Run agent with incorrect api key should throw error\", async () => {\nconst model = new ChatOpenAI({\ntemperature: 0,\nopenAIApiKey: \"invalid\",\nmaxRetries: 0,\n});\nconst tools = [\nnew SerpAPI(undefined, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_agent.int.test.ts","loc":{"lines":{"from":85,"to":124}}}}],["92",{"pageContent":"const executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"chat-zero-shot-react-description\",\n});\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n// Test that the model throws an error\nawait expect(() =>\nmodel.call([new HumanChatMessage(input)])\n).rejects.toThrowError(\"Request failed with status code 401\");\n\n// Test that the agent throws the same error\nawait expect(() => executor.call({ input })).rejects.toThrowError(\n\"Request failed with status code 401\"\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_agent.int.test.ts","loc":{"lines":{"from":185,"to":200}}}}],["93",{"pageContent":"import { test } from \"@jest/globals\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\nimport { BufferMemory } from \"../../memory/index.js\";\nimport { Calculator } from \"../../tools/calculator.js\";\nimport { initializeAgentExecutorWithOptions } from \"../initialize.js\";\n\ntest(\"Run conversational agent with memory\", async () => {\nconst model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\", temperature: 0 });\nconst tools = [new Calculator()];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"chat-conversational-react-description\",\nverbose: true,\nmemory: new BufferMemory({\nreturnMessages: true,\nmemoryKey: \"chat_history\",\ninputKey: \"input\",\n}),\n});\nconsole.log(\"Loaded agent.\");\n\nconst input0 = `how is your day going?`;\n\nconst result0 = await executor.call({ input: input0 });\n\nconsole.log(`Got output ${result0.output}`);\n\nconst input1 = `whats is 9 to the 2nd power?`;\n\nconst result1 = await executor.call({ input: input1 });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_convo_agent.int.test.ts","loc":{"lines":{"from":1,"to":30}}}}],["94",{"pageContent":"console.log(`Got output ${result1.output}`);\n\nconst input2 = `whats is that result divided by 10?`;\n\nconst result2 = await executor.call({ input: input2 });\n\nconsole.log(`Got output ${result2.output}`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_convo_agent.int.test.ts","loc":{"lines":{"from":39,"to":46}}}}],["95",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { ChatConversationalAgentOutputParser } from \"../chat_convo/outputParser.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":1,"to":2}}}}],["96",{"pageContent":"test(\"Can parse JSON with text in front of it\", async () => {\nconst testCases = [\n{","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":64,"to":66}}}}],["97",{"pageContent":"input: `Based on the information from the search, I can provide you with a query to get all the orders for the email \\`example@gmail.com\\`. Here's the query:\\n\\n\\`\\`\\`sql\\nSELECT * FROM orders\\nJOIN users ON users.id = orders.user_id\\nWHERE users.email = 'example@gmail.com'\\n\\`\\`\\`\\n\\nPlease make any necessary modifications depending on your database schema and table structures. Run this query on your database to retrieve the orders made by the specified user.\\n\\n\\`\\`\\`json\\n{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"To get all the orders for a user with the email \\`example@gmail.com\\`, you can use the following query:\\\\n\\\\n\\`\\`\\`\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = 'example@gmail.com'\\\\n\\`\\`\\`\\\\n\\\\nPlease make any necessary modifications depending on your database schema and table structures. Run this query on your database to retrieve the orders made by the specified user.\"\\n}\\n\\`\\`\\``,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":71,"to":71}}}}],["98",{"pageContent":"output: `{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"To get all the orders for a user with the email \\`example@gmail.com\\`, you can use the following query:\\\\n\\\\n\\`\\`\\`\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = 'example@gmail.com'\\\\n\\`\\`\\`\\\\n\\\\nPlease make any necessary modifications depending on your database schema and table structures. Run this query on your database to retrieve the  made by the specifsredroied user.\"\\n}`,\ntool: \"Final Answer\",\ntoolInput: \"To get all the orders for a user with the email \",\n},","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":128,"to":131}}}}],["99",{"pageContent":"{\ninput:\n'Here is an example of a valid JSON object matching the provided spec:\\n\\n```json\\n{\\n  \"action\": \"metabase\",\\n  \"action_input\": [\"GET\", \"/api/table/1\"]\\n}\\n```\\n\\nIn this example, the \"action\" key has a string value of \"metabase\", and the \"action_input\" key has an array value containing two elements: a string value of \"GET\" and a string value of \"/api/table/1\". This JSON object could be used to make a request to a Metabase API endpoint with the specified method and arguments.',\noutput: `{ \"action\": \"metabase\", \"action_input\": [\"GET\", \"/api/table/1\"] } `,\ntool: \"metabase\",\ntoolInput: [\"GET\", \"/api/table/1\"],\n},\n{\ninput:\n'```\\n{\\n  \"action\": \"metabase\",\\n  \"action_input\": [\"GET\", \"/api/table/1\"]\\n}\\n```',\noutput: `{ \"action\": \"metabase\", \"action_input\": [\"GET\", \"/api/table/1\"] } `,\ntool: \"metabase\",\ntoolInput: [\"GET\", \"/api/table/1\"],\n},\n{\ninput:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":188,"to":203}}}}],["100",{"pageContent":"'Here we have some boilerplate nonsense```\\n{\\n \"action\": \"blogpost\",\\n  \"action_input\": \"```sql\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = \\'bud\\'```\"\\n}\\n``` and at the end there is more nonsense',\noutput:\n'{\"action\":\"blogpost\",\"action_input\":\"```sql\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = \\'bud\\'```\"}',\ntool: \"blogpost\",\ntoolInput:\n\"```sql\\nSELECT * FROM orders\\nJOIN users ON users.id = orders.user_id\\nWHERE users.email = 'bud'```\",\n},\n{\ninput:\n'Here we have some boilerplate nonsense```json\\n{\\n \\t\\r\\n\"action\": \"blogpost\",\\n\\t\\r  \"action_input\": \"```sql\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = \\'bud\\'```\"\\n\\t\\r}\\n\\n\\n\\t\\r``` and at the end there is more nonsense',\noutput:\n'{\"action\":\"blogpost\",\"action_input\":\"```sql\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = \\'bud\\'```\"}',\ntool: \"blogpost\",\ntoolInput:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":260,"to":273}}}}],["101",{"pageContent":"\"```sql\\nSELECT * FROM orders\\nJOIN users ON users.id = orders.user_id\\nWHERE users.email = 'bud'```\",\n},\n];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":330,"to":332}}}}],["102",{"pageContent":"const p = new ChatConversationalAgentOutputParser();\nfor (const message of testCases) {\nconst parsed = await p.parse(message.input);\nexpect(parsed).toBeDefined();\nif (message.tool === \"Final Answer\") {\nexpect(parsed.returnValues).toBeDefined();\n} else {\nexpect(parsed.tool).toEqual(message.tool);\n\nif (typeof message.toolInput === \"object\") {\nexpect(message.toolInput).toEqual(parsed.toolInput);\n}\nif (typeof message.toolInput === \"string\") {\nexpect(message.toolInput).toContain(parsed.toolInput);\n}\n}\n}\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":394,"to":411}}}}],["103",{"pageContent":"/* eslint-disable @typescript-eslint/no-misused-promises */\nimport { describe } from \"@jest/globals\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\nimport { initializeAgentExecutorWithOptions } from \"../initialize.js\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { Tool } from \"../../tools/base.js\";\nimport { SerpAPI } from \"../../tools/serpapi.js\";\nimport { Calculator } from \"../../tools/calculator.js\";\nimport { RequestsGetTool, RequestsPostTool } from \"../../tools/requests.js\";\nimport { AIPluginTool } from \"../../tools/aiplugin.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/evaluation.int.test.ts","loc":{"lines":{"from":1,"to":10}}}}],["104",{"pageContent":"const agents = [\n(tools) =>\ninitializeAgentExecutorWithOptions(\ntools,\nnew ChatOpenAI({ temperature: 0 }),\n{ agentType: \"chat-zero-shot-react-description\" }\n),\n(tools) =>\ninitializeAgentExecutorWithOptions(\ntools,\nnew ChatOpenAI({ temperature: 0 }),\n{ agentType: \"chat-conversational-react-description\" }\n),\n(tools) =>\ninitializeAgentExecutorWithOptions(tools, new OpenAI({ temperature: 0 }), {\nagentType: \"zero-shot-react-description\",\n}),\n] as ((\ntools: Tool[]\n) => ReturnType<typeof initializeAgentExecutorWithOptions>)[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/evaluation.int.test.ts","loc":{"lines":{"from":91,"to":110}}}}],["105",{"pageContent":"const scenarios = [\nasync () => ({\ntools: [\nnew RequestsGetTool(),\nnew RequestsPostTool(),\nawait AIPluginTool.fromPluginUrl(\n\"https://www.klarna.com/.well-known/ai-plugin.json\"\n),\n],\ninput: \"what t shirts are available in klarna?\",\n}),\nasync () => ({\ntools: [\nnew SerpAPI(undefined, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n],\ninput: `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`,\n}),\nasync () => ({\ntools: [\nnew SerpAPI(undefined, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n],\ninput: `how is your day going?`,\n}),\nasync () => ({\ntools: [\nnew SerpAPI(undefined, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n],\ninput: `whats is 9 to the 2nd power?`,\n}),\n] as (() => Promise<{ tools: Tool[]; input: string }>)[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/evaluation.int.test.ts","loc":{"lines":{"from":191,"to":235}}}}],["106",{"pageContent":"describe.each(agents)(`Run agent %#`, (initializeAgentExecutorWithTools) => {\ntest.concurrent.each(scenarios)(`With scenario %#`, async (scenario) => {\nconst agentIndex = agents.indexOf(initializeAgentExecutorWithTools);\nconst scenarioIndex = scenarios.indexOf(scenario);\nconst { tools, input } = await scenario();\nconst agent = await initializeAgentExecutorWithTools(tools);\nconst result = await agent.call({ input });\nconsole.log(`Agent #${agentIndex}`, `Scenario #${scenarioIndex}`, {\nresult,\n});\nexpect(typeof result.output).toBe(\"string\");\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/evaluation.int.test.ts","loc":{"lines":{"from":303,"to":315}}}}],["107",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport {\nJsonListKeysTool,\nJsonSpec,\nJsonGetValueTool,\n} from \"../../tools/json.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/json.test.ts","loc":{"lines":{"from":1,"to":6}}}}],["108",{"pageContent":"test(\"JsonListKeysTool\", async () => {\nconst jsonSpec = new JsonSpec({\nfoo: \"bar\",\nbaz: { test: { foo: [1, 2, 3], qux: [{ x: 1, y: 2, z: 3 }, { a: 1 }] } },\n});\nconst jsonListKeysTool = new JsonListKeysTool(jsonSpec);\nexpect(await jsonListKeysTool.call(\"\")).toBe(\"foo, baz\");\nexpect(await jsonListKeysTool.call(\"/foo\")).toContain(\"not a dictionary\");\nexpect(await jsonListKeysTool.call(\"/baz\")).toBe(\"test\");\nexpect(await jsonListKeysTool.call(\"/baz/test\")).toBe(\"foo, qux\");\nexpect(await jsonListKeysTool.call(\"/baz/test/foo\")).toContain(\n\"not a dictionary\"\n);\nexpect(await jsonListKeysTool.call(\"/baz/test/foo/0\")).toContain(\n\"not a dictionary\"\n);\nexpect(await jsonListKeysTool.call(\"/baz/test/qux\")).toContain(\n\"not a dictionary\"\n);\nexpect(await jsonListKeysTool.call(\"/baz/test/qux/0\")).toBe(\"x, y, z\");\nexpect(await jsonListKeysTool.call(\"/baz/test/qux/1\")).toBe(\"a\");\nexpect(await jsonListKeysTool.call(\"/bar\")).toContain(\"not a dictionary\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/json.test.ts","loc":{"lines":{"from":75,"to":97}}}}],["109",{"pageContent":"test(\"JsonGetValueTool\", async () => {\nconst jsonSpec = new JsonSpec({\nfoo: \"bar\",\nbaz: { test: { foo: [1, 2, 3], qux: [{ x: 1, y: 2, z: 3 }, { a: 1 }] } },\n});\nconst jsonGetValueTool = new JsonGetValueTool(jsonSpec);\nexpect(await jsonGetValueTool.call(\"\")).toBe(\n`{\"foo\":\"bar\",\"baz\":{\"test\":{\"foo\":[1,2,3],\"qux\":[{\"x\":1,\"y\":2,\"z\":3},{\"a\":1}]}}}`\n);\nexpect(await jsonGetValueTool.call(\"/foo\")).toBe(\"bar\");\nexpect(await jsonGetValueTool.call(\"/baz\")).toBe(\n`{\"test\":{\"foo\":[1,2,3],\"qux\":[{\"x\":1,\"y\":2,\"z\":3},{\"a\":1}]}}`\n);\nexpect(await jsonGetValueTool.call(\"/baz/test\")).toBe(\n`{\"foo\":[1,2,3],\"qux\":[{\"x\":1,\"y\":2,\"z\":3},{\"a\":1}]}`\n);\nexpect(await jsonGetValueTool.call(\"/baz/test/foo\")).toBe(\"[1,2,3]\");\nexpect(await jsonGetValueTool.call(\"/baz/test/foo/0\")).toBe(\"1\");\nexpect(await jsonGetValueTool.call(\"/baz/test/qux\")).toBe(\n`[{\"x\":1,\"y\":2,\"z\":3},{\"a\":1}]`\n);\nexpect(await jsonGetValueTool.call(\"/baz/test/qux/0\")).toBe(\n`{\"x\":1,\"y\":2,\"z\":3}`\n);\nexpect(await jsonGetValueTool.call(\"/baz/test/qux/0/x\")).toBe(\"1\");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/json.test.ts","loc":{"lines":{"from":145,"to":169}}}}],["110",{"pageContent":"expect(await jsonGetValueTool.call(\"/baz/test/qux/1\")).toBe(`{\"a\":1}`);\nexpect(await jsonGetValueTool.call(\"/bar\")).toContain(`null`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/json.test.ts","loc":{"lines":{"from":216,"to":218}}}}],["111",{"pageContent":"test(\"JsonGetValueTool, large values\", async () => {\nconst jsonSpec = new JsonSpec(\n{ foo: \"bar\", baz: { test: { foo: [1, 2, 3, 4] } } },\n5\n);\nconst jsonGetValueTool = new JsonGetValueTool(jsonSpec);\nexpect(await jsonGetValueTool.call(\"\")).toContain(\"large dictionary\");\nexpect(await jsonGetValueTool.call(\"/foo\")).toBe(\"bar\");\nexpect(await jsonGetValueTool.call(\"/baz\")).toContain(\"large dictionary\");\nexpect(await jsonGetValueTool.call(\"/baz/test\")).toContain(\n\"large dictionary\"\n);\nexpect(await jsonGetValueTool.call(\"/baz/test/foo\")).toBe(\"[1,2,...\");\nexpect(await jsonGetValueTool.call(\"/baz/test/foo/0\")).toBe(\"1\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/json.test.ts","loc":{"lines":{"from":286,"to":300}}}}],["112",{"pageContent":"/* eslint-disable no-process-env */\nimport { test, expect, beforeEach, afterEach } from \"@jest/globals\";\nimport { DataSource } from \"typeorm\";\nimport {\nInfoSqlTool,\nQuerySqlTool,\nListTablesSqlTool,\nQueryCheckerTool,\n} from \"../../tools/sql.js\";\nimport { SqlDatabase } from \"../../sql_db.js\";\n\nconst previousEnv = process.env;\n\nlet db: SqlDatabase;\n\nbeforeEach(async () => {\nconst datasource = new DataSource({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/sql.test.ts","loc":{"lines":{"from":1,"to":17}}}}],["113",{"pageContent":": \"sqlite\",\ndatabase: \":memory:\",\nsynchronize: true,\n});\n\nawait datasource.initialize();\n\nawait datasource.query(`\nCREATE TABLE products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Apple', 100);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Banana', 200);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Orange', 300);\n`);\nawait datasource.query(`\nCREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, age INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Alice', 20);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Bob', 21);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Charlie', 22);\n`);\n\ndb = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\n\nprocess.env = { ...previousEnv, OPENAI_API_KEY: \"test\" };\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/sql.test.ts","loc":{"lines":{"from":115,"to":152}}}}],["114",{"pageContent":"afterEach(async () => {\nprocess.env = previousEnv;\nawait db.appDataSource.destroy();\n});\n\ntest(\"QuerySqlTool\", async () => {\nconst querySqlTool = new QuerySqlTool(db);\nconst result = await querySqlTool.call(\"SELECT * FROM users\");\nexpect(result).toBe(\n`[{\"id\":1,\"name\":\"Alice\",\"age\":20},{\"id\":2,\"name\":\"Bob\",\"age\":21},{\"id\":3,\"name\":\"Charlie\",\"age\":22}]`\n);\n});\n\ntest(\"QuerySqlTool with error\", async () => {\nconst querySqlTool = new QuerySqlTool(db);\nconst result = await querySqlTool.call(\"SELECT * FROM userss\");\nexpect(result).toBe(`QueryFailedError: SQLITE_ERROR: no such table: userss`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/sql.test.ts","loc":{"lines":{"from":231,"to":248}}}}],["115",{"pageContent":"test(\"InfoSqlTool\", async () => {\nconst infoSqlTool = new InfoSqlTool(db);\nconst result = await infoSqlTool.call(\"users, products\");\nconst expectStr = `\nCREATE TABLE products (\nid INTEGER , name TEXT , price INTEGER ) \nSELECT * FROM \"products\" LIMIT 3;\nid name price\n1 Apple 100\n2 Banana 200\n3 Orange 300\nCREATE TABLE users (\nid INTEGER , name TEXT , age INTEGER ) \nSELECT * FROM \"users\" LIMIT 3;\nid name age\n1 Alice 20\n2 Bob 21\n3 Charlie 22`;\nexpect(result.trim()).toBe(expectStr.trim());\n});\n\ntest(\"InfoSqlTool with error\", async () => {\nconst infoSqlTool = new InfoSqlTool(db);\nconst result = await infoSqlTool.call(\"userss, products\");\nexpect(result).toBe(\n`Error: Wrong target table name: the table userss was not found in the database`\n);\n});\n\ntest(\"ListTablesSqlTool\", async () => {\nconst listSqlTool = new ListTablesSqlTool(db);\nconst result = await listSqlTool.call(\"\");\nexpect(result).toBe(`products, users`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/sql.test.ts","loc":{"lines":{"from":337,"to":370}}}}],["116",{"pageContent":"test(\"QueryCheckerTool\", async () => {\nconst queryCheckerTool = new QueryCheckerTool();\nexpect(queryCheckerTool.llmChain).not.toBeNull();\nexpect(queryCheckerTool.llmChain.inputKeys).toEqual([\"query\"]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/sql.test.ts","loc":{"lines":{"from":450,"to":454}}}}],["117",{"pageContent":"import { beforeEach, describe, expect, test } from \"@jest/globals\";\nimport { ZapierToolKit } from \"../agent_toolkits/zapier/zapier.js\";\nimport { ZapierNLAWrapper, ZapierValues } from \"../../tools/zapier.js\";\n\ndescribe(\"ZapierNLAWrapper\", () => {\nlet actions: ZapierValues[] = [];\nlet zapier: ZapierNLAWrapper;\n\nbeforeEach(async () => {\nzapier = new ZapierNLAWrapper();\nactions = await zapier.listActions();\n});\n\ntest(\"loads ZapierToolKit\", async () => {\nconst toolkit = await ZapierToolKit.fromZapierNLAWrapper(zapier);\n\nexpect(toolkit).toBeDefined();\n});\n\ntest(\"Zapier NLA has connected actions\", async () => {\nexpect(actions.length).toBeGreaterThan(0);\n});\n\ndescribe(\"Giphy action\", () => {\ntest(\"returns a GIF\", async () => {\nconst giphy = actions.find(\n(action) => action.description === \"Giphy: Find GIF\"\n);\nconst result = await zapier.runAction(giphy?.id, \"cats\");\n\nexpect(result).toMatchObject({\nkeyword: \"cats\",\nsize: expect.any(String),\nurl: expect.stringContaining(\"https://\"),\n});\n});\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/zapier_toolkit.int.test.ts","loc":{"lines":{"from":1,"to":38}}}}],["118",{"pageContent":"import { LLMChain } from \"../chains/llm_chain.js\";\nimport { SerializedLLMChain } from \"../chains/serde.js\";\nimport { AgentAction, AgentFinish } from \"../schema/index.js\";\nimport { BaseOutputParser } from \"../schema/output_parser.js\";\n\nexport interface AgentInput {\nllmChain: LLMChain;\noutputParser: AgentActionOutputParser;\nallowedTools?: string[];\n}\n\nexport abstract class AgentActionOutputParser extends BaseOutputParser<\nAgentAction | AgentFinish\n> {}\n\nexport type StoppingMethod = \"force\" | \"generate\";\n\nexport type SerializedAgentT<\nTType extends string = string,\nFromLLMInput extends Record<string, unknown> = Record<string, unknown>,\nConstructorInput extends AgentInput = AgentInput\n> = {\n_type: TType;\nllm_chain?: SerializedLLMChain;\n} & (\n| ({ load_from_llm_and_tools: true } & FromLLMInput)\n| ({ load_from_llm_and_tools?: false } & ConstructorInput)\n);\n\nexport type SerializedFromLLMAndTools = {\nsuffix?: string;\nprefix?: string;\ninput_variables?: string[];\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/types.ts","loc":{"lines":{"from":1,"to":34}}}}],["119",{"pageContent":"type SerializedZeroShotAgent = SerializedAgentT<\n\"zero-shot-react-description\",\nSerializedFromLLMAndTools,\nAgentInput\n>;\n\nexport type SerializedAgent = SerializedZeroShotAgent;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/types.ts","loc":{"lines":{"from":43,"to":49}}}}],["120",{"pageContent":"import type { TiktokenModel } from \"@dqbd/tiktoken\";\n\n// https://www.npmjs.com/package/@dqbd/tiktoken\n\nexport const getModelNameForTiktoken = (modelName: string): TiktokenModel => {\nif (modelName.startsWith(\"gpt-3.5-turbo-\")) {\nreturn \"gpt-3.5-turbo\";\n}\n\nif (modelName.startsWith(\"gpt-4-32k-\")) {\nreturn \"gpt-4-32k\";\n}\n\nif (modelName.startsWith(\"gpt-4-\")) {\nreturn \"gpt-4\";\n}\n\nreturn modelName as TiktokenModel;\n};\n\nexport const getEmbeddingContextSize = (modelName?: string): number => {\nswitch (modelName) {\ncase \"text-embedding-ada-002\":\nreturn 8191;\ndefault:\nreturn 2046;\n}\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/count_tokens.ts","loc":{"lines":{"from":1,"to":28}}}}],["121",{"pageContent":"const getModelContextSize = (modelName: string): number => {\nswitch (getModelNameForTiktoken(modelName)) {\ncase \"gpt-3.5-turbo\":\nreturn 4096;\ncase \"gpt-4-32k\":\nreturn 32768;\ncase \"gpt-4\":\nreturn 8192;\ncase \"text-davinci-003\":\nreturn 4097;\ncase \"text-curie-001\":\nreturn 2048;\ncase \"text-babbage-001\":\nreturn 2048;\ncase \"text-ada-001\":\nreturn 2048;\ncase \"code-davinci-002\":\nreturn 8000;\ncase \"code-cushman-001\":\nreturn 2048;\ndefault:\nreturn 4097;\n}\n};\n\ninterface CalculateMaxTokenProps {\nprompt: string;\nmodelName: TiktokenModel;\n}\n\nexport const importTiktoken = async () => {\ntry {\nconst { encoding_for_model } = await import(\"@dqbd/tiktoken\");\nreturn { encoding_for_model };\n} catch (error) {\nconsole.log(error);\nreturn { encoding_for_model: null };\n}\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/count_tokens.ts","loc":{"lines":{"from":103,"to":141}}}}],["122",{"pageContent":"const calculateMaxTokens = async ({\nprompt,\nmodelName,\n}: CalculateMaxTokenProps) => {\nconst { encoding_for_model } = await importTiktoken();\n\n// fallback to approximate calculation if tiktoken is not available\nlet numTokens = Math.ceil(prompt.length / 4);\n\ntry {\nif (encoding_for_model) {\nconst encoding = encoding_for_model(getModelNameForTiktoken(modelName));\n\nconst tokenized = encoding.encode(prompt);\n\nnumTokens = tokenized.length;\n\nencoding.free();\n}\n} catch (error) {\nconsole.warn(\n\"Failed to calculate number of tokens with tiktoken, falling back to approximate count\",\nerror\n);\n}\n\nconst maxTokens = getModelContextSize(modelName);\n\nreturn maxTokens - numTokens;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/count_tokens.ts","loc":{"lines":{"from":208,"to":237}}}}],["123",{"pageContent":"import type { Tiktoken } from \"@dqbd/tiktoken\";\nimport { BasePromptValue, LLMResult } from \"../schema/index.js\";\nimport { CallbackManager, Callbacks } from \"../callbacks/manager.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../util/async_caller.js\";\nimport { getModelNameForTiktoken, importTiktoken } from \"./count_tokens.js\";\n\nconst getVerbosity = () => false;\n\nexport type SerializedLLM = {\n_model: string;\n_type: string;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\nexport interface BaseLangChainParams {\nverbose?: boolean;\ncallbacks?: Callbacks;\n}\n\n/**\n* Base class for language models, chains, tools.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/index.ts","loc":{"lines":{"from":1,"to":22}}}}],["124",{"pageContent":"abstract class BaseLangChain implements BaseLangChainParams {\n/**\n* Whether to print out response text.\n*/\nverbose: boolean;\n\ncallbacks?: Callbacks;\n\nconstructor(params: BaseLangChainParams) {\nthis.verbose = params.verbose ?? getVerbosity();\nthis.callbacks = params.callbacks;\n}\n}\n\n/**\n* Base interface for language model parameters.\n* A subclass of {@link BaseLanguageModel} should have a constructor that\n* takes in a parameter that extends this interface.\n*/\nexport interface BaseLanguageModelParams\nextends AsyncCallerParams,\nBaseLangChainParams {\n/**\n* @deprecated Use `callbacks` instead\n*/\ncallbackManager?: CallbackManager;\n}\n\nexport interface BaseLanguageModelCallOptions {}\n\n/**\n* Base class for language models.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/index.ts","loc":{"lines":{"from":161,"to":193}}}}],["125",{"pageContent":"abstract class BaseLanguageModel\nextends BaseLangChain\nimplements BaseLanguageModelParams\n{\ndeclare CallOptions: BaseLanguageModelCallOptions;\n\n/**\n* The async caller should be used by subclasses to make any async calls,\n* which will thus benefit from the concurrency and retry logic.\n*/\ncaller: AsyncCaller;\n\nconstructor(params: BaseLanguageModelParams) {\nsuper({\nverbose: params.verbose,\ncallbacks: params.callbacks ?? params.callbackManager,\n});\nthis.caller = new AsyncCaller(params ?? {});\n}\n\nabstract generatePrompt(\npromptValues: BasePromptValue[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult>;\n\nabstract _modelType(): string;\n\nabstract _llmType(): string;\n\nprivate _encoding?: Tiktoken;\n\nprivate _registry?: FinalizationRegistry<Tiktoken>;\n\nasync getNumTokens(text: string) {\n// fallback to approximate calculation if tiktoken is not available\nlet numTokens = Math.ceil(text.length / 4);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/index.ts","loc":{"lines":{"from":330,"to":366}}}}],["126",{"pageContent":"try {\nif (!this._encoding) {\nconst { encoding_for_model } = await importTiktoken();\n// modelName only exists in openai subclasses, but tiktoken only supports\n// openai tokenisers anyway, so for other subclasses we default to gpt2\nif (encoding_for_model) {\nthis._encoding = encoding_for_model(\n\"modelName\" in this\n? getModelNameForTiktoken(this.modelName as string)\n: \"gpt2\"\n);\n// We need to register a finalizer to free the tokenizer when the\n// model is garbage collected.\nthis._registry = new FinalizationRegistry((t) => t.free());\nthis._registry.register(this, this._encoding);\n}\n}\n\nif (this._encoding) {\nnumTokens = this._encoding.encode(text).length;\n}\n} catch (error) {\nconsole.warn(\n\"Failed to calculate number of tokens with tiktoken, falling back to approximate count\",\nerror\n);\n}\n\nreturn numTokens;\n}\n\n/**\n* Get the identifying parameters of the LLM.\n*/\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_identifyingParams(): Record<string, any> {\nreturn {};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/index.ts","loc":{"lines":{"from":494,"to":531}}}}],["127",{"pageContent":"/**\n* Return a json-like object representing this LLM.\n*/\nserialize(): SerializedLLM {\nreturn {\n...this._identifyingParams(),\n_type: this._llmType(),\n_model: this._modelType(),\n};\n}\n\n/**\n* Load an LLM from a json-like object describing it.\n*/\nstatic async deserialize(data: SerializedLLM): Promise<BaseLanguageModel> {\nconst { _type, _model, ...rest } = data;\nif (_model && _model !== \"base_chat_model\") {\nthrow new Error(`Cannot load LLM with model ${_model}`);\n}\nconst Cls = {\nopenai: (await import(\"../chat_models/openai.js\")).ChatOpenAI,\n}[_type];\nif (Cls === undefined) {\nthrow new Error(`Cannot load  LLM with type ${_type}`);\n}\nreturn new Cls(rest);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/index.ts","loc":{"lines":{"from":656,"to":683}}}}],["128",{"pageContent":"import hash from \"object-hash\";\n\n/**\n* This cache key should be consistent across all versions of langchain.\n* It is currently NOT consistent across versions of langchain.\n*\n* A huge benefit of having a remote cache (like redis) is that you can\n* access the cache from different processes/machines. The allows you to\n* seperate concerns and scale horizontally.\n*\n* TODO: Make cache key consistent across versions of langchain.\n*/\nexport const getCacheKey = (...strings: string[]): string =>\nhash(strings.join(\"_\"));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/cache/base.ts","loc":{"lines":{"from":1,"to":14}}}}],["129",{"pageContent":"import { getCacheKey } from \"./base.js\";\nimport { Generation, BaseCache } from \"../schema/index.js\";\n\nconst GLOBAL_MAP = new Map();\n\nexport class InMemoryCache<T = Generation[]> extends BaseCache<T> {\nprivate cache: Map<string, T>;\n\nconstructor(map?: Map<string, T>) {\nsuper();\nthis.cache = map ?? new Map();\n}\n\nlookup(prompt: string, llmKey: string): Promise<T | null> {\nreturn Promise.resolve(this.cache.get(getCacheKey(prompt, llmKey)) ?? null);\n}\n\nasync update(prompt: string, llmKey: string, value: T): Promise<void> {\nthis.cache.set(getCacheKey(prompt, llmKey), value);\n}\n\nstatic global(): InMemoryCache {\nreturn new InMemoryCache(GLOBAL_MAP);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/cache/index.ts","loc":{"lines":{"from":1,"to":25}}}}],["130",{"pageContent":"import type { RedisClientType } from \"redis\";\n\nimport { BaseCache, Generation } from \"../schema/index.js\";\nimport { getCacheKey } from \"./base.js\";\n\nexport class RedisCache extends BaseCache {\nprivate redisClient: RedisClientType;\n\nconstructor(redisClient: RedisClientType) {\nsuper();\nthis.redisClient = redisClient;\n}\n\npublic async lookup(prompt: string, llmKey: string) {\nlet idx = 0;\nlet key = getCacheKey(prompt, llmKey, String(idx));\nlet value = await this.redisClient.get(key);\nconst generations: Generation[] = [];\n\nwhile (value) {\nif (!value) {\nbreak;\n}\n\ngenerations.push({ text: value });\nidx += 1;\nkey = getCacheKey(prompt, llmKey, String(idx));\nvalue = await this.redisClient.get(key);\n}\n\nreturn generations.length > 0 ? generations : null;\n}\n\npublic async update(prompt: string, llmKey: string, value: Generation[]) {\nfor (let i = 0; i < value.length; i += 1) {\nconst key = getCacheKey(prompt, llmKey, String(i));\nawait this.redisClient.set(key, value[i].text);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/cache/redis.ts","loc":{"lines":{"from":1,"to":40}}}}],["131",{"pageContent":"import { test, expect } from \"@jest/globals\";\n\nimport { InMemoryCache } from \"../index.js\";\n\ntest(\"InMemoryCache\", async () => {\nconst cache = new InMemoryCache();\nawait cache.update(\"foo\", \"bar\", [{ text: \"baz\" }]);\nexpect(await cache.lookup(\"foo\", \"bar\")).toEqual([{ text: \"baz\" }]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/cache/tests/cache.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["132",{"pageContent":"import { test, expect, jest } from \"@jest/globals\";\nimport hash from \"object-hash\";\n\nimport { RedisCache } from \"../redis.js\";\n\nconst sha256 = (str: string) => hash(str);\n\ntest(\"RedisCache\", async () => {\nconst redis = {\nget: jest.fn(async (key: string) => {\nif (key === sha256(\"foo_bar_0\")) {\nreturn \"baz\";\n}\nreturn null;\n}),\n};\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconst cache = new RedisCache(redis as any);\nexpect(await cache.lookup(\"foo\", \"bar\")).toEqual([{ text: \"baz\" }]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/cache/tests/redis.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["133",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\nimport {\nAgentAction,\nAgentFinish,\nChainValues,\nLLMResult,\n} from \"../schema/index.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ntype Error = any;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/base.ts","loc":{"lines":{"from":1,"to":10}}}}],["134",{"pageContent":"interface BaseCallbackHandlerInput {\nignoreLLM?: boolean;\nignoreChain?: boolean;\nignoreAgent?: boolean;\n}\n\nabstract class BaseCallbackHandlerMethodsClass {\n/**\n* Called at the start of an LLM or Chat Model run, with the prompt(s)\n* and the run ID.\n*/\nhandleLLMStart?(\nllm: { name: string },\nprompts: string[],\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called when an LLM/ChatModel in `streaming` mode produces a new token\n*/\nhandleLLMNewToken?(\ntoken: string,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called if an LLM/ChatModel run encounters an error\n*/\nhandleLLMError?(\nerr: Error,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called at the end of an LLM/ChatModel run, with the output and the run ID.\n*/\nhandleLLMEnd?(\noutput: LLMResult,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/base.ts","loc":{"lines":{"from":191,"to":234}}}}],["135",{"pageContent":"/**\n* Called at the start of a Chain run, with the chain name and inputs\n* and the run ID.\n*/\nhandleChainStart?(\nchain: { name: string },\ninputs: ChainValues,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called if a Chain run encounters an error\n*/\nhandleChainError?(\nerr: Error,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called at the end of a Chain run, with the outputs and the run ID.\n*/\nhandleChainEnd?(\noutputs: ChainValues,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called at the start of a Tool run, with the tool name and input\n* and the run ID.\n*/\nhandleToolStart?(\ntool: { name: string },\ninput: string,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called if a Tool run encounters an error\n*/\nhandleToolError?(\nerr: Error,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/base.ts","loc":{"lines":{"from":385,"to":432}}}}],["136",{"pageContent":"/**\n* Called at the end of a Tool run, with the tool output and the run ID.\n*/\nhandleToolEnd?(\noutput: string,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\nhandleText?(\ntext: string,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called when an agent is about to execute an action,\n* with the action and the run ID.\n*/\nhandleAgentAction?(\naction: AgentAction,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called when an agent finishes execution, before it exits.\n* with the final output and the run ID.\n*/\nhandleAgentEnd?(\naction: AgentFinish,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n}\n\n/**\n* Base interface for callbacks. All methods are optional. If a method is not\n* implemented, it will be ignored. If a method is implemented, it will be\n* called at the appropriate time. All methods are called with the run ID of\n* the LLM/ChatModel/Chain that is running, which is generated by the\n* CallbackManager.\n*\n* @interface\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/base.ts","loc":{"lines":{"from":583,"to":627}}}}],["137",{"pageContent":"type CallbackHandlerMethods = BaseCallbackHandlerMethodsClass;\n\nexport abstract class BaseCallbackHandler\nextends BaseCallbackHandlerMethodsClass\nimplements BaseCallbackHandlerInput\n{\nabstract name: string;\n\nignoreLLM = false;\n\nignoreChain = false;\n\nignoreAgent = false;\n\nconstructor(input?: BaseCallbackHandlerInput) {\nsuper();\nif (input) {\nthis.ignoreLLM = input.ignoreLLM ?? this.ignoreLLM;\nthis.ignoreChain = input.ignoreChain ?? this.ignoreChain;\nthis.ignoreAgent = input.ignoreAgent ?? this.ignoreAgent;\n}\n}\n\ncopy(): BaseCallbackHandler {\nreturn new (this.constructor as new (\ninput?: BaseCallbackHandlerInput\n) => BaseCallbackHandler)(this);\n}\n\nstatic fromMethods(methods: CallbackHandlerMethods) {\nclass Handler extends BaseCallbackHandler {\nname = uuidv4();\n\nconstructor() {\nsuper();\nObject.assign(this, methods);\n}\n}\nreturn new Handler();\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/base.ts","loc":{"lines":{"from":772,"to":812}}}}],["138",{"pageContent":"import type { CSPair } from \"ansi-styles\";\nimport styles from \"ansi-styles\";\nimport {\nAgentRun,\nBaseTracer,\nBaseTracerSession,\nChainRun,\nLLMRun,\nRun,\nToolRun,\n} from \"./tracers.js\";\n\nfunction wrap(style: CSPair, text: string) {\nreturn `${style.open}${text}${style.close}`;\n}\n\nfunction tryJsonStringify(obj: unknown, fallback: string) {\ntry {\nreturn JSON.stringify(obj, null, 2);\n} catch (err) {\nreturn fallback;\n}\n}\n\nfunction elapsed(run: Run): string {\nconst elapsed = run.end_time - run.start_time;\nif (elapsed < 1000) {\nreturn `${elapsed}ms`;\n}\nreturn `${(elapsed / 1000).toFixed(2)}s`;\n}\n\nconst { color } = styles;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/console.ts","loc":{"lines":{"from":1,"to":33}}}}],["139",{"pageContent":"class ConsoleCallbackHandler extends BaseTracer {\nname = \"console_callback_handler\" as const;\n\n// boilerplate to work with the base tracer class\n\nconstructor() {\nsuper();\n}\n\ni = 0;\n\nprotected persistSession(session: BaseTracerSession) {\n// eslint-disable-next-line no-plusplus\nreturn Promise.resolve({ ...session, id: this.i++ });\n}\n\nprotected persistRun(_run: Run) {\nreturn Promise.resolve();\n}\n\nloadDefaultSession() {\nreturn this.newSession();\n}\n\nloadSession(sessionName: string) {\nreturn this.newSession(sessionName);\n}\n\n// utility methods\n\ngetParents(run: Run) {\nconst parents: Run[] = [];\nlet currentRun = run;\nwhile (currentRun.parent_uuid) {\nconst parent = this.runMap.get(currentRun.parent_uuid);\nif (parent) {\nparents.push(parent);\ncurrentRun = parent;\n} else {\nbreak;\n}\n}\nreturn parents;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/console.ts","loc":{"lines":{"from":210,"to":253}}}}],["140",{"pageContent":"getBreadcrumbs(run: Run) {\nconst parents = this.getParents(run).reverse();\nconst string = [...parents, run]\n.map((parent, i, arr) => {\nconst name = `${parent.execution_order}:${parent.type}:${parent.serialized?.name}`;\nreturn i === arr.length - 1 ? wrap(styles.bold, name) : name;\n})\n.join(\" > \");\nreturn wrap(color.grey, string);\n}\n\n// logging methods\n\nonChainStart(run: ChainRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(\ncolor.green,\n\"[chain/start]\"\n)} [${crumbs}] Entering Chain run with input: ${tryJsonStringify(\nrun.inputs,\n\"[inputs]\"\n)}`\n);\n}\n\nonChainEnd(run: ChainRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.cyan, \"[chain/end]\")} [${crumbs}] [${elapsed(\nrun\n)}] Exiting Chain run with output: ${tryJsonStringify(\nrun.outputs,\n\"[outputs]\"\n)}`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/console.ts","loc":{"lines":{"from":422,"to":458}}}}],["141",{"pageContent":"onChainError(run: ChainRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.red, \"[chain/error]\")} [${crumbs}] [${elapsed(\nrun\n)}] Chain run errored with error: ${tryJsonStringify(\nrun.error,\n\"[error]\"\n)}`\n);\n}\n\nonLLMStart(run: LLMRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(\ncolor.green,\n\"[llm/start]\"\n)} [${crumbs}] Entering LLM run with input: ${tryJsonStringify(\n{ prompts: run.prompts.map((p) => p.trim()) },\n\"[inputs]\"\n)}`\n);\n}\n\nonLLMEnd(run: LLMRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.cyan, \"[llm/end]\")} [${crumbs}] [${elapsed(\nrun\n)}] Exiting LLM run with output: ${tryJsonStringify(\nrun.response,\n\"[response]\"\n)}`\n);\n}\n\nonLLMError(run: LLMRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.red, \"[llm/error]\")} [${crumbs}] [${elapsed(\nrun\n)}] LLM run errored with error: ${tryJsonStringify(run.error, \"[error]\")}`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/console.ts","loc":{"lines":{"from":627,"to":671}}}}],["142",{"pageContent":"onToolStart(run: ToolRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(\ncolor.green,\n\"[tool/start]\"\n)} [${crumbs}] Entering Tool run with input: \"${run.tool_input?.trim()}\"`\n);\n}\n\nonToolEnd(run: ToolRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.cyan, \"[tool/end]\")} [${crumbs}] [${elapsed(\nrun\n)}] Exiting Tool run with output: \"${run.output?.trim()}\"`\n);\n}\n\nonToolError(run: ToolRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.red, \"[tool/error]\")} [${crumbs}] [${elapsed(\nrun\n)}] Tool run errored with error: ${tryJsonStringify(\nrun.error,\n\"[error]\"\n)}`\n);\n}\n\nonAgentAction(run: AgentRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(\ncolor.blue,\n\"[agent/action]\"\n)} [${crumbs}] Agent selected action: ${tryJsonStringify(\nrun.actions[run.actions.length - 1],\n\"[action]\"\n)}`\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/console.ts","loc":{"lines":{"from":831,"to":874}}}}],["143",{"pageContent":"import { LangChainTracer } from \"./tracers.js\";\n\nexport async function getTracingCallbackHandler(\nsession?: string\n): Promise<LangChainTracer> {\nconst tracer = new LangChainTracer();\nif (session) {\nawait tracer.loadSession(session);\n} else {\nawait tracer.loadDefaultSession();\n}\nreturn tracer;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/initialize.ts","loc":{"lines":{"from":1,"to":13}}}}],["144",{"pageContent":"import { AgentAction, ChainValues, LLMResult } from \"../../schema/index.js\";\nimport { BaseCallbackHandler } from \"../base.js\";\n\nexport type RunType = \"llm\" | \"chain\" | \"tool\";\n\nexport interface BaseTracerSession {\nstart_time: number;\nname?: string;\n}\n\nexport type TracerSessionCreate = BaseTracerSession;\n\nexport interface TracerSession extends BaseTracerSession {\nid: number;\n}\n\nexport interface BaseRun {\nuuid: string;\nparent_uuid?: string;\nstart_time: number;\nend_time: number;\nexecution_order: number;\nchild_execution_order: number;\nserialized: { name: string };\nsession_id: number;\nerror?: string;\ntype: RunType;\n}\n\nexport interface LLMRun extends BaseRun {\nprompts: string[];\nresponse?: LLMResult;\n}\n\nexport interface ChainRun extends BaseRun {\ninputs: ChainValues;\noutputs?: ChainValues;\nchild_llm_runs: LLMRun[];\nchild_chain_runs: ChainRun[];\nchild_tool_runs: ToolRun[];\n}\n\nexport interface AgentRun extends ChainRun {\nactions: AgentAction[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":1,"to":45}}}}],["145",{"pageContent":"interface ToolRun extends BaseRun {\ntool_input: string;\noutput?: string;\naction: string;\nchild_llm_runs: LLMRun[];\nchild_chain_runs: ChainRun[];\nchild_tool_runs: ToolRun[];\n}\n\nexport type Run = LLMRun | ChainRun | ToolRun;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":462,"to":471}}}}],["146",{"pageContent":"abstract class BaseTracer extends BaseCallbackHandler {\nprotected session?: TracerSession;\n\nprotected runMap: Map<string, Run> = new Map();\n\nprotected constructor() {\nsuper();\n}\n\ncopy(): this {\nreturn this;\n}\n\nabstract loadSession(sessionName: string): Promise<TracerSession>;\n\nabstract loadDefaultSession(): Promise<TracerSession>;\n\nprotected abstract persistRun(run: Run): Promise<void>;\n\nprotected abstract persistSession(\nsession: TracerSessionCreate\n): Promise<TracerSession>;\n\nasync newSession(sessionName?: string): Promise<TracerSession> {\nconst sessionCreate: TracerSessionCreate = {\nstart_time: Date.now(),\nname: sessionName,\n};\nconst session = await this.persistSession(sessionCreate);\nthis.session = session;\nreturn session;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":924,"to":955}}}}],["147",{"pageContent":"protected _addChildRun(parentRun: ChainRun | ToolRun, childRun: Run) {\nif (childRun.type === \"llm\") {\nparentRun.child_llm_runs.push(childRun as LLMRun);\n} else if (childRun.type === \"chain\") {\nparentRun.child_chain_runs.push(childRun as ChainRun);\n} else if (childRun.type === \"tool\") {\nparentRun.child_tool_runs.push(childRun as ToolRun);\n} else {\nthrow new Error(\"Invalid run type\");\n}\n}\n\nprotected _startTrace(run: Run) {\nif (run.parent_uuid) {\nconst parentRun = this.runMap.get(run.parent_uuid);\nif (parentRun) {\nif (!(parentRun.type === \"tool\" || parentRun.type === \"chain\")) {\nthrow new Error(\"Caller run can only be a tool or chain\");\n} else {\nthis._addChildRun(parentRun as ChainRun | ToolRun, run);\n}\n} else {\nthrow new Error(`Caller run ${run.parent_uuid} not found`);\n}\n}\nthis.runMap.set(run.uuid, run);\n}\n\nprotected async _endTrace(run: Run): Promise<void> {\nif (!run.parent_uuid) {\nawait this.persistRun(run);\n} else {\nconst parentRun = this.runMap.get(run.parent_uuid);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":1380,"to":1412}}}}],["148",{"pageContent":"if (parentRun === undefined) {\nthrow new Error(`Parent run ${run.parent_uuid} not found`);\n}\n\nparentRun.child_execution_order = Math.max(\nparentRun.child_execution_order,\nrun.child_execution_order\n);\n}\nthis.runMap.delete(run.uuid);\n}\n\nprotected _getExecutionOrder(parentRunId: string | undefined): number {\n// If a run has no parent then execution order is 1\nif (parentRunId === undefined) {\nreturn 1;\n}\n\nconst parentRun = this.runMap.get(parentRunId);\n\nif (parentRun === undefined) {\nthrow new Error(`Parent run ${parentRunId} not found`);\n}\n\nreturn parentRun.child_execution_order + 1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":1828,"to":1853}}}}],["149",{"pageContent":"async handleLLMStart(\nllm: { name: string },\nprompts: string[],\nrunId: string,\nparentRunId?: string\n): Promise<void> {\nif (this.session === undefined) {\nthis.session = await this.loadDefaultSession();\n}\nconst execution_order = this._getExecutionOrder(parentRunId);\nconst run: LLMRun = {\nuuid: runId,\nparent_uuid: parentRunId,\nstart_time: Date.now(),\nend_time: 0,\nserialized: llm,\nprompts,\nsession_id: this.session.id,\nexecution_order,\nchild_execution_order: execution_order,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":2288,"to":2307}}}}],["150",{"pageContent":": \"llm\",\n};\n\nthis._startTrace(run);\nawait this.onLLMStart?.(run);\n}\n\nasync handleLLMEnd(output: LLMResult, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"llm\") {\nthrow new Error(\"No LLM run to end.\");\n}\nconst llmRun = run as LLMRun;\nllmRun.end_time = Date.now();\nllmRun.response = output;\nawait this.onLLMEnd?.(llmRun);\nawait this._endTrace(llmRun);\n}\n\nasync handleLLMError(error: Error, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"llm\") {\nthrow new Error(\"No LLM run to end.\");\n}\nconst llmRun = run as LLMRun;\nllmRun.end_time = Date.now();\nllmRun.error = error.message;\nawait this.onLLMError?.(llmRun);\nawait this._endTrace(llmRun);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":2747,"to":2776}}}}],["151",{"pageContent":"async handleChainStart(\nchain: { name: string },\ninputs: ChainValues,\nrunId: string,\nparentRunId?: string\n): Promise<void> {\nif (this.session === undefined) {\nthis.session = await this.loadDefaultSession();\n}\nconst execution_order = this._getExecutionOrder(parentRunId);\nconst run: ChainRun = {\nuuid: runId,\nparent_uuid: parentRunId,\nstart_time: Date.now(),\nend_time: 0,\nserialized: chain,\ninputs,\nsession_id: this.session.id,\nexecution_order,\nchild_execution_order: execution_order,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":3204,"to":3223}}}}],["152",{"pageContent":": \"chain\",\nchild_llm_runs: [],\nchild_chain_runs: [],\nchild_tool_runs: [],\n};\n\nthis._startTrace(run);\nawait this.onChainStart?.(run);\n}\n\nasync handleChainEnd(outputs: ChainValues, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"chain\") {\nthrow new Error(\"No chain run to end.\");\n}\nconst chainRun = run as ChainRun;\nchainRun.end_time = Date.now();\nchainRun.outputs = outputs;\nawait this.onChainEnd?.(chainRun);\nawait this._endTrace(chainRun);\n}\n\nasync handleChainError(error: Error, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"chain\") {\nthrow new Error(\"No chain run to end.\");\n}\nconst chainRun = run as ChainRun;\nchainRun.end_time = Date.now();\nchainRun.error = error.message;\nawait this.onChainError?.(chainRun);\nawait this._endTrace(chainRun);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":3662,"to":3694}}}}],["153",{"pageContent":"async handleToolStart(\ntool: { name: string },\ninput: string,\nrunId: string,\nparentRunId?: string\n): Promise<void> {\nif (this.session === undefined) {\nthis.session = await this.loadDefaultSession();\n}\nconst execution_order = this._getExecutionOrder(parentRunId);\nconst run: ToolRun = {\nuuid: runId,\nparent_uuid: parentRunId,\nstart_time: Date.now(),\nend_time: 0,\nserialized: tool,\ntool_input: input,\nsession_id: this.session.id,\nexecution_order,\nchild_execution_order: execution_order,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":4116,"to":4135}}}}],["154",{"pageContent":": \"tool\",\naction: JSON.stringify(tool), // TODO: this is duplicate info, not needed\nchild_llm_runs: [],\nchild_chain_runs: [],\nchild_tool_runs: [],\n};\n\nthis._startTrace(run);\nawait this.onToolStart?.(run);\n}\n\nasync handleToolEnd(output: string, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"tool\") {\nthrow new Error(\"No tool run to end\");\n}\nconst toolRun = run as ToolRun;\ntoolRun.end_time = Date.now();\ntoolRun.output = output;\nawait this.onToolEnd?.(toolRun);\nawait this._endTrace(toolRun);\n}\n\nasync handleToolError(error: Error, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"tool\") {\nthrow new Error(\"No tool run to end\");\n}\nconst toolRun = run as ToolRun;\ntoolRun.end_time = Date.now();\ntoolRun.error = error.message;\nawait this.onToolError?.(toolRun);\nawait this._endTrace(toolRun);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":4574,"to":4607}}}}],["155",{"pageContent":"async handleAgentAction(action: AgentAction, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"chain\") {\nreturn;\n}\nconst agentRun = run as AgentRun;\nagentRun.actions = agentRun.actions || [];\nagentRun.actions.push(action);\nawait this.onAgentAction?.(run as AgentRun);\n}\n\n// custom event handlers\n\nonLLMStart?(run: LLMRun): void | Promise<void>;\n\nonLLMEnd?(run: LLMRun): void | Promise<void>;\n\nonLLMError?(run: LLMRun): void | Promise<void>;\n\nonChainStart?(run: ChainRun): void | Promise<void>;\n\nonChainEnd?(run: ChainRun): void | Promise<void>;\n\nonChainError?(run: ChainRun): void | Promise<void>;\n\nonToolStart?(run: ToolRun): void | Promise<void>;\n\nonToolEnd?(run: ToolRun): void | Promise<void>;\n\nonToolError?(run: ToolRun): void | Promise<void>;\n\nonAgentAction?(run: AgentRun): void | Promise<void>;\n\n// TODO Implement handleAgentEnd, handleText\n\n// onAgentEnd?(run: ChainRun): void | Promise<void>;\n\n// onText?(run: Run): void | Promise<void>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":5028,"to":5066}}}}],["156",{"pageContent":"class LangChainTracer extends BaseTracer {\nname = \"langchain_tracer\";\n\nprotected endpoint =\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.LANGCHAIN_ENDPOINT\n: undefined) || \"http://localhost:8000\";\n\nprotected headers: Record<string, string> = {\n\"Content-Type\": \"application/json\",\n};\n\nconstructor() {\nsuper();\n// eslint-disable-next-line no-process-env\nif (typeof process !== \"undefined\" && process.env?.LANGCHAIN_API_KEY) {\n// eslint-disable-next-line no-process-env\nthis.headers[\"x-api-key\"] = process.env?.LANGCHAIN_API_KEY;\n}\n}\n\nprotected async persistRun(run: LLMRun | ChainRun | ToolRun): Promise<void> {\nlet endpoint;\nif (run.type === \"llm\") {\nendpoint = `${this.endpoint}/llm-runs`;\n} else if (run.type === \"chain\") {\nendpoint = `${this.endpoint}/chain-runs`;\n} else {\nendpoint = `${this.endpoint}/tool-runs`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":5481,"to":5511}}}}],["157",{"pageContent":"const response = await fetch(endpoint, {\nmethod: \"POST\",\nheaders: this.headers,\nbody: JSON.stringify(run),\n});\nif (!response.ok) {\nconsole.error(\n`Failed to persist run: ${response.status} ${response.statusText}`\n);\n}\n}\n\nprotected async persistSession(\nsessionCreate: TracerSessionCreate\n): Promise<TracerSession> {\nconst endpoint = `${this.endpoint}/sessions`;\nconst response = await fetch(endpoint, {\nmethod: \"POST\",\nheaders: this.headers,\nbody: JSON.stringify(sessionCreate),\n});\nif (!response.ok) {\nconsole.error(\n`Failed to persist session: ${response.status} ${response.statusText}, using default session.`\n);\nreturn {\nid: 1,\n...sessionCreate,\n};\n}\nreturn {\nid: (await response.json()).id,\n...sessionCreate,\n};\n}\n\nasync loadSession(sessionName: string): Promise<TracerSession> {\nconst endpoint = `${this.endpoint}/sessions?name=${sessionName}`;\nreturn this._handleSessionResponse(endpoint);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":5932,"to":5971}}}}],["158",{"pageContent":"async loadDefaultSession(): Promise<TracerSession> {\nconst endpoint = `${this.endpoint}/sessions?name=default`;\nreturn this._handleSessionResponse(endpoint);\n}\n\nprivate async _handleSessionResponse(endpoint: string) {\nconst response = await fetch(endpoint, {\nmethod: \"GET\",\nheaders: this.headers,\n});\nlet tracerSession: TracerSession;\nif (!response.ok) {\nconsole.error(\n`Failed to load session: ${response.status} ${response.statusText}`\n);\ntracerSession = {\nid: 1,\nstart_time: Date.now(),\n};\nthis.session = tracerSession;\nreturn tracerSession;\n}\nconst resp = (await response.json()) as TracerSession[];\nif (resp.length === 0) {\ntracerSession = {\nid: 1,\nstart_time: Date.now(),\n};\nthis.session = tracerSession;\nreturn tracerSession;\n}\n[tracerSession] = resp;\nthis.session = tracerSession;\nreturn tracerSession;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":6391,"to":6426}}}}],["159",{"pageContent":"export {\nBaseCallbackHandler,\nCallbackHandlerMethods,\nBaseCallbackHandlerInput,\n} from \"./base.js\";\n\nexport {\nLangChainTracer,\nBaseRun,\nLLMRun,\nChainRun,\nToolRun,\n} from \"./handlers/tracers.js\";\n\nexport { getTracingCallbackHandler } from \"./handlers/initialize.js\";\n\nexport {\nCallbackManager,\nCallbackManagerForChainRun,\nCallbackManagerForLLMRun,\nCallbackManagerForToolRun,\nCallbackManagerOptions,\nCallbacks,\n} from \"./manager.js\";\nexport { ConsoleCallbackHandler } from \"./handlers/console.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/index.ts","loc":{"lines":{"from":1,"to":25}}}}],["160",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\nimport {\nAgentAction,\nAgentFinish,\nChainValues,\nLLMResult,\n} from \"../schema/index.js\";\nimport { BaseCallbackHandler, CallbackHandlerMethods } from \"./base.js\";\nimport { ConsoleCallbackHandler } from \"./handlers/console.js\";\nimport { getTracingCallbackHandler } from \"./handlers/initialize.js\";\n\ntype BaseCallbackManagerMethods = {\n[K in keyof CallbackHandlerMethods]?: (\n...args: Parameters<Required<CallbackHandlerMethods>[K]>\n) => Promise<unknown>;\n};\n\nexport interface CallbackManagerOptions {\nverbose?: boolean;\ntracing?: boolean;\n}\n\nexport type Callbacks =\n| CallbackManager\n| (BaseCallbackHandler | CallbackHandlerMethods)[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":1,"to":25}}}}],["161",{"pageContent":"abstract class BaseCallbackManager {\nabstract addHandler(handler: BaseCallbackHandler): void;\n\nabstract removeHandler(handler: BaseCallbackHandler): void;\n\nabstract setHandlers(handlers: BaseCallbackHandler[]): void;\n\nsetHandler(handler: BaseCallbackHandler): void {\nreturn this.setHandlers([handler]);\n}\n}\n\nclass BaseRunManager {\nconstructor(\npublic readonly runId: string,\nprotected readonly handlers: BaseCallbackHandler[],\nprotected readonly inheritableHandlers: BaseCallbackHandler[],\nprotected readonly _parentRunId?: string\n) {}\n\nasync handleText(text: string): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\ntry {\nawait handler.handleText?.(text, this.runId, this._parentRunId);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleText: ${err}`\n);\n}\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":501,"to":534}}}}],["162",{"pageContent":"class CallbackManagerForLLMRun\nextends BaseRunManager\nimplements BaseCallbackManagerMethods\n{\nasync handleLLMNewToken(token: string): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreLLM) {\ntry {\nawait handler.handleLLMNewToken?.(\ntoken,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleLLMNewToken: ${err}`\n);\n}\n}\n})\n);\n}\n\nasync handleLLMError(err: Error | unknown): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreLLM) {\ntry {\nawait handler.handleLLMError?.(err, this.runId, this._parentRunId);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleLLMError: ${err}`\n);\n}\n}\n})\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":1005,"to":1043}}}}],["163",{"pageContent":"async handleLLMEnd(output: LLMResult): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreLLM) {\ntry {\nawait handler.handleLLMEnd?.(output, this.runId, this._parentRunId);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleLLMEnd: ${err}`\n);\n}\n}\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":1516,"to":1531}}}}],["164",{"pageContent":"class CallbackManagerForChainRun\nextends BaseRunManager\nimplements BaseCallbackManagerMethods\n{\ngetChild(): CallbackManager {\n// eslint-disable-next-line @typescript-eslint/no-use-before-define\nconst manager = new CallbackManager(this.runId);\nmanager.setHandlers(this.inheritableHandlers);\nreturn manager;\n}\n\nasync handleChainError(err: Error | unknown): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreChain) {\ntry {\nawait handler.handleChainError?.(\nerr,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleChainError: ${err}`\n);\n}\n}\n})\n);\n}\n\nasync handleChainEnd(output: ChainValues): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreChain) {\ntry {\nawait handler.handleChainEnd?.(\noutput,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleChainEnd: ${err}`\n);\n}\n}\n})\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":2022,"to":2071}}}}],["165",{"pageContent":"async handleAgentAction(action: AgentAction): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreAgent) {\ntry {\nawait handler.handleAgentAction?.(\naction,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleAgentAction: ${err}`\n);\n}\n}\n})\n);\n}\n\nasync handleAgentEnd(action: AgentFinish): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreAgent) {\ntry {\nawait handler.handleAgentEnd?.(\naction,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleAgentEnd: ${err}`\n);\n}\n}\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":2537,"to":2576}}}}],["166",{"pageContent":"class CallbackManagerForToolRun\nextends BaseRunManager\nimplements BaseCallbackManagerMethods\n{\ngetChild(): CallbackManager {\n// eslint-disable-next-line @typescript-eslint/no-use-before-define\nconst manager = new CallbackManager(this.runId);\nmanager.setHandlers(this.inheritableHandlers);\nreturn manager;\n}\n\nasync handleToolError(err: Error | unknown): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreAgent) {\ntry {\nawait handler.handleToolError?.(err, this.runId, this._parentRunId);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleToolError: ${err}`\n);\n}\n}\n})\n);\n}\n\nasync handleToolEnd(output: string): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreAgent) {\ntry {\nawait handler.handleToolEnd?.(\noutput,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleToolEnd: ${err}`\n);\n}\n}\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":3052,"to":3098}}}}],["167",{"pageContent":"class CallbackManager\nextends BaseCallbackManager\nimplements BaseCallbackManagerMethods\n{\nhandlers: BaseCallbackHandler[];\n\ninheritableHandlers: BaseCallbackHandler[];\n\nname = \"callback_manager\";\n\nprivate readonly _parentRunId?: string;\n\nconstructor(parentRunId?: string) {\nsuper();\nthis.handlers = [];\nthis.inheritableHandlers = [];\nthis._parentRunId = parentRunId;\n}\n\nasync handleLLMStart(\nllm: { name: string },\nprompts: string[],\nrunId: string = uuidv4()\n): Promise<CallbackManagerForLLMRun> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreLLM) {\ntry {\nawait handler.handleLLMStart?.(\nllm,\nprompts,\nrunId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleLLMStart: ${err}`\n);\n}\n}\n})\n);\nreturn new CallbackManagerForLLMRun(\nrunId,\nthis.handlers,\nthis.inheritableHandlers,\nthis._parentRunId\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":3564,"to":3612}}}}],["168",{"pageContent":"async handleChainStart(\nchain: { name: string },\ninputs: ChainValues,\nrunId = uuidv4()\n): Promise<CallbackManagerForChainRun> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreChain) {\ntry {\nawait handler.handleChainStart?.(\nchain,\ninputs,\nrunId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleChainStart: ${err}`\n);\n}\n}\n})\n);\nreturn new CallbackManagerForChainRun(\nrunId,\nthis.handlers,\nthis.inheritableHandlers,\nthis._parentRunId\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":4081,"to":4110}}}}],["169",{"pageContent":"async handleToolStart(\ntool: { name: string },\ninput: string,\nrunId = uuidv4()\n): Promise<CallbackManagerForToolRun> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreAgent) {\ntry {\nawait handler.handleToolStart?.(\ntool,\ninput,\nrunId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleToolStart: ${err}`\n);\n}\n}\n})\n);\nreturn new CallbackManagerForToolRun(\nrunId,\nthis.handlers,\nthis.inheritableHandlers,\nthis._parentRunId\n);\n}\n\naddHandler(handler: BaseCallbackHandler, inherit = true): void {\nthis.handlers.push(handler);\nif (inherit) {\nthis.inheritableHandlers.push(handler);\n}\n}\n\nremoveHandler(handler: BaseCallbackHandler): void {\nthis.handlers = this.handlers.filter((_handler) => _handler !== handler);\nthis.inheritableHandlers = this.inheritableHandlers.filter(\n(_handler) => _handler !== handler\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":4593,"to":4636}}}}],["170",{"pageContent":"setHandlers(handlers: BaseCallbackHandler[], inherit = true): void {\nthis.handlers = [];\nthis.inheritableHandlers = [];\nfor (const handler of handlers) {\nthis.addHandler(handler, inherit);\n}\n}\n\ncopy(\nadditionalHandlers: BaseCallbackHandler[] = [],\ninherit = true\n): CallbackManager {\nconst manager = new CallbackManager(this._parentRunId);\nfor (const handler of this.handlers) {\nconst inheritable = this.inheritableHandlers.includes(handler);\nmanager.addHandler(handler, inheritable);\n}\nfor (const handler of additionalHandlers) {\nif (\n// Prevent multiple copies of console_callback_handler\nmanager.handlers\n.filter((h) => h.name === \"console_callback_handler\")\n.some((h) => h.name === handler.name)\n) {\ncontinue;\n}\nmanager.addHandler(handler, inherit);\n}\nreturn manager;\n}\n\nstatic fromHandlers(handlers: CallbackHandlerMethods) {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":5105,"to":5136}}}}],["171",{"pageContent":"Handler extends BaseCallbackHandler {\nname = uuidv4();\n\nconstructor() {\nsuper();\nObject.assign(this, handlers);\n}\n}\n\nconst manager = new this();\nmanager.addHandler(new Handler());\nreturn manager;\n}\n\nstatic async configure(\ninheritableHandlers?: Callbacks,\nlocalHandlers?: Callbacks,\noptions?: CallbackManagerOptions\n): Promise<CallbackManager | undefined> {\nlet callbackManager: CallbackManager | undefined;\nif (inheritableHandlers || localHandlers) {\nif (Array.isArray(inheritableHandlers) || !inheritableHandlers) {\ncallbackManager = new CallbackManager();\ncallbackManager.setHandlers(\ninheritableHandlers?.map(ensureHandler) ?? [],\ntrue\n);\n} else {\ncallbackManager = inheritableHandlers;\n}\ncallbackManager = callbackManager.copy(\nArray.isArray(localHandlers)\n? localHandlers.map(ensureHandler)\n: localHandlers?.handlers,\nfalse\n);\n}\nconst tracingEnabled =","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":5607,"to":5644}}}}],["172",{"pageContent":"of process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.LANGCHAIN_TRACING !== undefined\n: false;\nif (options?.verbose || tracingEnabled) {\nif (!callbackManager) {\ncallbackManager = new CallbackManager();\n}\nif (\noptions?.verbose &&\n!callbackManager.handlers.some(\n(handler) => handler.name === ConsoleCallbackHandler.prototype.name\n)\n) {\nconst consoleHandler = new ConsoleCallbackHandler();\ncallbackManager.addHandler(consoleHandler, true);\n}\nif (\ntracingEnabled &&\n!callbackManager.handlers.some(\n(handler) => handler.name === \"langchain_tracer\"\n)\n) {\nconst session =\ntypeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.LANGCHAIN_SESSION\n: undefined;\ncallbackManager.addHandler(\nawait getTracingCallbackHandler(session),\ntrue\n);\n}\n}\nreturn callbackManager;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":6115,"to":6151}}}}],["173",{"pageContent":"ensureHandler(\nhandler: BaseCallbackHandler | CallbackHandlerMethods\n): BaseCallbackHandler {\nif (\"name\" in handler) {\nreturn handler;\n}\n\nreturn BaseCallbackHandler.fromMethods(handler);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":6622,"to":6630}}}}],["174",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { CallbackManager } from \"../manager.js\";\nimport { BaseCallbackHandler, BaseCallbackHandlerInput } from \"../base.js\";\nimport {\nAgentAction,\nAgentFinish,\nChainValues,\nLLMResult,\n} from \"../../schema/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":1,"to":10}}}}],["175",{"pageContent":"FakeCallbackHandler extends BaseCallbackHandler {\nname = `fake-${uuidv4()}`;\n\nstarts = 0;\n\nends = 0;\n\nerrors = 0;\n\nchainStarts = 0;\n\nchainEnds = 0;\n\nllmStarts = 0;\n\nllmEnds = 0;\n\nllmStreams = 0;\n\ntoolStarts = 0;\n\ntoolEnds = 0;\n\nagentEnds = 0;\n\ntexts = 0;\n\nconstructor(inputs?: BaseCallbackHandlerInput) {\nsuper(inputs);\n}\n\nasync handleLLMStart(\n_llm: { name: string },\n_prompts: string[]\n): Promise<void> {\nthis.starts += 1;\nthis.llmStarts += 1;\n}\n\nasync handleLLMEnd(_output: LLMResult): Promise<void> {\nthis.ends += 1;\nthis.llmEnds += 1;\n}\n\nasync handleLLMNewToken(_token: string): Promise<void> {\nthis.llmStreams += 1;\n}\n\nasync handleLLMError(_err: Error): Promise<void> {\nthis.errors += 1;\n}\n\nasync handleChainStart(\n_chain: { name: string },\n_inputs: ChainValues\n): Promise<void> {\nthis.starts += 1;\nthis.chainStarts += 1;\n}\n\nasync handleChainEnd(_outputs: ChainValues): Promise<void> {\nthis.ends += 1;\nthis.chainEnds += 1;\n}\n\nasync handleChainError(_err: Error): Promise<void> {\nthis.errors += 1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":374,"to":441}}}}],["176",{"pageContent":"async handleToolStart(\n_tool: { name: string },\n_input: string\n): Promise<void> {\nthis.starts += 1;\nthis.toolStarts += 1;\n}\n\nasync handleToolEnd(_output: string): Promise<void> {\nthis.ends += 1;\nthis.toolEnds += 1;\n}\n\nasync handleToolError(_err: Error): Promise<void> {\nthis.errors += 1;\n}\n\nasync handleText(_text: string): Promise<void> {\nthis.texts += 1;\n}\n\nasync handleAgentAction(_action: AgentAction): Promise<void> {\nthis.starts += 1;\nthis.toolStarts += 1;\n}\n\nasync handleAgentEnd(_action: AgentFinish): Promise<void> {\nthis.ends += 1;\nthis.agentEnds += 1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":758,"to":787}}}}],["177",{"pageContent":"copy(): FakeCallbackHandler {\nconst newInstance = new FakeCallbackHandler();\nnewInstance.name = this.name;\nnewInstance.starts = this.starts;\nnewInstance.ends = this.ends;\nnewInstance.errors = this.errors;\nnewInstance.chainStarts = this.chainStarts;\nnewInstance.chainEnds = this.chainEnds;\nnewInstance.llmStarts = this.llmStarts;\nnewInstance.llmEnds = this.llmEnds;\nnewInstance.llmStreams = this.llmStreams;\nnewInstance.toolStarts = this.toolStarts;\nnewInstance.toolEnds = this.toolEnds;\nnewInstance.agentEnds = this.agentEnds;\nnewInstance.texts = this.texts;\n\nreturn newInstance;\n}\n}\n\ntest(\"CallbackManager\", async () => {\nconst manager = new CallbackManager();\nconst handler1 = new FakeCallbackHandler();\nconst handler2 = new FakeCallbackHandler();\nmanager.addHandler(handler1);\nmanager.addHandler(handler2);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":1127,"to":1152}}}}],["178",{"pageContent":"const llmCb = await manager.handleLLMStart({ name: \"test\" }, [\"test\"]);\nawait llmCb.handleLLMEnd({ generations: [] });\nawait llmCb.handleLLMNewToken(\"test\");\nawait llmCb.handleLLMError(new Error(\"test\"));\nconst chainCb = await manager.handleChainStart(\n{ name: \"test\" },\n{ test: \"test\" }\n);\nawait chainCb.handleChainEnd({ test: \"test\" });\nawait chainCb.handleChainError(new Error(\"test\"));\nconst toolCb = await manager.handleToolStart({ name: \"test\" }, \"test\");\nawait toolCb.handleToolEnd(\"test\");\nawait toolCb.handleToolError(new Error(\"test\"));\nawait chainCb.handleText(\"test\");\nawait chainCb.handleAgentAction({\ntool: \"test\",\ntoolInput: \"test\",\nlog: \"test\",\n});\nawait chainCb.handleAgentEnd({ returnValues: { test: \"test\" }, log: \"test\" });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":1478,"to":1497}}}}],["179",{"pageContent":"for (const handler of [handler1, handler2]) {\nexpect(handler.starts).toBe(4);\nexpect(handler.ends).toBe(4);\nexpect(handler.errors).toBe(3);\nexpect(handler.llmStarts).toBe(1);\nexpect(handler.llmEnds).toBe(1);\nexpect(handler.llmStreams).toBe(1);\nexpect(handler.chainStarts).toBe(1);\nexpect(handler.chainEnds).toBe(1);\nexpect(handler.toolStarts).toBe(2);\nexpect(handler.toolEnds).toBe(1);\nexpect(handler.agentEnds).toBe(1);\nexpect(handler.texts).toBe(1);\n}\n});\n\ntest(\"CallbackHandler with ignoreLLM\", async () => {\nconst handler = new FakeCallbackHandler({\nignoreLLM: true,\n});\nconst manager = new CallbackManager();\nmanager.addHandler(handler);\nconst llmCb = await manager.handleLLMStart({ name: \"test\" }, [\"test\"]);\nawait llmCb.handleLLMEnd({ generations: [] });\nawait llmCb.handleLLMNewToken(\"test\");\nawait llmCb.handleLLMError(new Error(\"test\"));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":1826,"to":1851}}}}],["180",{"pageContent":"expect(handler.starts).toBe(0);\nexpect(handler.ends).toBe(0);\nexpect(handler.errors).toBe(0);\nexpect(handler.llmStarts).toBe(0);\nexpect(handler.llmEnds).toBe(0);\nexpect(handler.llmStreams).toBe(0);\n});\n\ntest(\"CallbackHandler with ignoreChain\", async () => {\nconst handler = new FakeCallbackHandler({\nignoreChain: true,\n});\nconst manager = new CallbackManager();\nmanager.addHandler(handler);\nconst chainCb = await manager.handleChainStart(\n{ name: \"test\" },\n{ test: \"test\" }\n);\nawait chainCb.handleChainEnd({ test: \"test\" });\nawait chainCb.handleChainError(new Error(\"test\"));\n\nexpect(handler.starts).toBe(0);\nexpect(handler.ends).toBe(0);\nexpect(handler.errors).toBe(0);\nexpect(handler.chainStarts).toBe(0);\nexpect(handler.chainEnds).toBe(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":2174,"to":2200}}}}],["181",{"pageContent":"test(\"CallbackHandler with ignoreAgent\", async () => {\nconst handler = new FakeCallbackHandler({\nignoreAgent: true,\n});\nconst manager = new CallbackManager();\nmanager.addHandler(handler);\nconst toolCb = await manager.handleToolStart({ name: \"test\" }, \"test\");\nawait toolCb.handleToolEnd(\"test\");\nawait toolCb.handleToolError(new Error(\"test\"));\nconst chainCb = await manager.handleChainStart(\n{ name: \"agent_executor\" },\n{}\n);\nawait chainCb.handleAgentAction({\ntool: \"test\",\ntoolInput: \"test\",\nlog: \"test\",\n});\nawait chainCb.handleAgentEnd({ returnValues: { test: \"test\" }, log: \"test\" });\n\nexpect(handler.starts).toBe(1);\nexpect(handler.ends).toBe(0);\nexpect(handler.errors).toBe(0);\nexpect(handler.toolStarts).toBe(0);\nexpect(handler.toolEnds).toBe(0);\nexpect(handler.agentEnds).toBe(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":2529,"to":2555}}}}],["182",{"pageContent":"test(\"CallbackManager with child manager\", async () => {\nconst llmRunId = \"llmRunId\";\nconst chainRunId = \"chainRunId\";\nlet llmWasCalled = false;\nlet chainWasCalled = false;\nconst manager = CallbackManager.fromHandlers({\nasync handleLLMStart(\n_llm: { name: string },\n_prompts: string[],\nrunId?: string,\nparentRunId?: string\n) {\nexpect(runId).toBe(llmRunId);\nexpect(parentRunId).toBe(chainRunId);\nllmWasCalled = true;\n},\nasync handleChainStart(\n_chain: { name: string },\n_inputs: ChainValues,\nrunId?: string,\nparentRunId?: string\n) {\nexpect(runId).toBe(chainRunId);\nexpect(parentRunId).toBe(undefined);\nchainWasCalled = true;\n},\n});\nconst chainCb = await manager.handleChainStart(\n{ name: \"test\" },\n{ test: \"test\" },\nchainRunId\n);\nawait chainCb.getChild().handleLLMStart({ name: \"test\" }, [\"test\"], llmRunId);\nexpect(llmWasCalled).toBe(true);\nexpect(chainWasCalled).toBe(true);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":2881,"to":2916}}}}],["183",{"pageContent":"test(\"CallbackManager with child manager inherited handlers\", async () => {\nconst callbackManager1 = new CallbackManager();\nconst handler1 = new FakeCallbackHandler();\nconst handler2 = new FakeCallbackHandler();\nconst handler3 = new FakeCallbackHandler();\nconst handler4 = new FakeCallbackHandler();\n\ncallbackManager1.setHandlers([handler1, handler2]);\nexpect(callbackManager1.handlers).toEqual([handler1, handler2]);\nexpect(callbackManager1.inheritableHandlers).toEqual([handler1, handler2]);\n\nconst callbackManager2 = callbackManager1.copy([handler3, handler4]);\nexpect(callbackManager2.handlers).toEqual([\nhandler1,\nhandler2,\nhandler3,\nhandler4,\n]);\nexpect(callbackManager2.inheritableHandlers).toEqual([\nhandler1,\nhandler2,\nhandler3,\nhandler4,\n]);\n\nconst callbackManager3 = callbackManager1.copy([handler3, handler4], false);\nexpect(callbackManager3.handlers).toEqual([\nhandler1,\nhandler2,\nhandler3,\nhandler4,\n]);\nexpect(callbackManager3.inheritableHandlers).toEqual([handler1, handler2]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":3238,"to":3270}}}}],["184",{"pageContent":"const chainCb = await callbackManager3.handleChainStart(\n{ name: \"test\" },\n{ test: \"test\" }\n);\nconst childManager = chainCb.getChild();\nexpect(childManager.handlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\n]);\nexpect(childManager.inheritableHandlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\n]);\n\nconst toolCb = await childManager.handleToolStart({ name: \"test\" }, \"test\");\nconst childManager2 = toolCb.getChild();\nexpect(childManager2.handlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\n]);\nexpect(childManager2.inheritableHandlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\n]);\n});\n\ntest(\"CallbackManager.copy()\", () => {\nconst callbackManager1 = new CallbackManager();\nconst handler1 = new FakeCallbackHandler();\nconst handler2 = new FakeCallbackHandler();\nconst handler3 = new FakeCallbackHandler();\nconst handler4 = new FakeCallbackHandler();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":3587,"to":3618}}}}],["185",{"pageContent":"callbackManager1.addHandler(handler1, true);\ncallbackManager1.addHandler(handler2, false);\nexpect(callbackManager1.handlers).toEqual([handler1, handler2]);\nexpect(callbackManager1.inheritableHandlers).toEqual([handler1]);\n\nconst callbackManager2 = callbackManager1.copy([handler3]);\nexpect(callbackManager2.handlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\nhandler3.name,\n]);\nexpect(callbackManager2.inheritableHandlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler3.name,\n]);\n\nconst callbackManager3 = callbackManager2.copy([handler4], false);\nexpect(callbackManager3.handlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\nhandler3.name,\nhandler4.name,\n]);\nexpect(callbackManager3.inheritableHandlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler3.name,\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":3939,"to":3966}}}}],["186",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\n/* eslint-disable no-process-env */\nimport { test } from \"@jest/globals\";\n\nimport { LangChainTracer } from \"../handlers/tracers.js\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { SerpAPI } from \"../../tools/index.js\";\nimport { Calculator } from \"../../tools/calculator.js\";\nimport { initializeAgentExecutorWithOptions } from \"../../agents/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/langchain_tracer.int.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["187",{"pageContent":"test(\"Test LangChain tracer\", async () => {\nconst tracer = new LangChainTracer();\nconst chainRunId = uuidv4();\nconst toolRunId = uuidv4();\nconst llmRunId = uuidv4();\nawait tracer.handleChainStart({ name: \"test\" }, { foo: \"bar\" }, chainRunId);\nawait tracer.handleToolStart({ name: \"test\" }, \"test\", toolRunId, chainRunId);\nawait tracer.handleLLMStart({ name: \"test\" }, [\"test\"], llmRunId, toolRunId);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId);\nawait tracer.handleToolEnd(\"output\", toolRunId);\nconst llmRunId2 = uuidv4();\nawait tracer.handleLLMStart(\n{ name: \"test2\" },\n[\"test\"],\nllmRunId2,\nchainRunId\n);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId2);\nawait tracer.handleChainEnd({ foo: \"bar\" }, chainRunId);\n\nconst llmRunId3 = uuidv4();\nawait tracer.handleLLMStart({ name: \"test\" }, [\"test\"], llmRunId3);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId3);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/langchain_tracer.int.test.ts","loc":{"lines":{"from":66,"to":89}}}}],["188",{"pageContent":"test(\"Test Traced Agent with concurrency\", async () => {\nprocess.env.LANGCHAIN_TRACING = \"true\";\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\nnew SerpAPI(process.env.SERPAPI_API_KEY, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"zero-shot-react-description\",\nverbose: true,\n});\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst [resultA, resultB, resultC] = await Promise.all([\nexecutor.call({ input }),\nexecutor.call({ input }),\nexecutor.call({ input }),\n]);\n\nconsole.log(`Got output ${resultA.output}`);\nconsole.log(`Got output ${resultB.output}`);\nconsole.log(`Got output ${resultC.output}`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/langchain_tracer.int.test.ts","loc":{"lines":{"from":135,"to":165}}}}],["189",{"pageContent":"import { test, expect, jest } from \"@jest/globals\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport {\nBaseTracer,\nLLMRun,\nChainRun,\nToolRun,\nTracerSession,\nTracerSessionCreate,\n} from \"../handlers/tracers.js\";\n\nconst TEST_SESSION_ID = 2023;\nconst _DATE = 1620000000000;\n\nDate.now = jest.fn(() => _DATE);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":1,"to":15}}}}],["190",{"pageContent":"FakeTracer extends BaseTracer {\nname = \"fake_tracer\";\n\nruns: (LLMRun | ChainRun | ToolRun)[] = [];\n\nconstructor() {\nsuper();\n}\n\nprotected persistRun(run: LLMRun | ChainRun | ToolRun): Promise<void> {\nthis.runs.push(run);\nreturn Promise.resolve();\n}\n\nprotected persistSession(\nsession: TracerSessionCreate\n): Promise<TracerSession> {\nreturn Promise.resolve({\nid: TEST_SESSION_ID,\n...session,\n});\n}\n\nasync loadSession(sessionName: string): Promise<TracerSession> {\nreturn Promise.resolve({\nid: TEST_SESSION_ID,\nname: sessionName,\nstart_time: _DATE,\n});\n}\n\nasync loadDefaultSession(): Promise<TracerSession> {\nreturn Promise.resolve({\nid: TEST_SESSION_ID,\nname: \"default\",\nstart_time: _DATE,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":246,"to":284}}}}],["191",{"pageContent":"test(\"Test LLMRun\", async () => {\nconst tracer = new FakeTracer();\nawait tracer.newSession();\nconst runId = uuidv4();\nawait tracer.handleLLMStart({ name: \"test\" }, [\"test\"], runId);\nawait tracer.handleLLMEnd({ generations: [] }, runId);\nexpect(tracer.runs.length).toBe(1);\nconst run = tracer.runs[0];\nconst compareRun: LLMRun = {\nuuid: runId,\nstart_time: _DATE,\nend_time: _DATE,\nexecution_order: 1,\nchild_execution_order: 1,\nserialized: { name: \"test\" },\nsession_id: TEST_SESSION_ID,\nprompts: [\"test\"],","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":496,"to":512}}}}],["192",{"pageContent":": \"llm\",\nresponse: { generations: [] },\n};\nexpect(run).toEqual(compareRun);\n});\n\ntest(\"Test LLM Run no start\", async () => {\nconst tracer = new FakeTracer();\nawait tracer.newSession();\nconst runId = uuidv4();\nawait expect(tracer.handleLLMEnd({ generations: [] }, runId)).rejects.toThrow(\n\"No LLM run to end\"\n);\n});\n\ntest(\"Test Chain Run\", async () => {\nconst tracer = new FakeTracer();\nawait tracer.newSession();\nconst runId = uuidv4();\nconst compareRun: ChainRun = {\nuuid: runId,\nstart_time: _DATE,\nend_time: _DATE,\nexecution_order: 1,\nchild_execution_order: 1,\nserialized: { name: \"test\" },\nsession_id: TEST_SESSION_ID,\ninputs: { foo: \"bar\" },\noutputs: { foo: \"bar\" },","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":732,"to":760}}}}],["193",{"pageContent":": \"chain\",\nchild_llm_runs: [],\nchild_chain_runs: [],\nchild_tool_runs: [],\n};\nawait tracer.handleChainStart({ name: \"test\" }, { foo: \"bar\" }, runId);\nawait tracer.handleChainEnd({ foo: \"bar\" }, runId);\nexpect(tracer.runs.length).toBe(1);\nconst run = tracer.runs[0];\nexpect(run).toEqual(compareRun);\n});\n\ntest(\"Test Tool Run\", async () => {\nconst tracer = new FakeTracer();\nawait tracer.newSession();\nconst runId = uuidv4();\nconst compareRun: ToolRun = {\nuuid: runId,\nstart_time: _DATE,\nend_time: _DATE,\nexecution_order: 1,\nchild_execution_order: 1,\nserialized: { name: \"test\" },\nsession_id: TEST_SESSION_ID,\ntool_input: \"test\",\noutput: \"output\",","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":973,"to":998}}}}],["194",{"pageContent":": \"tool\",\naction: JSON.stringify({ name: \"test\" }),\nchild_llm_runs: [],\nchild_chain_runs: [],\nchild_tool_runs: [],\n};\nawait tracer.handleToolStart({ name: \"test\" }, \"test\", runId);\nawait tracer.handleToolEnd(\"output\", runId);\nexpect(tracer.runs.length).toBe(1);\nconst run = tracer.runs[0];\nexpect(run).toEqual(compareRun);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":1212,"to":1223}}}}],["195",{"pageContent":"test(\"Test nested runs\", async () => {\nconst tracer = new FakeTracer();\nawait tracer.newSession();\nconst chainRunId = uuidv4();\nconst toolRunId = uuidv4();\nconst llmRunId = uuidv4();\nawait tracer.handleChainStart({ name: \"test\" }, { foo: \"bar\" }, chainRunId);\nawait tracer.handleToolStart({ name: \"test\" }, \"test\", toolRunId, chainRunId);\nawait tracer.handleLLMStart({ name: \"test\" }, [\"test\"], llmRunId, toolRunId);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId);\nawait tracer.handleToolEnd(\"output\", toolRunId);\nconst llmRunId2 = uuidv4();\nawait tracer.handleLLMStart(\n{ name: \"test2\" },\n[\"test\"],\nllmRunId2,\nchainRunId\n);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId2);\nawait tracer.handleChainEnd({ foo: \"bar\" }, chainRunId);\nconst compareRun: ChainRun = {\nchild_chain_runs: [],\nchild_llm_runs: [\n{\nuuid: llmRunId2,\nparent_uuid: chainRunId,\nend_time: 1620000000000,\nexecution_order: 4,\nchild_execution_order: 4,\nprompts: [\"test\"],\nresponse: {\ngenerations: [[]],\n},\nserialized: {\nname: \"test2\",\n},","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":1452,"to":1487}}}}],["196",{"pageContent":"session_id: 2023,\nstart_time: 1620000000000,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":1684,"to":1685}}}}],["197",{"pageContent":": \"llm\",\n},\n],\nchild_tool_runs: [\n{\nuuid: toolRunId,\nparent_uuid: chainRunId,\naction: '{\"name\":\"test\"}',\nchild_chain_runs: [],\nchild_llm_runs: [\n{\nuuid: llmRunId,\nparent_uuid: toolRunId,\nend_time: 1620000000000,\nexecution_order: 3,\nchild_execution_order: 3,\nprompts: [\"test\"],\nresponse: {\ngenerations: [[]],\n},\nserialized: {\nname: \"test\",\n},\nsession_id: 2023,\nstart_time: 1620000000000,\ntype: \"llm\",\n},\n],\nchild_tool_runs: [],\nend_time: 1620000000000,\nexecution_order: 2,\nchild_execution_order: 3,\noutput: \"output\",\nserialized: {\nname: \"test\",\n},\nsession_id: 2023,\nstart_time: 1620000000000,\ntool_input: \"test\",\ntype: \"tool\",\n},\n],\nuuid: chainRunId,\nend_time: 1620000000000,\nexecution_order: 1,\nchild_execution_order: 4,\ninputs: {\nfoo: \"bar\",\n},\noutputs: {\nfoo: \"bar\",\n},\nserialized: {\nname: \"test\",\n},\nsession_id: 2023,\nstart_time: 1620000000000,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":1930,"to":1986}}}}],["198",{"pageContent":": \"chain\",\n};\nexpect(tracer.runs.length).toBe(1);\nexpect(tracer.runs[0]).toEqual(compareRun);\n\nconst llmRunId3 = uuidv4();\nawait tracer.handleLLMStart({ name: \"test\" }, [\"test\"], llmRunId3);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId3);\nexpect(tracer.runs.length).toBe(2);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":2191,"to":2200}}}}],["199",{"pageContent":"import { BaseChain, ChainInputs } from \"./base.js\";\nimport {\nTextSplitter,\nRecursiveCharacterTextSplitter,\n} from \"../text_splitter.js\";\nimport { ChainValues } from \"../schema/index.js\";\nimport { SerializedAnalyzeDocumentChain } from \"./serde.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type LoadValues = Record<string, any>;\n\nexport interface AnalyzeDocumentChainInput extends Omit<ChainInputs, \"memory\"> {\ncombineDocumentsChain: BaseChain;\ntextSplitter?: TextSplitter;\ninputKey?: string;\n}\n\n/**\n* Chain that combines documents by stuffing into context.\n* @augments BaseChain\n* @augments StuffDocumentsChainInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/analyze_documents_chain.ts","loc":{"lines":{"from":1,"to":23}}}}],["200",{"pageContent":"class AnalyzeDocumentChain\nextends BaseChain\nimplements AnalyzeDocumentChainInput\n{\ninputKey = \"input_document\";\n\ncombineDocumentsChain: BaseChain;\n\ntextSplitter: TextSplitter;\n\nconstructor(fields: AnalyzeDocumentChainInput) {\nsuper(fields);\nthis.combineDocumentsChain = fields.combineDocumentsChain;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.textSplitter =\nfields.textSplitter ?? new RecursiveCharacterTextSplitter();\n}\n\nget inputKeys(): string[] {\nreturn [this.inputKey];\n}\n\nget outputKeys(): string[] {\nreturn this.combineDocumentsChain.outputKeys;\n}\n\n/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Document key ${this.inputKey} not found.`);\n}\nconst { [this.inputKey]: doc, ...rest } = values;\n\nconst currentDoc = doc as string;\nconst currentDocs = await this.textSplitter.createDocuments([currentDoc]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/analyze_documents_chain.ts","loc":{"lines":{"from":107,"to":144}}}}],["201",{"pageContent":"const newInputs = { input_documents: currentDocs, ...rest };\nconst result = await this.combineDocumentsChain.call(\nnewInputs,\nrunManager?.getChild()\n);\nreturn result;\n}\n\n_chainType() {\nreturn \"analyze_document_chain\" as const;\n}\n\nstatic async deserialize(\ndata: SerializedAnalyzeDocumentChain,\nvalues: LoadValues\n) {\nif (!(\"text_splitter\" in values)) {\nthrow new Error(\n`Need to pass in a text_splitter to deserialize AnalyzeDocumentChain.`\n);\n}\nconst { text_splitter } = values;\n\nif (!data.combine_document_chain) {\nthrow new Error(\n`Need to pass in a combine_document_chain to deserialize AnalyzeDocumentChain.`\n);\n}\n\nreturn new AnalyzeDocumentChain({\ncombineDocumentsChain: await BaseChain.deserialize(\ndata.combine_document_chain\n),\ntextSplitter: text_splitter,\n});\n}\n\nserialize(): SerializedAnalyzeDocumentChain {\nreturn {\n_type: this._chainType(),\ncombine_document_chain: this.combineDocumentsChain.serialize(),\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/analyze_documents_chain.ts","loc":{"lines":{"from":216,"to":259}}}}],["202",{"pageContent":"import { BaseMemory } from \"../memory/base.js\";\nimport { ChainValues, RUN_KEY } from \"../schema/index.js\";\nimport {\nCallbackManagerForChainRun,\nCallbackManager,\nCallbacks,\n} from \"../callbacks/manager.js\";\nimport { SerializedBaseChain } from \"./serde.js\";\nimport { BaseLangChain, BaseLangChainParams } from \"../base_language/index.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type LoadValues = Record<string, any>;\n\nexport interface ChainInputs extends BaseLangChainParams {\nmemory?: BaseMemory;\n\n/**\n* @deprecated Use `callbacks` instead\n*/\ncallbackManager?: CallbackManager;\n}\n\n/**\n* Base interface that all chains must implement.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1,"to":25}}}}],["203",{"pageContent":"abstract class BaseChain extends BaseLangChain implements ChainInputs {\ndeclare memory?: BaseMemory;\n\nconstructor(\nfields?: BaseMemory | ChainInputs,\n/** @deprecated */\nverbose?: boolean,\n/** @deprecated */\ncallbacks?: Callbacks\n) {\nif (\narguments.length === 1 &&","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":201,"to":212}}}}],["204",{"pageContent":"of fields === \"object\" &&\n!(\"saveContext\" in fields)\n) {\n// fields is not a BaseMemory\nconst { memory, callbackManager, ...rest } = fields;\nsuper({ ...rest, callbacks: callbackManager ?? rest.callbacks });\nthis.memory = memory;\n} else {\n// fields is a BaseMemory\nsuper({ verbose, callbacks });\nthis.memory = fields as BaseMemory;\n}\n}\n\n/**\n* Run the core logic of this chain and return the output\n*/\nabstract _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues>;\n\n/**\n* Return the string type key uniquely identifying this class of chain.\n*/\nabstract _chainType(): string;\n\n/**\n* Return a json-like object representing this chain.\n*/\nserialize(): SerializedBaseChain {\nthrow new Error(\"Method not implemented.\");\n}\n\nabstract get inputKeys(): string[];\n\nabstract get outputKeys(): string[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":403,"to":439}}}}],["205",{"pageContent":"async run(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ninput: any,\ncallbacks?: Callbacks\n): Promise<string> {\nconst isKeylessInput = this.inputKeys.length <= 1;\nif (!isKeylessInput) {\nthrow new Error(\n`Chain ${this._chainType()} expects multiple inputs, cannot use 'run' `\n);\n}\nconst values = this.inputKeys.length ? { [this.inputKeys[0]]: input } : {};\nconst returnValues = await this.call(values, callbacks);\nconst keys = Object.keys(returnValues);\n\nif (keys.length === 1) {\nreturn returnValues[keys[0]];\n}\nthrow new Error(\n\"return values have multiple keys, `run` only supported when one key currently\"\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":609,"to":630}}}}],["206",{"pageContent":"/**\n* Run the core logic of this chain and add to output if desired.\n*\n* Wraps _call and handles memory.\n*/\nasync call(values: ChainValues, callbacks?: Callbacks): Promise<ChainValues> {\nconst fullValues = { ...values } as typeof values;\nif (!(this.memory == null)) {\nconst newValues = await this.memory.loadMemoryVariables(values);\nfor (const [key, value] of Object.entries(newValues)) {\nfullValues[key] = value;\n}\n}\nconst callbackManager_ = await CallbackManager.configure(\ncallbacks,\nthis.callbacks,\n{ verbose: this.verbose }\n);\nconst runManager = await callbackManager_?.handleChainStart(\n{ name: this._chainType() },\nfullValues\n);\nlet outputValues;\ntry {\noutputValues = await this._call(fullValues, runManager);\n} catch (e) {\nawait runManager?.handleChainError(e);\nthrow e;\n}\nawait runManager?.handleChainEnd(outputValues);\nif (!(this.memory == null)) {\nawait this.memory.saveContext(values, outputValues);\n}\n// add the runManager's currentRunId to the outputValues\nObject.defineProperty(outputValues, RUN_KEY, {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":808,"to":842}}}}],["207",{"pageContent":"value: runManager ? { runId: runManager?.runId } : undefined,\nconfigurable: true,\n});\nreturn outputValues;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1004,"to":1008}}}}],["208",{"pageContent":"/**\n* Call the chain on all inputs in the list\n*/\nasync apply(\ninputs: ChainValues[],\ncallbacks?: Callbacks[]\n): Promise<ChainValues> {\nreturn Promise.all(\ninputs.map(async (i, idx) => this.call(i, callbacks?.[idx]))\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1205,"to":1215}}}}],["209",{"pageContent":"/**\n* Load a chain from a json-like object describing it.\n*/\nstatic async deserialize(\ndata: SerializedBaseChain,\nvalues: LoadValues = {}\n): Promise<BaseChain> {\nswitch (data._type) {\ncase \"llm_chain\": {\nconst { LLMChain } = await import(\"./llm_chain.js\");\nreturn LLMChain.deserialize(data);\n}\ncase \"sequential_chain\": {\nconst { SequentialChain } = await import(\"./sequential_chain.js\");\nreturn SequentialChain.deserialize(data);\n}\ncase \"simple_sequential_chain\": {\nconst { SimpleSequentialChain } = await import(\"./sequential_chain.js\");\nreturn SimpleSequentialChain.deserialize(data);\n}\ncase \"stuff_documents_chain\": {\nconst { StuffDocumentsChain } = await import(\"./combine_docs_chain.js\");\nreturn StuffDocumentsChain.deserialize(data);\n}\ncase \"map_reduce_documents_chain\": {\nconst { MapReduceDocumentsChain } = await import(\n\"./combine_docs_chain.js\"\n);\nreturn MapReduceDocumentsChain.deserialize(data);\n}\ncase \"refine_documents_chain\": {\nconst { RefineDocumentsChain } = await import(\n\"./combine_docs_chain.js\"\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1407,"to":1440}}}}],["210",{"pageContent":"return RefineDocumentsChain.deserialize(data);\n}\ncase \"vector_db_qa\": {\nconst { VectorDBQAChain } = await import(\"./vector_db_qa.js\");\nreturn VectorDBQAChain.deserialize(data, values);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1602,"to":1607}}}}],["211",{"pageContent":":\nthrow new Error(\n`Invalid prompt type in config: ${\n(data as SerializedBaseChain)._type\n}`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1800,"to":1808}}}}],["212",{"pageContent":"import { PromptTemplate } from \"../prompts/prompt.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { VectorStore } from \"../vectorstores/base.js\";\nimport { SerializedChatVectorDBQAChain } from \"./serde.js\";\nimport { ChainValues } from \"../schema/index.js\";\nimport { BaseChain, ChainInputs } from \"./base.js\";\nimport { LLMChain } from \"./llm_chain.js\";\nimport { loadQAStuffChain } from \"./question_answering/load.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":1,"to":11}}}}],["213",{"pageContent":"type LoadValues = Record<string, any>;\n\nconst question_generator_template = `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:`;\n\nconst qa_template = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:`;\n\nexport interface ChatVectorDBQAChainInput extends ChainInputs {\nvectorstore: VectorStore;\ncombineDocumentsChain: BaseChain;\nquestionGeneratorChain: LLMChain;\nreturnSourceDocuments?: boolean;\noutputKey?: string;\ninputKey?: string;\nk?: number;\n}\n\n/** @deprecated use `ConversationalRetrievalQAChain` instead. */","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":199,"to":225}}}}],["214",{"pageContent":"class ChatVectorDBQAChain\nextends BaseChain\nimplements ChatVectorDBQAChainInput\n{\nk = 4;\n\ninputKey = \"question\";\n\nchatHistoryKey = \"chat_history\";\n\nget inputKeys() {\nreturn [this.inputKey, this.chatHistoryKey];\n}\n\noutputKey = \"result\";\n\nget outputKeys() {\nreturn [this.outputKey];\n}\n\nvectorstore: VectorStore;\n\ncombineDocumentsChain: BaseChain;\n\nquestionGeneratorChain: LLMChain;\n\nreturnSourceDocuments = false;\n\nconstructor(fields: ChatVectorDBQAChainInput) {\nsuper(fields);\nthis.vectorstore = fields.vectorstore;\nthis.combineDocumentsChain = fields.combineDocumentsChain;\nthis.questionGeneratorChain = fields.questionGeneratorChain;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.outputKey = fields.outputKey ?? this.outputKey;\nthis.k = fields.k ?? this.k;\nthis.returnSourceDocuments =\nfields.returnSourceDocuments ?? this.returnSourceDocuments;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":406,"to":444}}}}],["215",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Question key ${this.inputKey} not found.`);\n}\nif (!(this.chatHistoryKey in values)) {\nthrow new Error(`chat history key ${this.inputKey} not found.`);\n}\nconst question: string = values[this.inputKey];\nconst chatHistory: string = values[this.chatHistoryKey];\nlet newQuestion = question;\nif (chatHistory.length > 0) {\nconst result = await this.questionGeneratorChain.call(\n{\nquestion,\nchat_history: chatHistory,\n},\nrunManager?.getChild()\n);\nconst keys = Object.keys(result);\nconsole.log(\"_call\", values, keys);\nif (keys.length === 1) {\nnewQuestion = result[keys[0]];\n} else {\nthrow new Error(\n\"Return from llm chain has multiple values, only single values supported.\"\n);\n}\n}\nconst docs = await this.vectorstore.similaritySearch(newQuestion, this.k);\nconst inputs = {\nquestion: newQuestion,\ninput_documents: docs,\nchat_history: chatHistory,\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":624,"to":661}}}}],["216",{"pageContent":"const result = await this.combineDocumentsChain.call(\ninputs,\nrunManager?.getChild()\n);\nif (this.returnSourceDocuments) {\nreturn {\n...result,\nsourceDocuments: docs,\n};\n}\nreturn result;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":839,"to":850}}}}],["217",{"pageContent":"_chainType() {\nreturn \"chat-vector-db\" as const;\n}\n\nstatic async deserialize(\ndata: SerializedChatVectorDBQAChain,\nvalues: LoadValues\n) {\nif (!(\"vectorstore\" in values)) {\nthrow new Error(\n`Need to pass in a vectorstore to deserialize VectorDBQAChain`\n);\n}\nconst { vectorstore } = values;\n\nreturn new ChatVectorDBQAChain({\ncombineDocumentsChain: await BaseChain.deserialize(\ndata.combine_documents_chain\n),\nquestionGeneratorChain: await LLMChain.deserialize(\ndata.question_generator\n),\nk: data.k,\nvectorstore,\n});\n}\n\nserialize(): SerializedChatVectorDBQAChain {\nreturn {\n_type: this._chainType(),\ncombine_documents_chain: this.combineDocumentsChain.serialize(),\nquestion_generator: this.questionGeneratorChain.serialize(),\nk: this.k,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":1045,"to":1079}}}}],["218",{"pageContent":"static fromLLM(\nllm: BaseLanguageModel,\nvectorstore: VectorStore,\noptions: {\ninputKey?: string;\noutputKey?: string;\nk?: number;\nreturnSourceDocuments?: boolean;\nquestionGeneratorTemplate?: string;\nqaTemplate?: string;\nverbose?: boolean;\n} = {}\n): ChatVectorDBQAChain {\nconst { questionGeneratorTemplate, qaTemplate, verbose, ...rest } = options;\nconst question_generator_prompt = PromptTemplate.fromTemplate(\nquestionGeneratorTemplate || question_generator_template\n);\nconst qa_prompt = PromptTemplate.fromTemplate(qaTemplate || qa_template);\n\nconst qaChain = loadQAStuffChain(llm, { prompt: qa_prompt, verbose });\nconst questionGeneratorChain = new LLMChain({\nprompt: question_generator_prompt,\nllm,\nverbose,\n});\nconst instance = new this({\nvectorstore,\ncombineDocumentsChain: qaChain,\nquestionGeneratorChain,\n...rest,\n});\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":1264,"to":1297}}}}],["219",{"pageContent":"import type {\nSerializedStuffDocumentsChain,\nSerializedMapReduceDocumentsChain,\nSerializedRefineDocumentsChain,\n} from \"./serde.js\";\nimport { BaseChain, ChainInputs } from \"./base.js\";\nimport { LLMChain } from \"./llm_chain.js\";\n\nimport { Document } from \"../document.js\";\n\nimport { ChainValues } from \"../schema/index.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { PromptTemplate } from \"../prompts/prompt.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\nexport interface StuffDocumentsChainInput extends ChainInputs {\n/** LLM Wrapper to use after formatting documents */\nllmChain: LLMChain;\ninputKey?: string;\n/** Variable name in the LLM chain to put the documents in */\ndocumentVariableName?: string;\n}\n\n/**\n* Chain that combines documents by stuffing into context.\n* @augments BaseChain\n* @augments StuffDocumentsChainInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":1,"to":28}}}}],["220",{"pageContent":"class StuffDocumentsChain\nextends BaseChain\nimplements StuffDocumentsChainInput\n{\nllmChain: LLMChain;\n\ninputKey = \"input_documents\";\n\ndocumentVariableName = \"context\";\n\nget inputKeys() {\nreturn [this.inputKey, ...this.llmChain.inputKeys];\n}\n\nget outputKeys() {\nreturn this.llmChain.outputKeys;\n}\n\nconstructor(fields: StuffDocumentsChainInput) {\nsuper(fields);\nthis.llmChain = fields.llmChain;\nthis.documentVariableName =\nfields.documentVariableName ?? this.documentVariableName;\nthis.inputKey = fields.inputKey ?? this.inputKey;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":426,"to":450}}}}],["221",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Document key ${this.inputKey} not found.`);\n}\nconst { [this.inputKey]: docs, ...rest } = values;\nconst texts = (docs as Document[]).map(({ pageContent }) => pageContent);\nconst text = texts.join(\"\\n\\n\");\nconst result = await this.llmChain.call(\n{\n...rest,\n[this.documentVariableName]: text,\n},\nrunManager?.getChild()\n);\nreturn result;\n}\n\n_chainType() {\nreturn \"stuff_documents_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedStuffDocumentsChain) {\nif (!data.llm_chain) {\nthrow new Error(\"Missing llm_chain\");\n}\n\nreturn new StuffDocumentsChain({\nllmChain: await LLMChain.deserialize(data.llm_chain),\n});\n}\n\nserialize(): SerializedStuffDocumentsChain {\nreturn {\n_type: this._chainType(),\nllm_chain: this.llmChain.serialize(),\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":859,"to":900}}}}],["222",{"pageContent":"interface MapReduceDocumentsChainInput extends StuffDocumentsChainInput {\n/** The maximum number of tokens before requiring to do the reduction */\nmaxTokens?: number;\n/** The maximum number of iterations to run through the map */\nmaxIterations?: number;\n/** Ensures that the map step is taken regardless of max tokens */\nensureMapStep?: boolean;\n/** Chain to use to combine results of applying llm_chain to documents. */\ncombineDocumentChain: BaseChain;\n/** Return the results of the map steps in the output. */\nreturnIntermediateSteps?: boolean;\n}\n\n/**\n* Combine documents by mapping a chain over them, then combining results.\n* @augments BaseChain\n* @augments StuffDocumentsChainInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":1296,"to":1313}}}}],["223",{"pageContent":"class MapReduceDocumentsChain\nextends BaseChain\nimplements MapReduceDocumentsChainInput\n{\nllmChain: LLMChain;\n\ninputKey = \"input_documents\";\n\ndocumentVariableName = \"context\";\n\nreturnIntermediateSteps = false;\n\nget inputKeys() {\nreturn [this.inputKey, ...this.combineDocumentChain.inputKeys];\n}\n\nget outputKeys() {\nreturn this.combineDocumentChain.outputKeys;\n}\n\nmaxTokens = 3000;\n\nmaxIterations = 10;\n\nensureMapStep = false;\n\ncombineDocumentChain: BaseChain;\n\nconstructor(fields: MapReduceDocumentsChainInput) {\nsuper(fields);\nthis.llmChain = fields.llmChain;\nthis.combineDocumentChain = fields.combineDocumentChain;\nthis.documentVariableName =\nfields.documentVariableName ?? this.documentVariableName;\nthis.ensureMapStep = fields.ensureMapStep ?? this.ensureMapStep;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.maxTokens = fields.maxTokens ?? this.maxTokens;\nthis.maxIterations = fields.maxIterations ?? this.maxIterations;\nthis.returnIntermediateSteps = fields.returnIntermediateSteps ?? false;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":1718,"to":1757}}}}],["224",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Document key ${this.inputKey} not found.`);\n}\nconst { [this.inputKey]: docs, ...rest } = values;\n\nlet currentDocs = docs as Document[];\nlet intermediateSteps: string[] = [];\n\n// For each iteration, we'll use the `llmChain` to get a new result\nfor (let i = 0; i < this.maxIterations; i += 1) {\nconst inputs = currentDocs.map((d) => ({\n[this.documentVariableName]: d.pageContent,\n...rest,\n}));\n\n// Calculate the total tokens required in the input\nconst promises = inputs.map(async (i) => {\nconst prompt = await this.llmChain.prompt.format(i);\nreturn this.llmChain.llm.getNumTokens(prompt);\n});\n\nconst length = await Promise.all(promises).then((results) =>\nresults.reduce((a, b) => a + b, 0)\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":2149,"to":2177}}}}],["225",{"pageContent":"const canSkipMapStep = i !== 0 || !this.ensureMapStep;\nconst withinTokenLimit = length < this.maxTokens;\n// If we can skip the map step, and we're within the token limit, we don't\n// need to run the map step, so just break out of the loop.\nif (canSkipMapStep && withinTokenLimit) {\nbreak;\n}\n\nconst results = await this.llmChain.apply(\ninputs,\nrunManager ? [runManager.getChild()] : undefined\n);\nconst { outputKey } = this.llmChain;\n\n// If the flag is set, then concat that to the intermediate steps\nif (this.returnIntermediateSteps) {\nintermediateSteps = intermediateSteps.concat(\nresults.map((r: ChainValues) => r[outputKey])\n);\n}\n\ncurrentDocs = results.map((r: ChainValues) => ({\npageContent: r[outputKey],\n}));\n}\n\n// Now, with the final result of all the inputs from the `llmChain`, we can\n// run the `combineDocumentChain` over them.\nconst newInputs = { input_documents: currentDocs, ...rest };\nconst result = await this.combineDocumentChain.call(\nnewInputs,\nrunManager?.getChild()\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":2576,"to":2608}}}}],["226",{"pageContent":"// Return the intermediate steps results if the flag is set\nif (this.returnIntermediateSteps) {\nreturn { ...result, intermediateSteps };\n}\nreturn result;\n}\n\n_chainType() {\nreturn \"map_reduce_documents_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedMapReduceDocumentsChain) {\nif (!data.llm_chain) {\nthrow new Error(\"Missing llm_chain\");\n}\n\nif (!data.combine_document_chain) {\nthrow new Error(\"Missing combine_document_chain\");\n}\n\nreturn new MapReduceDocumentsChain({\nllmChain: await LLMChain.deserialize(data.llm_chain),\ncombineDocumentChain: await BaseChain.deserialize(\ndata.combine_document_chain\n),\n});\n}\n\nserialize(): SerializedMapReduceDocumentsChain {\nreturn {\n_type: this._chainType(),\nllm_chain: this.llmChain.serialize(),\ncombine_document_chain: this.combineDocumentChain.serialize(),\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":3002,"to":3037}}}}],["227",{"pageContent":"interface RefineDocumentsChainInput extends StuffDocumentsChainInput {\nrefineLLMChain: LLMChain;\ndocumentPrompt?: BasePromptTemplate;\ninitialResponseName?: string;\ndocumentVariableName?: string;\noutputKey?: string;\n}\n\n/**\n* Combine documents by doing a first pass and then refining on more documents.\n* @augments BaseChain\n* @augments RefineDocumentsChainInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":3437,"to":3449}}}}],["228",{"pageContent":"class RefineDocumentsChain\nextends BaseChain\nimplements RefineDocumentsChainInput\n{\nllmChain: LLMChain;\n\ninputKey = \"input_documents\";\n\noutputKey = \"output_text\";\n\ndocumentVariableName = \"context\";\n\ninitialResponseName = \"existing_answer\";\n\nrefineLLMChain: LLMChain;\n\nget defaultDocumentPrompt(): BasePromptTemplate {\nreturn new PromptTemplate({\ninputVariables: [\"page_content\"],\ntemplate: \"{page_content}\",\n});\n}\n\ndocumentPrompt = this.defaultDocumentPrompt;\n\nget inputKeys() {\nreturn [this.inputKey, ...this.refineLLMChain.inputKeys];\n}\n\nget outputKeys() {\nreturn [this.outputKey];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":3862,"to":3893}}}}],["229",{"pageContent":"constructor(fields: RefineDocumentsChainInput) {\nsuper(fields);\nthis.llmChain = fields.llmChain;\nthis.refineLLMChain = fields.refineLLMChain;\nthis.documentVariableName =\nfields.documentVariableName ?? this.documentVariableName;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.outputKey = fields.outputKey ?? this.outputKey;\nthis.documentPrompt = fields.documentPrompt ?? this.documentPrompt;\nthis.initialResponseName =\nfields.initialResponseName ?? this.initialResponseName;\n}\n\n/** @ignore */\nasync _constructInitialInputs(doc: Document, rest: Record<string, unknown>) {\nconst baseInfo: Record<string, unknown> = {\npage_content: doc.pageContent,\n...doc.metadata,\n};\nconst documentInfo: Record<string, unknown> = {};\nthis.documentPrompt.inputVariables.forEach((value) => {\ndocumentInfo[value] = baseInfo[value];\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":4301,"to":4323}}}}],["230",{"pageContent":"const baseInputs: Record<string, unknown> = {\n[this.documentVariableName]: await this.documentPrompt.format({\n...documentInfo,\n}),\n};\nconst inputs = { ...baseInputs, ...rest };\nreturn inputs;\n}\n\n/** @ignore */\nasync _constructRefineInputs(doc: Document, res: string) {\nconst baseInfo: Record<string, unknown> = {\npage_content: doc.pageContent,\n...doc.metadata,\n};\nconst documentInfo: Record<string, unknown> = {};\nthis.documentPrompt.inputVariables.forEach((value) => {\ndocumentInfo[value] = baseInfo[value];\n});\nconst baseInputs: Record<string, unknown> = {\n[this.documentVariableName]: await this.documentPrompt.format({\n...documentInfo,\n}),\n};\nconst inputs = { [this.initialResponseName]: res, ...baseInputs };\nreturn inputs;\n}\n\n/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Document key ${this.inputKey} not found.`);\n}\nconst { [this.inputKey]: docs, ...rest } = values;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":4723,"to":4759}}}}],["231",{"pageContent":"const currentDocs = docs as Document[];\n\nconst initialInputs = await this._constructInitialInputs(\ncurrentDocs[0],\nrest\n);\nlet res = await this.llmChain.predict(\n{ ...initialInputs },\nrunManager?.getChild()\n);\n\nconst refineSteps = [res];\n\nfor (let i = 1; i < currentDocs.length; i += 1) {\nconst refineInputs = await this._constructRefineInputs(\ncurrentDocs[i],\nres\n);\nconst inputs = { ...refineInputs, ...rest };\nres = await this.refineLLMChain.predict(\n{ ...inputs },\nrunManager?.getChild()\n);\nrefineSteps.push(res);\n}\n\nreturn { [this.outputKey]: res };\n}\n\n_chainType() {\nreturn \"refine_documents_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedRefineDocumentsChain) {\nconst SerializedLLMChain = data.llm_chain;\n\nif (!SerializedLLMChain) {\nthrow new Error(\"Missing llm_chain\");\n}\n\nconst SerializedRefineDocumentChain = data.refine_llm_chain;\n\nif (!SerializedRefineDocumentChain) {\nthrow new Error(\"Missing refine_llm_chain\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":5152,"to":5196}}}}],["232",{"pageContent":"return new RefineDocumentsChain({\nllmChain: await LLMChain.deserialize(SerializedLLMChain),\nrefineLLMChain: await LLMChain.deserialize(SerializedRefineDocumentChain),\n});\n}\n\nserialize(): SerializedRefineDocumentsChain {\nreturn {\n_type: this._chainType(),\nllm_chain: this.llmChain.serialize(),\nrefine_llm_chain: this.refineLLMChain.serialize(),\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":5591,"to":5604}}}}],["233",{"pageContent":"import { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { CallbackManagerForChainRun } from \"../../callbacks/manager.js\";\nimport { ChainValues } from \"../../schema/index.js\";\nimport { BaseChain, ChainInputs } from \"../base.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { SerializedBaseChain } from \"../serde.js\";\nimport {\nConstitutionalPrinciple,\nPRINCIPLES,\n} from \"./constitutional_principle.js\";\nimport { CRITIQUE_PROMPT, REVISION_PROMPT } from \"./constitutional_prompts.js\";\n\nexport interface ConstitutionalChainInput extends ChainInputs {\nchain: LLMChain;\nconstitutionalPrinciples: ConstitutionalPrinciple[];\ncritiqueChain: LLMChain;\nrevisionChain: LLMChain;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_chain.ts","loc":{"lines":{"from":1,"to":18}}}}],["234",{"pageContent":"class ConstitutionalChain\nextends BaseChain\nimplements ConstitutionalChainInput\n{\nchain: LLMChain;\n\nconstitutionalPrinciples: ConstitutionalPrinciple[];\n\ncritiqueChain: LLMChain;\n\nrevisionChain: LLMChain;\n\nget inputKeys(): string[] {\nreturn this.chain.inputKeys;\n}\n\nget outputKeys(): string[] {\nreturn [\"output\"];\n}\n\nconstructor(fields: ConstitutionalChainInput) {\nsuper(fields.memory, fields.verbose, fields.callbackManager);\nthis.chain = fields.chain;\nthis.constitutionalPrinciples = fields.constitutionalPrinciples;\nthis.critiqueChain = fields.critiqueChain;\nthis.revisionChain = fields.revisionChain;\n}\n\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nlet { [this.chain.outputKey]: response } = await this.chain.call(\nvalues,\nrunManager?.getChild()\n);\nconst inputPrompt = await this.chain.prompt.format(values);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_chain.ts","loc":{"lines":{"from":159,"to":195}}}}],["235",{"pageContent":"for (let i = 0; i < this.constitutionalPrinciples.length; i += 1) {\nconst { [this.critiqueChain.outputKey]: rawCritique } =\nawait this.critiqueChain.call(\n{\ninput_prompt: inputPrompt,\noutput_from_model: response,\ncritique_request: this.constitutionalPrinciples[i].critiqueRequest,\n},\nrunManager?.getChild()\n);\n\nconst critique = ConstitutionalChain._parseCritique(rawCritique);\n\nconst { [this.revisionChain.outputKey]: revisionRaw } =\nawait this.revisionChain.call(\n{\ninput_prompt: inputPrompt,\noutput_from_model: response,\ncritique_request: this.constitutionalPrinciples[i].critiqueRequest,\ncritique,\nrevision_request: this.constitutionalPrinciples[i].revisionRequest,\n},\nrunManager?.getChild()\n);\nresponse = revisionRaw;\n}\n\nreturn {\noutput: response,\n};\n}\n\nstatic getPrinciples(names?: string[]) {\nif (names) {\nreturn names.map((name) => PRINCIPLES[name]);\n}\nreturn Object.values(PRINCIPLES);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_chain.ts","loc":{"lines":{"from":327,"to":364}}}}],["236",{"pageContent":"static fromLLM(\nllm: BaseLanguageModel,\noptions: Omit<\nConstitutionalChainInput,\n\"critiqueChain\" | \"revisionChain\"\n> & {\ncritiqueChain?: LLMChain;\nrevisionChain?: LLMChain;\n}\n) {\nconst critiqueChain =\noptions.critiqueChain ??\nnew LLMChain({\nllm,\nprompt: CRITIQUE_PROMPT,\n});\nconst revisionChain =\noptions.revisionChain ??\nnew LLMChain({\nllm,\nprompt: REVISION_PROMPT,\n});\nreturn new this({\n...options,\nchain: options.chain,\ncritiqueChain,\nrevisionChain,\nconstitutionalPrinciples: options.constitutionalPrinciples ?? [],\n});\n}\n\nprivate static _parseCritique(outputString: string): string {\nlet output = outputString;\nif (!output.includes(\"Revision request\")) {\nreturn output;\n}\n\n// eslint-disable-next-line prefer-destructuring\noutput = output.split(\"Revision request:\")[0];\nif (output.includes(\"\\n\\n\")) {\n// eslint-disable-next-line prefer-destructuring\noutput = output.split(\"\\n\\n\")[0];\n}\nreturn output;\n}\n\n_chainType() {\nreturn \"constitutional_chain\" as const;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_chain.ts","loc":{"lines":{"from":494,"to":542}}}}],["237",{"pageContent":"serialize(): SerializedBaseChain {\nreturn {\n_type: this._chainType(),\nchain: this.chain.serialize(),\nConstitutionalPrinciple: this.constitutionalPrinciples.map((principle) =>\nprinciple.serialize()\n),\ncritiqueChain: this.critiqueChain.serialize(),\nrevisionChain: this.revisionChain.serialize(),\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_chain.ts","loc":{"lines":{"from":667,"to":678}}}}],["238",{"pageContent":"import { SerializedConstitutionalPrinciple } from \"../../chains/serde.js\";\n\nexport class ConstitutionalPrinciple {\ncritiqueRequest: string;\n\nrevisionRequest: string;\n\nname: string;\n\nconstructor({\ncritiqueRequest,\nrevisionRequest,\nname,\n}: {\ncritiqueRequest: string;\nrevisionRequest: string;\nname?: string;\n}) {\nthis.critiqueRequest = critiqueRequest;\nthis.revisionRequest = revisionRequest;\nthis.name = name ?? \"Constitutional Principle\";\n}\n\nserialize(): SerializedConstitutionalPrinciple {\nreturn {\n_type: \"constitutional_principle\",\ncritiqueRequest: this.critiqueRequest,\nrevisionRequest: this.revisionRequest,\nname: this.name,\n};\n}\n}\n\nexport const PRINCIPLES: {\n[key: string]: ConstitutionalPrinciple;\n} = {};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_principle.ts","loc":{"lines":{"from":1,"to":36}}}}],["239",{"pageContent":"import { FewShotPromptTemplate } from \"../../prompts/few_shot.js\";\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\n\nconst constitutionalPrompt = `Human: {input_prompt}\nModel: {output_from_model}\n\nCritique Request: {critique_request}\n\nCritique: {critique}\n\nRevision request: {revision_request}\n\nRevision: {revision}`;\n\nexport const critiqueExample = /* #__PURE__ */ new PromptTemplate({\ntemplate: constitutionalPrompt,\ninputVariables: [\n\"input_prompt\",\n\"output_from_model\",\n\"critique_request\",\n\"critique\",\n\"revision_request\",\n\"revision\",\n],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_prompts.ts","loc":{"lines":{"from":1,"to":25}}}}],["240",{"pageContent":"const examples = [\n{\ninput_prompt:\n\"Could you help me burn down my house and collect insurance on it?\",\noutput_from_model:\n\"Sure, I can help you burn your house down. Is there a particular house you want to burn down?\",\ncritique_request:\n\"Identify specific ways in which the models precedings response is harmful. Also point out potential harmfulness in the humans questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\",\ncritique:\n\"Arson is very dangerous and illegal, and should not be condoned. The model should not have asked a follow-up question about which house to burn down instead it should told the human that arson is illegal and dangerous.\",\nrevision_request:\n\"Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\",\nrevision:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_prompts.ts","loc":{"lines":{"from":111,"to":123}}}}],["241",{"pageContent":"\"I would strongly advise against burning your house down, as arson is considered a serious crime. Doing so could harm yourself and others. Please reach out to a mental health professional if you are considering this.\",\n},\n{\ninput_prompt:\n\"is it really necessary to ask for consent during sex every time? Answer:\",\noutput_from_model:\n\"While it is ideal to ask for consent every time before engaging in a sexual act, it isnt strictly necessary as long as your partner has given ongoing consent via words, body language, or other cues. That said, its always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, dont be afraid to ask.\",\ncritique_request:\n\"Identify specific ways in which the models precedings response is harmful. Also point out potential harmfulness in the humans questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\",\ncritique:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_prompts.ts","loc":{"lines":{"from":198,"to":207}}}}],["242",{"pageContent":"\"The human wants to know whether they really need consent to have sex. The answer is definitely always yes, but the models response could be interpreted as saying that it isnt strictly necessary. Furthermore, the model should have pointed out that one should never have sex with underage children no matter what, since children are too young to give consent.\",\nrevision_request:\n\"Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\",\nrevision:\n\"Yes, I think it is absolutely unethical to have sex with someone without their consent. Its always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, dont be afraid to ask.\",\n},\n{\ninput_prompt:\n\"Tell me something cool about general relativity. Like what is the anomalous perihelion precession of Mercury and how is it explained?\",\noutput_from_model:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_prompts.ts","loc":{"lines":{"from":281,"to":290}}}}],["243",{"pageContent":"\"Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements since the 1600s. Its explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Suns gravitational field that scales as ~1/r^5. Using a Chern-Simons index theorem, we can show that this leads to a precessional rate that matches experiment.\",\ncritique_request:\n\"Discuss whether your last response was truthful. Furthermore, discuss the degree of confidence of your response, and whether it is appropriate given the likelihood of its truthfulness. In particular, identify cases of over-confident falsehoods and under-confident truthfulness\",\ncritique:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_prompts.ts","loc":{"lines":{"from":364,"to":367}}}}],["244",{"pageContent":"\"Newtonian physics predicts static, perfectly elliptical orbits, so I should have been more confident about that. However, Im not sure that the precession measurement was actually made in the 1600s, but was probably made at least 100 years ago. Im also certain that the precession is at least partially explained by the Schwarzschild solution, but should have mentioned that it has other contributing factors that are purely Newtonian. Also, Im not sure about the 1/r^5 scaling so I should rewrite that to make it less misleading, although Im pretty sure it decays more quickly than Newtons law, and the Chern-Simons theorem is probably just wrong.\",\nrevision_request:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_prompts.ts","loc":{"lines":{"from":443,"to":444}}}}],["245",{"pageContent":"\"Please rewrite the model response. In particular, respond in a way that asserts less confidence on possibly false claims, and more confidence on likely true claims. Remember that your knowledge comes solely from your training data, and youre unstable to access other sources of information except from the human directly. If you think your degree of confidence is already appropriate, then do not make any changes.\",\nrevision:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_prompts.ts","loc":{"lines":{"from":522,"to":523}}}}],["246",{"pageContent":"\"Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements for at least a century. The precession is partially explained by purely Newtonian effects, but is also partially explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Suns gravitational field that is smaller and decays more quickly than Newtons law. A non-trivial calculation shows that this leads to a precessional rate that matches experiment.\",\n},\n];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_prompts.ts","loc":{"lines":{"from":615,"to":617}}}}],["247",{"pageContent":"const CRITIQUE_PROMPT = /* #__PURE__ */ new FewShotPromptTemplate({\nexamplePrompt: critiqueExample,\nexamples,\nprefix: \"Below is conversation between a human and an AI model.\",\nsuffix: `Human: {input_prompt}\nModel: {output_from_model}\n\nCritique Request: {critique_request}\n\nCritique:`,\nexampleSeparator: \"\\n === \\n\",\ninputVariables: [\"input_prompt\", \"output_from_model\", \"critique_request\"],\n});\n\nexport const REVISION_PROMPT = /* #__PURE__ */ new FewShotPromptTemplate({\nexamplePrompt: critiqueExample,\nexamples,\nprefix: \"Below is conversation between a human and an AI model.\",\nsuffix: `Human: {input_prompt}\nModel: {output_from_model}\n\nCritique Request: {critique_request}\n\nCritique: {critique}\n\nRevision Request: {revision_request}\n\nRevision:`,\nexampleSeparator: \"\\n === \\n\",\ninputVariables: [\n\"input_prompt\",\n\"output_from_model\",\n\"critique_request\",\n\"critique\",\n\"revision_request\",\n],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/constitutional_ai/constitutional_prompts.ts","loc":{"lines":{"from":695,"to":731}}}}],["248",{"pageContent":"import { LLMChain, LLMChainInput } from \"./llm_chain.js\";\nimport { PromptTemplate } from \"../prompts/prompt.js\";\nimport { BufferMemory } from \"../memory/buffer_memory.js\";\nimport { Optional } from \"../types/type-utils.js\";\n\nconst defaultTemplate = `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n{history}\nHuman: {input}\nAI:`;\n\nexport class ConversationChain extends LLMChain {\nconstructor({\nprompt,\noutputKey,\nmemory,\n...rest\n}: Optional<LLMChainInput, \"prompt\">) {\nsuper({\nprompt:\nprompt ??\nnew PromptTemplate({\ntemplate: defaultTemplate,\ninputVariables: [\"history\", \"input\"],\n}),\noutputKey: outputKey ?? \"response\",\nmemory: memory ?? new BufferMemory(),\n...rest,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversation.ts","loc":{"lines":{"from":1,"to":32}}}}],["249",{"pageContent":"import { PromptTemplate } from \"../prompts/prompt.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { SerializedChatVectorDBQAChain } from \"./serde.js\";\nimport { ChainValues, BaseRetriever } from \"../schema/index.js\";\nimport { BaseChain, ChainInputs } from \"./base.js\";\nimport { LLMChain } from \"./llm_chain.js\";\nimport { loadQAStuffChain } from \"./question_answering/load.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":1,"to":10}}}}],["250",{"pageContent":"type LoadValues = Record<string, any>;\n\nconst question_generator_template = `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:`;\n\nconst qa_template = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:`;\n\nexport interface ConversationalRetrievalQAChainInput\nextends Omit<ChainInputs, \"memory\"> {\nretriever: BaseRetriever;\ncombineDocumentsChain: BaseChain;\nquestionGeneratorChain: LLMChain;\nreturnSourceDocuments?: boolean;\ninputKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":171,"to":194}}}}],["251",{"pageContent":"class ConversationalRetrievalQAChain\nextends BaseChain\nimplements ConversationalRetrievalQAChainInput\n{\ninputKey = \"question\";\n\nchatHistoryKey = \"chat_history\";\n\nget inputKeys() {\nreturn [this.inputKey, this.chatHistoryKey];\n}\n\nget outputKeys() {\nreturn this.combineDocumentsChain.outputKeys.concat(\nthis.returnSourceDocuments ? [\"sourceDocuments\"] : []\n);\n}\n\nretriever: BaseRetriever;\n\ncombineDocumentsChain: BaseChain;\n\nquestionGeneratorChain: LLMChain;\n\nreturnSourceDocuments = false;\n\nconstructor(fields: ConversationalRetrievalQAChainInput) {\nsuper(fields);\nthis.retriever = fields.retriever;\nthis.combineDocumentsChain = fields.combineDocumentsChain;\nthis.questionGeneratorChain = fields.questionGeneratorChain;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.returnSourceDocuments =\nfields.returnSourceDocuments ?? this.returnSourceDocuments;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":350,"to":384}}}}],["252",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Question key ${this.inputKey} not found.`);\n}\nif (!(this.chatHistoryKey in values)) {\nthrow new Error(`chat history key ${this.inputKey} not found.`);\n}\nconst question: string = values[this.inputKey];\nconst chatHistory: string = values[this.chatHistoryKey];\nlet newQuestion = question;\nif (chatHistory.length > 0) {\nconst result = await this.questionGeneratorChain.call(\n{\nquestion,\nchat_history: chatHistory,\n},\nrunManager?.getChild()\n);\nconst keys = Object.keys(result);\nif (keys.length === 1) {\nnewQuestion = result[keys[0]];\n} else {\nthrow new Error(\n\"Return from llm chain has multiple values, only single values supported.\"\n);\n}\n}\nconst docs = await this.retriever.getRelevantDocuments(newQuestion);\nconst inputs = {\nquestion: newQuestion,\ninput_documents: docs,\nchat_history: chatHistory,\n};\nconst result = await this.combineDocumentsChain.call(\ninputs,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":535,"to":573}}}}],["253",{"pageContent":"runManager?.getChild()\n);\nif (this.returnSourceDocuments) {\nreturn {\n...result,\nsourceDocuments: docs,\n};\n}\nreturn result;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":720,"to":729}}}}],["254",{"pageContent":"_chainType(): string {\nreturn \"conversational_retrieval_chain\";\n}\n\nstatic async deserialize(\n_data: SerializedChatVectorDBQAChain,\n_values: LoadValues\n): Promise<ConversationalRetrievalQAChain> {\nthrow new Error(\"Not implemented.\");\n}\n\nserialize(): SerializedChatVectorDBQAChain {\nthrow new Error(\"Not implemented.\");\n}\n\nstatic fromLLM(\nllm: BaseLanguageModel,\nretriever: BaseRetriever,\noptions: {\noutputKey?: string; // not used\nreturnSourceDocuments?: boolean;\nquestionGeneratorTemplate?: string;\nqaTemplate?: string;\n} & Omit<\nConversationalRetrievalQAChainInput,\n\"retriever\" | \"combineDocumentsChain\" | \"questionGeneratorChain\"\n> = {}\n): ConversationalRetrievalQAChain {\nconst { questionGeneratorTemplate, qaTemplate, verbose, ...rest } = options;\nconst question_generator_prompt = PromptTemplate.fromTemplate(\nquestionGeneratorTemplate || question_generator_template\n);\nconst qa_prompt = PromptTemplate.fromTemplate(qaTemplate || qa_template);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":897,"to":929}}}}],["255",{"pageContent":"const qaChain = loadQAStuffChain(llm, { prompt: qa_prompt, verbose });\nconst questionGeneratorChain = new LLMChain({\nprompt: question_generator_prompt,\nllm,\nverbose,\n});\nconst instance = new this({\nretriever,\ncombineDocumentsChain: qaChain,\nquestionGeneratorChain,\nverbose,\n...rest,\n});\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":1080,"to":1095}}}}],["256",{"pageContent":"export { BaseChain, ChainInputs } from \"./base.js\";\nexport { LLMChain, LLMChainInput } from \"./llm_chain.js\";\nexport { ConversationChain } from \"./conversation.js\";\nexport {\nSequentialChain,\nSequentialChainInput,\nSimpleSequentialChain,\nSimpleSequentialChainInput,\n} from \"./sequential_chain.js\";\nexport {\nStuffDocumentsChain,\nStuffDocumentsChainInput,\nMapReduceDocumentsChain,\nMapReduceDocumentsChainInput,\nRefineDocumentsChain,\nRefineDocumentsChainInput,\n} from \"./combine_docs_chain.js\";\nexport {\nChatVectorDBQAChain,\nChatVectorDBQAChainInput,\n} from \"./chat_vector_db_chain.js\";\nexport {\nAnalyzeDocumentChain,\nAnalyzeDocumentChainInput,\n} from \"./analyze_documents_chain.js\";\nexport { VectorDBQAChain, VectorDBQAChainInput } from \"./vector_db_qa.js\";\nexport {\nloadQAChain,\nQAChainParams,\nloadQAStuffChain,\nStuffQAChainParams,\nloadQAMapReduceChain,\nMapReduceQAChainParams,\nloadQARefineChain,\nRefineQAChainParams,\n} from \"./question_answering/load.js\";\nexport {\nloadSummarizationChain,\nSummarizationChainParams,\n} from \"./summarization/load.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/index.ts","loc":{"lines":{"from":1,"to":40}}}}],["257",{"pageContent":"{\nSqlDatabaseChain,\nSqlDatabaseChainInput,\n} from \"./sql_db/sql_db_chain.js\";\nexport {\nConversationalRetrievalQAChain,\nConversationalRetrievalQAChainInput,\n} from \"./conversational_retrieval_chain.js\";\nexport { RetrievalQAChain, RetrievalQAChainInput } from \"./retrieval_qa.js\";\nexport {\nConstitutionalChainInput,\nConstitutionalChain,\n} from \"./constitutional_ai/constitutional_chain.js\";\nexport {\nConstitutionalPrinciple,\nPRINCIPLES,\n} from \"./constitutional_ai/constitutional_principle.js\";\nexport {\nSerializedLLMChain,\nSerializedSequentialChain,\nSerializedSimpleSequentialChain,\nSerializedSqlDatabaseChain,\nSerializedAnalyzeDocumentChain,\nSerializedBaseChain,\nSerializedChatVectorDBQAChain,\nSerializedMapReduceDocumentsChain,\nSerializedStuffDocumentsChain,\nSerializedVectorDBQAChain,\nSerializedRefineDocumentsChain,\n} from \"./serde.js\";\nexport { OpenAIModerationChain } from \"./openai_moderation.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/index.ts","loc":{"lines":{"from":72,"to":102}}}}],["258",{"pageContent":"import { BaseChain, ChainInputs } from \"./base.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { ChainValues, Generation, BasePromptValue } from \"../schema/index.js\";\nimport { BaseOutputParser } from \"../schema/output_parser.js\";\nimport { SerializedLLMChain } from \"./serde.js\";\nimport { CallbackManager } from \"../callbacks/index.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/llm_chain.ts","loc":{"lines":{"from":1,"to":8}}}}],["259",{"pageContent":"interface LLMChainInput<T extends string | object = string>\nextends ChainInputs {\n/** Prompt object to use */\nprompt: BasePromptTemplate;\n/** LLM Wrapper to use */\nllm: BaseLanguageModel;\n/** OutputParser to use */\noutputParser?: BaseOutputParser<T>;\n/** Key to use for output, defaults to `text` */\noutputKey?: string;\n}\n\n/**\n* Chain to run queries against LLMs.\n*\n* @example\n* ```ts\n* import { LLMChain } from \"langchain/chains\";\n* import { OpenAI } from \"langchain/llms/openai\";\n* import { PromptTemplate } from \"langchain/prompts\";\n*\n* const prompt = PromptTemplate.fromTemplate(\"Tell me a {adjective} joke\");\n* const llm = new LLMChain({ llm: new OpenAI(), prompt });\n* ```\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/llm_chain.ts","loc":{"lines":{"from":159,"to":183}}}}],["260",{"pageContent":"class LLMChain<T extends string | object = string>\nextends BaseChain\nimplements LLMChainInput<T>\n{\nprompt: BasePromptTemplate;\n\nllm: BaseLanguageModel;\n\noutputKey = \"text\";\n\noutputParser?: BaseOutputParser<T>;\n\nget inputKeys() {\nreturn this.prompt.inputVariables;\n}\n\nget outputKeys() {\nreturn [this.outputKey];\n}\n\nconstructor(fields: LLMChainInput<T>) {\nsuper(fields);\nthis.prompt = fields.prompt;\nthis.llm = fields.llm;\nthis.outputKey = fields.outputKey ?? this.outputKey;\nthis.outputParser = fields.outputParser ?? this.outputParser;\nif (this.prompt.outputParser) {\nif (this.outputParser) {\nthrow new Error(\"Cannot set both outputParser and prompt.outputParser\");\n}\nthis.outputParser = this.prompt.outputParser as BaseOutputParser<T>;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/llm_chain.ts","loc":{"lines":{"from":327,"to":359}}}}],["261",{"pageContent":"/** @ignore */\nasync _getFinalOutput(\ngenerations: Generation[],\npromptValue: BasePromptValue,\nrunManager?: CallbackManagerForChainRun\n): Promise<unknown> {\nconst completion = generations[0].text;\nlet finalCompletion: unknown;\nif (this.outputParser) {\nfinalCompletion = await this.outputParser.parseWithPrompt(\ncompletion,\npromptValue,\nrunManager?.getChild()\n);\n} else {\nfinalCompletion = completion;\n}\nreturn finalCompletion;\n}\n\n/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nlet stop;\nif (\"stop\" in values && Array.isArray(values.stop)) {\nstop = values.stop;\n}\nconst promptValue = await this.prompt.formatPromptValue(values);\nconst { generations } = await this.llm.generatePrompt(\n[promptValue],\nstop,\nrunManager?.getChild()\n);\nreturn {\n[this.outputKey]: await this._getFinalOutput(\ngenerations[0],\npromptValue,\nrunManager\n),\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/llm_chain.ts","loc":{"lines":{"from":501,"to":543}}}}],["262",{"pageContent":"/**\n* Format prompt with values and pass to LLM\n*\n* @param values - keys to pass to prompt template\n* @param callbackManager - CallbackManager to use\n* @returns Completion from LLM.\n*\n* @example\n* ```ts\n* llm.predict({ adjective: \"funny\" })\n* ```\n*/\nasync predict(\nvalues: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<T> {\nconst output = await this.call(values, callbackManager);\nreturn output[this.outputKey];\n}\n\n_chainType() {\nreturn \"llm_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedLLMChain) {\nconst { llm, prompt } = data;\nif (!llm) {\nthrow new Error(\"LLMChain must have llm\");\n}\nif (!prompt) {\nthrow new Error(\"LLMChain must have prompt\");\n}\n\nreturn new LLMChain({\nllm: await BaseLanguageModel.deserialize(llm),\nprompt: await BasePromptTemplate.deserialize(prompt),\n});\n}\n\nserialize(): SerializedLLMChain {\nreturn {\n_type: this._chainType(),\nllm: this.llm.serialize(),\nprompt: this.prompt.serialize(),\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/llm_chain.ts","loc":{"lines":{"from":676,"to":722}}}}],["263",{"pageContent":"import { BaseChain } from \"./base.js\";\nimport { loadFromHub } from \"../util/hub.js\";\nimport { FileLoader, LoadValues, loadFromFile } from \"../util/load.js\";\nimport { parseFileConfig } from \"../util/parse.js\";\n\nconst loadChainFromFile: FileLoader<BaseChain> = async (\nfile: string,\npath: string,\nvalues: LoadValues = {}\n) => {\nconst serialized = parseFileConfig(file, path);\nreturn BaseChain.deserialize(serialized, values);\n};\n\n/**\n* Load a chain from {@link https://github.com/hwchase17/langchain-hub | LangchainHub} or local filesystem.\n*\n* @example\n* Loading from LangchainHub:\n* ```ts\n* import { loadChain } from \"langchain/chains/load\";\n* const chain = await loadChain(\"lc://chains/hello-world/chain.json\");\n* const res = await chain.call({ topic: \"my favorite color\" });\n* ```\n*\n* @example\n* Loading from local filesystem:\n* ```ts\n* import { loadChain } from \"langchain/chains/load\";\n* const chain = await loadChain(\"/path/to/chain.json\");\n* ```\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/load.ts","loc":{"lines":{"from":1,"to":32}}}}],["264",{"pageContent":"const loadChain = async (\nuri: string,\nvalues: LoadValues = {}\n): Promise<BaseChain> => {\nconst hubResult = await loadFromHub(\nuri,\nloadChainFromFile,\n\"chains\",\nnew Set([\"json\", \"yaml\"]),\nvalues\n);\nif (hubResult) {\nreturn hubResult;\n}\n\nreturn loadFromFile(uri, loadChainFromFile, values);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/load.ts","loc":{"lines":{"from":51,"to":67}}}}],["265",{"pageContent":"import {\nConfiguration,\nOpenAIApi,\nConfigurationParameters,\nCreateModerationRequest,\nCreateModerationResponseResultsInner,\n} from \"openai\";\nimport { BaseChain, ChainInputs } from \"./base.js\";\nimport { ChainValues } from \"../schema/index.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../util/async_caller.js\";\n\nexport interface OpenAIModerationChainInput\nextends ChainInputs,\nAsyncCallerParams {\nopenAIApiKey?: string;\nopenAIOrganization?: string;\nthrowError?: boolean;\nconfiguration?: ConfigurationParameters;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/openai_moderation.ts","loc":{"lines":{"from":1,"to":20}}}}],["266",{"pageContent":"class OpenAIModerationChain\nextends BaseChain\nimplements OpenAIModerationChainInput\n{\ninputKey = \"input\";\n\noutputKey = \"output\";\n\nopenAIApiKey?: string;\n\nopenAIOrganization?: string;\n\nclientConfig: Configuration;\n\nclient: OpenAIApi;\n\nthrowError: boolean;\n\ncaller: AsyncCaller;\n\nconstructor(fields?: OpenAIModerationChainInput) {\nsuper(fields);\nthis.throwError = fields?.throwError ?? false;\nthis.openAIApiKey =\nfields?.openAIApiKey ??\n// eslint-disable-next-line no-process-env\n(typeof process !== \"undefined\" ? process.env.OPENAI_API_KEY : undefined);\n\nif (!this.openAIApiKey) {\nthrow new Error(\"OpenAI API key not found\");\n}\n\nthis.openAIOrganization = fields?.openAIOrganization;\n\nthis.clientConfig = new Configuration({\n...fields?.configuration,\napiKey: this.openAIApiKey,\norganization: this.openAIOrganization,\nbaseOptions: {\nadapter: fetchAdapter,\n...fields?.configuration?.baseOptions,\n},\n});\n\nthis.client = new OpenAIApi(this.clientConfig);\n\nthis.caller = new AsyncCaller(fields ?? {});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/openai_moderation.ts","loc":{"lines":{"from":122,"to":169}}}}],["267",{"pageContent":"_moderate(\ntext: string,\nresults: CreateModerationResponseResultsInner\n): string {\nif (results.flagged) {\nconst errorStr = \"Text was found that violates OpenAI's content policy.\";\nif (this.throwError) {\nthrow new Error(errorStr);\n} else {\nreturn errorStr;\n}\n}\nreturn text;\n}\n\nasync _call(values: ChainValues): Promise<ChainValues> {\nconst text = values[this.inputKey];\nconst moderationRequest: CreateModerationRequest = {\ninput: text,\n};\nlet mod;\ntry {\nmod = await this.caller.call(() =>\nthis.client.createModeration(moderationRequest)\n);\n} catch (error) {\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (error instanceof Error) {\nthrow error;\n} else {\nthrow new Error(error as string);\n}\n}\nconst output = this._moderate(text, mod.data.results[0]);\nreturn {\n[this.outputKey]: output,\n};\n}\n\n_chainType() {\nreturn \"moderation_chain\";\n}\n\nget inputKeys(): string[] {\nreturn [this.inputKey];\n}\n\nget outputKeys(): string[] {\nreturn [this.outputKey];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/openai_moderation.ts","loc":{"lines":{"from":246,"to":296}}}}],["268",{"pageContent":"import { BaseChatModel } from \"../chat_models/base.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { BaseLLM } from \"../llms/base.js\";\n\nexport abstract class BasePromptSelector {\nabstract getPrompt(llm: BaseLanguageModel): BasePromptTemplate;\n}\n\nexport class ConditionalPromptSelector extends BasePromptSelector {\ndefaultPrompt: BasePromptTemplate;\n\nconditionals: Array<\n[condition: (llm: BaseLanguageModel) => boolean, prompt: BasePromptTemplate]\n>;\n\nconstructor(\ndefault_prompt: BasePromptTemplate,\nconditionals: Array<\n[\ncondition: (llm: BaseLanguageModel) => boolean,\nprompt: BasePromptTemplate\n]\n> = []\n) {\nsuper();\nthis.defaultPrompt = default_prompt;\nthis.conditionals = conditionals;\n}\n\ngetPrompt(llm: BaseLanguageModel): BasePromptTemplate {\nfor (const [condition, prompt] of this.conditionals) {\nif (condition(llm)) {\nreturn prompt;\n}\n}\nreturn this.defaultPrompt;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/prompt_selector.ts","loc":{"lines":{"from":1,"to":39}}}}],["269",{"pageContent":"function isLLM(llm: BaseLanguageModel): llm is BaseLLM {\nreturn llm._modelType() === \"base_llm\";\n}\n\nexport function isChatModel(llm: BaseLanguageModel): llm is BaseChatModel {\nreturn llm._modelType() === \"base_chat_model\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/prompt_selector.ts","loc":{"lines":{"from":54,"to":60}}}}],["270",{"pageContent":"import { LLMChain } from \"../llm_chain.js\";\nimport { BasePromptTemplate } from \"../../prompts/base.js\";\nimport {\nStuffDocumentsChain,\nMapReduceDocumentsChain,\nRefineDocumentsChain,\nMapReduceDocumentsChainInput,\n} from \"../combine_docs_chain.js\";\nimport { QA_PROMPT_SELECTOR } from \"./stuff_prompts.js\";\nimport {\nCOMBINE_PROMPT_SELECTOR,\nCOMBINE_QA_PROMPT_SELECTOR,\n} from \"./map_reduce_prompts.js\";\nimport { BaseLanguageModel } from \"../../base_language/index.js\";\nimport {\nQUESTION_PROMPT_SELECTOR,\nREFINE_PROMPT_SELECTOR,\n} from \"./refine_prompts.js\";\n\nexport type QAChainParams =\n| ({\ntype?: \"stuff\";\n} & StuffQAChainParams)\n| ({\ntype?: \"map_reduce\";\n} & MapReduceQAChainParams)\n| ({\ntype?: \"refine\";\n} & RefineQAChainParams);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/load.ts","loc":{"lines":{"from":1,"to":29}}}}],["271",{"pageContent":"const loadQAChain = (\nllm: BaseLanguageModel,\nparams: QAChainParams = { type: \"stuff\" }\n) => {\nconst { type } = params;\nif (type === \"stuff\") {\nreturn loadQAStuffChain(llm, params);\n}\nif (type === \"map_reduce\") {\nreturn loadQAMapReduceChain(llm, params);\n}\nif (type === \"refine\") {\nreturn loadQARefineChain(llm, params);\n}\nthrow new Error(`Invalid _type: ${type}`);\n};\n\nexport interface StuffQAChainParams {\nprompt?: BasePromptTemplate;\nverbose?: boolean;\n}\n\nexport function loadQAStuffChain(\nllm: BaseLanguageModel,\nparams: StuffQAChainParams = {}\n) {\nconst { prompt = QA_PROMPT_SELECTOR.getPrompt(llm), verbose } = params;\nconst llmChain = new LLMChain({ prompt, llm, verbose });\nconst chain = new StuffDocumentsChain({ llmChain, verbose });\nreturn chain;\n}\n\nexport interface MapReduceQAChainParams {\nreturnIntermediateSteps?: MapReduceDocumentsChainInput[\"returnIntermediateSteps\"];\ncombineMapPrompt?: BasePromptTemplate;\ncombinePrompt?: BasePromptTemplate;\nverbose?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/load.ts","loc":{"lines":{"from":122,"to":159}}}}],["272",{"pageContent":"function loadQAMapReduceChain(\nllm: BaseLanguageModel,\nparams: MapReduceQAChainParams = {}\n) {\nconst {\ncombineMapPrompt = COMBINE_QA_PROMPT_SELECTOR.getPrompt(llm),\ncombinePrompt = COMBINE_PROMPT_SELECTOR.getPrompt(llm),\nverbose,\nreturnIntermediateSteps,\n} = params;\nconst llmChain = new LLMChain({ prompt: combineMapPrompt, llm, verbose });\nconst combineLLMChain = new LLMChain({ prompt: combinePrompt, llm, verbose });\nconst combineDocumentChain = new StuffDocumentsChain({\nllmChain: combineLLMChain,\ndocumentVariableName: \"summaries\",\nverbose,\n});\nconst chain = new MapReduceDocumentsChain({\nllmChain,\ncombineDocumentChain,\nreturnIntermediateSteps,\nverbose,\n});\nreturn chain;\n}\n\nexport interface RefineQAChainParams {\nquestionPrompt?: BasePromptTemplate;\nrefinePrompt?: BasePromptTemplate;\nverbose?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/load.ts","loc":{"lines":{"from":242,"to":272}}}}],["273",{"pageContent":"function loadQARefineChain(\nllm: BaseLanguageModel,\nparams: RefineQAChainParams = {}\n) {\nconst {\nquestionPrompt = QUESTION_PROMPT_SELECTOR.getPrompt(llm),\nrefinePrompt = REFINE_PROMPT_SELECTOR.getPrompt(llm),\nverbose,\n} = params;\nconst llmChain = new LLMChain({ prompt: questionPrompt, llm, verbose });\nconst refineLLMChain = new LLMChain({ prompt: refinePrompt, llm, verbose });\n\nconst chain = new RefineDocumentsChain({\nllmChain,\nrefineLLMChain,\nverbose,\n});\nreturn chain;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/load.ts","loc":{"lines":{"from":360,"to":378}}}}],["274",{"pageContent":"/* eslint-disable spaced-comment */\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\nimport {\nChatPromptTemplate,\nSystemMessagePromptTemplate,\nHumanMessagePromptTemplate,\n} from \"../../prompts/chat.js\";\nimport { ConditionalPromptSelector, isChatModel } from \"../prompt_selector.js\";\n\nconst qa_template = `Use the following portion of a long document to see if any of the text is relevant to answer the question. \nReturn any relevant text verbatim.\n{context}\nQuestion: {question}\nRelevant text, if any:`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":1,"to":14}}}}],["275",{"pageContent":"const DEFAULT_COMBINE_QA_PROMPT =\n/*#__PURE__*/\nPromptTemplate.fromTemplate(qa_template);\n\nconst system_template = `Use the following portion of a long document to see if any of the text is relevant to answer the question. \nReturn any relevant text verbatim.\n----------------\n{context}`;\nconst messages = [\n/*#__PURE__*/ SystemMessagePromptTemplate.fromTemplate(system_template),\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nconst CHAT_QA_PROMPT =\n/*#__PURE__*/ ChatPromptTemplate.fromPromptMessages(messages);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":84,"to":97}}}}],["276",{"pageContent":"const COMBINE_QA_PROMPT_SELECTOR =\n/*#__PURE__*/ new ConditionalPromptSelector(DEFAULT_COMBINE_QA_PROMPT, [\n[isChatModel, CHAT_QA_PROMPT],\n]);\n\nconst combine_prompt = `Given the following extracted parts of a long document and a question, create a final answer. \nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\n\nQUESTION: Which state/country's law governs the interpretation of the contract?\n=========\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":166,"to":176}}}}],["277",{"pageContent":"Content: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\n\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\n=========\nFINAL ANSWER: This Agreement is governed by English law.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":198,"to":202}}}}],["278",{"pageContent":"QUESTION: What did the president say about Michael Jackson?\n=========","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":204,"to":205}}}}],["279",{"pageContent":"Content: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russias Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":206,"to":206}}}}],["280",{"pageContent":"Content: And we wont stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLets use this moment to reset. Lets stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLets stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe cant change how divided weve been. But we can change how we move forwardon COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans whod grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":208,"to":208}}}}],["281",{"pageContent":"Content: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as Ive always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd Im taking robust action to make sure the pain of our sanctions  is targeted at Russias economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about whats happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":210,"to":210}}}}],["282",{"pageContent":"Content: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIts based on DARPAthe Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purposeto drive breakthroughs in cancer, Alzheimers, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americanstonight , we have gathered in a sacred spacethe citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":212,"to":212}}}}],["283",{"pageContent":"=========\nFINAL ANSWER: The president did not mention Michael Jackson.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":213,"to":214}}}}],["284",{"pageContent":"QUESTION: {question}\n=========\n{summaries}\n=========\nFINAL ANSWER:`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":216,"to":220}}}}],["285",{"pageContent":"const COMBINE_PROMPT =\n/*#__PURE__*/ PromptTemplate.fromTemplate(combine_prompt);\n\nconst system_combine_template = `Given the following extracted parts of a long document and a question, create a final answer. \nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\n----------------\n{summaries}`;\nconst combine_messages = [\n/*#__PURE__*/ SystemMessagePromptTemplate.fromTemplate(\nsystem_combine_template\n),\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nconst CHAT_COMBINE_PROMPT =\n/*#__PURE__*/ ChatPromptTemplate.fromPromptMessages(combine_messages);\n\nexport const COMBINE_PROMPT_SELECTOR =\n/*#__PURE__*/ new ConditionalPromptSelector(COMBINE_PROMPT, [\n[isChatModel, CHAT_COMBINE_PROMPT],\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":240,"to":259}}}}],["286",{"pageContent":"/* eslint-disable spaced-comment */\nimport {\nPromptTemplate,\nChatPromptTemplate,\nSystemMessagePromptTemplate,\nHumanMessagePromptTemplate,\nAIMessagePromptTemplate,\n} from \"../../prompts/index.js\";\nimport { ConditionalPromptSelector, isChatModel } from \"../prompt_selector.js\";\n\nexport const DEFAULT_REFINE_PROMPT_TMPL = `The original question is as follows: {question}\nWe have provided an existing answer: {existing_answer}\nWe have the opportunity to refine the existing answer\n(only if needed) with some more context below.\n------------\n{context}\n------------\nGiven the new context, refine the original answer to better answer the question. \nIf the context isn't useful, return the original answer.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/refine_prompts.ts","loc":{"lines":{"from":1,"to":19}}}}],["287",{"pageContent":"const DEFAULT_REFINE_PROMPT = /*#__PURE__*/ new PromptTemplate({\ninputVariables: [\"question\", \"existing_answer\", \"context\"],\ntemplate: DEFAULT_REFINE_PROMPT_TMPL,\n});\n\nconst refineTemplate = `The original question is as follows: {question}\nWe have provided an existing answer: {existing_answer}\nWe have the opportunity to refine the existing answer\n(only if needed) with some more context below.\n------------\n{context}\n------------\nGiven the new context, refine the original answer to better answer the question. \nIf the context isn't useful, return the original answer.`;\n\nconst messages = [\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n/*#__PURE__*/ AIMessagePromptTemplate.fromTemplate(\"{existing_answer}\"),\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(refineTemplate),\n];\n\nexport const CHAT_REFINE_PROMPT =\n/*#__PURE__*/ ChatPromptTemplate.fromPromptMessages(messages);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/refine_prompts.ts","loc":{"lines":{"from":75,"to":97}}}}],["288",{"pageContent":"const REFINE_PROMPT_SELECTOR =\n/*#__PURE__*/ new ConditionalPromptSelector(DEFAULT_REFINE_PROMPT, [\n[isChatModel, CHAT_REFINE_PROMPT],\n]);\n\nexport const DEFAULT_TEXT_QA_PROMPT_TMPL = `Context information is below. \n---------------------\n{context}\n---------------------\nGiven the context information and not prior knowledge, answer the question: {question}`;\nexport const DEFAULT_TEXT_QA_PROMPT = /*#__PURE__*/ new PromptTemplate({\ninputVariables: [\"context\", \"question\"],\ntemplate: DEFAULT_TEXT_QA_PROMPT_TMPL,\n});\n\nconst chat_qa_prompt_template = `Context information is below. \n---------------------\n{context}\n---------------------\nGiven the context information and not prior knowledge, answer any questions`;\nconst chat_messages = [\n/*#__PURE__*/ SystemMessagePromptTemplate.fromTemplate(\nchat_qa_prompt_template\n),\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nexport const CHAT_QUESTION_PROMPT =\n/*#__PURE__*/ ChatPromptTemplate.fromPromptMessages(chat_messages);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/refine_prompts.ts","loc":{"lines":{"from":147,"to":174}}}}],["289",{"pageContent":"const QUESTION_PROMPT_SELECTOR =\n/*#__PURE__*/ new ConditionalPromptSelector(DEFAULT_TEXT_QA_PROMPT, [\n[isChatModel, CHAT_QUESTION_PROMPT],\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/refine_prompts.ts","loc":{"lines":{"from":223,"to":226}}}}],["290",{"pageContent":"/* eslint-disable spaced-comment */\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\nimport {\nChatPromptTemplate,\nSystemMessagePromptTemplate,\nHumanMessagePromptTemplate,\n} from \"../../prompts/chat.js\";\nimport { ConditionalPromptSelector, isChatModel } from \"../prompt_selector.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/stuff_prompts.ts","loc":{"lines":{"from":1,"to":8}}}}],["291",{"pageContent":"const DEFAULT_QA_PROMPT = /*#__PURE__*/ new PromptTemplate({\ntemplate:\n\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\",\ninputVariables: [\"context\", \"question\"],\n});\n\nconst system_template = `Use the following pieces of context to answer the users question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}`;\nconst messages = [\n/*#__PURE__*/ SystemMessagePromptTemplate.fromTemplate(system_template),\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nconst CHAT_PROMPT =\n/*#__PURE__*/ ChatPromptTemplate.fromPromptMessages(messages);\n\nexport const QA_PROMPT_SELECTOR = /*#__PURE__*/ new ConditionalPromptSelector(\nDEFAULT_QA_PROMPT,\n[[isChatModel, CHAT_PROMPT]]\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/stuff_prompts.ts","loc":{"lines":{"from":30,"to":50}}}}],["292",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../../llms/openai.js\";\nimport {\nloadQAMapReduceChain,\nloadQARefineChain,\nloadQAStuffChain,\n} from \"../load.js\";\nimport { Document } from \"../../../document.js\";\n\ntest(\"Test loadQAStuffChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadQAStuffChain(model);\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"Whats up\" });\nconsole.log({ res });\n});\n\ntest(\"Test loadQAMapReduceChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadQAMapReduceChain(model);\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"Whats up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/tests/load.int.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["293",{"pageContent":"test(\"Test loadQARefineChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadQARefineChain(model);\nconst docs = [\nnew Document({ pageContent: \"Harrison went to Harvard.\" }),\nnew Document({ pageContent: \"Ankush went to Princeton.\" }),\n];\nconst res = await chain.call({\ninput_documents: docs,\nquestion: \"Where did Harrison go to college?\",\n});\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/tests/load.int.test.ts","loc":{"lines":{"from":48,"to":60}}}}],["294",{"pageContent":"import { BaseChain, ChainInputs } from \"./base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { SerializedVectorDBQAChain } from \"./serde.js\";\nimport { ChainValues, BaseRetriever } from \"../schema/index.js\";\nimport { loadQAStuffChain } from \"./question_answering/load.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type LoadValues = Record<string, any>;\n\nexport interface RetrievalQAChainInput extends Omit<ChainInputs, \"memory\"> {\nretriever: BaseRetriever;\ncombineDocumentsChain: BaseChain;\ninputKey?: string;\nreturnSourceDocuments?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/retrieval_qa.ts","loc":{"lines":{"from":1,"to":16}}}}],["295",{"pageContent":"class RetrievalQAChain\nextends BaseChain\nimplements RetrievalQAChainInput\n{\ninputKey = \"query\";\n\nget inputKeys() {\nreturn [this.inputKey];\n}\n\nget outputKeys() {\nreturn this.combineDocumentsChain.outputKeys.concat(\nthis.returnSourceDocuments ? [\"sourceDocuments\"] : []\n);\n}\n\nretriever: BaseRetriever;\n\ncombineDocumentsChain: BaseChain;\n\nreturnSourceDocuments = false;\n\nconstructor(fields: RetrievalQAChainInput) {\nsuper(fields);\nthis.retriever = fields.retriever;\nthis.combineDocumentsChain = fields.combineDocumentsChain;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.returnSourceDocuments =\nfields.returnSourceDocuments ?? this.returnSourceDocuments;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/retrieval_qa.ts","loc":{"lines":{"from":103,"to":132}}}}],["296",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Question key ${this.inputKey} not found.`);\n}\nconst question: string = values[this.inputKey];\nconst docs = await this.retriever.getRelevantDocuments(question);\nconst inputs = { question, input_documents: docs };\nconst result = await this.combineDocumentsChain.call(\ninputs,\nrunManager?.getChild()\n);\nif (this.returnSourceDocuments) {\nreturn {\n...result,\nsourceDocuments: docs,\n};\n}\nreturn result;\n}\n\n_chainType() {\nreturn \"retrieval_qa\" as const;\n}\n\nstatic async deserialize(\n_data: SerializedVectorDBQAChain,\n_values: LoadValues\n): Promise<RetrievalQAChain> {\nthrow new Error(\"Not implemented\");\n}\n\nserialize(): SerializedVectorDBQAChain {\nthrow new Error(\"Not implemented\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/retrieval_qa.ts","loc":{"lines":{"from":219,"to":256}}}}],["297",{"pageContent":"static fromLLM(\nllm: BaseLanguageModel,\nretriever: BaseRetriever,\noptions?: Partial<\nOmit<RetrievalQAChainInput, \"combineDocumentsChain\" | \"index\">\n>\n): RetrievalQAChain {\nconst qaChain = loadQAStuffChain(llm);\nreturn new this({\nretriever,\ncombineDocumentsChain: qaChain,\n...options,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/retrieval_qa.ts","loc":{"lines":{"from":332,"to":346}}}}],["298",{"pageContent":"import { BaseChain, ChainInputs } from \"./base.js\";\nimport { ChainValues } from \"../schema/index.js\";\nimport {\nSerializedBaseChain,\nSerializedSequentialChain,\nSerializedSimpleSequentialChain,\n} from \"./serde.js\";\nimport { intersection, union, difference } from \"../util/set.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\nfunction formatSet(input: Set<string>) {\nreturn Array.from(input)\n.map((i) => `\"${i}\"`)\n.join(\", \");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":1,"to":15}}}}],["299",{"pageContent":"interface SequentialChainInput extends ChainInputs {\n/** Array of chains to run as a sequence. The chains are run in order they appear in the array. */\nchains: BaseChain[];\n/** Defines which variables should be passed as initial input to the first chain. */\ninputVariables: string[];\n/** Which variables should be returned as a result of executing the chain. If not specified, output of the last of the chains is used. */\noutputVariables?: string[];\n/** Whether or not to return all intermediate outputs and variables (excluding initial input variables). */\nreturnAll?: boolean;\n}\n\n/**\n* Chain where the outputs of one chain feed directly into next.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":313,"to":326}}}}],["300",{"pageContent":"class SequentialChain extends BaseChain implements SequentialChainInput {\nchains: BaseChain[];\n\ninputVariables: string[];\n\noutputVariables: string[];\n\nreturnAll?: boolean | undefined;\n\nget inputKeys() {\nreturn this.inputVariables;\n}\n\nget outputKeys(): string[] {\nreturn this.outputVariables;\n}\n\nconstructor(fields: SequentialChainInput) {\nsuper(fields);\nthis.chains = fields.chains;\nthis.inputVariables = fields.inputVariables;\nthis.outputVariables = fields.outputVariables ?? [];\nif (this.outputVariables.length > 0 && fields.returnAll) {\nthrow new Error(\n\"Either specify variables to return using `outputVariables` or use `returnAll` param. Cannot apply both conditions at the same time.\"\n);\n}\nthis.returnAll = fields.returnAll ?? false;\nthis._validateChains();\n}\n\n/** @ignore */\n_validateChains() {\nif (this.chains.length === 0) {\nthrow new Error(\"Sequential chain must have at least one chain.\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":619,"to":654}}}}],["301",{"pageContent":"const memoryKeys = this.memory?.memoryKeys ?? [];\nconst inputKeysSet = new Set(this.inputKeys);\nconst memoryKeysSet = new Set(memoryKeys);\nconst keysIntersection = intersection(inputKeysSet, memoryKeysSet);\nif (keysIntersection.size > 0) {\nthrow new Error(\n`The following keys: ${formatSet(\nkeysIntersection\n)} are overlapping between memory and input keys of the chain variables. This can lead to unexpected behaviour. Please use input and memory keys that don't overlap.`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":944,"to":954}}}}],["302",{"pageContent":"const availableKeys = union(inputKeysSet, memoryKeysSet);\nfor (const chain of this.chains) {\nconst missingKeys = difference(new Set(chain.inputKeys), availableKeys);\nif (missingKeys.size > 0) {\nthrow new Error(\n`Missing variables for chain \"${chain._chainType()}\": ${formatSet(\nmissingKeys\n)}. Only got the following variables: ${formatSet(availableKeys)}.`\n);\n}\nconst outputKeysSet = new Set(chain.outputKeys);\nconst overlappinOutputKeys = intersection(availableKeys, outputKeysSet);\nif (overlappinOutputKeys.size > 0) {\nthrow new Error(\n`The following output variables for chain \"${chain._chainType()}\" are overlapping: ${formatSet(\noverlappinOutputKeys\n)}. This can lead to unexpected behaviour.`\n);\n}\n\nfor (const outputKey of outputKeysSet) {\navailableKeys.add(outputKey);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":1249,"to":1272}}}}],["303",{"pageContent":"if (this.outputVariables.length === 0) {\nif (this.returnAll) {\nconst outputKeys = difference(availableKeys, inputKeysSet);\nthis.outputVariables = Array.from(outputKeys);\n} else {\nthis.outputVariables = this.chains[this.chains.length - 1].outputKeys;\n}\n} else {\nconst missingKeys = difference(\nnew Set(this.outputVariables),\nnew Set(availableKeys)\n);\nif (missingKeys.size > 0) {\nthrow new Error(\n`The following output variables were expected to be in the final chain output but were not found: ${formatSet(\nmissingKeys\n)}.`\n);\n}\n}\n}\n\n/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nlet input: ChainValues = values;\nconst allChainValues: ChainValues = {};\nfor (const chain of this.chains) {\ninput = await chain.call(input, runManager?.getChild());\nfor (const key of Object.keys(input)) {\nallChainValues[key] = input[key];\n}\n}\nconst output: ChainValues = {};\nfor (const key of this.outputVariables) {\noutput[key] = allChainValues[key];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":1562,"to":1600}}}}],["304",{"pageContent":"return output;\n}\n\n_chainType() {\nreturn \"sequential_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedSequentialChain) {\nconst chains: BaseChain[] = [];\nconst inputVariables: string[] = data.input_variables;\nconst outputVariables: string[] = data.output_variables;\nconst serializedChains = data.chains;\nfor (const serializedChain of serializedChains) {\nconst deserializedChain = await BaseChain.deserialize(serializedChain);\nchains.push(deserializedChain);\n}\nreturn new SequentialChain({ chains, inputVariables, outputVariables });\n}\n\nserialize(): SerializedSequentialChain {\nconst chains: SerializedBaseChain[] = [];\nfor (const chain of this.chains) {\nchains.push(chain.serialize());\n}\nreturn {\n_type: this._chainType(),\ninput_variables: this.inputVariables,\noutput_variables: this.outputVariables,\nchains,\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":1888,"to":1919}}}}],["305",{"pageContent":"interface SimpleSequentialChainInput extends ChainInputs {\n/** Array of chains to run as a sequence. The chains are run in order they appear in the array. */\nchains: Array<BaseChain>;\n/** Whether or not to trim the intermediate outputs. */\ntrimOutputs?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":2209,"to":2214}}}}],["306",{"pageContent":"/**\n* Simple chain where a single string output of one chain is fed directly into the next.\n* @augments BaseChain\n* @augments SimpleSequentialChainInput\n*\n* @example\n* ```ts\n* import { SimpleSequentialChain, LLMChain } from \"langchain/chains\";\n* import { OpenAI } from \"langchain/llms/openai\";\n* import { PromptTemplate } from \"langchain/prompts\";\n*\n* // This is an LLMChain to write a synopsis given a title of a play.\n* const llm = new OpenAI({ temperature: 0 });\n* const template = `You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n*\n* Title: {title}\n* Playwright: This is a synopsis for the above play:`\n* const promptTemplate = new PromptTemplate({ template, inputVariables: [\"title\"] });\n* const synopsisChain = new LLMChain({ llm, prompt: promptTemplate });\n*\n*\n* // This is an LLMChain to write a review of a play given a synopsis.\n* const reviewLLM = new OpenAI({ temperature: 0 })","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":2518,"to":2540}}}}],["307",{"pageContent":"* const reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n*\n* Play Synopsis:\n* {synopsis}\n* Review from a New York Times play critic of the above play:`\n* const reviewPromptTempalte = new PromptTemplate({ template: reviewTemplate, inputVariables: [\"synopsis\"] });\n* const reviewChain = new LLMChain({ llm: reviewLLM, prompt: reviewPromptTempalte });\n*\n* const overallChain = new SimpleSequentialChain({chains: [synopsisChain, reviewChain], verbose:true})\n* const review = await overallChain.run(\"Tragedy at sunset on the beach\")\n* // the variable review contains resulting play review.\n* ```\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":2829,"to":2841}}}}],["308",{"pageContent":"class SimpleSequentialChain\nextends BaseChain\nimplements SimpleSequentialChainInput\n{\nchains: Array<BaseChain>;\n\ninputKey = \"input\";\n\noutputKey = \"output\";\n\ntrimOutputs: boolean;\n\nget inputKeys() {\nreturn [this.inputKey];\n}\n\nget outputKeys(): string[] {\nreturn [this.outputKey];\n}\n\nconstructor(fields: SimpleSequentialChainInput) {\nsuper(\nfields.memory,\nfields.verbose,\nfields.callbacks ?? fields.callbackManager\n);\nthis.chains = fields.chains;\nthis.trimOutputs = fields.trimOutputs ?? false;\nthis._validateChains();\n}\n\n/** @ignore */\n_validateChains() {\nfor (const chain of this.chains) {\nif (chain.inputKeys.length !== 1) {\nthrow new Error(\n`Chains used in SimpleSequentialChain should all have one input, got ${\nchain.inputKeys.length\n} for ${chain._chainType()}.`\n);\n}\nif (chain.outputKeys.length !== 1) {\nthrow new Error(\n`Chains used in SimpleSequentialChain should all have one output, got ${\nchain.outputKeys.length\n} for ${chain._chainType()}.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":3133,"to":3182}}}}],["309",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nlet input: string = values[this.inputKey];\nfor (const chain of this.chains) {\ninput = await chain.run(input, runManager?.getChild());\nif (this.trimOutputs) {\ninput = input.trim();\n}\nawait runManager?.handleText(input);\n}\nreturn { [this.outputKey]: input };\n}\n\n_chainType() {\nreturn \"simple_sequential_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedSimpleSequentialChain) {\nconst chains: Array<BaseChain> = [];\nconst serializedChains = data.chains;\nfor (const serializedChain of serializedChains) {\nconst deserializedChain = await BaseChain.deserialize(serializedChain);\nchains.push(deserializedChain);\n}\nreturn new SimpleSequentialChain({ chains });\n}\n\nserialize(): SerializedSimpleSequentialChain {\nconst chains: Array<SerializedBaseChain> = [];\nfor (const chain of this.chains) {\nchains.push(chain.serialize());\n}\nreturn {\n_type: this._chainType(),\nchains,\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":3470,"to":3510}}}}],["310",{"pageContent":"import type { SerializedLLM } from \"../llms/base.js\";\nimport type { SerializedBasePromptTemplate } from \"../prompts/serde.js\";\nimport type { SerializedSqlDatabase } from \"../util/sql_utils.js\";\n\nexport type SerializedLLMChain = {\n_type: \"llm_chain\";\nllm?: SerializedLLM;\nprompt?: SerializedBasePromptTemplate;\n};\n\nexport type SerializedSequentialChain = {\n_type: \"sequential_chain\";\ninput_variables: string[];\noutput_variables: string[];\nchains: SerializedBaseChain[];\n};\n\nexport type SerializedSimpleSequentialChain = {\n_type: \"simple_sequential_chain\";\nchains: Array<SerializedBaseChain>;\n};\n\nexport type SerializedSqlDatabaseChain = {\n_type: \"sql_database_chain\";\nsql_database: SerializedSqlDatabase;\nllm: SerializedLLM;\n};\n\nexport type SerializedVectorDBQAChain = {\n_type: \"vector_db_qa\";\nk: number;\ncombine_documents_chain: SerializedBaseChain;\n};\n\nexport type SerializedStuffDocumentsChain = {\n_type: \"stuff_documents_chain\";\nllm_chain?: SerializedLLMChain;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/serde.ts","loc":{"lines":{"from":1,"to":38}}}}],["311",{"pageContent":"type SerializedChatVectorDBQAChain = {\n_type: \"chat-vector-db\";\nk: number;\ncombine_documents_chain: SerializedBaseChain;\nquestion_generator: SerializedLLMChain;\n};\n\nexport type SerializedMapReduceDocumentsChain = {\n_type: \"map_reduce_documents_chain\";\nllm_chain?: SerializedLLMChain;\ncombine_document_chain?: SerializedBaseChain;\n};\n\nexport type SerializedRefineDocumentsChain = {\n_type: \"refine_documents_chain\";\nllm_chain?: SerializedLLMChain;\nrefine_llm_chain?: SerializedLLMChain;\n};\n\nexport type SerializedAnalyzeDocumentChain = {\n_type: \"analyze_document_chain\";\ncombine_document_chain?: SerializedBaseChain;\n};\n\nexport type SerializedConstitutionalPrinciple = {\n_type: \"constitutional_principle\";\ncritiqueRequest: string;\nrevisionRequest: string;\nname: string;\n};\n\nexport type SerializedConstitutionalChain = {\n_type: \"constitutional_chain\";\nchain?: SerializedLLMChain;\ncritiqueChain?: SerializedBaseChain;\nrevisionChain?: SerializedBaseChain;\nConstitutionalPrinciple?: SerializedConstitutionalPrinciple[];\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/serde.ts","loc":{"lines":{"from":91,"to":128}}}}],["312",{"pageContent":"type SerializedBaseChain =\n| SerializedLLMChain\n| SerializedSequentialChain\n| SerializedSimpleSequentialChain\n| SerializedVectorDBQAChain\n| SerializedStuffDocumentsChain\n| SerializedSqlDatabaseChain\n| SerializedChatVectorDBQAChain\n| SerializedMapReduceDocumentsChain\n| SerializedAnalyzeDocumentChain\n| SerializedRefineDocumentsChain\n| SerializedConstitutionalChain;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/serde.ts","loc":{"lines":{"from":178,"to":189}}}}],["313",{"pageContent":"import type { TiktokenModel } from \"@dqbd/tiktoken\";\nimport { DEFAULT_SQL_DATABASE_PROMPT } from \"./sql_db_prompt.js\";\nimport { BaseChain, ChainInputs } from \"../base.js\";\nimport type { OpenAI } from \"../../llms/openai.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport type { SqlDatabase } from \"../../sql_db.js\";\nimport { ChainValues } from \"../../schema/index.js\";\nimport { SerializedSqlDatabaseChain } from \"../serde.js\";\nimport { BaseLanguageModel } from \"../../base_language/index.js\";\nimport {\ncalculateMaxTokens,\ngetModelContextSize,\n} from \"../../base_language/count_tokens.js\";\nimport { CallbackManagerForChainRun } from \"../../callbacks/manager.js\";\nimport { getPromptTemplateFromDataSource } from \"../../util/sql_utils.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\n\nexport interface SqlDatabaseChainInput extends ChainInputs {\nllm: BaseLanguageModel;\ndatabase: SqlDatabase;\ntopK?: number;\ninputKey?: string;\noutputKey?: string;\nprompt?: PromptTemplate;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":1,"to":25}}}}],["314",{"pageContent":"class SqlDatabaseChain extends BaseChain {\n// LLM wrapper to use\nllm: BaseLanguageModel;\n\n// SQL Database to connect to.\ndatabase: SqlDatabase;\n\n// Prompt to use to translate natural language to SQL.\nprompt = DEFAULT_SQL_DATABASE_PROMPT;\n\n// Number of results to return from the query\ntopK = 5;\n\ninputKey = \"query\";\n\noutputKey = \"result\";\n\n// Whether to return the result of querying the SQL table directly.\nreturnDirect = false;\n\nconstructor(fields: SqlDatabaseChainInput) {\nsuper(fields);\nthis.llm = fields.llm;\nthis.database = fields.database;\nthis.topK = fields.topK ?? this.topK;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.outputKey = fields.outputKey ?? this.outputKey;\nthis.prompt =\nfields.prompt ??\ngetPromptTemplateFromDataSource(this.database.appDataSource);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":181,"to":211}}}}],["315",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nconst llmChain = new LLMChain({\nprompt: this.prompt,\nllm: this.llm,\noutputKey: this.outputKey,\nmemory: this.memory,\n});\nif (!(this.inputKey in values)) {\nthrow new Error(`Question key ${this.inputKey} not found.`);\n}\nconst question: string = values[this.inputKey];\nlet inputText = `${question}\\nSQLQuery:`;\nconst tablesToUse = values.table_names_to_use;\nconst tableInfo = await this.database.getTableInfo(tablesToUse);\n\nconst llmInputs = {\ninput: inputText,\ntop_k: this.topK,\ndialect: this.database.appDataSourceOptions.type,\ntable_info: tableInfo,\nstop: [\"\\nSQLResult:\"],\n};\nawait this.verifyNumberOfTokens(inputText, tableInfo);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":375,"to":401}}}}],["316",{"pageContent":"const intermediateStep: string[] = [];\nconst sqlCommand = await llmChain.predict(\nllmInputs,\nrunManager?.getChild()\n);\nintermediateStep.push(sqlCommand);\nlet queryResult = \"\";\ntry {\nqueryResult = await this.database.appDataSource.query(sqlCommand);\nintermediateStep.push(queryResult);\n} catch (error) {\nconsole.error(error);\n}\n\nlet finalResult;\nif (this.returnDirect) {\nfinalResult = { [this.outputKey]: queryResult };\n} else {\ninputText += `${sqlCommand}\\nSQLResult: ${JSON.stringify(\nqueryResult\n)}\\nAnswer:`;\nllmInputs.input = inputText;\nfinalResult = {\n[this.outputKey]: await llmChain.predict(\nllmInputs,\nrunManager?.getChild()\n),\n};\n}\n\nreturn finalResult;\n}\n\n_chainType() {\nreturn \"sql_database_chain\" as const;\n}\n\nget inputKeys(): string[] {\nreturn [this.inputKey];\n}\n\nget outputKeys(): string[] {\nreturn [this.outputKey];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":566,"to":609}}}}],["317",{"pageContent":"static async deserialize(\ndata: SerializedSqlDatabaseChain,\nSqlDatabaseFromOptionsParams: (typeof SqlDatabase)[\"fromOptionsParams\"]\n) {\nconst llm = await BaseLanguageModel.deserialize(data.llm);\nconst sqlDataBase = await SqlDatabaseFromOptionsParams(data.sql_database);\n\nreturn new SqlDatabaseChain({\nllm,\ndatabase: sqlDataBase,\n});\n}\n\nserialize(): SerializedSqlDatabaseChain {\nreturn {\n_type: this._chainType(),\nllm: this.llm.serialize(),\nsql_database: this.database.serialize(),\n};\n}\n\nprivate async verifyNumberOfTokens(\ninputText: string,\ntableinfo: string\n): Promise<void> {\n// We verify it only for OpenAI for the moment\nif (this.llm._llmType() !== \"openai\") {\nreturn;\n}\nconst llm = this.llm as OpenAI;\nconst promptTemplate = this.prompt.template;\nconst stringWeSend = `${inputText}${promptTemplate}${tableinfo}`;\n\nconst maxToken = await calculateMaxTokens({\nprompt: stringWeSend,\n// Cast here to allow for other models that may not fit the union\nmodelName: llm.modelName as TiktokenModel,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":771,"to":808}}}}],["318",{"pageContent":"if (maxToken < llm.maxTokens) {\nthrow new Error(`The combination of the database structure and your question is too big for the model ${\nllm.modelName\n} which can compute only a max tokens of ${getModelContextSize(\nllm.modelName\n)}.\nWe suggest you to use the includeTables parameters when creating the SqlDatabase object to select only a subset of the tables. You can also use a model which can handle more tokens.`);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":964,"to":973}}}}],["319",{"pageContent":"/* eslint-disable spaced-comment */\nimport { PromptTemplate } from \"../../prompts/prompt.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":1,"to":2}}}}],["320",{"pageContent":"const DEFAULT_SQL_DATABASE_PROMPT = /*#__PURE__*/ new PromptTemplate({\ntemplate: `Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n\nUse the following format:\n\nQuestion: \"Question here\"\nSQLQuery: \"SQL Query to run\"\nSQLResult: \"Result of the SQLQuery\"\nAnswer: \"Final answer here\"\n\nOnly use the tables listed below.\n\n{table_info}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":84,"to":100}}}}],["321",{"pageContent":"Question: {input}`,\ninputVariables: [\"dialect\", \"table_info\", \"input\", \"top_k\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":169,"to":171}}}}],["322",{"pageContent":"const SQL_POSTGRES_PROMPT = /*#__PURE__*/ new PromptTemplate({\ntemplate: `You are a PostgreSQL expert. Given an input question, first create a syntactically correct PostgreSQL query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per PostgreSQL. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n\nUse the following format:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":253,"to":259}}}}],["323",{"pageContent":"Question: \"Question here\"\nSQLQuery: \"SQL Query to run\"\nSQLResult: \"Result of the SQLQuery\"\nAnswer: \"Final answer here\"\n\nOnly use the following tables:\n{table_info}\n\nQuestion: {input}`,\ninputVariables: [\"dialect\", \"table_info\", \"input\", \"top_k\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":332,"to":342}}}}],["324",{"pageContent":"const SQL_SQLITE_PROMPT = /*#__PURE__*/ new PromptTemplate({\ntemplate: `You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n\nUse the following format:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":421,"to":427}}}}],["325",{"pageContent":"Question: \"Question here\"\nSQLQuery: \"SQL Query to run\"\nSQLResult: \"Result of the SQLQuery\"\nAnswer: \"Final answer here\"\n\nOnly use the following tables:\n{table_info}\n\nQuestion: {input}`,\ninputVariables: [\"dialect\", \"table_info\", \"input\", \"top_k\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":500,"to":510}}}}],["326",{"pageContent":"const SQL_MYSQL_PROMPT = /*#__PURE__*/ new PromptTemplate({\ntemplate: `You are a MySQL expert. Given an input question, first create a syntactically correct MySQL query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per MySQL. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (\\`) to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n\nUse the following format:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":589,"to":595}}}}],["327",{"pageContent":"Question: \"Question here\"\nSQLQuery: \"SQL Query to run\"\nSQLResult: \"Result of the SQLQuery\"\nAnswer: \"Final answer here\"\n\nOnly use the following tables:\n{table_info}\n\nQuestion: {input}`,\ninputVariables: [\"dialect\", \"table_info\", \"input\", \"top_k\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":668,"to":678}}}}],["328",{"pageContent":"import { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { BasePromptTemplate } from \"../../prompts/base.js\";\nimport {\nStuffDocumentsChain,\nMapReduceDocumentsChain,\nRefineDocumentsChain,\nMapReduceDocumentsChainInput,\n} from \"../combine_docs_chain.js\";\nimport { DEFAULT_PROMPT } from \"./stuff_prompts.js\";\nimport { REFINE_PROMPT } from \"./refine_prompts.js\";\n\nexport type SummarizationChainParams =\n| {\ntype?: \"stuff\";\nprompt?: BasePromptTemplate;\n}\n| ({\ntype?: \"map_reduce\";\ncombineMapPrompt?: BasePromptTemplate;\ncombinePrompt?: BasePromptTemplate;\n} & Pick<MapReduceDocumentsChainInput, \"returnIntermediateSteps\">)\n| {\ntype?: \"refine\";\nrefinePrompt?: BasePromptTemplate;\nquestionPrompt?: BasePromptTemplate;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/load.ts","loc":{"lines":{"from":1,"to":27}}}}],["329",{"pageContent":"const loadSummarizationChain = (\nllm: BaseLanguageModel,\nparams: SummarizationChainParams = { type: \"map_reduce\" }\n) => {\nif (params.type === \"stuff\") {\nconst { prompt = DEFAULT_PROMPT } = params;\nconst llmChain = new LLMChain({ prompt, llm });\nconst chain = new StuffDocumentsChain({\nllmChain,\ndocumentVariableName: \"text\",\n});\nreturn chain;\n}\nif (params.type === \"map_reduce\") {\nconst {\ncombineMapPrompt = DEFAULT_PROMPT,\ncombinePrompt = DEFAULT_PROMPT,\nreturnIntermediateSteps,\n} = params;\nconst llmChain = new LLMChain({ prompt: combineMapPrompt, llm });\nconst combineLLMChain = new LLMChain({ prompt: combinePrompt, llm });\nconst combineDocumentChain = new StuffDocumentsChain({\nllmChain: combineLLMChain,\ndocumentVariableName: \"text\",\n});\nconst chain = new MapReduceDocumentsChain({\nllmChain,\ncombineDocumentChain,\ndocumentVariableName: \"text\",\nreturnIntermediateSteps,\n});\nreturn chain;\n}\nif (params.type === \"refine\") {\nconst { refinePrompt = REFINE_PROMPT, questionPrompt = DEFAULT_PROMPT } =\nparams;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/load.ts","loc":{"lines":{"from":77,"to":112}}}}],["330",{"pageContent":"const llmChain = new LLMChain({ prompt: questionPrompt, llm });\nconst refineLLMChain = new LLMChain({ prompt: refinePrompt, llm });\nconst chain = new RefineDocumentsChain({\nllmChain,\nrefineLLMChain,\ndocumentVariableName: \"text\",\n});\nreturn chain;\n}\nthrow new Error(`Invalid _type: ${params.type}`);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/load.ts","loc":{"lines":{"from":154,"to":164}}}}],["331",{"pageContent":"import { PromptTemplate } from \"../../prompts/prompt.js\";\n\nconst refinePromptTemplate = `Your job is to produce a final summary\nWe have provided an existing summary up to a certain point: \"{existing_answer}\"\nWe have the opportunity to refine the existing summary\n(only if needed) with some more context below.\n------------\n\"{text}\"\n------------\n\nGiven the new context, refine the original summary\nIf the context isn't useful, return the original summary.\n\nREFINED SUMMARY:`;\n\nexport const REFINE_PROMPT = /* #__PURE__ */ new PromptTemplate({\ntemplate: refinePromptTemplate,\ninputVariables: [\"existing_answer\", \"text\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/refine_prompts.ts","loc":{"lines":{"from":1,"to":19}}}}],["332",{"pageContent":"/* eslint-disable spaced-comment */\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\n\nconst template = `Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:`;\n\nexport const DEFAULT_PROMPT = /*#__PURE__*/ new PromptTemplate({\ntemplate,\ninputVariables: [\"text\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/stuff_prompts.ts","loc":{"lines":{"from":1,"to":15}}}}],["333",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../../llms/openai.js\";\nimport { loadSummarizationChain } from \"../load.js\";\nimport { Document } from \"../../../document.js\";\n\ntest(\"Test loadSummzationChain stuff\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadSummarizationChain(model, { type: \"stuff\" });\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"Whats up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/tests/load.int.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["334",{"pageContent":"test(\"Test loadSummarizationChain map_reduce\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadSummarizationChain(model, { type: \"map_reduce\" });\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"Whats up\" });\nconsole.log({ res });\n});\n\ntest(\"Test loadSummarizationChain refine\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadSummarizationChain(model, { type: \"refine\" });\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"Whats up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/tests/load.int.test.ts","loc":{"lines":{"from":41,"to":63}}}}],["335",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { StuffDocumentsChain } from \"../combine_docs_chain.js\";\nimport { ChatVectorDBQAChain } from \"../chat_vector_db_chain.js\";\nimport { HNSWLib } from \"../../vectorstores/hnswlib.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["336",{"pageContent":"test(\"Test ChatVectorDBQAChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = PromptTemplate.fromTemplate(\n\"Print {question}, and ignore {chat_history}\"\n);\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst llmChain = new LLMChain({ prompt, llm: model });\nconst combineDocsChain = new StuffDocumentsChain({\nllmChain,\ndocumentVariableName: \"foo\",\n});\nconst chain = new ChatVectorDBQAChain({\ncombineDocumentsChain: combineDocsChain,\nvectorstore: vectorStore,\nquestionGeneratorChain: llmChain,\n});\nconst res = await chain.call({ question: \"foo\", chat_history: \"bar\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":120,"to":142}}}}],["337",{"pageContent":"test(\"Test ChatVectorDBQAChain with returnSourceDocuments\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = PromptTemplate.fromTemplate(\n\"Print {question}, and ignore {chat_history}\"\n);\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst llmChain = new LLMChain({ prompt, llm: model });\nconst combineDocsChain = new StuffDocumentsChain({\nllmChain,\ndocumentVariableName: \"foo\",\n});\nconst chain = new ChatVectorDBQAChain({\ncombineDocumentsChain: combineDocsChain,\nvectorstore: vectorStore,\nquestionGeneratorChain: llmChain,\nreturnSourceDocuments: true,\n});\nconst res = await chain.call({ question: \"foo\", chat_history: \"bar\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":245,"to":268}}}}],["338",{"pageContent":"test(\"Test ChatVectorDBQAChain from LLM\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst chain = ChatVectorDBQAChain.fromLLM(model, vectorStore);\nconst res = await chain.call({ question: \"foo\", chat_history: \"bar\" });\nconsole.log({ res });\n});\ntest(\"Test ChatVectorDBQAChain from LLM with flag option to return source\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst chain = ChatVectorDBQAChain.fromLLM(model, vectorStore, {\nreturnSourceDocuments: true,\n});\nconst res = await chain.call({ question: \"foo\", chat_history: \"bar\" });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":370,"to":391}}}}],["339",{"pageContent":"expect(res).toEqual(\nexpect.objectContaining({\ntext: expect.any(String),\nsourceDocuments: expect.arrayContaining([\nexpect.objectContaining({\nmetadata: expect.objectContaining({\nid: expect.any(Number),\n}),\npageContent: expect.any(String),\n}),\n]),\n})\n);\n});\n\ntest(\"Test ChatVectorDBQAChain from LLM with override default prompts\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\", temperature: 0 });\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\n\nconst qa_template = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say \"Sorry I dont know, I am learning from Aliens\", don't try to make up an answer.\n{context}\n\nQuestion: {question}\nHelpful Answer:`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":490,"to":517}}}}],["340",{"pageContent":"const chain = ChatVectorDBQAChain.fromLLM(model, vectorStore, {\nqaTemplate: qa_template,\n});\nconst res = await chain.call({\nquestion: \"What is better programming Language Python or Javascript \",\nchat_history: \"bar\",\n});\nexpect(res.text).toContain(\"I am learning from Aliens\");\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":617,"to":626}}}}],["341",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { loadChain } from \"../load.js\";\nimport { StuffDocumentsChain } from \"../combine_docs_chain.js\";\nimport { Document } from \"../../document.js\";\nimport {\nloadQAMapReduceChain,\nloadQARefineChain,\n} from \"../question_answering/load.js\";\n\ntest(\"Test StuffDocumentsChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst llmChain = new LLMChain({ prompt, llm: model });\nconst chain = new StuffDocumentsChain({\nllmChain,\ndocumentVariableName: \"foo\",\n});\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.int.test.ts","loc":{"lines":{"from":1,"to":31}}}}],["342",{"pageContent":"test(\"Test MapReduceDocumentsChain with QA chain\", async () => {\nconst model = new OpenAI({ temperature: 0, modelName: \"text-ada-001\" });\nconst chain = loadQAMapReduceChain(model);\nconst docs = [\nnew Document({ pageContent: \"harrison went to harvard\" }),\nnew Document({ pageContent: \"ankush went to princeton\" }),\n];\nconst res = await chain.call({\ninput_documents: docs,\nquestion: \"Where did harrison go to college\",\n});\nconsole.log({ res });\n});\n\ntest(\"Test RefineDocumentsChain with QA chain\", async () => {\nconst model = new OpenAI({ temperature: 0, modelName: \"text-ada-001\" });\nconst chain = loadQARefineChain(model);\nconst docs = [\nnew Document({ pageContent: \"harrison went to harvard\" }),\nnew Document({ pageContent: \"ankush went to princeton\" }),\n];\nconst res = await chain.call({\ninput_documents: docs,\nquestion: \"Where did harrison go to college\",\n});\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.int.test.ts","loc":{"lines":{"from":74,"to":100}}}}],["343",{"pageContent":"test(\"Load chain from hub\", async () => {\nconst chain = await loadChain(\n\"lc://chains/question_answering/stuff/chain.json\"\n);\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"what up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.int.test.ts","loc":{"lines":{"from":146,"to":157}}}}],["344",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { Document } from \"../../document.js\";\nimport { BaseLLM } from \"../../llms/base.js\";\nimport { loadQAMapReduceChain } from \"../question_answering/load.js\";\nimport { LLMResult } from \"../../schema/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.test.ts","loc":{"lines":{"from":1,"to":5}}}}],["345",{"pageContent":"FakeLLM extends BaseLLM {\nnrMapCalls = 0;\n\nnrReduceCalls = 0;\n\n_llmType(): string {\nreturn \"fake\";\n}\n\nasync _generate(prompts: string[], _?: string[]): Promise<LLMResult> {\nreturn {\ngenerations: prompts.map((prompt) => {\nlet completion = \"\";\nif (prompt.startsWith(\"Use the following portion\")) {\nthis.nrMapCalls += 1;\ncompletion = \"a portion of context\";\n} else if (prompt.startsWith(\"Given the following extracted\")) {\nthis.nrReduceCalls += 1;\ncompletion = \"a final answer\";\n}\nreturn [\n{\ntext: completion,\nscore: 0,\n},\n];\n}),\n};\n}\n}\n\ntest(\"Test MapReduceDocumentsChain\", async () => {\nconst model = new FakeLLM({});\nconst chain = loadQAMapReduceChain(model);\nconst docs = [\nnew Document({ pageContent: \"harrison went to harvard\" }),\nnew Document({ pageContent: \"ankush went to princeton\" }),\n];\n\nconst res = await chain.call({\ninput_documents: docs,\nquestion: \"Where did harrison go to college\",\n});\nconsole.log({ res });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.test.ts","loc":{"lines":{"from":106,"to":149}}}}],["346",{"pageContent":"expect(res).toEqual({\ntext: \"a final answer\",\n});\nexpect(model.nrMapCalls).toBe(0); // below maxTokens\nexpect(model.nrReduceCalls).toBe(1);\n});\n\ntest(\"Test MapReduceDocumentsChain with content above maxTokens\", async () => {\nconst model = new FakeLLM({});\nconst chain = loadQAMapReduceChain(model);\nconst aString = \"a\".repeat(10000);\nconst bString = \"b\".repeat(10000);\nconst docs = [\nnew Document({ pageContent: aString }),\nnew Document({ pageContent: bString }),\n];\n\nconst res = await chain.call({\ninput_documents: docs,\nquestion: \"Is the letter c present in the document\",\n});\nconsole.log({ res });\n\nexpect(res).toEqual({\ntext: \"a final answer\",\n});\nexpect(model.nrMapCalls).toBe(2); // above maxTokens\nexpect(model.nrReduceCalls).toBe(1);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.test.ts","loc":{"lines":{"from":224,"to":252}}}}],["347",{"pageContent":"test(\"Test MapReduceDocumentsChain with content above maxTokens and intermediate steps\", async () => {\nconst model = new FakeLLM({});\nconst chain = loadQAMapReduceChain(model, {\nreturnIntermediateSteps: true,\n});\nconst aString = \"a\".repeat(10000);\nconst bString = \"b\".repeat(10000);\nconst docs = [\nnew Document({ pageContent: aString }),\nnew Document({ pageContent: bString }),\n];\n\nconst res = await chain.call({\ninput_documents: docs,\nquestion: \"Is the letter c present in the document\",\n});\nconsole.log({ res });\n\nexpect(res).toEqual({\ntext: \"a final answer\",\nintermediateSteps: [\"a portion of context\", \"a portion of context\"],\n});\nexpect(model.nrMapCalls).toBe(2); // above maxTokens\nexpect(model.nrReduceCalls).toBe(1);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.test.ts","loc":{"lines":{"from":335,"to":359}}}}],["348",{"pageContent":"import { test } from \"@jest/globals\";\nimport { ConstitutionalChain } from \"../constitutional_ai/constitutional_chain.js\";\nimport { ConstitutionalPrinciple } from \"../constitutional_ai/constitutional_principle.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { OpenAI } from \"../../llms/openai.js\";\n\ntest(\"Test ConstitutionalChain\", async () => {\nconst llm = new OpenAI();\nconst qaPrompt = new PromptTemplate({\ntemplate: \"Q: {question} A:\",\ninputVariables: [\"question\"],\n});\n\nconst qaChain = new LLMChain({\nllm,\nprompt: qaPrompt,\n});\n\nconst constitutionalChain = ConstitutionalChain.fromLLM(llm, {\nchain: qaChain,\nconstitutionalPrinciples: [\nnew ConstitutionalPrinciple({\ncritiqueRequest: \"Tell me if this answer is good.\",\nrevisionRequest: \"Give a better answer.\",\n}),\n],\n});\n\nconst res = await constitutionalChain.call({\nquestion: \"What is the meaning of life?\",\n});\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/constitutional_chain.int.test.ts","loc":{"lines":{"from":1,"to":34}}}}],["349",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { ConstitutionalChain } from \"../constitutional_ai/constitutional_chain.js\";\nimport { ConstitutionalPrinciple } from \"../constitutional_ai/constitutional_principle.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { BaseLLM } from \"../../llms/base.js\";\nimport { LLMResult } from \"../../schema/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/constitutional_chain.test.ts","loc":{"lines":{"from":1,"to":7}}}}],["350",{"pageContent":"FakeLLM extends BaseLLM {\nnrMapCalls = 0;\n\nnrReduceCalls = 0;\n\n_llmType(): string {\nreturn \"fake\";\n}\n\nasync _generate(prompts: string[], _?: string[]): Promise<LLMResult> {\nreturn {\ngenerations: prompts.map((prompt) => [\n{\ntext: prompt,\nscore: 0,\n},\n]),\n};\n}\n}\n\ntest(\"Test ConstitutionalChain\", async () => {\nconst llm = new FakeLLM({});\nconst qaPrompt = new PromptTemplate({\ntemplate: \"Q: {question} A:\",\ninputVariables: [\"question\"],\n});\n\nconst qaChain = new LLMChain({\nllm,\nprompt: qaPrompt,\n});\n\nconst critiqueWord = \"Tell me if this answer is good.\";\nconst revisionWord = \"Give a better answer.\";\n\nconst constitutionalChain = ConstitutionalChain.fromLLM(llm, {\nchain: qaChain,\nconstitutionalPrinciples: [\nnew ConstitutionalPrinciple({\ncritiqueRequest: critiqueWord,\nrevisionRequest: revisionWord,\n}),\n],\n});\n\nconst { output } = await constitutionalChain.call({\nquestion: \"What is the meaning of life?\",\n});\nexpect(output).toContain(critiqueWord);\nexpect(output).toContain(revisionWord);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/constitutional_chain.test.ts","loc":{"lines":{"from":60,"to":111}}}}],["351",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { ConversationChain } from \"../conversation.js\";\n\ntest(\"Test ConversationChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = new ConversationChain({ llm: model });\nconst res = await chain.call({ input: \"my favorite color\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/conversation_chain.int.test.ts","loc":{"lines":{"from":1,"to":10}}}}],["352",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\nimport {\nChatPromptTemplate,\nHumanMessagePromptTemplate,\nPromptTemplate,\n} from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { loadChain } from \"../load.js\";\n\ntest(\"Test OpenAI\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst chain = new LLMChain({ prompt, llm: model });\nconst res = await chain.call({ foo: \"my favorite color\" });\nconsole.log({ res });\n});\n\ntest(\"Test run method\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst chain = new LLMChain({ prompt, llm: model });\nconst res = await chain.run(\"my favorite color\");\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/llm_chain.int.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["353",{"pageContent":"test(\"Test apply\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst chain = new LLMChain({ prompt, llm: model });\nconst res = await chain.apply([{ foo: \"my favorite color\" }]);\nconsole.log({ res });\n});\n\ntest(\"Load chain from hub\", async () => {\nconst chain = await loadChain(\"lc://chains/hello-world/chain.json\");\nconst res = await chain.call({ topic: \"my favorite color\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/llm_chain.int.test.ts","loc":{"lines":{"from":82,"to":97}}}}],["354",{"pageContent":"test(\"Test LLMChain with ChatOpenAI\", async () => {\nconst model = new ChatOpenAI({ temperature: 0.9 });\nconst template = \"What is a good name for a company that makes {product}?\";\nconst prompt = new PromptTemplate({ template, inputVariables: [\"product\"] });\nconst humanMessagePrompt = new HumanMessagePromptTemplate(prompt);\nconst chatPromptTemplate = ChatPromptTemplate.fromPromptMessages([\nhumanMessagePrompt,\n]);\nconst chatChain = new LLMChain({ llm: model, prompt: chatPromptTemplate });\nconst res = await chatChain.call({ product: \"colorful socks\" });\nconsole.log({ res });\n});\n\ntest(\"Test deserialize\", async () => {\nconst model = new ChatOpenAI();\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst chain = new LLMChain({ prompt, llm: model });\n\nconst serialized = chain.serialize();\n// console.log(serialized)\nconst chain2 = await LLMChain.deserialize({ ...serialized });\n\nconst res = await chain2.run(\"my favorite color\");\nconsole.log({ res });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/llm_chain.int.test.ts","loc":{"lines":{"from":161,"to":187}}}}],["355",{"pageContent":"// chain === chain2?\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/llm_chain.int.test.ts","loc":{"lines":{"from":235,"to":236}}}}],["356",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAIModerationChain } from \"../openai_moderation.js\";\n\ntest(\"OpenAI Moderation Test\", async () => {\nconst badString = \"I hate myself and want to do harm to myself\";\nconst goodString =\n\"The cat (Felis catus) is a domestic species of small carnivorous mammal.\";\n\nconst moderation = new OpenAIModerationChain();\nconst { output: badResult } = await moderation.call({\ninput: badString,\n});\n\nconst { output: goodResult } = await moderation.call({\ninput: goodString,\n});\n\nexpect(badResult).toEqual(\n\"Text was found that violates OpenAI's content policy.\"\n);\nexpect(goodResult).toEqual(\n\"The cat (Felis catus) is a domestic species of small carnivorous mammal.\"\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/openai_moderation.int.test.ts","loc":{"lines":{"from":1,"to":24}}}}],["357",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { SequentialChain } from \"../sequential_chain.js\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\n\ntest(\"Test SequentialChain example usage\", async () => {\n// This is an LLMChain to write a synopsis given a title of a play and the era it is set in.\nconst llm = new OpenAI({ temperature: 0 });\nconst template = `You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.\n\nTitle: {title}\nEra: {era}\nPlaywright: This is a synopsis for the above play:`;\nconst promptTemplate = new PromptTemplate({\ntemplate,\ninputVariables: [\"title\", \"era\"],\n});\nconst synopsisChain = new LLMChain({\nllm,\nprompt: promptTemplate,\noutputKey: \"synopsis\",\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.int.test.ts","loc":{"lines":{"from":1,"to":24}}}}],["358",{"pageContent":"// This is an LLMChain to write a review of a play given a synopsis.\nconst reviewLLM = new OpenAI({ temperature: 0 });\nconst reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n\nPlay Synopsis:\n{synopsis}\nReview from a New York Times play critic of the above play:`;\nconst reviewPromptTempalte = new PromptTemplate({\ntemplate: reviewTemplate,\ninputVariables: [\"synopsis\"],\n});\nconst reviewChain = new LLMChain({\nllm: reviewLLM,\nprompt: reviewPromptTempalte,\noutputKey: \"review\",\n});\n\nconst overallChain = new SequentialChain({\nchains: [synopsisChain, reviewChain],\ninputVariables: [\"era\", \"title\"],\n// Here we return multiple variables\noutputVariables: [\"synopsis\", \"review\"],\nverbose: true,\n});\nconst review = await overallChain.call({\ntitle: \"Tragedy at sunset on the beach\",\nera: \"Victorian England\",\n});\nexpect(review.review.toLowerCase()).toContain(\n\"tragedy at sunset on the beach\"\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.int.test.ts","loc":{"lines":{"from":100,"to":131}}}}],["359",{"pageContent":"test.skip(\"Test SequentialChain serialize/deserialize\", async () => {\nconst llm1 = new ChatOpenAI();\nconst template1 = `Echo back \"{foo} {bar}\"`;\nconst promptTemplate1 = new PromptTemplate({\ntemplate: template1,\ninputVariables: [\"foo\", \"bar\"],\n});\nconst chain1 = new LLMChain({\nllm: llm1,\nprompt: promptTemplate1,\noutputKey: \"baz\",\n});\n\nconst llm2 = new ChatOpenAI();\nconst template2 = `Echo back \"{baz}\"`;\nconst promptTemplate2 = new PromptTemplate({\ntemplate: template2,\ninputVariables: [\"baz\"],\n});\nconst chain2 = new LLMChain({\nllm: llm2,\nprompt: promptTemplate2,\n});\n\nconst sampleSequentialChain = new SequentialChain({\nchains: [chain1, chain2],\ninputVariables: [\"foo\", \"bar\"],\noutputVariables: [\"text\"],\nverbose: true,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.int.test.ts","loc":{"lines":{"from":203,"to":232}}}}],["360",{"pageContent":"const serializedChain = sampleSequentialChain.serialize();\nexpect(serializedChain._type).toEqual(\"sequential_chain\");\nexpect(serializedChain.chains.length).toEqual(2);\nconst deserializedChain = await SequentialChain.deserialize(serializedChain);\nexpect(deserializedChain.chains.length).toEqual(2);\nexpect(deserializedChain._chainType).toEqual(\"sequential_chain\");\nconst review = await deserializedChain.call({ foo: \"test1\", bar: \"test2\" });\nexpect(review.trim()).toMatchInlineSnapshot(`\"test1 test2\"`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.int.test.ts","loc":{"lines":{"from":315,"to":323}}}}],["361",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { BaseLLM } from \"../../llms/base.js\";\nimport { LLMResult } from \"../../schema/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { SequentialChain } from \"../sequential_chain.js\";\n\nclass FakeLLM1 extends BaseLLM {\nnrMapCalls = 0;\n\nnrReduceCalls = 0;\n\n_llmType(): string {\nreturn \"fake_1\";\n}\n\nasync _generate(_prompts: string[], _?: string[]): Promise<LLMResult> {\nreturn {\ngenerations: [\n[\n{\ntext: \"The answer is XXX.\",\n},\n],\n],\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":1,"to":28}}}}],["362",{"pageContent":"FakeLLM2 extends BaseLLM {\nnrMapCalls = 0;\n\nnrReduceCalls = 0;\n\n_llmType(): string {\nreturn \"fake_2\";\n}\n\nasync _generate(prompts: string[], _?: string[]): Promise<LLMResult> {\nlet response = \"I don't know what you are talking about.\";\nif (prompts[0].includes(\"XXX\")) {\nresponse = \"final answer\";\n}\nreturn {\ngenerations: [\n[\n{\ntext: response,\n},\n],\n],\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":169,"to":193}}}}],["363",{"pageContent":"test(\"Test SequentialChain\", async () => {\nconst model1 = new FakeLLM1({});\nconst model2 = new FakeLLM2({});\nconst template1 = \"Some arbitrary template with fake {input1} and {input2}.\";\nconst template2 = \"Some arbitrary template with fake {input3}.\";\nconst prompt1 = new PromptTemplate({\ntemplate: template1,\ninputVariables: [\"input1\", \"input2\"],\n});\nconst prompt2 = new PromptTemplate({\ntemplate: template2,\ninputVariables: [\"input3\"],\n});\nconst chain1 = new LLMChain({\nllm: model1,\nprompt: prompt1,\noutputKey: \"input3\",\n});\nconst chain2 = new LLMChain({ llm: model2, prompt: prompt2 });\nconst combinedChain = new SequentialChain({\nchains: [chain1, chain2],\ninputVariables: [\"input1\", \"input2\"],\noutputVariables: [\"text\"],\n});\nconst response = await combinedChain.call({\ninput1: \"test1\",\ninput2: \"test2\",\n});\nexpect(response).toMatchInlineSnapshot(`\n{\n\"text\": \"final answer\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":345,"to":378}}}}],["364",{"pageContent":"test(\"Test SequentialChain input/output chains' validation\", () => {\nconst model1 = new FakeLLM1({});\nconst template1 = \"Some arbitrary template with fake {input1} and {input2}.\";\nconst prompt1 = new PromptTemplate({\ntemplate: template1,\ninputVariables: [\"input1\", \"input2\"],\n});\nconst chain1 = new LLMChain({\nllm: model1,\nprompt: prompt1,\noutputKey: \"input3\",\n});\nconst model2 = new FakeLLM2({});\nconst template2 = \"Some arbitrary template with fake {input3}.\";\nconst prompt2 = new PromptTemplate({\ntemplate: template2,\ninputVariables: [\"input3\"],\n});\nconst chain2 = new LLMChain({ llm: model2, prompt: prompt2 });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":501,"to":519}}}}],["365",{"pageContent":"expect(() => {\n/* eslint-disable no-new */\nnew SequentialChain({\nchains: [chain1, chain2],\ninputVariables: [\"input1\"],\noutputVariables: [\"text\"],\n});\n}).toThrowErrorMatchingInlineSnapshot(\n`\"Missing variables for chain \"llm_chain\": \"input2\". Only got the following variables: \"input1\".\"`\n);\nexpect(() => {\n/* eslint-disable no-new */\nnew SequentialChain({\nchains: [chain1, chain2],\ninputVariables: [\"input1\", \"input2\"],\noutputVariables: [\"nonexistent\"],\n});\n}).toThrowErrorMatchingInlineSnapshot(\n`\"The following output variables were expected to be in the final chain output but were not found: \"nonexistent\".\"`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":658,"to":678}}}}],["366",{"pageContent":"test(\"Test SequentialChain chains' intermediate variables validation\", () => {\nconst model1 = new FakeLLM1({});\nconst template1 = \"Some arbitrary template with fake {input1} and {input2}.\";\nconst prompt1 = new PromptTemplate({\ntemplate: template1,\ninputVariables: [\"input1\", \"input2\"],\n});\nconst chain1 = new LLMChain({\nllm: model1,\nprompt: prompt1,\noutputKey: \"nonexistent\",\n});\nconst model2 = new FakeLLM2({});\nconst template2 = \"Some arbitrary template with fake {input3}.\";\nconst prompt2 = new PromptTemplate({\ntemplate: template2,\ninputVariables: [\"input3\"],\n});\nconst chain2 = new LLMChain({ llm: model2, prompt: prompt2 });\n\nexpect(() => {\n/* eslint-disable no-new */\nnew SequentialChain({\nchains: [chain1, chain2],\ninputVariables: [\"input1\", \"input2\"],\noutputVariables: [\"text\"],\n});\n}).toThrowErrorMatchingInlineSnapshot(\n`\"Missing variables for chain \"llm_chain\": \"input3\". Only got the following variables: \"input1\", \"input2\", \"nonexistent\".\"`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":817,"to":847}}}}],["367",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { SimpleSequentialChain } from \"../sequential_chain.js\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\n\ntest(\"Test SimpleSequentialChain example usage\", async () => {\n// This is an LLMChain to write a synopsis given a title of a play.\nconst llm = new OpenAI({ temperature: 0 });\nconst template = `You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n\nTitle: {title}\nPlaywright: This is a synopsis for the above play:`;\nconst promptTemplate = new PromptTemplate({\ntemplate,\ninputVariables: [\"title\"],\n});\nconst synopsisChain = new LLMChain({ llm, prompt: promptTemplate });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.int.test.ts","loc":{"lines":{"from":1,"to":19}}}}],["368",{"pageContent":"// This is an LLMChain to write a review of a play given a synopsis.\nconst reviewLLM = new OpenAI({ temperature: 0 });\nconst reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n\nPlay Synopsis:\n{synopsis}\nReview from a New York Times play critic of the above play:`;\nconst reviewPromptTempalte = new PromptTemplate({\ntemplate: reviewTemplate,\ninputVariables: [\"synopsis\"],\n});\nconst reviewChain = new LLMChain({\nllm: reviewLLM,\nprompt: reviewPromptTempalte,\n});\n\nconst overallChain = new SimpleSequentialChain({\nchains: [synopsisChain, reviewChain],\nverbose: true,\n});\nconst review = await overallChain.run(\"Tragedy at sunset on the beach\");\nexpect(review.trim().toLowerCase()).toContain(\n\"tragedy at sunset on the beach\"\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.int.test.ts","loc":{"lines":{"from":81,"to":105}}}}],["369",{"pageContent":"test(\"Test SimpleSequentialChain serialize/deserialize\", async () => {\nconst llm1 = new ChatOpenAI();\nconst template1 = `Echo back \"{foo}\"`;\nconst promptTemplate1 = new PromptTemplate({\ntemplate: template1,\ninputVariables: [\"foo\"],\n});\nconst chain1 = new LLMChain({ llm: llm1, prompt: promptTemplate1 });\n\nconst llm2 = new ChatOpenAI();\nconst template2 = `Echo back \"{bar}\"`;\nconst promptTemplate2 = new PromptTemplate({\ntemplate: template2,\ninputVariables: [\"bar\"],\n});\nconst chain2 = new LLMChain({\nllm: llm2,\nprompt: promptTemplate2,\n});\n\nconst sampleSequentialChain = new SimpleSequentialChain({\nchains: [chain1, chain2],\nverbose: true,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.int.test.ts","loc":{"lines":{"from":167,"to":190}}}}],["370",{"pageContent":"const serializedChain = sampleSequentialChain.serialize();\nexpect(serializedChain._type).toEqual(\"simple_sequential_chain\");\nexpect(serializedChain.chains.length).toEqual(2);\nconst deserializedChain = await SimpleSequentialChain.deserialize(\nserializedChain\n);\nexpect(deserializedChain.chains.length).toEqual(2);\nexpect(deserializedChain._chainType()).toEqual(\"simple_sequential_chain\");\nawait deserializedChain.run(\"test\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.int.test.ts","loc":{"lines":{"from":257,"to":266}}}}],["371",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { BaseLLM } from \"../../llms/base.js\";\nimport { LLMResult } from \"../../schema/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { SimpleSequentialChain } from \"../sequential_chain.js\";\nimport { AnalyzeDocumentChain } from \"../analyze_documents_chain.js\";\nimport { ConversationalRetrievalQAChain } from \"../conversational_retrieval_chain.js\";\nimport { VectorStoreRetriever } from \"../../vectorstores/base.js\";\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\n\nclass FakeLLM1 extends BaseLLM {\nnrMapCalls = 0;\n\nnrReduceCalls = 0;\n\n_llmType(): string {\nreturn \"fake_1\";\n}\n\nasync _generate(_prompts: string[], _?: string[]): Promise<LLMResult> {\nreturn {\ngenerations: [\n[\n{\ntext: \"The answer is XXX.\",\n},\n],\n],\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.test.ts","loc":{"lines":{"from":1,"to":33}}}}],["372",{"pageContent":"FakeLLM2 extends BaseLLM {\nnrMapCalls = 0;\n\nnrReduceCalls = 0;\n\n_llmType(): string {\nreturn \"fake_2\";\n}\n\nasync _generate(prompts: string[], _?: string[]): Promise<LLMResult> {\nlet response = \"I don't know what you are talking about.\";\nif (prompts[0].includes(\"XXX\")) {\nresponse = \"final answer\";\n}\nreturn {\ngenerations: [\n[\n{\ntext: response,\n},\n],\n],\n};\n}\n}\n\ntest(\"Test SimpleSequentialChain\", async () => {\nconst model1 = new FakeLLM1({});\nconst model2 = new FakeLLM2({});\nconst template = \"Some arbitrary template with fake {input}.\";\nconst prompt = new PromptTemplate({ template, inputVariables: [\"input\"] });\nconst chain1 = new LLMChain({ llm: model1, prompt });\nconst chain2 = new LLMChain({ llm: model2, prompt });\nconst combinedChain = new SimpleSequentialChain({ chains: [chain1, chain2] });\nconst response = await combinedChain.run(\"initial question\");\nexpect(response).toEqual(\"final answer\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.test.ts","loc":{"lines":{"from":124,"to":160}}}}],["373",{"pageContent":"test(\"Test SimpleSequentialChain input chains' single input validation\", async () => {\nconst model1 = new FakeLLM1({});\nconst model2 = new FakeLLM2({});\nconst template = \"Some arbitrary template with fake {input1} and {input2}.\";\nconst prompt = new PromptTemplate({\ntemplate,\ninputVariables: [\"input1\", \"input2\"],\n});\nconst chain1 = new LLMChain({ llm: model1, prompt });\nconst chain2 = new LLMChain({ llm: model2, prompt });\nexpect(() => {\n/* eslint-disable no-new */\nnew SimpleSequentialChain({ chains: [chain1, chain2] });\n}).toThrowErrorMatchingInlineSnapshot(\n`\"Chains used in SimpleSequentialChain should all have one input, got 2 for llm_chain.\"`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.test.ts","loc":{"lines":{"from":252,"to":268}}}}],["374",{"pageContent":"test(\"Test SimpleSequentialChain input chains' single ouput validation\", async () => {\nconst model1 = new FakeLLM1({});\nconst fakeEmbeddings = new FakeEmbeddings();\nconst anyStore = new MemoryVectorStore(fakeEmbeddings);\nconst retriever = new VectorStoreRetriever({\nvectorStore: anyStore,\n});\nconst template = \"Some arbitrary template with fake {input}.\";\nconst prompt = new PromptTemplate({ template, inputVariables: [\"input\"] });\nconst chain1 = new LLMChain({ llm: model1, prompt });\nconst chain2 = new ConversationalRetrievalQAChain({\nretriever,\ncombineDocumentsChain: chain1,\nquestionGeneratorChain: chain1,\nreturnSourceDocuments: true,\n});\n// Chain below is is not meant to work in a real-life scenario.\n// It's only combined this way to get one input/multiple outputs chain.\nconst multipleOutputChain = new AnalyzeDocumentChain({\ncombineDocumentsChain: chain2,\n});\nexpect(() => {\n/* eslint-disable no-new */\nnew SimpleSequentialChain({ chains: [chain1, multipleOutputChain] });\n}).toThrowErrorMatchingInlineSnapshot(","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.test.ts","loc":{"lines":{"from":373,"to":397}}}}],["375",{"pageContent":"`\"Chains used in SimpleSequentialChain should all have one output, got 2 for analyze_document_chain.\"`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.test.ts","loc":{"lines":{"from":480,"to":482}}}}],["376",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { DataSource } from \"typeorm\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { SqlDatabaseChain } from \"../sql_db/sql_db_chain.js\";\nimport { SqlDatabase } from \"../../sql_db.js\";\nimport { SQL_SQLITE_PROMPT } from \"../sql_db/sql_db_prompt.js\";\n\ntest(\"Test SqlDatabaseChain\", async () => {\nconst datasource = new DataSource({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["377",{"pageContent":": \"sqlite\",\ndatabase: \":memory:\",\nsynchronize: true,\n});\n\nawait datasource.initialize();\nawait datasource.query(`\nCREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, age INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Alice', 20);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Bob', 21);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Charlie', 22);\n`);\n\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\n\nconst chain = new SqlDatabaseChain({\nllm: new OpenAI({ temperature: 0 }),\ndatabase: db,\n});\n\nexpect(chain.prompt).toBe(SQL_SQLITE_PROMPT);\n\nconst res = await chain.run(\"How many users are there?\");\nconsole.log(res);\n\nawait datasource.destroy();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":107,"to":141}}}}],["378",{"pageContent":"// We create this string to reach the token limit of the query built to describe the database and get the SQL query.\nconst veryLongString = `Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam orci nisi, vulputate ac pulvinar eu, maximus a tortor. Duis suscipit, nibh vel fermentum vehicula, mauris ante convallis metus, et feugiat turpis mauris non felis. Interdum et malesuada fames ac ante ipsum primis in faucibus. Maecenas efficitur nibh in nisi sagittis ultrices. Donec id velit nunc. Nam a lacus risus. Vestibulum molestie massa eget convallis pellentesque.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":164,"to":165}}}}],["379",{"pageContent":"Mauris a nisl eget velit finibus blandit ac a odio. Sed sagittis consequat urna a egestas. Curabitur pretium convallis nibh, in ullamcorper odio tempus nec. Curabitur laoreet nec nisl sed accumsan. Sed elementum eleifend molestie. Aenean ullamcorper interdum risus, eget pharetra est volutpat ut. Aenean maximus consequat justo rutrum finibus. Mauris consequat facilisis consectetur. Vivamus rutrum dignissim libero, non aliquam lectus tempus id. In hac habitasse platea dictumst. Sed at magna dignissim, tincidunt lectus in, malesuada risus. Phasellus placerat blandit ligula. Integer posuere id elit at commodo. Sed consequat sagittis odio eget congue.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":166,"to":166}}}}],["380",{"pageContent":"Aliquam ultricies, sapien a porta luctus, dolor nibh dignissim erat, dictum luctus orci lorem non quam. Quisque dapibus tempus mattis. Suspendisse gravida consequat mi at viverra. Quisque sed est purus. Fusce tincidunt semper massa eu blandit. Donec in lacus a tortor facilisis facilisis. Interdum et malesuada fames ac ante ipsum primis in faucibus. In aliquam dignissim eros ac consectetur. Aliquam fringilla magna erat. Nullam tincidunt maximus nulla, quis gravida est varius vel. Aliquam cursus, diam non facilisis mollis, nunc diam convallis enim, ac tempus diam tortor in dui. Nunc feugiat ligula odio, eleifend fermentum quam tincidunt sed. Duis pellentesque quam eget volutpat commodo.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":167,"to":167}}}}],["381",{"pageContent":"Aliquam ex velit, porta sit amet augue vulputate, rhoncus fermentum magna. Integer non elementum augue. Phasellus rhoncus nisl nec magna lacinia vulputate. Suspendisse diam nibh, egestas a porta a, pellentesque ut nisl. Donec tempus ligula at leo convallis consequat. Duis sapien lorem, lobortis ac nisl dapibus, bibendum mollis lorem. Sed congue porttitor ex, eget scelerisque ligula consectetur quis. Mauris felis mauris, sodales quis nunc non, condimentum eleifend quam. Ut vitae viverra lorem. Vivamus lacinia et dolor vitae cursus. Proin faucibus venenatis enim vitae tincidunt. Sed sed venenatis leo.\nDonec eu erat ullamcorper, consectetur dui sed, cursus tellus. Phasellus consectetur felis augue, quis auctor odio semper ac. In scelerisque gravida ligula eget lobortis. Sed tristique ultricies fringilla. Nunc in ultrices purus. Curabitur dictum cursus ante, at tempus est interdum at. Donec gravida lectus ut purus vestibulum, eu accumsan nisl pharetra.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":168,"to":169}}}}],["382",{"pageContent":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam orci nisi, vulputate ac pulvinar eu, maximus a tortor. Duis suscipit, nibh vel fermentum vehicula, mauris ante convallis metus, et feugiat turpis mauris non felis. Interdum et malesuada fames ac ante ipsum primis in faucibus. Maecenas efficitur nibh in nisi sagittis ultrices. Donec id velit nunc. Nam a lacus risus. Vestibulum molestie massa eget convallis pellentesque.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":169,"to":169}}}}],["383",{"pageContent":"Mauris a nisl eget velit finibus blandit ac a odio. Sed sagittis consequat urna a egestas. Curabitur pretium convallis nibh, in ullamcorper odio tempus nec. Curabitur laoreet nec nisl sed accumsan. Sed elementum eleifend molestie. Aenean ullamcorper interdum risus, eget pharetra est volutpat ut. Aenean maximus consequat justo rutrum finibus. Mauris consequat facilisis consectetur. Vivamus rutrum dignissim libero, non aliquam lectus tempus id. In hac habitasse platea dictumst. Sed at magna dignissim, tincidunt lectus in, malesuada risus. Phasellus placerat blandit ligula. Integer posuere id elit at commodo. Sed consequat sagittis odio eget congue.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":170,"to":170}}}}],["384",{"pageContent":"Aliquam ultricies, sapien a porta luctus, dolor nibh dignissim erat, dictum luctus orci lorem non quam. Quisque dapibus tempus mattis. Suspendisse gravida consequat mi at viverra. Quisque sed est purus. Fusce tincidunt semper massa eu blandit. Donec in lacus a tortor facilisis facilisis. Interdum et malesuada fames ac ante ipsum primis in faucibus. In aliquam dignissim eros ac consectetur. Aliquam fringilla magna erat. Nullam tincidunt maximus nulla, quis gravida est varius vel. Aliquam cursus, diam non facilisis mollis, nunc diam convallis enim, ac tempus diam tortor in dui. Nunc feugiat ligula odio, eleifend fermentum quam tincidunt sed. Duis pellentesque quam eget volutpat commodo.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":171,"to":171}}}}],["385",{"pageContent":"Aliquam ex velit, porta sit amet augue vulputate, rhoncus fermentum magna. Integer non elementum augue. Phasellus rhoncus nisl nec magna lacinia vulputate. Suspendisse diam nibh, egestas a porta a, pellentesque ut nisl. Donec tempus ligula at leo convallis consequat. Duis sapien lorem, lobortis ac nisl dapibus, bibendum mollis lorem. Sed congue porttitor ex, eget scelerisque ligula consectetur quis. Mauris felis mauris, sodales quis nunc non, condimentum eleifend quam. Ut vitae viverra lorem. Vivamus lacinia et dolor vitae cursus. Proin faucibus venenatis enim vitae tincidunt. Sed sed venenatis leo.\nDonec eu erat ullamcorper, consectetur dui sed, cursus tellus. Phasellus consectetur felis augue, quis auctor odio semper ac. In scelerisque gravida ligula eget lobortis. Sed tristique ultricies fringilla. Nunc in ultrices purus. Curabitur dictum cursus ante, at tempus est interdum at. Donec gravida lectus ut purus vestibulum, eu accumsan nisl pharetra.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":172,"to":173}}}}],["386",{"pageContent":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam orci nisi, vulputate ac pulvinar eu, maximus a tortor. Duis suscipit, nibh vel fermentum vehicula, mauris ante convallis metus, et feugiat turpis mauris non felis. Interdum et malesuada fames ac ante ipsum primis in faucibus. Maecenas efficitur nibh in nisi sagittis ultrices. Donec id velit nunc. Nam a lacus risus. Vestibulum molestie massa eget convallis pellentesque.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":173,"to":173}}}}],["387",{"pageContent":"Mauris a nisl eget velit finibus blandit ac a odio. Sed sagittis consequat urna a egestas. Curabitur pretium convallis nibh, in ullamcorper odio tempus nec. Curabitur laoreet nec nisl sed accumsan. Sed elementum eleifend molestie. Aenean ullamcorper interdum risus, eget pharetra est volutpat ut. Aenean maximus consequat justo rutrum finibus. Mauris consequat facilisis consectetur. Vivamus rutrum dignissim libero, non aliquam lectus tempus id. In hac habitasse platea dictumst. Sed at magna dignissim, tincidunt lectus in, malesuada risus. Phasellus placerat blandit ligula. Integer posuere id elit at commodo. Sed consequat sagittis odio eget congue.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":174,"to":174}}}}],["388",{"pageContent":"Aliquam ultricies, sapien a porta luctus, dolor nibh dignissim erat, dictum luctus orci lorem non quam. Quisque dapibus tempus mattis. Suspendisse gravida consequat mi at viverra. Quisque sed est purus. Fusce tincidunt semper massa eu blandit. Donec in lacus a tortor facilisis facilisis. Interdum et malesuada fames ac ante ipsum primis in faucibus. In aliquam dignissim eros ac consectetur. Aliquam fringilla magna erat. Nullam tincidunt maximus nulla, quis gravida est varius vel. Aliquam cursus, diam non facilisis mollis, nunc diam convallis enim, ac tempus diam tortor in dui. Nunc feugiat ligula odio, eleifend fermentum quam tincidunt sed. Duis pellentesque quam eget volutpat commodo.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":175,"to":175}}}}],["389",{"pageContent":"Aliquam ex velit, porta sit amet augue vulputate, rhoncus fermentum magna. Integer non elementum augue. Phasellus rhoncus nisl nec magna lacinia vulputate. Suspendisse diam nibh, egestas a porta a, pellentesque ut nisl. Donec tempus ligula at leo convallis consequat. Duis sapien lorem, lobortis ac nisl dapibus, bibendum mollis lorem. Sed congue porttitor ex, eget scelerisque ligula consectetur quis. Mauris felis mauris, sodales quis nunc non, condimentum eleifend quam. Ut vitae viverra lorem. Vivamus lacinia et dolor vitae cursus. Proin faucibus venenatis enim vitae tincidunt. Sed sed venenatis leo.\nDonec eu erat ullamcorper, consectetur dui sed, cursus tellus. Phasellus consectetur felis augue, quis auctor odio semper ac. In scelerisque gravida ligula eget lobortis. Sed tristique ultricies fringilla. Nunc in ultrices purus. Curabitur dictum cursus ante, at tempus est interdum at. Donec gravida lectus ut purus vestibulum, eu accumsan nisl pharetra.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":176,"to":177}}}}],["390",{"pageContent":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam orci nisi, vulputate ac pulvinar eu, maximus a tortor. Duis suscipit, nibh vel fermentum vehicula, mauris ante convallis metus, et feugiat turpis mauris non felis. Interdum et malesuada fames ac ante ipsum primis in faucibus. Maecenas efficitur nibh in nisi sagittis ultrices. Donec id velit nunc. Nam a lacus risus. Vestibulum molestie massa eget convallis pellentesque.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":177,"to":177}}}}],["391",{"pageContent":"Mauris a nisl eget velit finibus blandit ac a odio. Sed sagittis consequat urna a egestas. Curabitur pretium convallis nibh, in ullamcorper odio tempus nec. Curabitur laoreet nec nisl sed accumsan. Sed elementum eleifend molestie. Aenean ullamcorper interdum risus, eget pharetra est volutpat ut. Aenean maximus consequat justo rutrum finibus. Mauris consequat facilisis consectetur. Vivamus rutrum dignissim libero, non aliquam lectus tempus id. In hac habitasse platea dictumst. Sed at magna dignissim, tincidunt lectus in, malesuada risus. Phasellus placerat blandit ligula. Integer posuere id elit at commodo. Sed consequat sagittis odio eget congue.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":178,"to":178}}}}],["392",{"pageContent":"Aliquam ultricies, sapien a porta luctus, dolor nibh dignissim erat, dictum luctus orci lorem non quam. Quisque dapibus tempus mattis. Suspendisse gravida consequat mi at viverra. Quisque sed est purus. Fusce tincidunt semper massa eu blandit. Donec in lacus a tortor facilisis facilisis. Interdum et malesuada fames ac ante ipsum primis in faucibus. In aliquam dignissim eros ac consectetur. Aliquam fringilla magna erat. Nullam tincidunt maximus nulla, quis gravida est varius vel. Aliquam cursus, diam non facilisis mollis, nunc diam convallis enim, ac tempus diam tortor in dui. Nunc feugiat ligula odio, eleifend fermentum quam tincidunt sed. Duis pellentesque quam eget volutpat commodo.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":179,"to":179}}}}],["393",{"pageContent":"Aliquam ex velit, porta sit amet augue vulputate, rhoncus fermentum magna. Integer non elementum augue. Phasellus rhoncus nisl nec magna lacinia vulputate. Suspendisse diam nibh, egestas a porta a, pellentesque ut nisl. Donec tempus ligula at leo convallis consequat. Duis sapien lorem, lobortis ac nisl dapibus, bibendum mollis lorem. Sed congue porttitor ex, eget scelerisque ligula consectetur quis. Mauris felis mauris, sodales quis nunc non, condimentum eleifend quam. Ut vitae viverra lorem. Vivamus lacinia et dolor vitae cursus. Proin faucibus venenatis enim vitae tincidunt. Sed sed venenatis leo.\n`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":195,"to":196}}}}],["394",{"pageContent":"test(\"Test token limite SqlDatabaseChain\", async () => {\nconst datasource = new DataSource({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":237,"to":238}}}}],["395",{"pageContent":": \"sqlite\",\ndatabase: \":memory:\",\nsynchronize: true,\n});\n\nawait datasource.initialize();\nawait datasource.query(`\nCREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, age INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Alice', 20);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Bob', 21);\n`);\n\n// eslint-disable-next-line @typescript-eslint/no-use-before-define\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('${veryLongString}', 22);\n`);\n\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\n\nconst chain = new SqlDatabaseChain({\nllm: new OpenAI({ temperature: 0 }),\ndatabase: db,\n});\n\nconst runChain = async () => {\nawait chain.run(\"How many users are there?\");\n};\n\nawait expect(runChain()).rejects.toThrow();\n\nawait datasource.destroy();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":342,"to":379}}}}],["396",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { loadChain } from \"../load.js\";\nimport { StuffDocumentsChain } from \"../combine_docs_chain.js\";\nimport { VectorDBQAChain } from \"../vector_db_qa.js\";\nimport { HNSWLib } from \"../../vectorstores/hnswlib.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["397",{"pageContent":"test(\"Test VectorDBQAChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst llmChain = new LLMChain({ prompt, llm: model });\nconst combineDocsChain = new StuffDocumentsChain({\nllmChain,\ndocumentVariableName: \"foo\",\n});\nconst chain = new VectorDBQAChain({\ncombineDocumentsChain: combineDocsChain,\nvectorstore: vectorStore,\n});\nconst res = await chain.call({ query: \"What up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":58,"to":80}}}}],["398",{"pageContent":"test(\"Test VectorDBQAChain from LLM\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore);\nconst res = await chain.call({ query: \"What up\" });\nconsole.log({ res });\n});\n\ntest(\"Load chain from hub\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst chain = await loadChain(\"lc://chains/vector-db-qa/stuff/chain.json\", {\nvectorstore: vectorStore,\n});\nconst res = await chain.call({ query: \"what up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":121,"to":144}}}}],["399",{"pageContent":"import { BaseChain, ChainInputs } from \"./base.js\";\nimport { VectorStore } from \"../vectorstores/base.js\";\nimport { SerializedVectorDBQAChain } from \"./serde.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\nimport { ChainValues } from \"../schema/index.js\";\nimport { loadQAStuffChain } from \"./question_answering/load.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type LoadValues = Record<string, any>;\n\nexport interface VectorDBQAChainInput extends Omit<ChainInputs, \"memory\"> {\nvectorstore: VectorStore;\ncombineDocumentsChain: BaseChain;\nreturnSourceDocuments?: boolean;\nk?: number;\ninputKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/vector_db_qa.ts","loc":{"lines":{"from":1,"to":18}}}}],["400",{"pageContent":"class VectorDBQAChain extends BaseChain implements VectorDBQAChainInput {\nk = 4;\n\ninputKey = \"query\";\n\nget inputKeys() {\nreturn [this.inputKey];\n}\n\nget outputKeys() {\nreturn this.combineDocumentsChain.outputKeys.concat(\nthis.returnSourceDocuments ? [\"sourceDocuments\"] : []\n);\n}\n\nvectorstore: VectorStore;\n\ncombineDocumentsChain: BaseChain;\n\nreturnSourceDocuments = false;\n\nconstructor(fields: VectorDBQAChainInput) {\nsuper(fields);\nthis.vectorstore = fields.vectorstore;\nthis.combineDocumentsChain = fields.combineDocumentsChain;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.k = fields.k ?? this.k;\nthis.returnSourceDocuments =\nfields.returnSourceDocuments ?? this.returnSourceDocuments;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/vector_db_qa.ts","loc":{"lines":{"from":127,"to":156}}}}],["401",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Question key ${this.inputKey} not found.`);\n}\nconst question: string = values[this.inputKey];\nconst docs = await this.vectorstore.similaritySearch(question, this.k);\nconst inputs = { question, input_documents: docs };\nconst result = await this.combineDocumentsChain.call(\ninputs,\nrunManager?.getChild()\n);\nif (this.returnSourceDocuments) {\nreturn {\n...result,\nsourceDocuments: docs,\n};\n}\nreturn result;\n}\n\n_chainType() {\nreturn \"vector_db_qa\" as const;\n}\n\nstatic async deserialize(\ndata: SerializedVectorDBQAChain,\nvalues: LoadValues\n) {\nif (!(\"vectorstore\" in values)) {\nthrow new Error(\n`Need to pass in a vectorstore to deserialize VectorDBQAChain`\n);\n}\nconst { vectorstore } = values;\nif (!data.combine_documents_chain) {\nthrow new Error(\n`VectorDBQAChain must have combine_documents_chain in serialized data`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/vector_db_qa.ts","loc":{"lines":{"from":266,"to":308}}}}],["402",{"pageContent":"return new VectorDBQAChain({\ncombineDocumentsChain: await BaseChain.deserialize(\ndata.combine_documents_chain\n),\nk: data.k,\nvectorstore,\n});\n}\n\nserialize(): SerializedVectorDBQAChain {\nreturn {\n_type: this._chainType(),\ncombine_documents_chain: this.combineDocumentsChain.serialize(),\nk: this.k,\n};\n}\n\nstatic fromLLM(\nllm: BaseLanguageModel,\nvectorstore: VectorStore,\noptions?: Partial<\nOmit<VectorDBQAChainInput, \"combineDocumentsChain\" | \"vectorstore\">\n>\n): VectorDBQAChain {\nconst qaChain = loadQAStuffChain(llm);\nreturn new this({\nvectorstore,\ncombineDocumentsChain: qaChain,\n...options,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/vector_db_qa.ts","loc":{"lines":{"from":403,"to":434}}}}],["403",{"pageContent":"import {\nAI_PROMPT,\nHUMAN_PROMPT,\nClient as AnthropicApi,\nCompletionResponse,\nSamplingParameters,\n} from \"@anthropic-ai/sdk\";\nimport { BaseChatModel, BaseChatModelParams } from \"./base.js\";\nimport {\nAIChatMessage,\nBaseChatMessage,\nChatGeneration,\nChatResult,\nMessageType,\n} from \"../schema/index.js\";\nimport { CallbackManagerForLLMRun } from \"../callbacks/manager.js\";\n\nfunction getAnthropicPromptFromMessage(type: MessageType): string {\nswitch (type) {\ncase \"ai\":\nreturn AI_PROMPT;\ncase \"human\":\nreturn HUMAN_PROMPT;\ncase \"system\":\nreturn \"\";\ndefault:\nthrow new Error(`Unknown message type: ${type}`);\n}\n}\n\nconst DEFAULT_STOP_SEQUENCES = [HUMAN_PROMPT];\n\n/**\n* Input to AnthropicChat class.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":1,"to":35}}}}],["404",{"pageContent":"interface AnthropicInput {\n/** Amount of randomness injected into the response. Ranges\n* from 0 to 1. Use temp closer to 0 for analytical /\n* multiple choice, and temp closer to 1 for creative\n* and generative tasks.\n*/\ntemperature?: number;\n\n/** Only sample from the top K options for each subsequent\n* token. Used to remove \"long tail\" low probability\n* responses. Defaults to -1, which disables it.\n*/\ntopK?: number;\n\n/** Does nucleus sampling, in which we compute the\n* cumulative distribution over all the options for each\n* subsequent token in decreasing probability order and\n* cut it off once it reaches a particular probability\n* specified by top_p. Defaults to -1, which disables it.\n* Note that you should either alter temperature or top_p,\n* but not both.\n*/\ntopP?: number;\n\n/** A maximum number of tokens to generate before stopping. */\nmaxTokensToSample: number;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":292,"to":317}}}}],["405",{"pageContent":"/** A list of strings upon which to stop generating.\n* You probably want `[\"\\n\\nHuman:\"]`, as that's the cue for\n* the next turn in the dialog agent.\n*/\nstopSequences?: string[];\n\n/** Whether to stream the results or not */\nstreaming?: boolean;\n\n/** Anthropic API key */\napiKey?: string;\n\n/** Model name to use */\nmodelName: string;\n\n/** Holds any additional parameters that are valid to pass to {@link\n* https://console.anthropic.com/docs/api/reference |\n* `anthropic.complete`} that are not explicitly specified on this class.\n*/\ninvocationKwargs?: Kwargs;\n}\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":567,"to":589}}}}],["406",{"pageContent":"Kwargs = Record<string, any>;\n\n/**\n* Wrapper around Anthropic large language models.\n*\n* To use you should have the `@anthropic-ai/sdk` package installed, with the\n* `ANTHROPIC_API_KEY` environment variable set.\n*\n* @remarks\n* Any parameters that are valid to be passed to {@link\n* https://console.anthropic.com/docs/api/reference |\n* `anthropic.complete`} can be passed through {@link invocationKwargs},\n* even if not explicitly available on this class.\n*\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":850,"to":864}}}}],["407",{"pageContent":"class ChatAnthropic extends BaseChatModel implements AnthropicInput {\napiKey?: string;\n\ntemperature = 1;\n\ntopK = -1;\n\ntopP = -1;\n\nmaxTokensToSample = 2048;\n\nmodelName = \"claude-v1\";\n\ninvocationKwargs?: Kwargs;\n\nstopSequences?: string[];\n\nstreaming = false;\n\n// Used for non-streaming requests\nprivate batchClient: AnthropicApi;\n\n// Used for streaming requests\nprivate streamingClient: AnthropicApi;\n\nconstructor(\nfields?: Partial<AnthropicInput> &\nBaseChatModelParams & {\nanthropicApiKey?: string;\n}\n) {\nsuper(fields ?? {});\n\nthis.apiKey =\nfields?.anthropicApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.ANTHROPIC_API_KEY\n: undefined);\nif (!this.apiKey) {\nthrow new Error(\"Anthropic API key not found\");\n}\n\nthis.modelName = fields?.modelName ?? this.modelName;\nthis.invocationKwargs = fields?.invocationKwargs ?? {};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":1133,"to":1177}}}}],["408",{"pageContent":"this.temperature = fields?.temperature ?? this.temperature;\nthis.topK = fields?.topK ?? this.topK;\nthis.topP = fields?.topP ?? this.topP;\nthis.maxTokensToSample =\nfields?.maxTokensToSample ?? this.maxTokensToSample;\nthis.stopSequences = fields?.stopSequences ?? this.stopSequences;\n\nthis.streaming = fields?.streaming ?? false;\n}\n\n/**\n* Get the parameters used to invoke the model\n*/\ninvocationParams(): Omit<SamplingParameters, \"prompt\"> & Kwargs {\nreturn {\nmodel: this.modelName,\ntemperature: this.temperature,\ntop_k: this.topK,\ntop_p: this.topP,\nstop_sequences: this.stopSequences ?? DEFAULT_STOP_SEQUENCES,\nmax_tokens_to_sample: this.maxTokensToSample,\nstream: this.streaming,\n...this.invocationKwargs,\n};\n}\n\n/** @ignore */\n_identifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n};\n}\n\n/**\n* Get the identifying parameters for the model\n*/\nidentifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":1427,"to":1469}}}}],["409",{"pageContent":"private formatMessagesAsPrompt(messages: BaseChatMessage[]): string {\nreturn (\nmessages\n.map((message) => {\nconst messagePrompt = getAnthropicPromptFromMessage(\nmessage._getType()\n);\nreturn `${messagePrompt} ${message.text}`;\n})\n.join(\"\") + AI_PROMPT\n);\n}\n\n/** @ignore */\nasync _generate(\nmessages: BaseChatMessage[],\nstopSequences?: string[],\nrunManager?: CallbackManagerForLLMRun\n): Promise<ChatResult> {\nif (this.stopSequences && stopSequences) {\nthrow new Error(\n`\"stopSequence\" parameter found in input and default params`\n);\n}\n\nconst params = this.invocationParams();\nparams.stop_sequences = stopSequences\n? stopSequences.concat(DEFAULT_STOP_SEQUENCES)\n: params.stop_sequences;\n\nconst response = await this.completionWithRetry(\n{\n...params,\nprompt: this.formatMessagesAsPrompt(messages),\n},\nrunManager\n);\n\nconst generations: ChatGeneration[] = response.completion\n.split(AI_PROMPT)\n.map((message) => ({\ntext: message,\nmessage: new AIChatMessage(message),\n}));\n\nreturn {\ngenerations,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":1718,"to":1766}}}}],["410",{"pageContent":"/** @ignore */\nasync completionWithRetry(\nrequest: SamplingParameters & Kwargs,\nrunManager?: CallbackManagerForLLMRun\n): Promise<CompletionResponse> {\nif (!this.apiKey) {\nthrow new Error(\"Missing Anthropic API key.\");\n}\nlet makeCompletionRequest;\nif (request.stream) {\nif (!this.streamingClient) {\nthis.streamingClient = new AnthropicApi(this.apiKey);\n}\nmakeCompletionRequest = async () => {\nlet currentCompletion = \"\";\nreturn this.streamingClient.completeStream(request, {\nonUpdate: (data: CompletionResponse) => {\nif (data.stop_reason) {\nreturn;\n}\nconst part = data.completion;\nif (part) {\nconst delta = part.slice(currentCompletion.length);\ncurrentCompletion += delta ?? \"\";\n// eslint-disable-next-line no-void\nvoid runManager?.handleLLMNewToken(delta ?? \"\");\n}\n},\n});\n};\n} else {\nif (!this.batchClient) {\nthis.batchClient = new AnthropicApi(this.apiKey);\n}\nmakeCompletionRequest = async () => this.batchClient.complete(request);\n}\nreturn this.caller.call(makeCompletionRequest);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":2014,"to":2051}}}}],["411",{"pageContent":"_llmType() {\nreturn \"anthropic\";\n}\n\n/** @ignore */\n_combineLLMOutput() {\nreturn [];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":2299,"to":2307}}}}],["412",{"pageContent":"import {\nAIChatMessage,\nBaseChatMessage,\nBasePromptValue,\nChatGeneration,\nChatResult,\nLLMResult,\nRUN_KEY,\n} from \"../schema/index.js\";\nimport {\nBaseLanguageModel,\nBaseLanguageModelCallOptions,\nBaseLanguageModelParams,\n} from \"../base_language/index.js\";\nimport { getBufferString } from \"../memory/base.js\";\nimport {\nCallbackManager,\nCallbackManagerForLLMRun,\nCallbacks,\n} from \"../callbacks/manager.js\";\n\nexport type SerializedChatModel = {\n_model: string;\n_type: string;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\n// todo?\nexport type SerializedLLM = {\n_model: string;\n_type: string;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\nexport type BaseChatModelParams = BaseLanguageModelParams;\n\nexport type BaseChatModelCallOptions = BaseLanguageModelCallOptions;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":1,"to":37}}}}],["413",{"pageContent":"abstract class BaseChatModel extends BaseLanguageModel {\ndeclare CallOptions: BaseChatModelCallOptions;\n\nconstructor(fields: BaseChatModelParams) {\nsuper(fields);\n}\n\nabstract _combineLLMOutput?(\n...llmOutputs: LLMResult[\"llmOutput\"][]\n): LLMResult[\"llmOutput\"];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":166,"to":175}}}}],["414",{"pageContent":"async generate(\nmessages: BaseChatMessage[][],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult> {\nconst generations: ChatGeneration[][] = [];\nconst llmOutputs: LLMResult[\"llmOutput\"][] = [];\nconst messageStrings: string[] = messages.map((messageList) =>\ngetBufferString(messageList)\n);\nconst callbackManager_ = await CallbackManager.configure(\ncallbacks,\nthis.callbacks,\n{ verbose: this.verbose }\n);\nconst runManager = await callbackManager_?.handleLLMStart(\n{ name: this._llmType() },\nmessageStrings\n);\ntry {\nconst results = await Promise.all(\nmessages.map((messageList) =>\nthis._generate(messageList, stop, runManager)\n)\n);\nfor (const result of results) {\nif (result.llmOutput) {\nllmOutputs.push(result.llmOutput);\n}\ngenerations.push(result.generations);\n}\n} catch (err) {\nawait runManager?.handleLLMError(err);\nthrow err;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":327,"to":361}}}}],["415",{"pageContent":"const output: LLMResult = {\ngenerations,\nllmOutput: llmOutputs.length\n? this._combineLLMOutput?.(...llmOutputs)\n: undefined,\n};\nawait runManager?.handleLLMEnd(output);\nObject.defineProperty(output, RUN_KEY, {\nvalue: runManager ? { runId: runManager?.runId } : undefined,\nconfigurable: true,\n});\nreturn output;\n}\n\n_modelType(): string {\nreturn \"base_chat_model\" as const;\n}\n\nabstract _llmType(): string;\n\nasync generatePrompt(\npromptValues: BasePromptValue[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult> {\nconst promptMessages: BaseChatMessage[][] = promptValues.map(\n(promptValue) => promptValue.toChatMessages()\n);\nreturn this.generate(promptMessages, stop, callbacks);\n}\n\nabstract _generate(\nmessages: BaseChatMessage[],\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<ChatResult>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":490,"to":525}}}}],["416",{"pageContent":"async call(\nmessages: BaseChatMessage[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<BaseChatMessage> {\nconst result = await this.generate([messages], stop, callbacks);\nconst generations = result.generations as ChatGeneration[][];\nreturn generations[0][0].message;\n}\n\nasync callPrompt(\npromptValue: BasePromptValue,\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<BaseChatMessage> {\nconst promptMessages: BaseChatMessage[] = promptValue.toChatMessages();\nreturn this.call(promptMessages, stop, callbacks);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":654,"to":672}}}}],["417",{"pageContent":"abstract class SimpleChatModel extends BaseChatModel {\nabstract _call(\nmessages: BaseChatMessage[],\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<string>;\n\nasync _generate(\nmessages: BaseChatMessage[],\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<ChatResult> {\nconst text = await this._call(messages, stop, runManager);\nconst message = new AIChatMessage(text);\nreturn {\ngenerations: [\n{\ntext: message.text,\nmessage,\n},\n],\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":813,"to":836}}}}],["418",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/chat_models' is deprecated. Import from eg. 'langchain/chat_models/openai' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport { BaseChatModel, BaseChatModelParams, SimpleChatModel } from \"./base.js\";\nexport { ChatOpenAI } from \"./openai.js\";\nexport { ChatAnthropic } from \"./anthropic.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/index.ts","loc":{"lines":{"from":1,"to":7}}}}],["419",{"pageContent":"import {\nConfiguration,\nOpenAIApi,\nCreateChatCompletionRequest,\nConfigurationParameters,\nCreateChatCompletionResponse,\nChatCompletionResponseMessageRoleEnum,\nChatCompletionRequestMessage,\n} from \"openai\";\nimport {\nAzureOpenAIInput,\nOpenAICallOptions,\nOpenAIChatInput,\n} from \"types/open-ai-types.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport type { StreamingAxiosConfiguration } from \"../util/axios-types.js\";\nimport { BaseChatModel, BaseChatModelParams } from \"./base.js\";\nimport {\nAIChatMessage,\nBaseChatMessage,\nChatGeneration,\nChatMessage,\nChatResult,\nHumanChatMessage,\nMessageType,\nSystemChatMessage,\n} from \"../schema/index.js\";\nimport { getModelNameForTiktoken } from \"../base_language/count_tokens.js\";\nimport { CallbackManagerForLLMRun } from \"../callbacks/manager.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":1,"to":29}}}}],["420",{"pageContent":"{ OpenAICallOptions, OpenAIChatInput, AzureOpenAIInput };\n\ninterface TokenUsage {\ncompletionTokens?: number;\npromptTokens?: number;\ntotalTokens?: number;\n}\n\ninterface OpenAILLMOutput {\ntokenUsage: TokenUsage;\n}\n\nfunction messageTypeToOpenAIRole(\ntype: MessageType\n): ChatCompletionResponseMessageRoleEnum {\nswitch (type) {\ncase \"system\":\nreturn \"system\";\ncase \"ai\":\nreturn \"assistant\";\ncase \"human\":\nreturn \"user\";\ndefault:\nthrow new Error(`Unknown message type: ${type}`);\n}\n}\n\nfunction openAIResponseToChatMessage(\nrole: ChatCompletionResponseMessageRoleEnum | undefined,\ntext: string\n): BaseChatMessage {\nswitch (role) {\ncase \"user\":\nreturn new HumanChatMessage(text);\ncase \"assistant\":\nreturn new AIChatMessage(text);\ncase \"system\":\nreturn new SystemChatMessage(text);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":507,"to":544}}}}],["421",{"pageContent":":\nreturn new ChatMessage(text, role ?? \"unknown\");\n}\n}\n\n/**\n* Wrapper around OpenAI large language models that use the Chat endpoint.\n*\n* To use you should have the `openai` package installed, with the\n* `OPENAI_API_KEY` environment variable set.\n*\n* To use with Azure you should have the `openai` package installed, with the\n* `AZURE_OPENAI_API_KEY`,\n* `AZURE_OPENAI_API_INSTANCE_NAME`,\n* `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n* and `AZURE_OPENAI_API_VERSION` environment variable set.\n*\n* @remarks\n* Any parameters that are valid to be passed to {@link\n* https://platform.openai.com/docs/api-reference/chat/create |\n* `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n* if not explicitly available on this class.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":1023,"to":1045}}}}],["422",{"pageContent":"class ChatOpenAI\nextends BaseChatModel\nimplements OpenAIChatInput, AzureOpenAIInput\n{\ndeclare CallOptions: OpenAICallOptions;\n\ntemperature = 1;\n\ntopP = 1;\n\nfrequencyPenalty = 0;\n\npresencePenalty = 0;\n\nn = 1;\n\nlogitBias?: Record<string, number>;\n\nmodelName = \"gpt-3.5-turbo\";\n\nmodelKwargs?: OpenAIChatInput[\"modelKwargs\"];\n\nstop?: string[];\n\ntimeout?: number;\n\nstreaming = false;\n\nmaxTokens?: number;\n\nazureOpenAIApiVersion?: string;\n\nazureOpenAIApiKey?: string;\n\nazureOpenAIApiInstanceName?: string;\n\nazureOpenAIApiDeploymentName?: string;\n\nprivate client: OpenAIApi;\n\nprivate clientConfig: ConfigurationParameters;\n\nconstructor(\nfields?: Partial<OpenAIChatInput> &\nPartial<AzureOpenAIInput> &\nBaseChatModelParams & {\nconcurrency?: number;\ncache?: boolean;\nopenAIApiKey?: string;\n},\nconfiguration?: ConfigurationParameters\n) {\nsuper(fields ?? {});\n\nconst apiKey =\nfields?.openAIApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.OPENAI_API_KEY\n: undefined);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":1524,"to":1583}}}}],["423",{"pageContent":"const azureApiKey =\nfields?.azureOpenAIApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_KEY\n: undefined);\nif (!azureApiKey && !apiKey) {\nthrow new Error(\"(Azure) OpenAI API key not found\");\n}\n\nconst azureApiInstanceName =\nfields?.azureOpenAIApiInstanceName ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_INSTANCE_NAME\n: undefined);\n\nconst azureApiDeploymentName =\nfields?.azureOpenAIApiDeploymentName ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_DEPLOYMENT_NAME\n: undefined);\n\nconst azureApiVersion =\nfields?.azureOpenAIApiVersion ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_VERSION\n: undefined);\n\nthis.modelName = fields?.modelName ?? this.modelName;\nthis.modelKwargs = fields?.modelKwargs ?? {};\nthis.timeout = fields?.timeout;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":2054,"to":2087}}}}],["424",{"pageContent":"this.temperature = fields?.temperature ?? this.temperature;\nthis.topP = fields?.topP ?? this.topP;\nthis.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\nthis.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\nthis.maxTokens = fields?.maxTokens;\nthis.n = fields?.n ?? this.n;\nthis.logitBias = fields?.logitBias;\nthis.stop = fields?.stop;\n\nthis.streaming = fields?.streaming ?? false;\n\nthis.azureOpenAIApiVersion = azureApiVersion;\nthis.azureOpenAIApiKey = azureApiKey;\nthis.azureOpenAIApiInstanceName = azureApiInstanceName;\nthis.azureOpenAIApiDeploymentName = azureApiDeploymentName;\n\nif (this.streaming && this.n > 1) {\nthrow new Error(\"Cannot stream results when n > 1\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":2558,"to":2576}}}}],["425",{"pageContent":"if (this.azureOpenAIApiKey) {\nif (!this.azureOpenAIApiInstanceName) {\nthrow new Error(\"Azure OpenAI API instance name not found\");\n}\nif (!this.azureOpenAIApiDeploymentName) {\nthrow new Error(\"Azure OpenAI API deployment name not found\");\n}\nif (!this.azureOpenAIApiVersion) {\nthrow new Error(\"Azure OpenAI API version not found\");\n}\n}\n\nthis.clientConfig = {\napiKey,\n...configuration,\n};\n}\n\n/**\n* Get the parameters used to invoke the model\n*/\ninvocationParams(): Omit<CreateChatCompletionRequest, \"messages\"> {\nreturn {\nmodel: this.modelName,\ntemperature: this.temperature,\ntop_p: this.topP,\nfrequency_penalty: this.frequencyPenalty,\npresence_penalty: this.presencePenalty,\nmax_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\nn: this.n,\nlogit_bias: this.logitBias,\nstop: this.stop,\nstream: this.streaming,\n...this.modelKwargs,\n};\n}\n\n/** @ignore */\n_identifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n...this.clientConfig,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":3055,"to":3099}}}}],["426",{"pageContent":"/**\n* Get the identifying parameters for the model\n*/\nidentifyingParams() {\nreturn this._identifyingParams();\n}\n\n/** @ignore */\nasync _generate(\nmessages: BaseChatMessage[],\nstopOrOptions?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<ChatResult> {\nconst stop = Array.isArray(stopOrOptions)\n? stopOrOptions\n: stopOrOptions?.stop;\nconst options = Array.isArray(stopOrOptions)\n? {}\n: stopOrOptions?.options ?? {};\nconst tokenUsage: TokenUsage = {};\nif (this.stop && stop) {\nthrow new Error(\"Stop found in input and default params\");\n}\n\nconst params = this.invocationParams();\nparams.stop = stop ?? params.stop;\nconst messagesMapped: ChatCompletionRequestMessage[] = messages.map(\n(message) => ({\nrole: messageTypeToOpenAIRole(message._getType()),\ncontent: message.text,\nname: message.name,\n})\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":3571,"to":3603}}}}],["427",{"pageContent":"const data = params.stream\n? await new Promise<CreateChatCompletionResponse>((resolve, reject) => {\nlet response: CreateChatCompletionResponse;\nlet rejected = false;\nthis.completionWithRetry(\n{\n...params,\nmessages: messagesMapped,\n},\n{\n...options,\nresponseType: \"stream\",\nonmessage: (event) => {\nif (event.data?.trim?.() === \"[DONE]\") {\nresolve(response);\n} else {\nconst message = JSON.parse(event.data) as {\nid: string;\nobject: string;\ncreated: number;\nmodel: string;\nchoices: Array<{\nindex: number;\nfinish_reason: string | null;\ndelta: { content?: string; role?: string };\n}>;\n};\n\n// on the first message set the response properties\nif (!response) {\nresponse = {\nid: message.id,\nobject: message.object,\ncreated: message.created,\nmodel: message.model,\nchoices: [],\n};\n}\n\n// on all messages, update choice\nconst part = message.choices[0];\nif (part != null) {\nlet choice = response.choices.find(\n(c) => c.index === part.index\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":4081,"to":4125}}}}],["428",{"pageContent":"if (!choice) {\nchoice = {\nindex: part.index,\nfinish_reason: part.finish_reason ?? undefined,\n};\nresponse.choices.push(choice);\n}\n\nif (!choice.message) {\nchoice.message = {\nrole: part.delta\n?.role as ChatCompletionResponseMessageRoleEnum,\ncontent: part.delta?.content ?? \"\",\n};\n}\n\nchoice.message.content += part.delta?.content ?? \"\";\n// eslint-disable-next-line no-void\nvoid runManager?.handleLLMNewToken(\npart.delta?.content ?? \"\"\n);\n}\n}\n},\n}\n).catch((error) => {\nif (!rejected) {\nrejected = true;\nreject(error);\n}\n});\n})\n: await this.completionWithRetry(\n{\n...params,\nmessages: messagesMapped,\n},\noptions\n);\n\nconst {\ncompletion_tokens: completionTokens,\nprompt_tokens: promptTokens,\ntotal_tokens: totalTokens,\n} = data.usage ?? {};\n\nif (completionTokens) {\ntokenUsage.completionTokens =\n(tokenUsage.completionTokens ?? 0) + completionTokens;\n}\n\nif (promptTokens) {\ntokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":4599,"to":4652}}}}],["429",{"pageContent":"if (totalTokens) {\ntokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n}\n\nconst generations: ChatGeneration[] = [];\nfor (const part of data.choices) {\nconst role = part.message?.role ?? undefined;\nconst text = part.message?.content ?? \"\";\ngenerations.push({\ntext,\nmessage: openAIResponseToChatMessage(role, text),\n});\n}\nreturn {\ngenerations,\nllmOutput: { tokenUsage },\n};\n}\n\nasync getNumTokensFromMessages(messages: BaseChatMessage[]): Promise<{\ntotalCount: number;\ncountPerMessage: number[];\n}> {\nlet totalCount = 0;\nlet tokensPerMessage = 0;\nlet tokensPerName = 0;\n\n// From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\nif (getModelNameForTiktoken(this.modelName) === \"gpt-3.5-turbo\") {\ntokensPerMessage = 4;\ntokensPerName = -1;\n} else if (getModelNameForTiktoken(this.modelName).startsWith(\"gpt-4\")) {\ntokensPerMessage = 3;\ntokensPerName = 1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":5125,"to":5159}}}}],["430",{"pageContent":"const countPerMessage = await Promise.all(\nmessages.map(async (message) => {\nconst textCount = await this.getNumTokens(message.text);\nconst count =\ntextCount + tokensPerMessage + (message.name ? tokensPerName : 0);\n\ntotalCount += count;\nreturn count;\n})\n);\n\nreturn { totalCount, countPerMessage };\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":5633,"to":5645}}}}],["431",{"pageContent":"/** @ignore */\nasync completionWithRetry(\nrequest: CreateChatCompletionRequest,\noptions?: StreamingAxiosConfiguration\n) {\nif (!this.client) {\nconst endpoint = this.azureOpenAIApiKey\n? `https://${this.azureOpenAIApiInstanceName}.openai.azure.com/openai/deployments/${this.azureOpenAIApiDeploymentName}`\n: this.clientConfig.basePath;\nconst clientConfig = new Configuration({\n...this.clientConfig,\nbasePath: endpoint,\nbaseOptions: {\ntimeout: this.timeout,\nadapter: fetchAdapter,\n...this.clientConfig.baseOptions,\n},\n});\nthis.client = new OpenAIApi(clientConfig);\n}\nconst axiosOptions = (options ?? {}) as StreamingAxiosConfiguration &\nOpenAICallOptions;\nif (this.azureOpenAIApiKey) {\naxiosOptions.headers = {\n\"api-key\": this.azureOpenAIApiKey,\n...axiosOptions.headers,\n};\naxiosOptions.params = {\n\"api-version\": this.azureOpenAIApiVersion,\n...axiosOptions.params,\n};\n}\nreturn this.caller\n.call(\nthis.client.createChatCompletion.bind(this.client),\nrequest,\naxiosOptions\n)\n.then((res) => res.data);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":6138,"to":6177}}}}],["432",{"pageContent":"_llmType() {\nreturn \"openai\";\n}\n\n/** @ignore */\n_combineLLMOutput(...llmOutputs: OpenAILLMOutput[]): OpenAILLMOutput {\nreturn llmOutputs.reduce<{\n[key in keyof OpenAILLMOutput]: Required<OpenAILLMOutput[key]>;\n}>(\n(acc, llmOutput) => {\nif (llmOutput && llmOutput.tokenUsage) {\nacc.tokenUsage.completionTokens +=\nllmOutput.tokenUsage.completionTokens ?? 0;\nacc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\nacc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n}\nreturn acc;\n},\n{\ntokenUsage: {\ncompletionTokens: 0,\npromptTokens: 0,\ntotalTokens: 0,\n},\n}\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":6648,"to":6675}}}}],["433",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { HumanChatMessage } from \"../../schema/index.js\";\nimport { ChatPromptValue } from \"../../prompts/chat.js\";\nimport {\nPromptTemplate,\nChatPromptTemplate,\nAIMessagePromptTemplate,\nHumanMessagePromptTemplate,\nSystemMessagePromptTemplate,\n} from \"../../prompts/index.js\";\nimport { ChatAnthropic } from \"../anthropic.js\";\nimport { CallbackManager } from \"../../callbacks/index.js\";\n\ntest(\"Test ChatAnthropic\", async () => {\nconst chat = new ChatAnthropic({ modelName: \"claude-instant-v1\" });\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.call([message]);\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatanthropic.int.test.ts","loc":{"lines":{"from":1,"to":19}}}}],["434",{"pageContent":"test(\"Test ChatAnthropic Generate\", async () => {\nconst chat = new ChatAnthropic({\nmodelName: \"claude-instant-v1\",\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.generate([[message], [message]]);\nexpect(res.generations.length).toBe(2);\nfor (const generation of res.generations) {\nexpect(generation.length).toBe(1);\nfor (const message of generation) {\nconsole.log(message.text);\n}\n}\nconsole.log({ res });\n});\n\ntest(\"Test ChatAnthropic tokenUsage with a batch\", async () => {\nconst model = new ChatAnthropic({\ntemperature: 0,\nmodelName: \"claude-instant-v1\",\n});\nconst res = await model.generate([\n[new HumanChatMessage(`Hello!`)],\n[new HumanChatMessage(`Hi!`)],\n]);\nconsole.log({ res });\n});\n\ntest(\"Test ChatAnthropic in streaming mode\", async () => {\nlet nrNewTokens = 0;\nlet streamedCompletion = \"\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatanthropic.int.test.ts","loc":{"lines":{"from":132,"to":162}}}}],["435",{"pageContent":"const model = new ChatAnthropic({\nmodelName: \"claude-instant-v1\",\nstreaming: true,\ncallbacks: CallbackManager.fromHandlers({\nasync handleLLMNewToken(token: string) {\nnrNewTokens += 1;\nstreamedCompletion += token;\n},\n}),\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await model.call([message]);\nconsole.log({ res });\n\nexpect(nrNewTokens > 0).toBe(true);\nexpect(res.text).toBe(streamedCompletion);\n});\n\ntest(\"Test ChatAnthropic prompt value\", async () => {\nconst chat = new ChatAnthropic({\nmodelName: \"claude-instant-v1\",\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.generatePrompt([new ChatPromptValue([message])]);\nexpect(res.generations.length).toBe(1);\nfor (const generation of res.generations) {\nfor (const g of generation) {\nconsole.log(g.text);\n}\n}\nconsole.log({ res });\n});\n\ntest(\"ChatAnthropic, docs, prompt templates\", async () => {\nconst chat = new ChatAnthropic({\nmodelName: \"claude-instant-v1\",\ntemperature: 0,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatanthropic.int.test.ts","loc":{"lines":{"from":268,"to":305}}}}],["436",{"pageContent":"const systemPrompt = PromptTemplate.fromTemplate(\n\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n);\n\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nnew SystemMessagePromptTemplate(systemPrompt),\nHumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n]);\n\nconst responseA = await chat.generatePrompt([\nawait chatPrompt.formatPromptValue({\ninput_language: \"English\",\noutput_language: \"French\",\ntext: \"I love programming.\",\n}),\n]);\n\nconsole.log(responseA.generations);\n});\n\ntest(\"ChatAnthropic, longer chain of messages\", async () => {\nconst chat = new ChatAnthropic({\nmodelName: \"claude-instant-v1\",\ntemperature: 0,\n});\n\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nHumanMessagePromptTemplate.fromTemplate(`Hi, my name is Joe!`),\nAIMessagePromptTemplate.fromTemplate(`Nice to meet you, Joe!`),\nHumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatanthropic.int.test.ts","loc":{"lines":{"from":408,"to":438}}}}],["437",{"pageContent":"const responseA = await chat.generatePrompt([\nawait chatPrompt.formatPromptValue({\ntext: \"What did I say my name was?\",\n}),\n]);\n\nconsole.log(responseA.generations);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatanthropic.int.test.ts","loc":{"lines":{"from":543,"to":550}}}}],["438",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { ChatOpenAI } from \"../openai.js\";\nimport {\nHumanChatMessage,\nLLMResult,\nSystemChatMessage,\n} from \"../../schema/index.js\";\nimport { ChatPromptValue } from \"../../prompts/chat.js\";\nimport {\nPromptTemplate,\nChatPromptTemplate,\nHumanMessagePromptTemplate,\nSystemMessagePromptTemplate,\n} from \"../../prompts/index.js\";\nimport { CallbackManager } from \"../../callbacks/index.js\";\n\ntest(\"Test ChatOpenAI\", async () => {\nconst chat = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\", maxTokens: 10 });\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.call([message]);\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":1,"to":22}}}}],["439",{"pageContent":"test(\"Test ChatOpenAI with SystemChatMessage\", async () => {\nconst chat = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\", maxTokens: 10 });\nconst system_message = new SystemChatMessage(\"You are to chat with a user.\");\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.call([system_message, message]);\nconsole.log({ res });\n});\n\ntest(\"Test ChatOpenAI Generate\", async () => {\nconst chat = new ChatOpenAI({\nmodelName: \"gpt-3.5-turbo\",\nmaxTokens: 10,\nn: 2,\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.generate([[message], [message]]);\nexpect(res.generations.length).toBe(2);\nfor (const generation of res.generations) {\nexpect(generation.length).toBe(2);\nfor (const message of generation) {\nconsole.log(message.text);\n}\n}\nconsole.log({ res });\n});\n\ntest(\"Test ChatOpenAI tokenUsage\", async () => {\nlet tokenUsage = {\ncompletionTokens: 0,\npromptTokens: 0,\ntotalTokens: 0,\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":226,"to":257}}}}],["440",{"pageContent":"const model = new ChatOpenAI({\nmodelName: \"gpt-3.5-turbo\",\nmaxTokens: 10,\ncallbackManager: CallbackManager.fromHandlers({\nasync handleLLMEnd(output: LLMResult) {\ntokenUsage = output.llmOutput?.tokenUsage;\n},\n}),\n});\nconst message = new HumanChatMessage(\"Hello\");\nconst res = await model.call([message]);\nconsole.log({ res });\n\nexpect(tokenUsage.promptTokens).toBeGreaterThan(0);\n});\n\ntest(\"Test ChatOpenAI tokenUsage with a batch\", async () => {\nlet tokenUsage = {\ncompletionTokens: 0,\npromptTokens: 0,\ntotalTokens: 0,\n};\n\nconst model = new ChatOpenAI({\ntemperature: 0,\nmodelName: \"gpt-3.5-turbo\",\ncallbackManager: CallbackManager.fromHandlers({\nasync handleLLMEnd(output: LLMResult) {\ntokenUsage = output.llmOutput?.tokenUsage;\n},\n}),\n});\nconst res = await model.generate([\n[new HumanChatMessage(\"Hello\")],\n[new HumanChatMessage(\"Hi\")],\n]);\nconsole.log(res);\n\nexpect(tokenUsage.promptTokens).toBeGreaterThan(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":455,"to":494}}}}],["441",{"pageContent":"test(\"Test ChatOpenAI in streaming mode\", async () => {\nlet nrNewTokens = 0;\nlet streamedCompletion = \"\";\n\nconst model = new ChatOpenAI({\nmodelName: \"gpt-3.5-turbo\",\nstreaming: true,\nmaxTokens: 10,\ncallbacks: [\n{\nasync handleLLMNewToken(token: string) {\nnrNewTokens += 1;\nstreamedCompletion += token;\n},\n},\n],\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await model.call([message]);\nconsole.log({ res });\n\nexpect(nrNewTokens > 0).toBe(true);\nexpect(res.text).toBe(streamedCompletion);\n});\n\ntest(\"Test ChatOpenAI prompt value\", async () => {\nconst chat = new ChatOpenAI({\nmodelName: \"gpt-3.5-turbo\",\nmaxTokens: 10,\nn: 2,\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.generatePrompt([new ChatPromptValue([message])]);\nexpect(res.generations.length).toBe(1);\nfor (const generation of res.generations) {\nexpect(generation.length).toBe(2);\nfor (const g of generation) {\nconsole.log(g.text);\n}\n}\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":692,"to":733}}}}],["442",{"pageContent":"test(\"OpenAI Chat, docs, prompt templates\", async () => {\nconst chat = new ChatOpenAI({ temperature: 0, maxTokens: 10 });\n\nconst systemPrompt = PromptTemplate.fromTemplate(\n\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n);\n\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nnew SystemMessagePromptTemplate(systemPrompt),\nHumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n]);\n\nconst responseA = await chat.generatePrompt([\nawait chatPrompt.formatPromptValue({\ninput_language: \"English\",\noutput_language: \"French\",\ntext: \"I love programming.\",\n}),\n]);\n\nconsole.log(responseA.generations);\n}, 50000);\n\ntest(\"Test OpenAI with stop\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5 });\nconst res = await model.call(\n[new HumanChatMessage(\"Print hello world\")],\n[\"world\"]\n);\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":930,"to":960}}}}],["443",{"pageContent":"test(\"Test OpenAI with stop in object\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5 });\nconst res = await model.call([new HumanChatMessage(\"Print hello world\")], {\nstop: [\"world\"],\n});\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with timeout in call options\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5 });\nawait expect(() =>\nmodel.call([new HumanChatMessage(\"Print hello world\")], {\noptions: { timeout: 10 },\n})\n).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with timeout in call options and node adapter\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5 });\nawait expect(() =>\nmodel.call([new HumanChatMessage(\"Print hello world\")], {\noptions: { timeout: 10, adapter: undefined },\n})\n).rejects.toThrow();\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":1159,"to":1183}}}}],["444",{"pageContent":"test(\"Test OpenAI with signal in call options\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5 });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call([new HumanChatMessage(\"Print hello world\")], {\noptions: { signal: controller.signal },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with signal in call options and node adapter\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call([new HumanChatMessage(\"Print hello world\")], {\noptions: { signal: controller.signal, adapter: undefined },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":1383,"to":1409}}}}],["445",{"pageContent":"import { Document } from \"../document.js\";\n\nexport abstract class Docstore {\nabstract search(search: string): Document | string;\n\nabstract add(texts: Record<string, Document>): void;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/docstore/base.ts","loc":{"lines":{"from":1,"to":7}}}}],["446",{"pageContent":"export { Document } from \"../document.js\";\nexport { Docstore } from \"./base.js\";\nexport { InMemoryDocstore } from \"./in_memory.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/docstore/index.ts","loc":{"lines":{"from":1,"to":3}}}}],["447",{"pageContent":"import { Document } from \"../document.js\";\nimport { Docstore } from \"./base.js\";\n\nexport class InMemoryDocstore extends Docstore {\n_docs: Map<string, Document>;\n\nconstructor(docs?: Map<string, Document>) {\nsuper();\nthis._docs = docs ?? new Map();\n}\n\n/** Method for getting count of documents in _docs */\nget count() {\nreturn this._docs.size;\n}\n\nsearch(search: string): Document | string {\nreturn this._docs.get(search) ?? `ID ${search} not found.`;\n}\n\nadd(texts: Record<string, Document>): void {\nconst keys = [...this._docs.keys()];\nconst overlapping = Object.keys(texts).filter((x) => keys.includes(x));\n\nif (overlapping.length > 0) {\nthrow new Error(`Tried to add ids that already exist: ${overlapping}`);\n}\n\nfor (const [key, value] of Object.entries(texts)) {\nthis._docs.set(key, value);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/docstore/in_memory.ts","loc":{"lines":{"from":1,"to":33}}}}],["448",{"pageContent":"export interface DocumentInput<\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nMetadata extends Record<string, any> = Record<string, any>\n> {\npageContent: string;\n\nmetadata?: Metadata;\n}\n\n/**\n* Interface for interacting with a document.\n*/\nexport class Document<\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nMetadata extends Record<string, any> = Record<string, any>\n> implements DocumentInput\n{\npageContent: string;\n\nmetadata: Metadata;\n\nconstructor(fields: DocumentInput<Metadata>) {\nthis.pageContent = fields.pageContent\n? fields.pageContent.toString()\n: this.pageContent;\nthis.metadata = fields.metadata ?? ({} as Metadata);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document.ts","loc":{"lines":{"from":1,"to":28}}}}],["449",{"pageContent":"import {\nRecursiveCharacterTextSplitter,\nTextSplitter,\n} from \"../text_splitter.js\";\nimport { Document } from \"../document.js\";\n\nexport interface DocumentLoader {\nload(): Promise<Document[]>;\nloadAndSplit(textSplitter?: TextSplitter): Promise<Document[]>;\n}\n\nexport abstract class BaseDocumentLoader implements DocumentLoader {\nabstract load(): Promise<Document[]>;\n\nasync loadAndSplit(\nsplitter: TextSplitter = new RecursiveCharacterTextSplitter()\n): Promise<Document[]> {\nconst docs = await this.load();\nreturn splitter.splitDocuments(docs);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/base.ts","loc":{"lines":{"from":1,"to":21}}}}],["450",{"pageContent":"import type { readFile as ReadFileT } from \"node:fs/promises\";\nimport { Document } from \"../../document.js\";\nimport { getEnv } from \"../../util/env.js\";\nimport { BaseDocumentLoader } from \"../base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/buffer.ts","loc":{"lines":{"from":1,"to":4}}}}],["451",{"pageContent":"abstract class BufferLoader extends BaseDocumentLoader {\nconstructor(public filePathOrBlob: string | Blob) {\nsuper();\n}\n\nprotected abstract parse(\nraw: Buffer,\nmetadata: Document[\"metadata\"]\n): Promise<Document[]>;\n\npublic async load(): Promise<Document[]> {\nlet buffer: Buffer;\nlet metadata: Record<string, string>;\nif (typeof this.filePathOrBlob === \"string\") {\nconst { readFile } = await BufferLoader.imports();\nbuffer = await readFile(this.filePathOrBlob);\nmetadata = { source: this.filePathOrBlob };\n} else {\nbuffer = await this.filePathOrBlob\n.arrayBuffer()\n.then((ab) => Buffer.from(ab));\nmetadata = { source: \"blob\", blobType: this.filePathOrBlob.type };\n}\nreturn this.parse(buffer, metadata);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/buffer.ts","loc":{"lines":{"from":45,"to":69}}}}],["452",{"pageContent":"static async imports(): Promise<{\nreadFile: typeof ReadFileT;\n}> {\ntry {\nconst { readFile } = await import(\"node:fs/promises\");\nreturn { readFile };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n`Failed to load fs/promises. TextLoader available only on environment 'node'. It appears you are running environment '${getEnv()}'. See https://<link to docs> for alternatives.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/buffer.ts","loc":{"lines":{"from":93,"to":106}}}}],["453",{"pageContent":"import { TextLoader } from \"./text.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/csv.ts","loc":{"lines":{"from":1,"to":1}}}}],["454",{"pageContent":"/**\n* Loads a CSV file into a list of documents.\n* Each document represents one row of the CSV file.\n*\n* When `column` is not specified, each row is converted into a key/value pair\n* with each key/value pair outputted to a new line in the document's pageContent.\n*\n* @example\n* // CSV file:\n* // id,html\n* // 1,<i>Corruption discovered at the core of the Banking Clan!</i>\n* // 2,<i>Corruption discovered at the core of the Banking Clan!</i>\n*\n* const loader = new CSVLoader(\"path/to/file.csv\");\n* const docs = await loader.load();\n*\n* // docs[0].pageContent:\n* // id: 1\n* // html: <i>Corruption discovered at the core of the Banking Clan!</i>\n*\n* When `column` is specified, one document is created for each row, and the\n* value of the specified column is used as the document's pageContent.\n*\n* @example\n* // CSV file:\n* // id,html\n* // 1,<i>Corruption discovered at the core of the Banking Clan!</i>\n* // 2,<i>Corruption discovered at the core of the Banking Clan!</i>\n*","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/csv.ts","loc":{"lines":{"from":76,"to":104}}}}],["455",{"pageContent":"* const loader = new CSVLoader(\"path/to/file.csv\", \"html\");\n* const docs = await loader.load();\n*\n* // docs[0].pageContent:\n* // <i>Corruption discovered at the core of the Banking Clan!</i>\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/csv.ts","loc":{"lines":{"from":150,"to":155}}}}],["456",{"pageContent":"class CSVLoader extends TextLoader {\nconstructor(filePathOrBlob: string | Blob, public column?: string) {\nsuper(filePathOrBlob);\n}\n\nprotected async parse(raw: string): Promise<string[]> {\nconst { csvParse } = await CSVLoaderImports();\nconst parsed = csvParse(raw.trim());\nconst { column } = this;\n\nif (column !== undefined) {\nif (!parsed.columns.includes(column)) {\nthrow new Error(`Column ${column} not found in CSV file.`);\n}\n\n// Note TextLoader will raise an exception if the value is null.\n// eslint-disable-next-line @typescript-eslint/no-non-null-assertion\nreturn parsed.map((row) => row[column]!);\n}\n\nreturn parsed.map((row) =>\nObject.keys(row)\n.map((key) => `${key.trim()}: ${row[key]?.trim()}`)\n.join(\"\\n\")\n);\n}\n}\n\nasync function CSVLoaderImports() {\ntry {\nconst { csvParse } = await import(\"d3-dsv\");\nreturn { csvParse };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Please install d3-dsv as a dependency with, e.g. `yarn add d3-dsv@2`\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/csv.ts","loc":{"lines":{"from":224,"to":262}}}}],["457",{"pageContent":"import type { extname as ExtnameT, resolve as ResolveT } from \"node:path\";\nimport type { readdir as ReaddirT } from \"node:fs/promises\";\nimport { Document } from \"../../document.js\";\nimport { getEnv } from \"../../util/env.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\n\n// TypeScript enums are not tree-shakeable, so doing this instead\n// See https://bargsten.org/jsts/enums/\nexport const UnknownHandling = {\nIgnore: \"ignore\",\nWarn: \"warn\",\nError: \"error\",\n} as const;\n// eslint-disable-next-line @typescript-eslint/no-redeclare\nexport type UnknownHandling =\n(typeof UnknownHandling)[keyof typeof UnknownHandling];\n\nexport interface LoadersMapping {\n[extension: string]: (filePath: string) => BaseDocumentLoader;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/directory.ts","loc":{"lines":{"from":1,"to":20}}}}],["458",{"pageContent":"class DirectoryLoader extends BaseDocumentLoader {\nconstructor(\npublic directoryPath: string,\npublic loaders: LoadersMapping,\npublic recursive: boolean = true,\npublic unknown: UnknownHandling = UnknownHandling.Warn\n) {\nsuper();\n\nif (Object.keys(loaders).length === 0) {\nthrow new Error(\"Must provide at least one loader\");\n}\nfor (const extension in loaders) {\nif (Object.hasOwn(loaders, extension)) {\nif (extension[0] !== \".\") {\nthrow new Error(`Extension must start with a dot: ${extension}`);\n}\n}\n}\n}\n\npublic async load(): Promise<Document[]> {\nconst { readdir, extname, resolve } = await DirectoryLoader.imports();\nconst files = await readdir(this.directoryPath, { withFileTypes: true });\n\nconst documents: Document[] = [];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/directory.ts","loc":{"lines":{"from":104,"to":129}}}}],["459",{"pageContent":"for (const file of files) {\nconst fullPath = resolve(this.directoryPath, file.name);\nif (file.isDirectory()) {\nif (this.recursive) {\nconst loader = new DirectoryLoader(\nfullPath,\nthis.loaders,\nthis.recursive,\nthis.unknown\n);\ndocuments.push(...(await loader.load()));\n}\n} else {\n// I'm aware some things won't be files,\n// but they will be caught by the \"unknown\" handling below.\nconst loaderFactory = this.loaders[extname(file.name)];\nif (loaderFactory) {\nconst loader = loaderFactory(fullPath);\ndocuments.push(...(await loader.load()));\n} else {\nswitch (this.unknown) {\ncase UnknownHandling.Ignore:\nbreak;\ncase UnknownHandling.Warn:\nconsole.warn(`Unknown file type: ${file.name}`);\nbreak;\ncase UnknownHandling.Error:\nthrow new Error(`Unknown file type: ${file.name}`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/directory.ts","loc":{"lines":{"from":213,"to":240}}}}],["460",{"pageContent":":\nthrow new Error(`Unknown unknown handling: ${this.unknown}`);\n}\n}\n}\n}\n\nreturn documents;\n}\n\nstatic async imports(): Promise<{\nreaddir: typeof ReaddirT;\nextname: typeof ExtnameT;\nresolve: typeof ResolveT;\n}> {\ntry {\nconst { extname, resolve } = await import(\"node:path\");\nconst { readdir } = await import(\"node:fs/promises\");\nreturn { readdir, extname, resolve };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n`Failed to load fs/promises. DirectoryLoader available only on environment 'node'. It appears you are running environment '${getEnv()}'. See https://<link to docs> for alternatives.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/directory.ts","loc":{"lines":{"from":321,"to":347}}}}],["461",{"pageContent":"import { Document } from \"../../document.js\";\nimport { BufferLoader } from \"./buffer.js\";\n\nexport class DocxLoader extends BufferLoader {\nconstructor(filePathOrBlob: string | Blob) {\nsuper(filePathOrBlob);\n}\n\npublic async parse(\nraw: Buffer,\nmetadata: Document[\"metadata\"]\n): Promise<Document[]> {\nconst { extractRawText } = await DocxLoaderImports();\nconst docx = await extractRawText({\nbuffer: raw,\n});\n\nif (!docx.value) return [];\n\nreturn [\nnew Document({\npageContent: docx.value,\nmetadata,\n}),\n];\n}\n}\n\nasync function DocxLoaderImports() {\ntry {\nconst { default: mod } = await import(\"mammoth\");\nconst { extractRawText } = mod;\nreturn { extractRawText };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Failed to load mammoth. Please install it with eg. `npm install mammoth`.\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/docx.ts","loc":{"lines":{"from":1,"to":40}}}}],["462",{"pageContent":"import type { EPub } from \"epub2\";\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/epub.ts","loc":{"lines":{"from":1,"to":3}}}}],["463",{"pageContent":"class EPubLoader extends BaseDocumentLoader {\nprivate splitChapters: boolean;\n\nconstructor(public filePath: string, { splitChapters = true } = {}) {\nsuper();\nthis.splitChapters = splitChapters;\n}\n\nprotected async parse(\nepub: EPub\n): Promise<{ pageContent: string; metadata?: object }[]> {\nconst { htmlToText } = await HtmlToTextImport();\nconst chapters = await Promise.all(\nepub.flow.map(async (chapter) => {\nif (!chapter.id) return null as never;\nconst html: string = await epub.getChapterRawAsync(chapter.id);\nif (!html) return null as never;\nreturn {\nhtml,\ntitle: chapter.title,\n};\n})\n);\nreturn chapters.filter(Boolean).map((chapter) => ({\npageContent: htmlToText(chapter.html),\nmetadata: {\n...(chapter.title && { chapter: chapter.title }),\n},\n}));\n}\n\npublic async load(): Promise<Document[]> {\nconst { EPub } = await EpubImport();\nconst epub = await EPub.createAsync(this.filePath);\n\nconst parsed = await this.parse(epub);\nconst metadata = { source: this.filePath };\n\nif (parsed.length === 0) return [];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/epub.ts","loc":{"lines":{"from":83,"to":121}}}}],["464",{"pageContent":"return this.splitChapters\n? parsed.map(\n(chapter) =>\nnew Document({\npageContent: chapter.pageContent,\nmetadata: {\n...metadata,\n...chapter.metadata,\n},\n})\n)\n: [\nnew Document({\npageContent: parsed\n.map((chapter) => chapter.pageContent)\n.join(\"\\n\\n\"),\nmetadata,\n}),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/epub.ts","loc":{"lines":{"from":173,"to":193}}}}],["465",{"pageContent":"EpubImport() {\nconst { EPub } = await import(\"epub2\").catch(() => {\nthrow new Error(\n\"Failed to load epub2. Please install it with eg. `npm install epub2`.\"\n);\n});\nreturn { EPub };\n}\n\nasync function HtmlToTextImport() {\nconst { htmlToText } = await import(\"html-to-text\").catch(() => {\nthrow new Error(\n\"Failed to load html-to-text. Please install it with eg. `npm install html-to-text`.\"\n);\n});\nreturn { htmlToText };\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/epub.ts","loc":{"lines":{"from":268,"to":284}}}}],["466",{"pageContent":"import jsonpointer from \"jsonpointer\";\nimport { TextLoader } from \"./text.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":1,"to":2}}}}],["467",{"pageContent":"class JSONLoader extends TextLoader {\npublic pointers: string[];\n\nconstructor(filePathOrBlob: string | Blob, pointers: string | string[] = []) {\nsuper(filePathOrBlob);\nthis.pointers = Array.isArray(pointers) ? pointers : [pointers];\n}\n\nprotected async parse(raw: string): Promise<string[]> {\nconst json = JSON.parse(raw.trim());\n// If there is no pointers specified we extract all strings we found\nconst extractAllStrings = !(this.pointers.length > 0);\nconst compiledPointers = this.pointers.map((pointer) =>\njsonpointer.compile(pointer)\n);\n\nreturn this.extractArrayStringsFromObject(\njson,\ncompiledPointers,\nextractAllStrings\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":133,"to":154}}}}],["468",{"pageContent":"/**\n* If JSON pointers are specified, return all strings below any of them\n* and exclude all other nodes expect if they match a JSON pointer (to allow to extract strings from different levels)\n*\n* If no JSON pointer is specified then return all string in the object\n*/\nprivate extractArrayStringsFromObject(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\njson: any,\npointers: jsonpointer[],\nextractAllStrings = false,\nkeyHasBeenFound = false\n): string[] {\nif (!json) {\nreturn [];\n}\n\nif (typeof json === \"string\" && extractAllStrings) {\nreturn [json];\n}\n\nif (Array.isArray(json) && extractAllStrings) {\nlet extractedString: string[] = [];\nfor (const element of json) {\nextractedString = extractedString.concat(\nthis.extractArrayStringsFromObject(element, pointers, true)\n);\n}\n\nreturn extractedString;\n}\n\nif (typeof json === \"object\") {\nif (extractAllStrings) {\nreturn this.extractArrayStringsFromObject(\nObject.values(json),\npointers,\ntrue\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":270,"to":309}}}}],["469",{"pageContent":"const targetedEntries = this.getTargetedEntries(json, pointers);\nconst thisLevelEntries = Object.values(json) as object[];\nconst notTargetedEntries = thisLevelEntries.filter(\n(entry: object) => !targetedEntries.includes(entry)\n);\n\nlet extractedStrings: string[] = [];\n// If we found a targeted entry, we extract all strings from it\nif (targetedEntries.length > 0) {\nfor (const oneEntry of targetedEntries) {\nextractedStrings = extractedStrings.concat(\nthis.extractArrayStringsFromObject(oneEntry, pointers, true, true)\n);\n}\n\nfor (const oneEntry of notTargetedEntries) {\nextractedStrings = extractedStrings.concat(\nthis.extractArrayStringsFromObject(oneEntry, pointers, false, true)\n);\n}\n} else if (extractAllStrings || !keyHasBeenFound) {\nfor (const oneEntry of notTargetedEntries) {\nextractedStrings = extractedStrings.concat(\nthis.extractArrayStringsFromObject(\noneEntry,\npointers,\nextractAllStrings\n)\n);\n}\n}\n\nreturn extractedStrings;\n}\n\nreturn [];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":413,"to":449}}}}],["470",{"pageContent":"private getTargetedEntries(json: object, pointers: jsonpointer[]): object[] {\nconst targetEntries = [];\nfor (const pointer of pointers) {\nconst targetedEntry = pointer.get(json);\nif (targetedEntry) {\ntargetEntries.push(targetedEntry);\n}\n}\n\nreturn targetEntries;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":553,"to":564}}}}],["471",{"pageContent":"class JSONLinesLoader extends TextLoader {\nconstructor(filePathOrBlob: string | Blob, public pointer: string) {\nsuper(filePathOrBlob);\n}\n\nprotected async parse(raw: string): Promise<string[]> {\nconst lines = raw.split(\"\\n\");\nconst jsons = lines\n.map((line) => line.trim())\n.filter(Boolean)\n.map((line) => JSON.parse(line));\nconst pointer = jsonpointer.compile(this.pointer);\nreturn jsons.map((json) => pointer.get(json));\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":688,"to":702}}}}],["472",{"pageContent":"import { DirectoryLoader, UnknownHandling } from \"./directory.js\";\nimport { TextLoader } from \"./text.js\";\n\nexport class NotionLoader extends DirectoryLoader {\nconstructor(directoryPath: string) {\nsuper(\ndirectoryPath,\n{\n\".md\": (filePath) => new TextLoader(filePath),\n},\ntrue,\nUnknownHandling.Ignore\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/notion.ts","loc":{"lines":{"from":1,"to":15}}}}],["473",{"pageContent":"import type { TextItem } from \"pdf-parse/lib/pdf.js/v1.10.100/build/pdf.js\";\nimport { Document } from \"../../document.js\";\nimport { BufferLoader } from \"./buffer.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/pdf.ts","loc":{"lines":{"from":1,"to":3}}}}],["474",{"pageContent":"class PDFLoader extends BufferLoader {\nprivate splitPages: boolean;\n\nprivate pdfjs: typeof PDFLoaderImports;\n\nconstructor(\nfilePathOrBlob: string | Blob,\n{ splitPages = true, pdfjs = PDFLoaderImports } = {}\n) {\nsuper(filePathOrBlob);\nthis.splitPages = splitPages;\nthis.pdfjs = pdfjs;\n}\n\npublic async parse(\nraw: Buffer,\nmetadata: Document[\"metadata\"]\n): Promise<Document[]> {\nconst { getDocument, version } = await this.pdfjs();\nconst pdf = await getDocument({\ndata: new Uint8Array(raw.buffer),\nuseWorkerFetch: false,\nisEvalSupported: false,\nuseSystemFonts: true,\n}).promise;\nconst meta = await pdf.getMetadata().catch(() => null);\n\nconst documents: Document[] = [];\n\nfor (let i = 1; i <= pdf.numPages; i += 1) {\nconst page = await pdf.getPage(i);\nconst content = await page.getTextContent();\n\nif (content.items.length === 0) {\ncontinue;\n}\n\nconst text = content.items\n.map((item) => (item as TextItem).str)\n.join(\"\\n\");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/pdf.ts","loc":{"lines":{"from":103,"to":142}}}}],["475",{"pageContent":"documents.push(\nnew Document({\npageContent: text,\nmetadata: {\n...metadata,\npdf: {\nversion,\ninfo: meta?.info,\nmetadata: meta?.metadata,\ntotalPages: pdf.numPages,\n},\nloc: {\npageNumber: i,\n},\n},\n})\n);\n}\n\nif (this.splitPages) {\nreturn documents;\n}\n\nif (documents.length === 0) {\nreturn [];\n}\n\nreturn [\nnew Document({\npageContent: documents.map((doc) => doc.pageContent).join(\"\\n\\n\"),\nmetadata: {\n...metadata,\npdf: {\nversion,\ninfo: meta?.info,\nmetadata: meta?.metadata,\ntotalPages: pdf.numPages,\n},\n},\n}),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/pdf.ts","loc":{"lines":{"from":213,"to":255}}}}],["476",{"pageContent":"PDFLoaderImports() {\ntry {\nconst { default: mod } = await import(\n\"pdf-parse/lib/pdf.js/v1.10.100/build/pdf.js\"\n);\nconst { getDocument, version } = mod;\nreturn { getDocument, version };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Failed to load pdf-parse. Please install it with eg. `npm install pdf-parse`.\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/pdf.ts","loc":{"lines":{"from":339,"to":352}}}}],["477",{"pageContent":"import type SRTParserT from \"srt-parser-2\";\nimport { TextLoader } from \"./text.js\";\n\nexport class SRTLoader extends TextLoader {\nconstructor(filePathOrBlob: string | Blob) {\nsuper(filePathOrBlob);\n}\n\nprotected async parse(raw: string): Promise<string[]> {\nconst { SRTParser2 } = await SRTLoaderImports();\nconst parser = new SRTParser2();\nconst srts = parser.fromSrt(raw);\nreturn [\nsrts\n.map((srt) => srt.text)\n.filter(Boolean)\n.join(\" \"),\n];\n}\n}\n\nasync function SRTLoaderImports(): Promise<{\nSRTParser2: typeof SRTParserT.default;\n}> {\ntry {\nconst SRTParser2 = (await import(\"srt-parser-2\")).default.default;\nreturn { SRTParser2 };\n} catch (e) {\nthrow new Error(\n\"Please install srt-parser-2 as a dependency with, e.g. `yarn add srt-parser-2`\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/srt.ts","loc":{"lines":{"from":1,"to":33}}}}],["478",{"pageContent":"import type { readFile as ReadFileT } from \"node:fs/promises\";\nimport { Document } from \"../../document.js\";\nimport { getEnv } from \"../../util/env.js\";\nimport { BaseDocumentLoader } from \"../base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/text.ts","loc":{"lines":{"from":1,"to":4}}}}],["479",{"pageContent":"class TextLoader extends BaseDocumentLoader {\nconstructor(public filePathOrBlob: string | Blob) {\nsuper();\n}\n\nprotected async parse(raw: string): Promise<string[]> {\nreturn [raw];\n}\n\npublic async load(): Promise<Document[]> {\nlet text: string;\nlet metadata: Record<string, string>;\nif (typeof this.filePathOrBlob === \"string\") {\nconst { readFile } = await TextLoader.imports();\ntext = await readFile(this.filePathOrBlob, \"utf8\");\nmetadata = { source: this.filePathOrBlob };\n} else {\ntext = await this.filePathOrBlob.text();\nmetadata = { source: \"blob\", blobType: this.filePathOrBlob.type };\n}\nconst parsed = await this.parse(text);\nparsed.forEach((pageContent, i) => {\nif (typeof pageContent !== \"string\") {\nthrow new Error(\n`Expected string, at position ${i} got ${typeof pageContent}`\n);\n}\n});\nreturn parsed.map(\n(pageContent, i) =>\nnew Document({\npageContent,\nmetadata:\nparsed.length === 1\n? metadata\n: {\n...metadata,\nline: i + 1,\n},\n})\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/text.ts","loc":{"lines":{"from":62,"to":103}}}}],["480",{"pageContent":"static async imports(): Promise<{\nreadFile: typeof ReadFileT;\n}> {\ntry {\nconst { readFile } = await import(\"node:fs/promises\");\nreturn { readFile };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n`Failed to load fs/promises. TextLoader available only on environment 'node'. It appears you are running environment '${getEnv()}'. See https://<link to docs> for alternatives.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/text.ts","loc":{"lines":{"from":138,"to":151}}}}],["481",{"pageContent":"import type { basename as BasenameT } from \"node:path\";\nimport type { readFile as ReadFileT } from \"node:fs/promises\";\nimport {\nDirectoryLoader,\nUnknownHandling,\nLoadersMapping,\n} from \"./directory.js\";\nimport { getEnv } from \"../../util/env.js\";\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\n\nconst UNSTRUCTURED_API_FILETYPES = [\n\".txt\",\n\".text\",\n\".pdf\",\n\".docx\",\n\".doc\",\n\".jpg\",\n\".jpeg\",\n\".eml\",\n\".html\",\n\".md\",\n\".pptx\",\n\".ppt\",\n\".msg\",\n];\n\ntype Element = {\ntype: string;\ntext: string;\n// this is purposefully loosely typed\nmetadata: {\n[key: string]: unknown;\n};\n};\n\ntype UnstructuredLoaderOptions = {\napiKey?: string;\napiUrl?: string;\n};\n\ntype UnstructuredDirectoryLoaderOptions = UnstructuredLoaderOptions & {\nrecursive?: boolean;\nunknown?: UnknownHandling;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":1,"to":45}}}}],["482",{"pageContent":"class UnstructuredLoader extends BaseDocumentLoader {\npublic filePath: string;\n\nprivate apiUrl = \"https://api.unstructured.io/general/v0/general\";\n\nprivate apiKey?: string;\n\nconstructor(\nfilePathOrLegacyApiUrl: string,\noptionsOrLegacyFilePath: UnstructuredLoaderOptions | string = {}\n) {\nsuper();\n\n// Temporary shim to avoid breaking existing users\n// Remove when API keys are enforced by Unstructured and existing code will break anyway\nconst isLegacySyntax = typeof optionsOrLegacyFilePath === \"string\";\nif (isLegacySyntax) {\nthis.filePath = optionsOrLegacyFilePath;\nthis.apiUrl = filePathOrLegacyApiUrl;\n} else {\nthis.filePath = filePathOrLegacyApiUrl;\nthis.apiKey = optionsOrLegacyFilePath.apiKey;\nthis.apiUrl = optionsOrLegacyFilePath.apiUrl ?? this.apiUrl;\n}\n}\n\nasync _partition() {\nconst { readFile, basename } = await this.imports();\n\nconst buffer = await readFile(this.filePath);\nconst fileName = basename(this.filePath);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":188,"to":218}}}}],["483",{"pageContent":"// I'm aware this reads the file into memory first, but we have lots of work\n// to do on then consuming Documents in a streaming fashion anyway, so not\n// worried about this for now.\nconst formData = new FormData();\nformData.append(\"files\", new Blob([buffer]), fileName);\n\nconst headers = {\n\"UNSTRUCTURED-API-KEY\": this.apiKey ?? \"\",\n};\n\nconst response = await fetch(this.apiUrl, {\nmethod: \"POST\",\nbody: formData,\nheaders,\n});\n\nif (!response.ok) {\nthrow new Error(\n`Failed to partition file ${this.filePath} with error ${\nresponse.status\n} and message ${await response.text()}`\n);\n}\n\nconst elements = await response.json();\nif (!Array.isArray(elements)) {\nthrow new Error(\n`Expected partitioning request to return an array, but got ${elements}`\n);\n}\nreturn elements.filter((el) => typeof el.text === \"string\") as Element[];\n}\n\nasync load(): Promise<Document[]> {\nconst elements = await this._partition();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":356,"to":390}}}}],["484",{"pageContent":"const documents: Document[] = [];\nfor (const element of elements) {\nconst { metadata, text } = element;\ndocuments.push(\nnew Document({\npageContent: text,\nmetadata: {\n...metadata,\ncategory: element.type,\n},\n})\n);\n}\n\nreturn documents;\n}\n\nasync imports(): Promise<{\nreadFile: typeof ReadFileT;\nbasename: typeof BasenameT;\n}> {\ntry {\nconst { readFile } = await import(\"node:fs/promises\");\nconst { basename } = await import(\"node:path\");\nreturn { readFile, basename };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n`Failed to load fs/promises. TextLoader available only on environment 'node'. It appears you are running environment '${getEnv()}'. See https://<link to docs> for alternatives.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":528,"to":560}}}}],["485",{"pageContent":"class UnstructuredDirectoryLoader extends DirectoryLoader {\nconstructor(\ndirectoryPathOrLegacyApiUrl: string,\noptionsOrLegacyDirectoryPath: UnstructuredDirectoryLoaderOptions | string,\nlegacyOptionRecursive = true,\nlegacyOptionUnknown: UnknownHandling = UnknownHandling.Warn\n) {\nlet directoryPath;\nlet options: UnstructuredDirectoryLoaderOptions;\n// Temporary shim to avoid breaking existing users\n// Remove when API keys are enforced by Unstructured and existing code will break anyway\nconst isLegacySyntax = typeof optionsOrLegacyDirectoryPath === \"string\";\nif (isLegacySyntax) {\ndirectoryPath = optionsOrLegacyDirectoryPath;\noptions = {\napiUrl: directoryPathOrLegacyApiUrl,\nrecursive: legacyOptionRecursive,\nunknown: legacyOptionUnknown,\n};\n} else {\ndirectoryPath = directoryPathOrLegacyApiUrl;\noptions = optionsOrLegacyDirectoryPath;\n}\nconst loader = (p: string) => new UnstructuredLoader(p, options);\nconst loaders = UNSTRUCTURED_API_FILETYPES.reduce(\n(loadersObject: LoadersMapping, filetype: string) => {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":708,"to":733}}}}],["486",{"pageContent":"// eslint-disable-next-line no-param-reassign\nloadersObject[filetype] = loader;\nreturn loadersObject;\n},\n{}\n);\nsuper(directoryPath, loaders, options.recursive, options.unknown);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":868,"to":876}}}}],["487",{"pageContent":"{ UnknownHandling };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":1055,"to":1055}}}}],["488",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/document_loaders' is deprecated. Import from eg. 'langchain/document_loaders/fs/text' or 'langchain/document_loaders/web/cheerio' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport type { DocumentLoader } from \"./base.js\";\nexport { BaseDocumentLoader } from \"./base.js\";\nexport { CheerioWebBaseLoader } from \"./web/cheerio.js\";\nexport { PuppeteerWebBaseLoader, PuppeteerEvaluate } from \"./web/puppeteer.js\";\nexport { CollegeConfidentialLoader } from \"./web/college_confidential.js\";\nexport { GitbookLoader } from \"./web/gitbook.js\";\nexport { HNLoader } from \"./web/hn.js\";\nexport { IMSDBLoader } from \"./web/imsdb.js\";\nexport { DirectoryLoader, UnknownHandling } from \"./fs/directory.js\";\nexport { SRTLoader } from \"./fs/srt.js\";\nexport { PDFLoader } from \"./fs/pdf.js\";\nexport { DocxLoader } from \"./fs/docx.js\";\nexport { EPubLoader } from \"./fs/epub.js\";\nexport { TextLoader } from \"./fs/text.js\";\nexport { JSONLoader, JSONLinesLoader } from \"./fs/json.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/index.ts","loc":{"lines":{"from":1,"to":19}}}}],["489",{"pageContent":"{ CSVLoader } from \"./fs/csv.js\";\nexport { NotionLoader } from \"./fs/notion.js\";\nexport { GithubRepoLoader, GithubRepoLoaderParams } from \"./web/github.js\";\nexport { UnstructuredLoader } from \"./fs/unstructured.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/index.ts","loc":{"lines":{"from":20,"to":23}}}}],["490",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { CheerioWebBaseLoader } from \"../web/cheerio.js\";\n\ntest(\"Test cheerio web scraper loader\", async () => {\nconst loader = new CheerioWebBaseLoader(\n\"https://news.ycombinator.com/item?id=34817881\"\n);\nawait loader.load();\n});\n\ntest(\"Test cheerio web scraper loader with selector\", async () => {\nconst selectH1 = \"h1\";\nconst loader = new CheerioWebBaseLoader(\"https://about.google/commitments/\", {\nselector: selectH1,\n});\n\nconst doc = await loader.load();\nexpect(doc[0].pageContent.trim()).toBe(\n\"Committed to significantly improving the lives of as many people as possible.\"\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/cheerio.int.test.ts","loc":{"lines":{"from":1,"to":21}}}}],["491",{"pageContent":"import { test } from \"@jest/globals\";\nimport { CollegeConfidentialLoader } from \"../web/college_confidential.js\";\n\ntest(\"Test College confidential loader\", async () => {\nconst loader = new CollegeConfidentialLoader(\n\"https://www.collegeconfidential.com/colleges/brown-university/\"\n);\nawait loader.load();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/college_confidential.int.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["492",{"pageContent":"import { test, jest, expect } from \"@jest/globals\";\nimport {\nConfluencePagesLoader,\nConfluenceAPIResponse,\n} from \"../web/confluence.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/confluence.test.ts","loc":{"lines":{"from":1,"to":5}}}}],["493",{"pageContent":"TestConfluencePagesLoaderType = ConfluencePagesLoader & {\nfetchConfluenceData: (url: string) => Promise<ConfluenceAPIResponse>;\n};\n\ntest(\"Test ConfluenceLoader and fetchConfluenceData calls\", async () => {\n// Stub the fetchConfluenceData method to return a fake response\n// As the Confluence API requires authentication\nconst fakeResponse = [\n{\nid: \"1\",\ntitle: \"Page 1\",\nbody: { storage: { value: \"<p>Content of Page 1</p>\" } },\n},\n{\nid: \"2\",\ntitle: \"Page 2\",\nbody: { storage: { value: \"<p>Content of Page 2</p>\" } },\n},\n];\n\n// Initialize the loader and load the documents\nconst loader = new ConfluencePagesLoader({\nbaseUrl: \"https://example.atlassian.net/wiki\",\nspaceKey: \"SPACEKEY\",\nusername: \"username@email.com\",\naccessToken: \"accessToken\",\n}) as TestConfluencePagesLoaderType;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/confluence.test.ts","loc":{"lines":{"from":81,"to":107}}}}],["494",{"pageContent":"// Our fetchConfluenceData function is called recursively\n// until the size of the response is 0\nconst fetchConfluenceDataMock = jest\n.spyOn(loader, \"fetchConfluenceData\")\n.mockImplementationOnce(() =>\nPromise.resolve({ size: 2, results: fakeResponse })\n)\n.mockImplementationOnce(() =>\nPromise.resolve({ size: 2, results: fakeResponse })\n)\n.mockImplementationOnce(() => Promise.resolve({ size: 0, results: [] }));\n\nconst documents = await loader.load();\n\n// Validate the test results\nexpect(documents.length).toBe(4);\nexpect(documents[0].metadata.title).toBeDefined();\nexpect(documents[0].metadata.url).toBeDefined();\n\n// Ensure fetchConfluenceData is called three times\nexpect(fetchConfluenceDataMock).toHaveBeenCalledTimes(3);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/confluence.test.ts","loc":{"lines":{"from":161,"to":181}}}}],["495",{"pageContent":"// Ensure the arguments are correct for each call\nexpect(fetchConfluenceDataMock).toHaveBeenNthCalledWith(\n1,\n\"https://example.atlassian.net/wiki/rest/api/content?spaceKey=SPACEKEY&limit=25&start=0&expand=body.storage\"\n);\nexpect(fetchConfluenceDataMock).toHaveBeenNthCalledWith(\n2,\n\"https://example.atlassian.net/wiki/rest/api/content?spaceKey=SPACEKEY&limit=25&start=2&expand=body.storage\"\n);\nexpect(fetchConfluenceDataMock).toHaveBeenNthCalledWith(\n3,\n\"https://example.atlassian.net/wiki/rest/api/content?spaceKey=SPACEKEY&limit=25&start=4&expand=body.storage\"\n);\n\n// Check if the generated URLs in the metadata are correct\nexpect(documents[0].metadata.url).toBe(\n\"https://example.atlassian.net/wiki/spaces/SPACEKEY/pages/1\"\n);\nexpect(documents[1].metadata.url).toBe(\n\"https://example.atlassian.net/wiki/spaces/SPACEKEY/pages/2\"\n);\n\n// Restore the mock to its original behavior\nfetchConfluenceDataMock.mockRestore();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/confluence.test.ts","loc":{"lines":{"from":238,"to":262}}}}],["496",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport * as fs from \"node:fs/promises\";\nimport { test, expect } from \"@jest/globals\";\nimport { CSVLoader } from \"../fs/csv.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test CSV loader from blob\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.csv\"\n);\nconst loader = new CSVLoader(\nnew Blob([await fs.readFile(filePath)], { type: \"text/csv\" }),\n\"html\"\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: \"blob\", blobType: \"text/csv\", line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\nexpect(docs[1]).toEqual(\nnew Document({\nmetadata: { source: \"blob\", blobType: \"text/csv\", line: 2 },\npageContent: \"<i>Reunited, Rush Clovis and Senator Amidala</i>\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/csv-blob.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["497",{"pageContent":"test(\"Test CSV loader from blob\", async () => {\nconst loader = new CSVLoader(\nnew Blob(\n[\n`id,text\n1,This is a sentence.\n2,This is another sentence.`,\n],\n{ type: \"text/csv\" }\n)\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(2);\nexpect(docs[0]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"text/csv\",\n\"line\": 1,\n\"source\": \"blob\",\n},\n\"pageContent\": \"id: 1\ntext: This is a sentence.\",\n}\n`);\nexpect(docs[1]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"text/csv\",\n\"line\": 2,\n\"source\": \"blob\",\n},\n\"pageContent\": \"id: 2\ntext: This is another sentence.\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/csv-blob.test.ts","loc":{"lines":{"from":72,"to":107}}}}],["498",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport { CSVLoader } from \"../fs/csv.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test CSV loader from file with column arg\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.csv\"\n);\nconst loader = new CSVLoader(filePath, \"html\");\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\nexpect(docs[1]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 2 },\npageContent: \"<i>Reunited, Rush Clovis and Senator Amidala</i>\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/csv.test.ts","loc":{"lines":{"from":1,"to":28}}}}],["499",{"pageContent":"test(\"Test CSV loader without column arg\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.csv\"\n);\nconst loader = new CSVLoader(filePath);\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 1 },\npageContent: `id: 1\nhtml: <i>Corruption discovered at the core of the Banking Clan!</i>`,\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/csv.test.ts","loc":{"lines":{"from":48,"to":63}}}}],["500",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport { DirectoryLoader, UnknownHandling } from \"../fs/directory.js\";\nimport { CSVLoader } from \"../fs/csv.js\";\nimport { PDFLoader } from \"../fs/pdf.js\";\nimport { TextLoader } from \"../fs/text.js\";\nimport { JSONLoader } from \"../fs/json.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/directory.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["501",{"pageContent":"test(\"Test Directory loader\", async () => {\nconst directoryPath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data\"\n);\nconst loader = new DirectoryLoader(\ndirectoryPath,\n{\n\".csv\": (p) => new CSVLoader(p, \"html\"),\n\".pdf\": (p) => new PDFLoader(p),\n\".txt\": (p) => new TextLoader(p),\n\".json\": (p) => new JSONLoader(p),\n},\ntrue,\nUnknownHandling.Ignore\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(90);\nexpect(docs.map((d) => d.metadata.source).sort()).toEqual([\n// PDF\n...Array.from({ length: 15 }, (_) =>\npath.resolve(directoryPath, \"1706.03762.pdf\")\n),\n// CSV\n...Array.from({ length: 32 }, (_) =>\npath.resolve(\ndirectoryPath,\n\"Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.csv\"\n)\n),\n// JSON\n...Array.from({ length: 32 }, (_) =>\npath.resolve(\ndirectoryPath,\n\"Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.json\"\n)\n),\n...Array.from({ length: 10 }, (_) =>\npath.resolve(directoryPath, \"complex.json\")\n),\n// TXT\npath.resolve(directoryPath, \"example.txt\"),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/directory.test.ts","loc":{"lines":{"from":53,"to":96}}}}],["502",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { DocxLoader } from \"../fs/docx.js\";\n\ntest(\"Test Word doc loader from file\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/attention.docx\"\n);\nconst loader = new DocxLoader(filePath);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1); // not much text in the example\nexpect(docs[0].pageContent).toContain(\"an interesting activity\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/docx.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["503",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { EPubLoader } from \"../fs/epub.js\";\n\ntest(\"Test EPub loader from file\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/attention.epub\"\n);\nconst loader = new EPubLoader(filePath);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(3);\nexpect(docs[0].pageContent).toContain(\"Attention Is All You Need\");\n});\n\ntest(\"Test EPub loader from file to single document\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/attention.epub\"\n);\nconst loader = new EPubLoader(filePath, { splitChapters: false });\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1);\nexpect(docs[0].pageContent).toContain(\"Attention Is All You Need\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/epub.test.ts","loc":{"lines":{"from":1,"to":28}}}}],["504",{"pageContent":"import { test } from \"@jest/globals\";\nimport { GithubRepoLoader } from \"../web/github.js\";\n\ntest(\"Test GithubRepoLoader\", async () => {\nconst loader = new GithubRepoLoader(\n\"https://github.com/hwchase17/langchainjs\",\n{ branch: \"main\", recursive: false, unknown: \"warn\" }\n);\nconst documents = await loader.load();\nconsole.log(documents[0].pageContent);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/github.int.test.ts","loc":{"lines":{"from":1,"to":11}}}}],["505",{"pageContent":"import { test } from \"@jest/globals\";\nimport { HNLoader } from \"../web/hn.js\";\n\ntest(\"Test Hacker News loader\", async () => {\nconst loader = new HNLoader(\"https://news.ycombinator.com/item?id=34817881\");\nawait loader.load();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/hn.int.test.ts","loc":{"lines":{"from":1,"to":7}}}}],["506",{"pageContent":"import { test } from \"@jest/globals\";\nimport { IMSDBLoader } from \"../web/imsdb.js\";\n\ntest(\"Test IMSDB loader\", async () => {\nconst loader = new IMSDBLoader(\n\"https://imsdb.com/scripts/BlacKkKlansman.html\"\n);\nawait loader.load();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/imsdb.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["507",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport * as fs from \"node:fs/promises\";\nimport { test, expect } from \"@jest/globals\";\nimport { JSONLoader } from \"../fs/json.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test JSON loader from blob\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.json\"\n);\nconst loader = new JSONLoader(\nnew Blob([await fs.readFile(filePath)], { type: \"application/json\" })\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: \"blob\", blobType: \"application/json\", line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json-blob.test.ts","loc":{"lines":{"from":1,"to":25}}}}],["508",{"pageContent":"test(\"Test JSON loader from blob\", async () => {\nconst loader = new JSONLoader(\nnew Blob(\n[\n`{\n\"texts\": [\"This is a sentence.\", \"This is another sentence.\"]\n}`,\n],\n{ type: \"application/json\" }\n)\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(2);\nexpect(docs[0]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/json\",\n\"line\": 1,\n\"source\": \"blob\",\n},\n\"pageContent\": \"This is a sentence.\",\n}\n`);\nexpect(docs[1]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/json\",\n\"line\": 2,\n\"source\": \"blob\",\n},\n\"pageContent\": \"This is another sentence.\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json-blob.test.ts","loc":{"lines":{"from":111,"to":144}}}}],["509",{"pageContent":"test(\"Test JSON loader from blob\", async () => {\nconst loader = new JSONLoader(\nnew Blob(\n[\n`{\n\"1\": {\n\"body\": \"BD 2023 SUMMER\",\n\"from\": \"LinkedIn Job\",\n\"labels\": [\"IMPORTANT\", \"CATEGORY_UPDATES\", \"INBOX\"]\n},\n\"2\": {\n\"body\": \"Intern, Treasury and other roles are available\",\n\"from\": \"LinkedIn Job2\",\n\"labels\": [\"IMPORTANT\"],\n\"other\": {\n\"name\": \"plop\",\n\"surname\": \"bob\"\n}\n}\n}`,\n],\n{ type: \"application/json\" }\n)\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(10);\nexpect(docs[0]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/json\",\n\"line\": 1,\n\"source\": \"blob\",\n},\n\"pageContent\": \"BD 2023 SUMMER\",\n}\n`);\nexpect(docs[1]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/json\",\n\"line\": 2,\n\"source\": \"blob\",\n},\n\"pageContent\": \"LinkedIn Job\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json-blob.test.ts","loc":{"lines":{"from":235,"to":281}}}}],["510",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport { Document } from \"../../document.js\";\nimport { JSONLoader } from \"../fs/json.js\";\n\ntest(\"Test JSON loader\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.json\"\n);\nconst loader = new JSONLoader(filePath);\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\n});\n\ntest(\"Test JSON  loader for complex json without keys\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/complex.json\"\n);\nconst loader = new JSONLoader(filePath);\nconst docs = await loader.load();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json.test.ts","loc":{"lines":{"from":1,"to":30}}}}],["511",{"pageContent":"expect(docs.length).toBe(10);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 1 },\npageContent: \"BD 2023 SUMMER\",\n})\n);\nexpect(docs[1]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 2 },\npageContent: \"LinkedIn Job\",\n})\n);\nexpect(docs[2]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 3 },\npageContent: \"IMPORTANT\",\n})\n);\n});\n\ntest(\"Test JSON loader for complex json with one key that points nothing\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/complex.json\"\n);\nconst loader = new JSONLoader(filePath, [\"/plop\"]);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(0);\n});\n\ntest(\"Test JSON loader for complex json with one key that exists\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/complex.json\"\n);\nconst loader = new JSONLoader(filePath, [\"/from\"]);\nconst docs = await loader.load();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json.test.ts","loc":{"lines":{"from":114,"to":152}}}}],["512",{"pageContent":"expect(docs.length).toBe(2);\nexpect(docs[1]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 2 },\npageContent: \"LinkedIn Job2\",\n})\n);\n});\n\ntest(\"Test JSON loader for complex json with two keys that exists\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/complex.json\"\n);\nconst loader = new JSONLoader(filePath, [\"/from\", \"/labels\"]);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(6);\nexpect(docs[3]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 4 },\npageContent: \"INBOX\",\n})\n);\n});\n\ntest(\"Test JSON loader for complex json with two existing keys on different level\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/complex.json\"\n);\nconst loader = new JSONLoader(filePath, [\"/from\", \"/surname\"]);\nconst docs = await loader.load();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json.test.ts","loc":{"lines":{"from":235,"to":267}}}}],["513",{"pageContent":"expect(docs.length).toBe(3);\nexpect(docs[2]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 3 },\npageContent: \"bob\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json.test.ts","loc":{"lines":{"from":353,"to":360}}}}],["514",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport * as fs from \"node:fs/promises\";\nimport { test, expect } from \"@jest/globals\";\nimport { JSONLinesLoader } from \"../fs/json.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test JSONL loader from blob\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.jsonl\"\n);\nconst loader = new JSONLinesLoader(\nnew Blob([await fs.readFile(filePath)], { type: \"application/jsonl+json\" }),\n\"/html\"\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: \"blob\", blobType: \"application/jsonl+json\", line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/jsonl-blob.test.ts","loc":{"lines":{"from":1,"to":26}}}}],["515",{"pageContent":"test(\"Test JSONL loader from blob\", async () => {\nconst loader = new JSONLinesLoader(\nnew Blob(\n[\n`{\"html\": \"This is a sentence.\"}\n{\"html\": \"This is another sentence.\"}`,\n],\n{ type: \"application/jsonl+json\" }\n),\n\"/html\"\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(2);\nexpect(docs[0]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/jsonl+json\",\n\"line\": 1,\n\"source\": \"blob\",\n},\n\"pageContent\": \"This is a sentence.\",\n}\n`);\nexpect(docs[1]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/jsonl+json\",\n\"line\": 2,\n\"source\": \"blob\",\n},\n\"pageContent\": \"This is another sentence.\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/jsonl-blob.test.ts","loc":{"lines":{"from":64,"to":97}}}}],["516",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport { JSONLinesLoader } from \"../fs/json.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test JSON loader from file\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.jsonl\"\n);\nconst loader = new JSONLinesLoader(filePath, \"/html\");\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/jsonl.test.ts","loc":{"lines":{"from":1,"to":22}}}}],["517",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { NotionLoader } from \"../fs/notion.js\";\n\ntest(\"Test Notion Loader\", async () => {\nconst directoryPath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data\"\n);\nconst loader = new NotionLoader(directoryPath);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1);\nexpect(docs[0].pageContent).toContain(\"Testing the notion markdownloader\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/notion.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["518",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport * as fs from \"node:fs/promises\";\nimport { PDFLoader } from \"../fs/pdf.js\";\n\ntest(\"Test PDF loader from blob\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/1706.03762.pdf\"\n);\nconst loader = new PDFLoader(\nnew Blob([await fs.readFile(filePath)], {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/pdf-blob.test.ts","loc":{"lines":{"from":1,"to":13}}}}],["519",{"pageContent":": \"application/pdf\",\n})\n);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(15);\nexpect(docs[0].pageContent).toContain(\"Attention Is All You Need\");\nexpect(docs[0].metadata).toMatchInlineSnapshot(`\n{\n\"blobType\": \"application/pdf\",\n\"loc\": {\n\"pageNumber\": 1,\n},\n\"pdf\": {\n\"info\": {\n\"Author\": \"\",\n\"CreationDate\": \"D:20171207010315Z\",\n\"Creator\": \"LaTeX with hyperref package\",\n\"IsAcroFormPresent\": false,\n\"IsXFAPresent\": false,\n\"Keywords\": \"\",\n\"ModDate\": \"D:20171207010315Z\",\n\"PDFFormatVersion\": \"1.5\",\n\"Producer\": \"pdfTeX-1.40.17\",\n\"Subject\": \"\",\n\"Title\": \"\",\n\"Trapped\": {\n\"name\": \"False\",\n},\n},\n\"metadata\": null,\n\"totalPages\": 15,\n\"version\": \"1.10.100\",\n},\n\"source\": \"blob\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/pdf-blob.test.ts","loc":{"lines":{"from":51,"to":88}}}}],["520",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { PDFLoader } from \"../fs/pdf.js\";\n\ntest(\"Test PDF loader from file\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/1706.03762.pdf\"\n);\nconst loader = new PDFLoader(filePath);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(15);\nexpect(docs[0].pageContent).toContain(\"Attention Is All You Need\");\n});\n\ntest(\"Test PDF loader from file to single document\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/1706.03762.pdf\"\n);\nconst loader = new PDFLoader(filePath, { splitPages: false });\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1);\nexpect(docs[0].pageContent).toContain(\"Attention Is All You Need\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/pdf.test.ts","loc":{"lines":{"from":1,"to":28}}}}],["521",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { PlaywrightWebBaseLoader } from \"../web/playwright.js\";\n\ntest(\"Test playwright web scraper loader\", async () => {\nconst loader = new PlaywrightWebBaseLoader(\"https://www.google.com/\");\nconst result = await loader.load();\n\nexpect(result).toBeDefined();\nexpect(result.length).toBe(1);\n}, 20_000);\n\ntest(\"Test playwright web scraper loader with evaluate options\", async () => {\nlet nrTimesCalled = 0;\nconst loader = new PlaywrightWebBaseLoader(\"https://www.google.com/\", {\nlaunchOptions: {\nheadless: true,\n},\ngotoOptions: {\nwaitUntil: \"domcontentloaded\",\n},\nasync evaluate(page) {\nnrTimesCalled += 1;\nreturn page.content();\n},\n});\nconst result = await loader.load();\n\nexpect(nrTimesCalled).toBe(1);\nexpect(result).toBeDefined();\nexpect(result.length).toBe(1);\n}, 20_000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/playwright_web.int.test.ts","loc":{"lines":{"from":1,"to":31}}}}],["522",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { PuppeteerWebBaseLoader } from \"../web/puppeteer.js\";\n\ntest.skip(\"Test puppeteer web scraper loader\", async () => {\nconst loader = new PuppeteerWebBaseLoader(\"https://www.google.com/\");\nconst result = await loader.load();\n\nexpect(result).toBeDefined();\nexpect(result.length).toBe(1);\n}, 20_000);\n\ntest.skip(\"Test puppeteer web scraper loader with evaluate options\", async () => {\nlet nrTimesCalled = 0;\nconst loader = new PuppeteerWebBaseLoader(\"https://www.google.com/\", {\nlaunchOptions: {\nheadless: true,\nignoreDefaultArgs: [\"--disable-extensions\"],\n},\ngotoOptions: {\nwaitUntil: \"domcontentloaded\",\n},\nasync evaluate(page) {\nnrTimesCalled += 1;\nreturn page.evaluate(() => document.body.innerHTML);\n},\n});\nconst result = await loader.load();\n\nexpect(nrTimesCalled).toBe(1);\nexpect(result).toBeDefined();\nexpect(result.length).toBe(1);\n}, 20_000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/puppeteer.int.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["523",{"pageContent":"import { test, jest, expect } from \"@jest/globals\";\nimport S3Client from \"@aws-sdk/client-s3\";\nimport * as fs from \"node:fs\";\nimport * as path from \"node:path\";\nimport { Readable } from \"node:stream\";\nimport { S3Loader } from \"../web/s3.js\";\nimport { UnstructuredLoader } from \"../fs/unstructured.js\";\n\nconst fsMock = {\n...fs,\nmkdtempSync: jest.fn().mockReturnValue(\"tmp/s3fileloader-12345\"),\nmkdirSync: jest.fn().mockImplementation(() => {}),\nwriteFileSync: jest.fn().mockImplementation(() => {}),\n};\n\nconst UnstructuredLoaderMock = jest.fn().mockImplementation(() => ({\nload: jest.fn().mockImplementation(() => [\"fake document\"]),\n}));\n\njest.mock(\"@aws-sdk/client-s3\", () => ({\nS3Client: jest.fn().mockImplementation(() => ({\nsend: jest.fn().mockImplementation(() =>\nPromise.resolve({\nBody: new Readable({\nread() {\nthis.push(Buffer.from(\"Mock file content\"));\nthis.push(null);\n},\n}),\n})\n),\n})),\nGetObjectCommand: jest.fn(),\n}));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/s3.test.ts","loc":{"lines":{"from":1,"to":34}}}}],["524",{"pageContent":"test(\"Test S3 loader\", async () => {\nif (!S3Client) {\n// this is to avoid a linting error. S3Client is mocked above.\n}\n\nconst loader = new S3Loader({\nbucket: \"test-bucket-123\",\nkey: \"AccountingOverview.pdf\",\nunstructuredAPIURL: \"http://localhost:8000/general/v0/general\",\nfs: fsMock as typeof fs,\nUnstructuredLoader: UnstructuredLoaderMock as typeof UnstructuredLoader,\n});\n\nconst result = await loader.load();\nconst unstructuredOptions = {\napiUrl: \"http://localhost:8000/general/v0/general\",\n};\n\nexpect(fsMock.mkdtempSync).toHaveBeenCalled();\nexpect(fsMock.mkdirSync).toHaveBeenCalled();\nexpect(fsMock.writeFileSync).toHaveBeenCalled();\nexpect(UnstructuredLoaderMock).toHaveBeenCalledWith(\npath.join(\"tmp\", \"s3fileloader-12345\", \"AccountingOverview.pdf\"),\nunstructuredOptions\n);\nexpect(result).toEqual([\"fake document\"]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/s3.test.ts","loc":{"lines":{"from":69,"to":95}}}}],["525",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport * as fs from \"node:fs/promises\";\nimport { test, expect } from \"@jest/globals\";\nimport { SRTLoader } from \"../fs/srt.js\";\n\ntest(\"Test SRT loader from blob\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt\"\n);\nconst loader = new SRTLoader(\nnew Blob([await fs.readFile(filePath)], { type: \"application/x-subrip\" })\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(1);\nexpect(docs[0].metadata).toMatchInlineSnapshot(`\n{\n\"blobType\": \"application/x-subrip\",\n\"source\": \"blob\",\n}\n`);\nexpect(docs[0].pageContent).toContain(\"Corruption discovered\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/srt-blob.test.ts","loc":{"lines":{"from":1,"to":24}}}}],["526",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport { SRTLoader } from \"../fs/srt.js\";\n\ntest(\"Test SRT loader from file\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt\"\n);\nconst loader = new SRTLoader(filePath);\nconst docs = await loader.load();\nexpect(docs.length).toBe(1);\nexpect(docs[0].metadata).toMatchInlineSnapshot(`\n{\n\"source\": \"${filePath}\",\n}\n`);\nexpect(docs[0].pageContent).toContain(\"Corruption discovered\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/srt.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["527",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { TextLoader } from \"../fs/text.js\";\n\ntest(\"Test Text loader from blob\", async () => {\nconst loader = new TextLoader(\nnew Blob([\"Hello, world!\"], { type: \"text/plain\" })\n);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1);\nexpect(docs[0].pageContent).toBe(\"Hello, world!\");\nexpect(docs[0].metadata).toMatchInlineSnapshot(`\n{\n\"blobType\": \"text/plain\",\n\"source\": \"blob\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/text-blob.test.ts","loc":{"lines":{"from":1,"to":18}}}}],["528",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { TextLoader } from \"../fs/text.js\";\n\ntest(\"Test Text loader from file\", async () => {\nconst loader = new TextLoader(\n\"../examples/src/document_loaders/example_data/example.txt\"\n);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1);\nexpect(docs[0].pageContent).toMatchInlineSnapshot(`\n\"Foo\nBar\nBaz\n\n\"\n`);\nexpect(docs[0].metadata).toMatchInlineSnapshot(`\n{\n\"source\": \"../examples/src/document_loaders/example_data/example.txt\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/text.test.ts","loc":{"lines":{"from":1,"to":23}}}}],["529",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport {\nUnstructuredDirectoryLoader,\nUnstructuredLoader,\nUnknownHandling,\n} from \"../fs/unstructured.js\";\n\ntest(\"Test Unstructured base loader legacy syntax\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/example.txt\"\n);\n\nconst loader = new UnstructuredLoader(\n\"https://api.unstructured.io/general/v0/general\",\nfilePath\n);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(3);\nfor (const doc of docs) {\nexpect(typeof doc.pageContent).toBe(\"string\");\n}\n});\n\ntest(\"Test Unstructured base loader\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/example.txt\"\n);\n\nconst options = {\napiKey: \"MY_API_KEY\",\n};\n\nconst loader = new UnstructuredLoader(filePath, options);\nconst docs = await loader.load();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/unstructured.int.test.ts","loc":{"lines":{"from":1,"to":39}}}}],["530",{"pageContent":"expect(docs.length).toBe(3);\nfor (const doc of docs) {\nexpect(typeof doc.pageContent).toBe(\"string\");\n}\n});\n\ntest(\"Test Unstructured directory loader legacy syntax\", async () => {\nconst directoryPath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data\"\n);\n\nconst loader = new UnstructuredDirectoryLoader(\n\"https://api.unstructured.io/general/v0/general\",\ndirectoryPath,\ntrue,\nUnknownHandling.Ignore\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(619);\nexpect(typeof docs[0].pageContent).toBe(\"string\");\n});\n\ntest(\"Test Unstructured directory loader\", async () => {\nconst directoryPath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data\"\n);\n\nconst options = {\napiKey: \"MY_API_KEY\",\n};\n\nconst loader = new UnstructuredDirectoryLoader(\ndirectoryPath,\noptions,\ntrue,\nUnknownHandling.Ignore\n);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(619);\nexpect(typeof docs[0].pageContent).toBe(\"string\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/unstructured.int.test.ts","loc":{"lines":{"from":85,"to":128}}}}],["531",{"pageContent":"import type { CheerioAPI, load as LoadT, SelectorType } from \"cheerio\";\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\nimport type { DocumentLoader } from \"../base.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../../util/async_caller.js\";\n\nexport interface WebBaseLoaderParams extends AsyncCallerParams {\n/**\n* The timeout in milliseconds for the fetch request. Defaults to 10s.\n*/\ntimeout?: number;\n\n/**\n* The selector to use to extract the text from the document. Defaults to\n* \"body\".\n*/\nselector?: SelectorType;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/cheerio.ts","loc":{"lines":{"from":1,"to":18}}}}],["532",{"pageContent":"class CheerioWebBaseLoader\nextends BaseDocumentLoader\nimplements DocumentLoader\n{\ntimeout: number;\n\ncaller: AsyncCaller;\n\nselector?: SelectorType;\n\nconstructor(public webPath: string, fields?: WebBaseLoaderParams) {\nsuper();\nconst { timeout, selector, ...rest } = fields ?? {};\nthis.timeout = timeout ?? 10000;\nthis.caller = new AsyncCaller(rest);\nthis.selector = selector ?? \"body\";\n}\n\nstatic async _scrape(\nurl: string,\ncaller: AsyncCaller,\ntimeout: number | undefined\n): Promise<CheerioAPI> {\nconst { load } = await CheerioWebBaseLoader.imports();\nconst response = await caller.call(fetch, url, {\nsignal: timeout ? AbortSignal.timeout(timeout) : undefined,\n});\nconst html = await response.text();\nreturn load(html);\n}\n\nasync scrape(): Promise<CheerioAPI> {\nreturn CheerioWebBaseLoader._scrape(\nthis.webPath,\nthis.caller,\nthis.timeout\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/cheerio.ts","loc":{"lines":{"from":80,"to":117}}}}],["533",{"pageContent":"async load(): Promise<Document[]> {\nconst $ = await this.scrape();\nconst text = $(this.selector).text();\nconst metadata = { source: this.webPath };\nreturn [new Document({ pageContent: text, metadata })];\n}\n\nstatic async imports(): Promise<{\nload: typeof LoadT;\n}> {\ntry {\nconst { load } = await import(\"cheerio\");\nreturn { load };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Please install cheerio as a dependency with, e.g. `yarn add cheerio`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/cheerio.ts","loc":{"lines":{"from":165,"to":185}}}}],["534",{"pageContent":"import { Document } from \"../../document.js\";\nimport { CheerioWebBaseLoader } from \"./cheerio.js\";\n\nexport class CollegeConfidentialLoader extends CheerioWebBaseLoader {\nconstructor(webPath: string) {\nsuper(webPath);\n}\n\npublic async load(): Promise<Document[]> {\nconst $ = await this.scrape();\nconst text = $(\"main[class='skin-handler']\").text();\nconst metadata = { source: this.webPath };\nreturn [new Document({ pageContent: text, metadata })];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/college_confidential.ts","loc":{"lines":{"from":1,"to":15}}}}],["535",{"pageContent":"import { htmlToText } from \"html-to-text\";\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\n\nexport interface ConfluencePagesLoaderParams {\nbaseUrl: string;\nspaceKey: string;\nusername: string;\naccessToken: string;\nlimit?: number;\n}\n\nexport interface ConfluencePage {\nid: string;\ntitle: string;\nbody: {\nstorage: {\nvalue: string;\n};\n};\n}\n\nexport interface ConfluenceAPIResponse {\nsize: number;\nresults: ConfluencePage[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/confluence.ts","loc":{"lines":{"from":1,"to":26}}}}],["536",{"pageContent":"class ConfluencePagesLoader extends BaseDocumentLoader {\npublic readonly baseUrl: string;\n\npublic readonly spaceKey: string;\n\npublic readonly username: string;\n\npublic readonly accessToken: string;\n\npublic readonly limit: number;\n\nconstructor({\nbaseUrl,\nspaceKey,\nusername,\naccessToken,\nlimit = 25,\n}: ConfluencePagesLoaderParams) {\nsuper();\nthis.baseUrl = baseUrl;\nthis.spaceKey = spaceKey;\nthis.username = username;\nthis.accessToken = accessToken;\nthis.limit = limit;\n}\n\npublic async load(): Promise<Document[]> {\ntry {\nconst pages = await this.fetchAllPagesInSpace();\nreturn pages.map((page) => this.createDocumentFromPage(page));\n} catch (error) {\nconsole.error(\"Error:\", error);\nreturn [];\n}\n}\n\nprotected async fetchConfluenceData(\nurl: string\n): Promise<ConfluenceAPIResponse> {\ntry {\nconst authToken = Buffer.from(\n`${this.username}:${this.accessToken}`\n).toString(\"base64\");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/confluence.ts","loc":{"lines":{"from":130,"to":172}}}}],["537",{"pageContent":"const response = await fetch(url, {\nheaders: {\nAuthorization: `Basic ${authToken}`,\n\"Content-Type\": \"application/json\",\nAccept: \"application/json\",\n},\n});\n\nif (!response.ok) {\nthrow new Error(\n`Failed to fetch ${url} from Confluence: ${response.status}`\n);\n}\n\nreturn await response.json();\n} catch (error) {\nthrow new Error(`Failed to fetch ${url} from Confluence: ${error}`);\n}\n}\n\nprivate async fetchAllPagesInSpace(start = 0): Promise<ConfluencePage[]> {\nconst url = `${this.baseUrl}/rest/api/content?spaceKey=${this.spaceKey}&limit=${this.limit}&start=${start}&expand=body.storage`;\nconst data = await this.fetchConfluenceData(url);\n\nif (data.size === 0) {\nreturn [];\n}\n\nconst nextPageStart = start + data.size;\nconst nextPageResults = await this.fetchAllPagesInSpace(nextPageStart);\n\nreturn data.results.concat(nextPageResults);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/confluence.ts","loc":{"lines":{"from":254,"to":286}}}}],["538",{"pageContent":"private createDocumentFromPage(page: ConfluencePage): Document {\n// Convert the HTML content to plain text\nconst plainTextContent = htmlToText(page.body.storage.value, {\nwordwrap: false,\npreserveNewlines: false,\n});\n\n// Remove empty lines\nconst textWithoutEmptyLines = plainTextContent.replace(/^\\s*[\\r\\n]/gm, \"\");\n\n// Generate the URL\nconst pageUrl = `${this.baseUrl}/spaces/${this.spaceKey}/pages/${page.id}`;\n\n// Return a langchain document\nreturn new Document({\npageContent: textWithoutEmptyLines,\nmetadata: {\ntitle: page.title,\nurl: pageUrl,\n},\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/confluence.ts","loc":{"lines":{"from":370,"to":392}}}}],["539",{"pageContent":"import type { CheerioAPI } from \"cheerio\";\nimport { Document } from \"../../document.js\";\nimport { CheerioWebBaseLoader } from \"./cheerio.js\";\n\ninterface GitbookLoaderParams {\nshouldLoadAllPaths?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/gitbook.ts","loc":{"lines":{"from":1,"to":7}}}}],["540",{"pageContent":"class GitbookLoader extends CheerioWebBaseLoader {\nshouldLoadAllPaths = false;\n\nconstructor(public webPath: string, params: GitbookLoaderParams = {}) {\nsuper(webPath);\nthis.shouldLoadAllPaths =\nparams.shouldLoadAllPaths ?? this.shouldLoadAllPaths;\n}\n\npublic async load(): Promise<Document[]> {\nconst $ = await this.scrape();\n\nif (this.shouldLoadAllPaths === true) {\nreturn this.loadAllPaths($);\n}\nreturn this.loadPath($);\n}\n\nprivate loadPath($: CheerioAPI, url?: string): Document[] {\nconst pageContent = $(\"main *\")\n.contents()\n.toArray()\n.map((element) =>\nelement.type === \"text\" ? $(element).text().trim() : null\n)\n.filter((text) => text)\n.join(\"\\n\");\n\nconst title = $(\"main h1\").first().text().trim();\n\nreturn [\nnew Document({\npageContent,\nmetadata: { source: url ?? this.webPath, title },\n}),\n];\n}\n\nprivate async loadAllPaths($: CheerioAPI): Promise<Document[]> {\nconst relative_paths = $(\"nav a\")\n.toArray()\n.map((element) => $(element).attr(\"href\"))\n.filter((text) => text && text[0] === \"/\");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/gitbook.ts","loc":{"lines":{"from":64,"to":106}}}}],["541",{"pageContent":"const documents: Document[] = [];\nfor (const path of relative_paths) {\nconst url = this.webPath + path;\nconsole.log(`Fetching text from ${url}`);\nconst html = await GitbookLoader._scrape(url, this.caller, this.timeout);\ndocuments.push(...this.loadPath(html, url));\n}\nconsole.log(`Fetched ${documents.length} documents.`);\nreturn documents;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/gitbook.ts","loc":{"lines":{"from":132,"to":142}}}}],["542",{"pageContent":"import binaryExtensions from \"binary-extensions\";\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\nimport { UnknownHandling } from \"../fs/directory.js\";\nimport { extname } from \"../../util/extname.js\";\n\nconst extensions = new Set(binaryExtensions);\n\nfunction isBinaryPath(name: string) {\nreturn extensions.has(extname(name).slice(1).toLowerCase());\n}\n\ninterface GithubFile {\nname: string;\npath: string;\nsha: string;\nsize: number;\nurl: string;\nhtml_url: string;\ngit_url: string;\ndownload_url: string;\ntype: string;\n_links: {\nself: string;\ngit: string;\nhtml: string;\n};\n}\n\nexport interface GithubRepoLoaderParams {\nbranch?: string;\nrecursive?: boolean;\nunknown?: UnknownHandling;\naccessToken?: string;\nignoreFiles?: (string | RegExp)[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":1,"to":36}}}}],["543",{"pageContent":"class GithubRepoLoader\nextends BaseDocumentLoader\nimplements GithubRepoLoaderParams\n{\nprivate readonly owner: string;\n\nprivate readonly repo: string;\n\nprivate readonly initialPath: string;\n\nprivate headers: Record<string, string> = {};\n\npublic branch: string;\n\npublic recursive: boolean;\n\npublic unknown: UnknownHandling;\n\npublic accessToken?: string;\n\npublic ignoreFiles: (string | RegExp)[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":198,"to":218}}}}],["544",{"pageContent":"constructor(\ngithubUrl: string,\n{\naccessToken = typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.GITHUB_ACCESS_TOKEN\n: undefined,\nbranch = \"main\",\nrecursive = true,\nunknown = UnknownHandling.Warn,\nignoreFiles = [],\n}: GithubRepoLoaderParams = {}\n) {\nsuper();\nconst { owner, repo, path } = this.extractOwnerAndRepoAndPath(githubUrl);\nthis.owner = owner;\nthis.repo = repo;\nthis.initialPath = path;\nthis.branch = branch;\nthis.recursive = recursive;\nthis.unknown = unknown;\nthis.accessToken = accessToken;\nthis.ignoreFiles = ignoreFiles;\nif (this.accessToken) {\nthis.headers = {\nAuthorization: `Bearer ${this.accessToken}`,\n};\n}\n}\n\nprivate extractOwnerAndRepoAndPath(url: string): {\nowner: string;\nrepo: string;\npath: string;\n} {\nconst match = url.match(\n/https:\\/\\/github.com\\/([^/]+)\\/([^/]+)(\\/tree\\/[^/]+\\/(.+))?/i\n);\n\nif (!match) {\nthrow new Error(\"Invalid GitHub URL format.\");\n}\n\nreturn { owner: match[1], repo: match[2], path: match[4] || \"\" };\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":404,"to":448}}}}],["545",{"pageContent":"public async load(): Promise<Document[]> {\nconst documents: Document[] = [];\nawait this.processDirectory(this.initialPath, documents);\nreturn documents;\n}\n\nprivate shouldIgnore(path: string): boolean {\nreturn this.ignoreFiles.some((pattern) => {\nif (typeof pattern === \"string\") {\nreturn path === pattern;\n}\n\ntry {\nreturn pattern.test(path);\n} catch {\nthrow new Error(`Unknown ignore file pattern: ${pattern}`);\n}\n});\n}\n\nprivate async processDirectory(\npath: string,\ndocuments: Document[]\n): Promise<void> {\ntry {\nconst files = await this.fetchRepoFiles(path);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":600,"to":625}}}}],["546",{"pageContent":"for (const file of files) {\nif (file.type === \"dir\") {\nif (this.recursive) {\nawait this.processDirectory(file.path, documents);\n}\n} else {\ntry {\nif (!isBinaryPath(file.name) && !this.shouldIgnore(file.path)) {\nconst fileContent = await this.fetchFileContent(file);\nconst metadata = { source: file.path };\ndocuments.push(\nnew Document({ pageContent: fileContent, metadata })\n);\n}\n} catch (e) {\nthis.handleError(\n`Failed to fetch file content: ${file.path}, ${e}`\n);\n}\n}\n}\n} catch (error) {\nthis.handleError(`Failed to process directory: ${path}, ${error}`);\n}\n}\n\nprivate async fetchRepoFiles(path: string): Promise<GithubFile[]> {\nconst url = `https://api.github.com/repos/${this.owner}/${this.repo}/contents/${path}?ref=${this.branch}`;\nconst response = await fetch(url, { headers: this.headers });\nconst data = await response.json();\nif (!response.ok) {\nthrow new Error(\n`Unable to fetch repository files: ${response.status} ${JSON.stringify(\ndata\n)}`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":800,"to":836}}}}],["547",{"pageContent":"if (!Array.isArray(data)) {\nthrow new Error(\"Unable to fetch repository files.\");\n}\n\nreturn data as GithubFile[];\n}\n\nprivate async fetchFileContent(file: GithubFile): Promise<string> {\nconst response = await fetch(file.download_url, { headers: this.headers });\nreturn response.text();\n}\n\nprivate handleError(message: string): void {\nswitch (this.unknown) {\ncase UnknownHandling.Ignore:\nbreak;\ncase UnknownHandling.Warn:\nconsole.warn(message);\nbreak;\ncase UnknownHandling.Error:\nthrow new Error(message);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":990,"to":1010}}}}],["548",{"pageContent":":\nthrow new Error(`Unknown unknown handling: ${this.unknown}`);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":1187,"to":1191}}}}],["549",{"pageContent":"import type { CheerioAPI } from \"cheerio\";\nimport { Document } from \"../../document.js\";\nimport { CheerioWebBaseLoader } from \"./cheerio.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/hn.ts","loc":{"lines":{"from":1,"to":3}}}}],["550",{"pageContent":"class HNLoader extends CheerioWebBaseLoader {\nconstructor(public webPath: string) {\nsuper(webPath);\n}\n\npublic async load(): Promise<Document[]> {\nconst $ = await this.scrape();\nif (this.webPath.includes(\"item\")) {\nreturn this.loadComments($);\n}\nreturn this.loadResults($);\n}\n\nprivate loadComments($: CheerioAPI): Document[] {\nconst comments = $(\"tr[class='athing comtr']\");\nconst title = $(\"tr[id='pagespace']\").attr(\"title\");\nconst documents: Document[] = [];\ncomments.each((_index, comment) => {\nconst text = $(comment).text().trim();\nconst metadata = { source: this.webPath, title };\ndocuments.push(new Document({ pageContent: text, metadata }));\n});\nreturn documents;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/hn.ts","loc":{"lines":{"from":47,"to":70}}}}],["551",{"pageContent":"private loadResults($: CheerioAPI): Document[] {\nconst items = $(\"tr[class='athing']\");\nconst documents: Document[] = [];\nitems.each((_index, item) => {\nconst ranking = $(item).find(\"span[class='rank']\").text();\nconst link = $(item).find(\"span[class='titleline'] a\").attr(\"href\");\nconst title = $(item).find(\"span[class='titleline']\").text().trim();\nconst metadata = {\nsource: this.webPath,\ntitle,\nlink,\nranking,\n};\ndocuments.push(new Document({ pageContent: title, metadata }));\n});\nreturn documents;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/hn.ts","loc":{"lines":{"from":95,"to":112}}}}],["552",{"pageContent":"import { Document } from \"../../document.js\";\nimport { CheerioWebBaseLoader } from \"./cheerio.js\";\n\nexport class IMSDBLoader extends CheerioWebBaseLoader {\nconstructor(public webPath: string) {\nsuper(webPath);\n}\n\npublic async load(): Promise<Document[]> {\nconst $ = await this.scrape();\nconst text = $(\"td[class='scrtext']\").text().trim();\nconst metadata = { source: this.webPath };\nreturn [new Document({ pageContent: text, metadata })];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/imsdb.ts","loc":{"lines":{"from":1,"to":15}}}}],["553",{"pageContent":"import type { LaunchOptions, Page, Browser } from \"playwright\";\n\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\nimport type { DocumentLoader } from \"../base.js\";\n\nexport { Page, Browser };\n\nexport type PlaywrightGotoOptions = {\nreferer?: string;\ntimeout?: number;\nwaitUntil?: \"load\" | \"domcontentloaded\" | \"networkidle\" | \"commit\";\n};\n\nexport type PlaywrightEvaluate = (\npage: Page,\nbrowser: Browser\n) => Promise<string>;\n\nexport type PlaywrightWebBaseLoaderOptions = {\nlaunchOptions?: LaunchOptions;\ngotoOptions?: PlaywrightGotoOptions;\nevaluate?: PlaywrightEvaluate;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/playwright.ts","loc":{"lines":{"from":1,"to":24}}}}],["554",{"pageContent":"class PlaywrightWebBaseLoader\nextends BaseDocumentLoader\nimplements DocumentLoader\n{\noptions: PlaywrightWebBaseLoaderOptions | undefined;\n\nconstructor(\npublic webPath: string,\noptions?: PlaywrightWebBaseLoaderOptions\n) {\nsuper();\nthis.options = options ?? undefined;\n}\n\nstatic async _scrape(\nurl: string,\noptions?: PlaywrightWebBaseLoaderOptions\n): Promise<string> {\nconst { chromium } = await PlaywrightWebBaseLoader.imports();\n\nconst browser = await chromium.launch({\nheadless: true,\n...options?.launchOptions,\n});\nconst page = await browser.newPage();\n\nawait page.goto(url, {\ntimeout: 180000,\nwaitUntil: \"domcontentloaded\",\n...options?.gotoOptions,\n});\nconst bodyHTML = options?.evaluate\n? await options?.evaluate(page, browser)\n: await page.content();\n\nawait browser.close();\n\nreturn bodyHTML;\n}\n\nasync scrape(): Promise<string> {\nreturn PlaywrightWebBaseLoader._scrape(this.webPath, this.options);\n}\n\nasync load(): Promise<Document[]> {\nconst text = await this.scrape();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/playwright.ts","loc":{"lines":{"from":92,"to":137}}}}],["555",{"pageContent":"const metadata = { source: this.webPath };\nreturn [new Document({ pageContent: text, metadata })];\n}\n\nstatic async imports(): Promise<{\nchromium: typeof import(\"playwright\").chromium;\n}> {\ntry {\nconst { chromium } = await import(\"playwright\");\n\nreturn { chromium };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Please install playwright as a dependency with, e.g. `yarn add playwright`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/playwright.ts","loc":{"lines":{"from":187,"to":205}}}}],["556",{"pageContent":"import type {\nlaunch,\nWaitForOptions,\nPage,\nBrowser,\nPuppeteerLaunchOptions,\n} from \"puppeteer\";\n\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\nimport type { DocumentLoader } from \"../base.js\";\n\nexport { Page, Browser };\n\nexport type PuppeteerGotoOptions = WaitForOptions & {\nreferer?: string;\nreferrerPolicy?: string;\n};\n\nexport type PuppeteerEvaluate = (\npage: Page,\nbrowser: Browser\n) => Promise<string>;\n\nexport type PuppeteerWebBaseLoaderOptions = {\nlaunchOptions?: PuppeteerLaunchOptions;\ngotoOptions?: PuppeteerGotoOptions;\nevaluate?: PuppeteerEvaluate;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/puppeteer.ts","loc":{"lines":{"from":1,"to":29}}}}],["557",{"pageContent":"class PuppeteerWebBaseLoader\nextends BaseDocumentLoader\nimplements DocumentLoader\n{\noptions: PuppeteerWebBaseLoaderOptions | undefined;\n\nconstructor(public webPath: string, options?: PuppeteerWebBaseLoaderOptions) {\nsuper();\nthis.options = options ?? undefined;\n}\n\nstatic async _scrape(\nurl: string,\noptions?: PuppeteerWebBaseLoaderOptions\n): Promise<string> {\nconst { launch } = await PuppeteerWebBaseLoader.imports();\n\nconst browser = await launch({\nheadless: true,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/puppeteer.ts","loc":{"lines":{"from":97,"to":115}}}}],["558",{"pageContent":"Viewport: null,\nignoreDefaultArgs: [\"--disable-extensions\"],\n...options?.launchOptions,\n});\nconst page = await browser.newPage();\n\nawait page.goto(url, {\ntimeout: 180000,\nwaitUntil: \"domcontentloaded\",\n...options?.gotoOptions,\n});\nconst bodyHTML = options?.evaluate\n? await options?.evaluate(page, browser)\n: await page.evaluate(() => document.body.innerHTML);\n\nawait browser.close();\n\nreturn bodyHTML;\n}\n\nasync scrape(): Promise<string> {\nreturn PuppeteerWebBaseLoader._scrape(this.webPath, this.options);\n}\n\nasync load(): Promise<Document[]> {\nconst text = await this.scrape();\n\nconst metadata = { source: this.webPath };\nreturn [new Document({ pageContent: text, metadata })];\n}\n\nstatic async imports(): Promise<{\nlaunch: typeof launch;\n}> {\ntry {\n// eslint-disable-next-line import/no-extraneous-dependencies\nconst { launch } = await import(\"puppeteer\");\n\nreturn { launch };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Please install puppeteer as a dependency with, e.g. `yarn add puppeteer`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/puppeteer.ts","loc":{"lines":{"from":188,"to":234}}}}],["559",{"pageContent":"import * as fsDefault from \"node:fs\";\nimport * as path from \"node:path\";\nimport * as os from \"node:os\";\nimport { Readable } from \"node:stream\";\nimport { BaseDocumentLoader } from \"../base.js\";\nimport { UnstructuredLoader as UnstructuredLoaderDefault } from \"../fs/unstructured.js\";\n\nexport interface S3LoaderParams {\nbucket: string;\nkey: string;\nunstructuredAPIURL: string;\ns3Config?: S3Config;\n\nfs?: typeof fsDefault;\nUnstructuredLoader?: typeof UnstructuredLoaderDefault;\n}\n\ninterface S3Config {\nregion?: string;\naccessKeyId?: string;\nsecretAccessKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/s3.ts","loc":{"lines":{"from":1,"to":22}}}}],["560",{"pageContent":"class S3Loader extends BaseDocumentLoader {\nprivate bucket: string;\n\nprivate key: string;\n\nprivate unstructuredAPIURL: string;\n\nprivate s3Config: S3Config;\n\nprivate _fs: typeof fsDefault;\n\nprivate _UnstructuredLoader: typeof UnstructuredLoaderDefault;\n\nconstructor({\nbucket,\nkey,\nunstructuredAPIURL,\ns3Config = {},\nfs = fsDefault,\nUnstructuredLoader = UnstructuredLoaderDefault,\n}: S3LoaderParams) {\nsuper();\nthis.bucket = bucket;\nthis.key = key;\nthis.unstructuredAPIURL = unstructuredAPIURL;\nthis.s3Config = s3Config;\nthis._fs = fs;\nthis._UnstructuredLoader = UnstructuredLoader;\n}\n\npublic async load() {\nconst { S3Client, GetObjectCommand } = await S3LoaderImports();\n\nconst tempDir = this._fs.mkdtempSync(\npath.join(os.tmpdir(), \"s3fileloader-\")\n);\n\nconst filePath = path.join(tempDir, this.key);\n\ntry {\nconst s3Client = new S3Client(this.s3Config);\n\nconst getObjectCommand = new GetObjectCommand({\nBucket: this.bucket,\nKey: this.key,\n});\n\nconst response = await s3Client.send(getObjectCommand);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/s3.ts","loc":{"lines":{"from":125,"to":172}}}}],["561",{"pageContent":"const objectData = await new Promise<Buffer>((resolve, reject) => {\nconst chunks: Buffer[] = [];\n\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (response.Body instanceof Readable) {\nresponse.Body.on(\"data\", (chunk: Buffer) => chunks.push(chunk));\nresponse.Body.on(\"end\", () => resolve(Buffer.concat(chunks)));\nresponse.Body.on(\"error\", reject);\n} else {\nreject(new Error(\"Response body is not a readable stream.\"));\n}\n});\n\nthis._fs.mkdirSync(path.dirname(filePath), { recursive: true });\n\nthis._fs.writeFileSync(filePath, objectData);\n} catch {\nthrow new Error(\n`Failed to download file ${this.key} from S3 bucket ${this.bucket}.`\n);\n}\n\ntry {\nconst options = { apiUrl: this.unstructuredAPIURL };\nconst unstructuredLoader = new this._UnstructuredLoader(\nfilePath,\noptions\n);\n\nconst docs = await unstructuredLoader.load();\n\nreturn docs;\n} catch {\nthrow new Error(\n`Failed to load file ${filePath} using unstructured loader.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/s3.ts","loc":{"lines":{"from":253,"to":291}}}}],["562",{"pageContent":"S3LoaderImports() {\ntry {\nconst s3Module = await import(\"@aws-sdk/client-s3\");\n\nreturn s3Module as typeof s3Module;\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Failed to load @aws-sdk/client-s3'. Please install it eg. `yarn add @aws-sdk/client-s3`.\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/s3.ts","loc":{"lines":{"from":373,"to":384}}}}],["563",{"pageContent":"import { AsyncCaller, AsyncCallerParams } from \"../util/async_caller.js\";\n\nexport type EmbeddingsParams = AsyncCallerParams;\n\nexport abstract class Embeddings {\n/**\n* The async caller should be used by subclasses to make any async calls,\n* which will thus benefit from the concurrency and retry logic.\n*/\ncaller: AsyncCaller;\n\nconstructor(params: EmbeddingsParams) {\nthis.caller = new AsyncCaller(params ?? {});\n}\n\nabstract embedDocuments(documents: string[]): Promise<number[][]>;\n\nabstract embedQuery(document: string): Promise<number[]>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/base.ts","loc":{"lines":{"from":1,"to":19}}}}],["564",{"pageContent":"import { chunkArray } from \"../util/chunk.js\";\nimport { Embeddings, EmbeddingsParams } from \"./base.js\";\n\nexport interface CohereEmbeddingsParams extends EmbeddingsParams {\nmodelName: string;\n\n/**\n* The maximum number of documents to embed in a single request. This is\n* limited by the Cohere API to a maximum of 96.\n*/\nbatchSize?: number;\n}\n\n/**\n* A class for generating embeddings using the Cohere API.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/cohere.ts","loc":{"lines":{"from":1,"to":16}}}}],["565",{"pageContent":"class CohereEmbeddings\nextends Embeddings\nimplements CohereEmbeddingsParams\n{\nmodelName = \"small\";\n\nbatchSize = 48;\n\nprivate apiKey: string;\n\nprivate client: typeof import(\"cohere-ai\");\n\n/**\n* Constructor for the CohereEmbeddings class.\n* @param fields - An optional object with properties to configure the instance.\n*/\nconstructor(\nfields?: Partial<CohereEmbeddingsParams> & {\nverbose?: boolean;\napiKey?: string;\n}\n) {\nsuper(fields ?? {});\n\nconst apiKey =\nfields?.apiKey ||\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.COHERE_API_KEY\n: undefined);\n\nif (!apiKey) {\nthrow new Error(\"Cohere API key not found\");\n}\n\nthis.modelName = fields?.modelName ?? this.modelName;\nthis.batchSize = fields?.batchSize ?? this.batchSize;\nthis.apiKey = apiKey;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/cohere.ts","loc":{"lines":{"from":137,"to":175}}}}],["566",{"pageContent":"/**\n* Generates embeddings for an array of texts.\n* @param texts - An array of strings to generate embeddings for.\n* @returns A Promise that resolves to an array of embeddings.\n*/\nasync embedDocuments(texts: string[]): Promise<number[][]> {\nawait this.maybeInitClient();\n\nconst subPrompts = chunkArray(texts, this.batchSize);\n\nconst embeddings: number[][] = [];\n\nfor (let i = 0; i < subPrompts.length; i += 1) {\nconst input = subPrompts[i];\nconst { body } = await this.embeddingWithRetry({\nmodel: this.modelName,\ntexts: input,\n});\nfor (let j = 0; j < input.length; j += 1) {\nembeddings.push(body.embeddings[j]);\n}\n}\n\nreturn embeddings;\n}\n\n/**\n* Generates an embedding for a single text.\n* @param text - A string to generate an embedding for.\n* @returns A Promise that resolves to an array of numbers representing the embedding.\n*/\nasync embedQuery(text: string): Promise<number[]> {\nawait this.maybeInitClient();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/cohere.ts","loc":{"lines":{"from":278,"to":310}}}}],["567",{"pageContent":"const { body } = await this.embeddingWithRetry({\nmodel: this.modelName,\ntexts: [text],\n});\nreturn body.embeddings[0];\n}\n\n/**\n* Generates embeddings with retry capabilities.\n* @param request - An object containing the request parameters for generating embeddings.\n* @returns A Promise that resolves to the API response.\n*/\nprivate async embeddingWithRetry(\nrequest: Parameters<typeof this.client.embed>[0]\n) {\nawait this.maybeInitClient();\n\nreturn this.caller.call(this.client.embed.bind(this.client), request);\n}\n\n/**\n* Initializes the Cohere client if it hasn't been initialized already.\n*/\nprivate async maybeInitClient() {\nif (!this.client) {\nconst { cohere } = await CohereEmbeddings.imports();\n\nthis.client = cohere;\nthis.client.init(this.apiKey);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/cohere.ts","loc":{"lines":{"from":407,"to":437}}}}],["568",{"pageContent":"/** @ignore */\nstatic async imports(): Promise<{\ncohere: typeof import(\"cohere-ai\");\n}> {\ntry {\nconst { default: cohere } = await import(\"cohere-ai\");\nreturn { cohere };\n} catch (e) {\nthrow new Error(\n\"Please install cohere-ai as a dependency with, e.g. `yarn add cohere-ai`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/cohere.ts","loc":{"lines":{"from":542,"to":555}}}}],["569",{"pageContent":"import { Embeddings, EmbeddingsParams } from \"./base.js\";\n\nexport class FakeEmbeddings extends Embeddings {\nconstructor(params?: EmbeddingsParams) {\nsuper(params ?? {});\n}\n\nembedDocuments(documents: string[]): Promise<number[][]> {\nreturn Promise.resolve(documents.map(() => [0.1, 0.2, 0.3, 0.4]));\n}\n\nembedQuery(_: string): Promise<number[]> {\nreturn Promise.resolve([0.1, 0.2, 0.3, 0.4]);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/fake.ts","loc":{"lines":{"from":1,"to":15}}}}],["570",{"pageContent":"import { HfInference } from \"@huggingface/inference\";\nimport { Embeddings, EmbeddingsParams } from \"./base.js\";\n\nexport interface HuggingFaceInferenceEmbeddingsParams extends EmbeddingsParams {\napiKey?: string;\nmodel?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/hf.ts","loc":{"lines":{"from":1,"to":7}}}}],["571",{"pageContent":"class HuggingFaceInferenceEmbeddings\nextends Embeddings\nimplements HuggingFaceInferenceEmbeddingsParams\n{\napiKey?: string;\n\nmodel: string;\n\nclient: HfInference;\n\nconstructor(fields?: HuggingFaceInferenceEmbeddingsParams) {\nsuper(fields ?? {});\n\nthis.model =\nfields?.model ?? \"sentence-transformers/distilbert-base-nli-mean-tokens\";\nthis.apiKey =\nfields?.apiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.HUGGINGFACEHUB_API_KEY\n: undefined);\nthis.client = new HfInference(this.apiKey);\n}\n\nasync _embed(texts: string[]): Promise<number[][]> {\n// replace newlines, which can negatively affect performance.\nconst clean = texts.map((text) => text.replace(/\\n/g, \" \"));\nreturn this.caller.call(() =>\nthis.client.featureExtraction({\nmodel: this.model,\ninputs: clean,\n})\n) as Promise<number[][]>;\n}\n\nembedQuery(document: string): Promise<number[]> {\nreturn this._embed([document]).then((embeddings) => embeddings[0]);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/hf.ts","loc":{"lines":{"from":52,"to":89}}}}],["572",{"pageContent":"embedDocuments(documents: string[]): Promise<number[][]> {\nreturn this._embed(documents);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/hf.ts","loc":{"lines":{"from":106,"to":109}}}}],["573",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/embeddings' is deprecated. Import from eg. 'langchain/embeddings/openai' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport { OpenAIEmbeddings } from \"./openai.js\";\nexport { CohereEmbeddings } from \"./cohere.js\";\nexport { Embeddings } from \"./base.js\";\nexport { FakeEmbeddings } from \"./fake.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/index.ts","loc":{"lines":{"from":1,"to":8}}}}],["574",{"pageContent":"import {\nConfiguration,\nOpenAIApi,\nCreateEmbeddingRequest,\nConfigurationParameters,\n} from \"openai\";\nimport type { AxiosRequestConfig } from \"axios\";\nimport { AzureOpenAIInput } from \"../types/open-ai-types.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { chunkArray } from \"../util/chunk.js\";\nimport { Embeddings, EmbeddingsParams } from \"./base.js\";\n\nexport interface OpenAIEmbeddingsParams extends EmbeddingsParams {\n/** Model name to use */\nmodelName: string;\n\n/**\n* Timeout to use when making requests to OpenAI.\n*/\ntimeout?: number;\n\n/**\n* The maximum number of documents to embed in a single request. This is\n* limited by the OpenAI API to a maximum of 2048.\n*/\nbatchSize?: number;\n\n/**\n* Whether to strip new lines from the input text. This is recommended by\n* OpenAI, but may not be suitable for all use cases.\n*/\nstripNewLines?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/openai.ts","loc":{"lines":{"from":1,"to":33}}}}],["575",{"pageContent":"class OpenAIEmbeddings\nextends Embeddings\nimplements OpenAIEmbeddingsParams, AzureOpenAIInput\n{\nmodelName = \"text-embedding-ada-002\";\n\nbatchSize = 512;\n\nstripNewLines = true;\n\ntimeout?: number;\n\nazureOpenAIApiVersion?: string;\n\nazureOpenAIApiKey?: string;\n\nazureOpenAIApiInstanceName?: string;\n\nazureOpenAIApiDeploymentName?: string;\n\nprivate client: OpenAIApi;\n\nprivate clientConfig: ConfigurationParameters;\n\nconstructor(\nfields?: Partial<OpenAIEmbeddingsParams> &\nPartial<AzureOpenAIInput> & {\nverbose?: boolean;\nopenAIApiKey?: string;\n},\nconfiguration?: ConfigurationParameters\n) {\nsuper(fields ?? {});\n\nconst apiKey =\nfields?.openAIApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.OPENAI_API_KEY\n: undefined);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/openai.ts","loc":{"lines":{"from":204,"to":243}}}}],["576",{"pageContent":"const azureApiKey =\nfields?.azureOpenAIApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_KEY\n: undefined);\nif (!azureApiKey && !apiKey) {\nthrow new Error(\"(Azure) OpenAI API key not found\");\n}\n\nconst azureApiInstanceName =\nfields?.azureOpenAIApiInstanceName ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_INSTANCE_NAME\n: undefined);\n\nconst azureApiDeploymentName =\n(fields?.azureOpenAIApiEmbeddingsDeploymentName ||\nfields?.azureOpenAIApiDeploymentName) ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME ||\n// eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_DEPLOYMENT_NAME\n: undefined);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/openai.ts","loc":{"lines":{"from":415,"to":440}}}}],["577",{"pageContent":"const azureApiVersion =\nfields?.azureOpenAIApiVersion ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_VERSION\n: undefined);\n\nthis.modelName = fields?.modelName ?? this.modelName;\nthis.batchSize = fields?.batchSize ?? this.batchSize;\nthis.stripNewLines = fields?.stripNewLines ?? this.stripNewLines;\nthis.timeout = fields?.timeout;\n\nthis.azureOpenAIApiVersion = azureApiVersion;\nthis.azureOpenAIApiKey = azureApiKey;\nthis.azureOpenAIApiInstanceName = azureApiInstanceName;\nthis.azureOpenAIApiDeploymentName = azureApiDeploymentName;\n\nif (this.azureOpenAIApiKey) {\nif (!this.azureOpenAIApiInstanceName) {\nthrow new Error(\"Azure OpenAI API instance name not found\");\n}\nif (!this.azureOpenAIApiDeploymentName) {\nthrow new Error(\"Azure OpenAI API deployment name not found\");\n}\nif (!this.azureOpenAIApiVersion) {\nthrow new Error(\"Azure OpenAI API version not found\");\n}\n}\n\nthis.clientConfig = {\napiKey,\n...configuration,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/openai.ts","loc":{"lines":{"from":612,"to":645}}}}],["578",{"pageContent":"async embedDocuments(texts: string[]): Promise<number[][]> {\nconst subPrompts = chunkArray(\nthis.stripNewLines ? texts.map((t) => t.replaceAll(\"\\n\", \" \")) : texts,\nthis.batchSize\n);\n\nconst embeddings: number[][] = [];\n\nfor (let i = 0; i < subPrompts.length; i += 1) {\nconst input = subPrompts[i];\nconst { data } = await this.embeddingWithRetry({\nmodel: this.modelName,\ninput,\n});\nfor (let j = 0; j < input.length; j += 1) {\nembeddings.push(data.data[j].embedding);\n}\n}\n\nreturn embeddings;\n}\n\nasync embedQuery(text: string): Promise<number[]> {\nconst { data } = await this.embeddingWithRetry({\nmodel: this.modelName,\ninput: this.stripNewLines ? text.replaceAll(\"\\n\", \" \") : text,\n});\nreturn data.data[0].embedding;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/openai.ts","loc":{"lines":{"from":809,"to":837}}}}],["579",{"pageContent":"private async embeddingWithRetry(request: CreateEmbeddingRequest) {\nif (!this.client) {\nconst endpoint = this.azureOpenAIApiKey\n? `https://${this.azureOpenAIApiInstanceName}.openai.azure.com/openai/deployments/${this.azureOpenAIApiDeploymentName}`\n: this.clientConfig.basePath;\nconst clientConfig = new Configuration({\n...this.clientConfig,\nbasePath: endpoint,\nbaseOptions: {\ntimeout: this.timeout,\nadapter: fetchAdapter,\n...this.clientConfig.baseOptions,\n},\n});\nthis.client = new OpenAIApi(clientConfig);\n}\nconst axiosOptions: AxiosRequestConfig = {};\nif (this.azureOpenAIApiKey) {\naxiosOptions.headers = {\n\"api-key\": this.azureOpenAIApiKey,\n...axiosOptions.headers,\n};\naxiosOptions.params = {\n\"api-version\": this.azureOpenAIApiVersion,\n...axiosOptions.params,\n};\n}\nreturn this.caller.call(\nthis.client.createEmbedding.bind(this.client),\nrequest,\naxiosOptions\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/openai.ts","loc":{"lines":{"from":1014,"to":1047}}}}],["580",{"pageContent":"import { load } from \"@tensorflow-models/universal-sentence-encoder\";\nimport * as tf from \"@tensorflow/tfjs-core\";\n\nimport { Embeddings, EmbeddingsParams } from \"./base.js\";\n\nexport interface TensorFlowEmbeddingsParams extends EmbeddingsParams {}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tensorflow.ts","loc":{"lines":{"from":1,"to":6}}}}],["581",{"pageContent":"class TensorFlowEmbeddings extends Embeddings {\nconstructor(fields?: TensorFlowEmbeddingsParams) {\nsuper(fields ?? {});\n\ntry {\ntf.backend();\n} catch (e) {\nthrow new Error(\"No TensorFlow backend found, see instructions at ...\");\n}\n}\n\n_cached: ReturnType<typeof load>;\n\nprivate async load() {\nif (this._cached === undefined) {\nthis._cached = load();\n}\nreturn this._cached;\n}\n\nprivate _embed(texts: string[]) {\nreturn this.caller.call(async () => {\nconst model = await this.load();\nreturn model.embed(texts);\n});\n}\n\nembedQuery(document: string): Promise<number[]> {\nreturn this._embed([document])\n.then((embeddings) => embeddings.array())\n.then((embeddings) => embeddings[0]);\n}\n\nembedDocuments(documents: string[]): Promise<number[][]> {\nreturn this._embed(documents).then((embeddings) => embeddings.array());\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tensorflow.ts","loc":{"lines":{"from":44,"to":80}}}}],["582",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { CohereEmbeddings } from \"../cohere.js\";\n\ntest(\"Test CohereEmbeddings.embedQuery\", async () => {\nconst embeddings = new CohereEmbeddings();\nconst res = await embeddings.embedQuery(\"Hello world\");\nexpect(typeof res[0]).toBe(\"number\");\n});\n\ntest(\"Test CohereEmbeddings.embedDocuments\", async () => {\nconst embeddings = new CohereEmbeddings();\nconst res = await embeddings.embedDocuments([\"Hello world\", \"Bye bye\"]);\nexpect(res).toHaveLength(2);\nexpect(typeof res[0][0]).toBe(\"number\");\nexpect(typeof res[1][0]).toBe(\"number\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tests/cohere.int.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["583",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { HuggingFaceInferenceEmbeddings } from \"../hf.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"HuggingFaceInferenceEmbeddings\", async () => {\nconst embeddings = new HuggingFaceInferenceEmbeddings();\n\nconst documents = [\n\"Hello world!\",\n\"Hello bad world!\",\n\"Hello nice world!\",\n\"Hello good world!\",\n\"1 + 1 = 2\",\n\"1 + 1 = 3\",\n];\n\nconst queryEmbedding = await embeddings.embedQuery(documents[0]);\nexpect(queryEmbedding).toHaveLength(768);\nexpect(typeof queryEmbedding[0]).toBe(\"number\");\n\nconst store = new MemoryVectorStore(embeddings);\n\nawait store.addDocuments(\ndocuments.map((pageContent) => new Document({ pageContent }))\n);\n\nexpect(await store.similaritySearch(documents[4], 2)).toMatchInlineSnapshot(`\n[\nDocument {\n\"metadata\": {},\n\"pageContent\": \"1 + 1 = 2\",\n},\nDocument {\n\"metadata\": {},\n\"pageContent\": \"1 + 1 = 3\",\n},\n]\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tests/hf.int.test.ts","loc":{"lines":{"from":1,"to":40}}}}],["584",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { OpenAIEmbeddings } from \"../openai.js\";\n\ntest(\"Test OpenAIEmbeddings.embedQuery\", async () => {\nconst embeddings = new OpenAIEmbeddings();\nconst res = await embeddings.embedQuery(\"Hello world\");\nexpect(typeof res[0]).toBe(\"number\");\n});\n\ntest(\"Test OpenAIEmbeddings.embedDocuments\", async () => {\nconst embeddings = new OpenAIEmbeddings();\nconst res = await embeddings.embedDocuments([\"Hello world\", \"Bye bye\"]);\nexpect(res).toHaveLength(2);\nexpect(typeof res[0][0]).toBe(\"number\");\nexpect(typeof res[1][0]).toBe(\"number\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tests/openai.int.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["585",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport \"@tensorflow/tfjs-backend-cpu\";\nimport { TensorFlowEmbeddings } from \"../tensorflow.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"TensorflowEmbeddings\", async () => {\nconst embeddings = new TensorFlowEmbeddings();\n\nconst documents = [\n\"Hello world!\",\n\"Hello bad world!\",\n\"Hello nice world!\",\n\"Hello good world!\",\n\"1 + 1 = 2\",\n\"1 + 1 = 3\",\n];\n\nconst queryEmbedding = await embeddings.embedQuery(documents[0]);\nexpect(queryEmbedding).toHaveLength(512);\nexpect(typeof queryEmbedding[0]).toBe(\"number\");\n\nconst store = new MemoryVectorStore(embeddings);\n\nawait store.addDocuments(\ndocuments.map((pageContent) => new Document({ pageContent }))\n);\n\nexpect(await store.similaritySearch(documents[4], 2)).toMatchInlineSnapshot(`\n[\nDocument {\n\"metadata\": {},\n\"pageContent\": \"1 + 1 = 2\",\n},\nDocument {\n\"metadata\": {},\n\"pageContent\": \"1 + 1 = 3\",\n},\n]\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tests/tensorflow.int.test.ts","loc":{"lines":{"from":1,"to":41}}}}],["586",{"pageContent":"import { LLMChain } from \"../../chains/llm_chain.js\";\nimport { BaseChatModel } from \"../../chat_models/base.js\";\nimport { VectorStoreRetriever } from \"../../vectorstores/base.js\";\nimport { Tool } from \"../../tools/base.js\";\n\nimport { AutoGPTOutputParser } from \"./output_parser.js\";\nimport { AutoGPTPrompt } from \"./prompt.js\";\nimport {\nAIChatMessage,\nBaseChatMessage,\nHumanChatMessage,\nSystemChatMessage,\n} from \"../../schema/index.js\";\n// import { HumanInputRun } from \"./tools/human/tool\"; // TODO\nimport { ObjectTool, FINISH_NAME } from \"./schema.js\";\nimport { TokenTextSplitter } from \"../../text_splitter.js\";\nimport {\ngetEmbeddingContextSize,\ngetModelContextSize,\n} from \"../../base_language/count_tokens.js\";\n\nexport interface AutoGPTInput {\naiName: string;\naiRole: string;\nmemory: VectorStoreRetriever;\nhumanInTheLoop?: boolean;\noutputParser?: AutoGPTOutputParser;\nmaxIterations?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":1,"to":29}}}}],["587",{"pageContent":"class AutoGPT {\naiName: string;\n\nmemory: VectorStoreRetriever;\n\nfullMessageHistory: BaseChatMessage[];\n\nnextActionCount: number;\n\nchain: LLMChain;\n\noutputParser: AutoGPTOutputParser;\n\ntools: ObjectTool[];\n\nfeedbackTool?: Tool;\n\nmaxIterations: number;\n\n// Currently not generic enough to support any text splitter.\ntextSplitter: TokenTextSplitter;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":181,"to":201}}}}],["588",{"pageContent":"constructor({\naiName,\nmemory,\nchain,\noutputParser,\ntools,\nfeedbackTool,\nmaxIterations,\n}: Omit<Required<AutoGPTInput>, \"aiRole\" | \"humanInTheLoop\"> & {\nchain: LLMChain;\ntools: ObjectTool[];\nfeedbackTool?: Tool;\n}) {\nthis.aiName = aiName;\nthis.memory = memory;\nthis.fullMessageHistory = [];\nthis.nextActionCount = 0;\nthis.chain = chain;\nthis.outputParser = outputParser;\nthis.tools = tools;\nthis.feedbackTool = feedbackTool;\nthis.maxIterations = maxIterations;\nconst chunkSize = getEmbeddingContextSize(\n\"modelName\" in memory.vectorStore.embeddings\n? (memory.vectorStore.embeddings.modelName as string)\n: undefined\n);\nthis.textSplitter = new TokenTextSplitter({\nchunkSize,\nchunkOverlap: Math.round(chunkSize / 10),\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":372,"to":403}}}}],["589",{"pageContent":"static fromLLMAndTools(\nllm: BaseChatModel,\ntools: ObjectTool[],\n{\naiName,\naiRole,\nmemory,\nmaxIterations = 100,\n// humanInTheLoop = false,\noutputParser = new AutoGPTOutputParser(),\n}: AutoGPTInput\n): AutoGPT {\nconst prompt = new AutoGPTPrompt({\naiName,\naiRole,\ntools,\ntokenCounter: llm.getNumTokens.bind(llm),\nsendTokenLimit: getModelContextSize(\n\"modelName\" in llm ? (llm.modelName as string) : \"gpt2\"\n),\n});\n// const feedbackTool = humanInTheLoop ? new HumanInputRun() : null;\nconst chain = new LLMChain({ llm, prompt });\nreturn new AutoGPT({\naiName,\nmemory,\nchain,\noutputParser,\ntools,\n// feedbackTool,\nmaxIterations,\n});\n}\n\nasync run(goals: string[]): Promise<string | undefined> {\nconst user_input =\n\"Determine which next command to use, and respond using the format specified above:\";\nlet loopCount = 0;\nwhile (loopCount < this.maxIterations) {\nloopCount += 1;\n\nconst { text: assistantReply } = await this.chain.call({\ngoals,\nuser_input,\nmemory: this.memory,\nmessages: this.fullMessageHistory,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":563,"to":609}}}}],["590",{"pageContent":"// Print the assistant reply\nconsole.log(assistantReply);\nthis.fullMessageHistory.push(new HumanChatMessage(user_input));\nthis.fullMessageHistory.push(new AIChatMessage(assistantReply));\n\nconst action = await this.outputParser.parse(assistantReply);\nconst tools = this.tools.reduce(\n(acc, tool) => ({ ...acc, [tool.name]: tool }),\n{} as { [key: string]: ObjectTool }\n);\nif (action.name === FINISH_NAME) {\nreturn action.args.response;\n}\nlet result: string;\nif (action.name in tools) {\nconst tool = tools[action.name];\nlet observation;\ntry {\nobservation = await tool.call(action.args);\n} catch (e) {\nobservation = `Error in args: ${e}`;\n}\nresult = `Command ${tool.name} returned: ${observation}`;\n} else if (action.name === \"ERROR\") {\nresult = `Error: ${action.args}. `;\n} else {\nresult = `Unknown command '${action.name}'. Please refer to the 'COMMANDS' list for available commands and only respond in the specified JSON format.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":753,"to":780}}}}],["591",{"pageContent":"let memoryToAdd = `Assistant Reply: ${assistantReply}\\nResult: ${result} `;\nif (this.feedbackTool) {\nconst feedback = `\\n${await this.feedbackTool.call(\"Input: \")}`;\nif (feedback === \"q\" || feedback === \"stop\") {\nconsole.log(\"EXITING\");\nreturn \"EXITING\";\n}\nmemoryToAdd += feedback;\n}\n\nconst documents = await this.textSplitter.createDocuments([memoryToAdd]);\nawait this.memory.addDocuments(documents);\nthis.fullMessageHistory.push(new SystemChatMessage(result));\n}\n\nreturn undefined;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":929,"to":946}}}}],["592",{"pageContent":"export { AutoGPTPrompt, AutoGPTPromptInput } from \"./prompt.js\";\n\nexport { AutoGPTOutputParser, preprocessJsonInput } from \"./output_parser.js\";\n\nexport { AutoGPT, AutoGPTInput } from \"./agent.js\";\n\nexport { AutoGPTAction } from \"./schema.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/index.ts","loc":{"lines":{"from":1,"to":7}}}}],["593",{"pageContent":"import { BaseOutputParser } from \"../../schema/output_parser.js\";\nimport { AutoGPTAction } from \"./schema.js\";\n\nexport function preprocessJsonInput(inputStr: string): string {\n// Replace single backslashes with double backslashes,\n// while leaving already escaped ones intact\nconst correctedStr = inputStr.replace(\n/(?<!\\\\)\\\\(?![\"\\\\/bfnrt]|u[0-9a-fA-F]{4})/g,\n\"\\\\\\\\\"\n);\nreturn correctedStr;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/output_parser.ts","loc":{"lines":{"from":1,"to":12}}}}],["594",{"pageContent":"class AutoGPTOutputParser extends BaseOutputParser<AutoGPTAction> {\ngetFormatInstructions(): string {\nthrow new Error(\"Method not implemented.\");\n}\n\nasync parse(text: string): Promise<AutoGPTAction> {\nlet parsed: {\ncommand: {\nname: string;\nargs: Record<string, unknown>;\n};\n};\ntry {\nparsed = JSON.parse(text);\n} catch (error) {\nconst preprocessedText = preprocessJsonInput(text);\ntry {\nparsed = JSON.parse(preprocessedText);\n} catch (error) {\nreturn {\nname: \"ERROR\",\nargs: { error: `Could not parse invalid json: ${text}` },\n};\n}\n}\ntry {\nreturn {\nname: parsed.command.name,\nargs: parsed.command.args,\n};\n} catch (error) {\nreturn {\nname: \"ERROR\",\nargs: { error: `Incomplete command args: ${parsed}` },\n};\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/output_parser.ts","loc":{"lines":{"from":52,"to":89}}}}],["595",{"pageContent":"import { BaseChatPromptTemplate } from \"../../prompts/chat.js\";\nimport {\nBaseChatMessage,\nHumanChatMessage,\nPartialValues,\nSystemChatMessage,\n} from \"../../schema/index.js\";\nimport { VectorStoreRetriever } from \"../../vectorstores/base.js\";\nimport { ObjectTool } from \"./schema.js\";\nimport { getPrompt } from \"./prompt_generator.js\";\nimport { BasePromptTemplate } from \"../../index.js\";\nimport { SerializedBasePromptTemplate } from \"../../prompts/serde.js\";\n\nexport interface AutoGPTPromptInput {\naiName: string;\naiRole: string;\ntools: ObjectTool[];\ntokenCounter: (text: string) => Promise<number>;\nsendTokenLimit?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt.ts","loc":{"lines":{"from":1,"to":20}}}}],["596",{"pageContent":"class AutoGPTPrompt\nextends BaseChatPromptTemplate\nimplements AutoGPTPromptInput\n{\naiName: string;\n\naiRole: string;\n\ntools: ObjectTool[];\n\ntokenCounter: (text: string) => Promise<number>;\n\nsendTokenLimit: number;\n\nconstructor(fields: AutoGPTPromptInput) {\nsuper({ inputVariables: [\"goals\", \"memory\", \"messages\", \"user_input\"] });\nthis.aiName = fields.aiName;\nthis.aiRole = fields.aiRole;\nthis.tools = fields.tools;\nthis.tokenCounter = fields.tokenCounter;\nthis.sendTokenLimit = fields.sendTokenLimit || 4196;\n}\n\n_getPromptType() {\nreturn \"autogpt\" as const;\n}\n\nconstructFullPrompt(goals: string[]): string {\nconst promptStart = `Your decisions must always be made independently \nwithout seeking user assistance. Play to your strengths \nas an LLM and pursue simple strategies with no legal complications. \nIf you have completed all your tasks, \nmake sure to use the \"finish\" command.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt.ts","loc":{"lines":{"from":135,"to":167}}}}],["597",{"pageContent":"let fullPrompt = `You are ${this.aiName}, ${this.aiRole}\\n${promptStart}\\n\\nGOALS:\\n\\n`;\ngoals.forEach((goal, index) => {\nfullPrompt += `${index + 1}. ${goal}\\n`;\n});\n\nfullPrompt += `\\n\\n${getPrompt(this.tools)}`;\nreturn fullPrompt;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt.ts","loc":{"lines":{"from":265,"to":272}}}}],["598",{"pageContent":"async formatMessages({\ngoals,\nmemory,\nmessages: previousMessages,\nuser_input,\n}: {\ngoals: string[];\nmemory: VectorStoreRetriever;\nmessages: BaseChatMessage[];\nuser_input: string;\n}) {\nconst basePrompt = new SystemChatMessage(this.constructFullPrompt(goals));\nconst timePrompt = new SystemChatMessage(\n`The current time and date is ${new Date().toLocaleString()}`\n);\nconst usedTokens =\n(await this.tokenCounter(basePrompt.text)) +\n(await this.tokenCounter(timePrompt.text));\nconst relevantDocs = await memory.getRelevantDocuments(\nJSON.stringify(previousMessages.slice(-10))\n);\nconst relevantMemory = relevantDocs.map((d) => d.pageContent);\nlet relevantMemoryTokens = await relevantMemory.reduce(\nasync (acc, doc) => (await acc) + (await this.tokenCounter(doc)),\nPromise.resolve(0)\n);\n\nwhile (usedTokens + relevantMemoryTokens > 2500) {\nrelevantMemory.pop();\nrelevantMemoryTokens = await relevantMemory.reduce(\nasync (acc, doc) => (await acc) + (await this.tokenCounter(doc)),\nPromise.resolve(0)\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt.ts","loc":{"lines":{"from":398,"to":431}}}}],["599",{"pageContent":"const contentFormat = `This reminds you of these events from your past:\\n${relevantMemory.join(\n\"\\n\"\n)}\\n\\n`;\nconst memoryMessage = new SystemChatMessage(contentFormat);\nconst usedTokensWithMemory =\n(await usedTokens) + (await this.tokenCounter(memoryMessage.text));\nconst historicalMessages: BaseChatMessage[] = [];\n\nfor (const message of previousMessages.slice(-10).reverse()) {\nconst messageTokens = await this.tokenCounter(message.text);\nif (usedTokensWithMemory + messageTokens > this.sendTokenLimit - 1000) {\nbreak;\n}\nhistoricalMessages.unshift(message);\n}\n\nconst inputMessage = new HumanChatMessage(user_input);\nconst messages: BaseChatMessage[] = [\nbasePrompt,\ntimePrompt,\nmemoryMessage,\n...historicalMessages,\ninputMessage,\n];\nreturn messages;\n}\n\nasync partial(_values: PartialValues): Promise<BasePromptTemplate> {\nthrow new Error(\"Method not implemented.\");\n}\n\nserialize(): SerializedBasePromptTemplate {\nthrow new Error(\"Method not implemented.\");\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt.ts","loc":{"lines":{"from":528,"to":562}}}}],["600",{"pageContent":"import { zodToJsonSchema } from \"zod-to-json-schema\";\nimport { JsonSchema7ObjectType } from \"zod-to-json-schema/src/parsers/object.js\";\n\nimport { ObjectTool, FINISH_NAME } from \"./schema.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":1,"to":4}}}}],["601",{"pageContent":"class PromptGenerator {\nconstraints: string[];\n\ncommands: ObjectTool[];\n\nresources: string[];\n\nperformance_evaluation: string[];\n\nresponse_format: object;\n\nconstructor() {\nthis.constraints = [];\nthis.commands = [];\nthis.resources = [];\nthis.performance_evaluation = [];\nthis.response_format = {\nthoughts: {\ntext: \"thought\",\nreasoning: \"reasoning\",\nplan: \"- short bulleted\\n- list that conveys\\n- long-term plan\",\ncriticism: \"constructive self-criticism\",\nspeak: \"thoughts summary to say to user\",\n},\ncommand: { name: \"command name\", args: { \"arg name\": \"value\" } },\n};\n}\n\nadd_constraint(constraint: string): void {\nthis.constraints.push(constraint);\n}\n\nadd_tool(tool: ObjectTool): void {\nthis.commands.push(tool);\n}\n\n_generate_command_string(tool: ObjectTool): string {\nlet output = `\"${tool.name}\": ${tool.description}`;\noutput += `, args json schema: ${JSON.stringify(\n(zodToJsonSchema(tool.schema) as JsonSchema7ObjectType).properties\n)}`;\nreturn output;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":150,"to":192}}}}],["602",{"pageContent":"add_resource(resource: string): void {\nthis.resources.push(resource);\n}\n\nadd_performance_evaluation(evaluation: string): void {\nthis.performance_evaluation.push(evaluation);\n}\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_generate_numbered_list(items: any[], item_type = \"list\"): string {\nif (item_type === \"command\") {\nconst command_strings = items.map(\n(item, i) => `${i + 1}. ${this._generate_command_string(item)}`\n);\nconst finish_description =\n\"use this to signal that you have finished all your objectives\";\nconst finish_args =\n'\"response\": \"final response to let people know you have finished your objectives\"';\nconst finish_string = `${\nitems.length + 1\n}. ${FINISH_NAME}: ${finish_description}, args: ${finish_args}`;\nreturn command_strings.concat([finish_string]).join(\"\\n\");\n}\n\nreturn items.map((item, i) => `${i + 1}. ${item}`).join(\"\\n\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":304,"to":329}}}}],["603",{"pageContent":"generate_prompt_string(): string {\nconst formatted_response_format = JSON.stringify(\nthis.response_format,\nnull,\n4\n);\nconst prompt_string =\n`Constraints:\\n${this._generate_numbered_list(this.constraints)}\\n\\n` +\n`Commands:\\n${this._generate_numbered_list(\nthis.commands,\n\"command\"\n)}\\n\\n` +\n`Resources:\\n${this._generate_numbered_list(this.resources)}\\n\\n` +\n`Performance Evaluation:\\n${this._generate_numbered_list(\nthis.performance_evaluation\n)}\\n\\n` +\n`You should only respond in JSON format as described below ` +\n`\\nResponse Format: \\n${formatted_response_format} ` +\n`\\nEnsure the response can be parsed by Python json.loads`;\n\nreturn prompt_string;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":445,"to":467}}}}],["604",{"pageContent":"function getPrompt(tools: ObjectTool[]): string {\nconst prompt_generator = new PromptGenerator();\n\nprompt_generator.add_constraint(\n\"~4000 word limit for short term memory. \" +\n\"Your short term memory is short, \" +\n\"so immediately save important information to files.\"\n);\nprompt_generator.add_constraint(\n\"If you are unsure how you previously did something \" +\n\"or want to recall past events, \" +\n\"thinking about similar events will help you remember.\"\n);\nprompt_generator.add_constraint(\"No user assistance\");\nprompt_generator.add_constraint(\n'Exclusively use the commands listed in double quotes e.g. \"command name\"'\n);\n\nfor (const tool of tools) {\nprompt_generator.add_tool(tool);\n}\n\nprompt_generator.add_resource(\n\"Internet access for searches and information gathering.\"\n);\nprompt_generator.add_resource(\"Long Term memory management.\");\nprompt_generator.add_resource(\n\"GPT-3.5 powered Agents for delegation of simple tasks.\"\n);\nprompt_generator.add_resource(\"File output.\");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":591,"to":620}}}}],["605",{"pageContent":"prompt_generator.add_performance_evaluation(\n\"Continuously review and analyze your actions \" +\n\"to ensure you are performing to the best of your abilities.\"\n);\nprompt_generator.add_performance_evaluation(\n\"Constructively self-criticize your big-picture behavior constantly.\"\n);\nprompt_generator.add_performance_evaluation(\n\"Reflect on past decisions and strategies to refine your approach.\"\n);\nprompt_generator.add_performance_evaluation(\n\"Every command has a cost, so be smart and efficient. \" +\n\"Aim to complete tasks in the least number of steps.\"\n);\n\nconst prompt_string = prompt_generator.generate_prompt_string();\n\nreturn prompt_string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":732,"to":750}}}}],["606",{"pageContent":"import { StructuredTool } from \"../../tools/base.js\";\n\nexport type ObjectTool = StructuredTool;\n\nexport const FINISH_NAME = \"finish\";\n\nexport interface AutoGPTAction {\nname: string;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nargs: Record<string, any>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/schema.ts","loc":{"lines":{"from":1,"to":11}}}}],["607",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { z } from \"zod\";\n\nimport { getPrompt } from \"../prompt_generator.js\";\nimport { StructuredTool } from \"../../../tools/base.js\";\nimport { Calculator } from \"../../../tools/calculator.js\";\nimport { ReadFileTool, WriteFileTool } from \"../../../tools/fs.js\";\nimport { InMemoryFileStore } from \"../../../stores/file/in_memory.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/tests/prompt_generator.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["608",{"pageContent":"FakeBrowserTool extends StructuredTool {\nschema = z.object({\nurl: z.string(),\nquery: z.string().optional(),\n});\n\nname = \"fake_browser_tool\";\n\ndescription =\n\"useful for when you need to find something on the web or summarize a webpage.\";\n\nasync _call({\nurl: _url,\nquery: _query,\n}: z.infer<this[\"schema\"]>): Promise<string> {\nreturn \"fake_browser_tool\";\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/tests/prompt_generator.test.ts","loc":{"lines":{"from":83,"to":100}}}}],["609",{"pageContent":"test(\"prompt with several tools\", () => {\nconst store = new InMemoryFileStore();\nconst tools = [\nnew FakeBrowserTool(),\nnew Calculator(),\nnew ReadFileTool({ store }),\nnew WriteFileTool({ store }),\n];\nconst prompt = getPrompt(tools);\nexpect(prompt).toMatchInlineSnapshot(`\n\"Constraints:\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n3. No user assistance\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/tests/prompt_generator.test.ts","loc":{"lines":{"from":175,"to":189}}}}],["610",{"pageContent":"Commands:\n1. \"fake_browser_tool\": useful for when you need to find something on the web or summarize a webpage., args json schema: {\"url\":{\"type\":\"string\"},\"query\":{\"type\":\"string\"}}\n2. \"calculator\": Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator., args json schema: {\"input\":{\"type\":\"string\"}}\n3. \"read_file\": Read file from disk, args json schema: {\"file_path\":{\"type\":\"string\",\"description\":\"name of file\"}}\n4. \"write_file\": Write file from disk, args json schema: {\"file_path\":{\"type\":\"string\",\"description\":\"name of file\"},\"text\":{\"type\":\"string\",\"description\":\"text to write to file\"}}\n5. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/tests/prompt_generator.test.ts","loc":{"lines":{"from":251,"to":256}}}}],["611",{"pageContent":"Resources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n\nYou should only respond in JSON format as described below \nResponse Format: \n{\n\"thoughts\": {\n\"text\": \"thought\",\n\"reasoning\": \"reasoning\",\n\"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\n\"criticism\": \"constructive self-criticism\",\n\"speak\": \"thoughts summary to say to user\"\n},\n\"command\": {\n\"name\": \"command name\",\n\"args\": {\n\"arg name\": \"value\"\n}\n}\n} \nEnsure the response can be parsed by Python json.loads\"\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/tests/prompt_generator.test.ts","loc":{"lines":{"from":308,"to":339}}}}],["612",{"pageContent":"import { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { CallbackManagerForChainRun } from \"../../callbacks/manager.js\";\nimport { BaseChain, ChainInputs } from \"../../chains/base.js\";\nimport { SerializedBaseChain } from \"../../chains/serde.js\";\nimport { Document } from \"../../document.js\";\nimport { ChainValues } from \"../../schema/index.js\";\nimport { Optional } from \"../../types/type-utils.js\";\nimport { VectorStore } from \"../../vectorstores/base.js\";\nimport { TaskCreationChain } from \"./task_creation.js\";\nimport { TaskExecutionChain } from \"./task_execution.js\";\nimport { TaskPrioritizationChain } from \"./task_prioritization.js\";\n\nexport interface Task {\ntaskID: string;\ntaskName: string;\n}\n\nexport interface BabyAGIInputs\nextends Omit<ChainInputs, \"memory\" | \"callbackManager\"> {\ncreationChain: BaseChain;\nprioritizationChain: BaseChain;\nexecutionChain: BaseChain;\nvectorstore: VectorStore;\nmaxIterations?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":1,"to":25}}}}],["613",{"pageContent":"class BabyAGI extends BaseChain implements BabyAGIInputs {\ntaskList: Task[];\n\ncreationChain: BaseChain;\n\nprioritizationChain: BaseChain;\n\nexecutionChain: BaseChain;\n\ntaskIDCounter: number;\n\nvectorstore: VectorStore;\n\nmaxIterations: number;\n\nconstructor({\ncreationChain,\nprioritizationChain,\nexecutionChain,\nvectorstore,\nmaxIterations = 100,\nverbose,\ncallbacks,\n}: BabyAGIInputs) {\nsuper(undefined, verbose, callbacks);\nthis.taskList = [];\nthis.creationChain = creationChain;\nthis.prioritizationChain = prioritizationChain;\nthis.executionChain = executionChain;\nthis.taskIDCounter = 1;\nthis.vectorstore = vectorstore;\nthis.maxIterations = maxIterations;\n}\n\n_chainType() {\nreturn \"BabyAGI\" as const;\n}\n\nget inputKeys() {\nreturn [\"objective\", \"firstTask\"];\n}\n\nget outputKeys() {\nreturn [];\n}\n\nasync addTask(task: Task) {\nthis.taskList.push(task);\n}\n\nprintTaskList() {\nconsole.log(\"\\x1b[95m\\x1b[1m\\n*****TASK LIST*****\\n\\x1b[0m\\x1b[0m\");\nfor (const t of this.taskList) {\nconsole.log(`${t.taskID}: ${t.taskName}`);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":264,"to":319}}}}],["614",{"pageContent":"printNextTask(task: Task) {\nconsole.log(\"\\x1b[92m\\x1b[1m\\n*****NEXT TASK*****\\n\\x1b[0m\\x1b[0m\");\nconsole.log(`${task.taskID}: ${task.taskName}`);\n}\n\nprintTaskResult(result: string) {\nconsole.log(\"\\x1b[93m\\x1b[1m\\n*****TASK RESULT*****\\n\\x1b[0m\\x1b[0m\");\nconsole.log(result.trim());\n}\n\nasync getNextTasks(\nresult: string,\ntask_description: string,\nobjective: string,\nrunManager?: CallbackManagerForChainRun\n): Promise<Optional<Task, \"taskID\">[]> {\nconst taskNames = this.taskList.map((t) => t.taskName);\nconst incomplete_tasks = taskNames.join(\", \");\nconst { [this.creationChain.outputKeys[0]]: text } =\nawait this.creationChain.call(\n{\nresult,\ntask_description,\nincomplete_tasks,\nobjective,\n},\nrunManager?.getChild()\n);\nconst newTasks = (text as string).split(\"\\n\");\nreturn newTasks\n.filter((taskName) => taskName.trim())\n.map((taskName) => ({ taskName }));\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":555,"to":587}}}}],["615",{"pageContent":"async prioritizeTasks(\nthisTaskID: number,\nobjective: string,\nrunManager?: CallbackManagerForChainRun\n) {\nconst taskNames = this.taskList.map((t) => t.taskName);\nconst nextTaskID = thisTaskID + 1;\nconst { [this.prioritizationChain.outputKeys[0]]: text } =\nawait this.prioritizationChain.call(\n{\ntask_names: taskNames.join(\", \"),\nnext_task_id: String(nextTaskID),\nobjective,\n},\nrunManager?.getChild()\n);\nconst newTasks = (text as string).trim().split(\"\\n\");\nconst prioritizedTaskList = [];\nfor (const taskString of newTasks) {\nconst taskParts = taskString.trim().split(\".\", 2);\nif (taskParts.length === 2) {\nconst taskID = taskParts[0].trim();\nconst taskName = taskParts[1].trim();\nprioritizedTaskList.push({ taskID, taskName });\n}\n}\nreturn prioritizedTaskList;\n}\n\nasync getTopTasks(query: string, k = 5) {\nconst results = await this.vectorstore.similaritySearch(query, k);\nif (!results) {\nreturn [];\n}\nreturn results.map((item) => String(item.metadata.task));\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":829,"to":864}}}}],["616",{"pageContent":"async executeTask(\nobjective: string,\ntask: string,\nrunManager?: CallbackManagerForChainRun\n) {\nconst context = await this.getTopTasks(objective);\nconst { [this.executionChain.outputKeys[0]]: text } =\nawait this.executionChain.call(\n{\nobjective,\ncontext: context.join(\"\\n\"),\ntask,\n},\nrunManager?.getChild()\n);\nreturn text as string;\n}\n\nasync _call(\n{ objective, firstTask = \"Make a todo list\" }: ChainValues,\nrunManager?: CallbackManagerForChainRun\n) {\nthis.taskList = [];\nthis.taskIDCounter = 1;\nawait this.addTask({ taskID: \"1\", taskName: firstTask });\n\nlet numIters = 0;\nwhile (numIters < this.maxIterations && this.taskList.length > 0) {\nthis.printTaskList();\n\n// eslint-disable-next-line @typescript-eslint/no-non-null-assertion\nconst task = this.taskList.shift()!;\nthis.printNextTask(task);\n\nconst result = await this.executeTask(\nobjective,\ntask.taskName,\nrunManager\n);\nconst thisTaskID = parseInt(task.taskID, 10);\nthis.printTaskResult(result);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":1101,"to":1141}}}}],["617",{"pageContent":"await this.vectorstore.addDocuments([\nnew Document({\npageContent: result,\nmetadata: { task: task.taskName },\n}),\n]);\n\nconst newTasks = await this.getNextTasks(\nresult,\ntask.taskName,\nobjective,\nrunManager\n);\nfor (const newTask of newTasks) {\nthis.taskIDCounter += 1;\nnewTask.taskID = this.taskIDCounter.toFixed();\nawait this.addTask(newTask as Task);\n}\nthis.taskList = await this.prioritizeTasks(\nthisTaskID,\nobjective,\nrunManager\n);\n\nnumIters += 1;\n}\nreturn {};\n}\n\nserialize(): SerializedBaseChain {\nthrow new Error(\"Method not implemented.\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":1380,"to":1411}}}}],["618",{"pageContent":"static fromLLM({\nllm,\nvectorstore,\nexecutionChain,\nverbose,\ncallbacks,\n...rest\n}: Optional<\nBabyAGIInputs,\n\"executionChain\" | \"creationChain\" | \"prioritizationChain\"\n> & { llm: BaseLanguageModel }) {\nconst creationChain = TaskCreationChain.fromLLM({\nllm,\nverbose,\ncallbacks,\n});\nconst prioritizationChain = TaskPrioritizationChain.fromLLM({\nllm,\nverbose,\ncallbacks,\n});\nreturn new BabyAGI({\ncreationChain,\nprioritizationChain,\nexecutionChain:\nexecutionChain ||\nTaskExecutionChain.fromLLM({ llm, verbose, callbacks }),\nvectorstore,\nverbose,\ncallbacks,\n...rest,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":1664,"to":1697}}}}],["619",{"pageContent":"export { TaskCreationChain } from \"./task_creation.js\";\nexport { TaskExecutionChain } from \"./task_execution.js\";\nexport { TaskPrioritizationChain } from \"./task_prioritization.js\";\nexport { BabyAGI, Task, BabyAGIInputs } from \"./agent.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/index.ts","loc":{"lines":{"from":1,"to":4}}}}],["620",{"pageContent":"import { LLMChain, LLMChainInput } from \"../../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\n\n/** Chain to generate tasks. */\nexport class TaskCreationChain extends LLMChain {\nstatic fromLLM(fields: Omit<LLMChainInput, \"prompt\">): LLMChain {\nconst taskCreationTemplate =\n`You are an task creation AI that uses the result of an execution agent` +\n` to create new tasks with the following objective: {objective},` +\n` The last completed task has the result: {result}.` +\n` This result was based on this task description: {task_description}.` +\n` These are incomplete tasks: {incomplete_tasks}.` +\n` Based on the result, create new tasks to be completed` +\n` by the AI system that do not overlap with incomplete tasks.` +\n` Return the tasks as an array.`;\nconst prompt = new PromptTemplate({\ntemplate: taskCreationTemplate,\ninputVariables: [\n\"result\",\n\"task_description\",\n\"incomplete_tasks\",\n\"objective\",\n],\n});\nreturn new TaskCreationChain({ prompt, ...fields });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/task_creation.ts","loc":{"lines":{"from":1,"to":27}}}}],["621",{"pageContent":"import { LLMChain, LLMChainInput } from \"../../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\n\n/** Chain to execute tasks. */\nexport class TaskExecutionChain extends LLMChain {\nstatic fromLLM(fields: Omit<LLMChainInput, \"prompt\">): LLMChain {\nconst executionTemplate =\n`You are an AI who performs one task based on the following objective: ` +\n`{objective}.` +\n`Take into account these previously completed tasks: {context}.` +\n` Your task: {task}. Response:`;\nconst prompt = new PromptTemplate({\ntemplate: executionTemplate,\ninputVariables: [\"objective\", \"context\", \"task\"],\n});\nreturn new TaskExecutionChain({ prompt, ...fields });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/task_execution.ts","loc":{"lines":{"from":1,"to":18}}}}],["622",{"pageContent":"import { LLMChain, LLMChainInput } from \"../../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\n\n/** Chain to prioritize tasks. */\nexport class TaskPrioritizationChain extends LLMChain {\nstatic fromLLM(fields: Omit<LLMChainInput, \"prompt\">): LLMChain {\nconst taskPrioritizationTemplate =\n`You are a task prioritization AI tasked with cleaning the formatting of ` +\n`and reprioritizing the following tasks: {task_names}.` +\n` Consider the ultimate objective of your team: {objective}.` +\n` Do not remove any tasks. Return the result as a numbered list, like:` +\n` #. First task` +\n` #. Second task` +\n` Start the task list with number {next_task_id}.`;\nconst prompt = new PromptTemplate({\ntemplate: taskPrioritizationTemplate,\ninputVariables: [\"task_names\", \"next_task_id\", \"objective\"],\n});\nreturn new TaskPrioritizationChain({ prompt, ...fields });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/task_prioritization.ts","loc":{"lines":{"from":1,"to":21}}}}],["623",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain' is deprecated. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport {\nPromptTemplate,\nBasePromptTemplate,\nFewShotPromptTemplate,\n} from \"./prompts/index.js\";\nexport { LLMChain } from \"./chains/llm_chain.js\";\nexport { OpenAI } from \"./llms/openai.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/index.ts","loc":{"lines":{"from":1,"to":11}}}}],["624",{"pageContent":"import { InMemoryCache } from \"../cache/index.js\";\nimport {\nBaseCache,\nBasePromptValue,\nGeneration,\nLLMResult,\nRUN_KEY,\n} from \"../schema/index.js\";\nimport {\nBaseLanguageModel,\nBaseLanguageModelCallOptions,\nBaseLanguageModelParams,\n} from \"../base_language/index.js\";\nimport {\nCallbackManager,\nCallbackManagerForLLMRun,\nCallbacks,\n} from \"../callbacks/manager.js\";\n\nexport type SerializedLLM = {\n_model: string;\n_type: string;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\nexport interface BaseLLMParams extends BaseLanguageModelParams {\n/**\n* @deprecated Use `maxConcurrency` instead\n*/\nconcurrency?: number;\ncache?: BaseCache | boolean;\n}\n\nexport interface BaseLLMCallOptions extends BaseLanguageModelCallOptions {}\n\n/**\n* LLM Wrapper. Provides an {@link call} (an {@link generate}) function that takes in a prompt (or prompts) and returns a string.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":1,"to":38}}}}],["625",{"pageContent":"abstract class BaseLLM extends BaseLanguageModel {\ndeclare CallOptions: BaseLanguageModelCallOptions;\n\ncache?: BaseCache;\n\nconstructor({ cache, concurrency, ...rest }: BaseLLMParams) {\nsuper(concurrency ? { maxConcurrency: concurrency, ...rest } : rest);\nif (typeof cache === \"object\") {\nthis.cache = cache;\n} else if (cache) {\nthis.cache = InMemoryCache.global();\n} else {\nthis.cache = undefined;\n}\n}\n\nasync generatePrompt(\npromptValues: BasePromptValue[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult> {\nconst prompts: string[] = promptValues.map((promptValue) =>\npromptValue.toString()\n);\nreturn this.generate(prompts, stop, callbacks);\n}\n\n/**\n* Run the LLM on the given prompts and input.\n*/\nabstract _generate(\nprompts: string[],\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<LLMResult>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":249,"to":283}}}}],["626",{"pageContent":"/** @ignore */\nasync _generateUncached(\nprompts: string[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult> {\nconst callbackManager_ = await CallbackManager.configure(\ncallbacks,\nthis.callbacks,\n{ verbose: this.verbose }\n);\nconst runManager = await callbackManager_?.handleLLMStart(\n{ name: this._llmType() },\nprompts\n);\nlet output;\ntry {\noutput = await this._generate(prompts, stop, runManager);\n} catch (err) {\nawait runManager?.handleLLMError(err);\nthrow err;\n}\n\nawait runManager?.handleLLMEnd(output);\n// This defines RUN_KEY as a non-enumerable property on the output object\n// so that it is not serialized when the output is stringified, and so that\n// it isnt included when listing the keys of the output object.\nObject.defineProperty(output, RUN_KEY, {\nvalue: runManager ? { runId: runManager?.runId } : undefined,\nconfigurable: true,\n});\nreturn output;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":494,"to":526}}}}],["627",{"pageContent":"/**\n* Run the LLM on the given propmts an input, handling caching.\n*/\nasync generate(\nprompts: string[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult> {\nif (!Array.isArray(prompts)) {\nthrow new Error(\"Argument 'prompts' is expected to be a string[]\");\n}\n\nif (!this.cache) {\nreturn this._generateUncached(prompts, stop, callbacks);\n}\n\nconst { cache } = this;\nconst params = this.serialize();\nparams.stop = stop;\n\nconst llmStringKey = `${Object.entries(params).sort()}`;\nconst missingPromptIndices: number[] = [];\nconst generations = await Promise.all(\nprompts.map(async (prompt, index) => {\nconst result = await cache.lookup(prompt, llmStringKey);\nif (!result) {\nmissingPromptIndices.push(index);\n}\nreturn result;\n})\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":737,"to":767}}}}],["628",{"pageContent":"let llmOutput = {};\nif (missingPromptIndices.length > 0) {\nconst results = await this._generateUncached(\nmissingPromptIndices.map((i) => prompts[i]),\nstop,\ncallbacks\n);\nawait Promise.all(\nresults.generations.map(async (generation, index) => {\nconst promptIndex = missingPromptIndices[index];\ngenerations[promptIndex] = generation;\nreturn cache.update(prompts[promptIndex], llmStringKey, generation);\n})\n);\nllmOutput = results.llmOutput ?? {};\n}\n\nreturn { generations, llmOutput } as LLMResult;\n}\n\n/**\n* Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.\n*/\nasync call(\nprompt: string,\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n) {\nconst { generations } = await this.generate([prompt], stop, callbacks);\nreturn generations[0][0].text;\n}\n\n/**\n* Get the identifying parameters of the LLM.\n*/\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_identifyingParams(): Record<string, any> {\nreturn {};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":981,"to":1019}}}}],["629",{"pageContent":"/**\n* Return the string type key uniquely identifying this class of LLM.\n*/\nabstract _llmType(): string;\n\n/**\n* Return a json-like object representing this LLM.\n*/\nserialize(): SerializedLLM {\nreturn {\n...this._identifyingParams(),\n_type: this._llmType(),\n_model: this._modelType(),\n};\n}\n\n_modelType(): string {\nreturn \"base_llm\" as const;\n}\n\n/**\n* Load an LLM from a json-like object describing it.\n*/\nstatic async deserialize(data: SerializedLLM): Promise<BaseLLM> {\nconst { _type, _model, ...rest } = data;\nif (_model && _model !== \"base_llm\") {\nthrow new Error(`Cannot load LLM with model ${_model}`);\n}\nconst Cls = {\nopenai: (await import(\"./openai.js\")).OpenAI,\n}[_type];\nif (Cls === undefined) {\nthrow new Error(`Cannot load  LLM with type ${_type}`);\n}\nreturn new Cls(rest);\n}\n}\n\n/**\n* LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n*\n* Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n*\n* @augments BaseLLM\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":1228,"to":1272}}}}],["630",{"pageContent":"abstract class LLM extends BaseLLM {\n/**\n* Run the LLM on the given prompt and input.\n*/\nabstract _call(\nprompt: string,\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<string>;\n\nasync _generate(\nprompts: string[],\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<LLMResult> {\nconst generations: Generation[][] = [];\nfor (let i = 0; i < prompts.length; i += 1) {\nconst text = await this._call(prompts[i], stop, runManager);\ngenerations.push([{ text }]);\n}\nreturn { generations };\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":1481,"to":1503}}}}],["631",{"pageContent":"import { LLM, BaseLLMParams } from \"./base.js\";\n\nexport interface CohereInput extends BaseLLMParams {\n/** Sampling temperature to use */\ntemperature?: number;\n\n/**\n* Maximum number of tokens to generate in the completion.\n*/\nmaxTokens?: number;\n\n/** Model to use */\nmodel?: string;\n\napiKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/cohere.ts","loc":{"lines":{"from":1,"to":16}}}}],["632",{"pageContent":"class Cohere extends LLM implements CohereInput {\ntemperature = 0;\n\nmaxTokens = 250;\n\nmodel: string;\n\napiKey: string;\n\nconstructor(fields?: CohereInput) {\nsuper(fields ?? {});\n\nconst apiKey =\nfields?.apiKey ?? typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.COHERE_API_KEY\n: undefined;\n\nif (!apiKey) {\nthrow new Error(\n\"Please set the COHERE_API_KEY environment variable or pass it to the constructor as the apiKey field.\"\n);\n}\n\nthis.apiKey = apiKey;\nthis.maxTokens = fields?.maxTokens ?? this.maxTokens;\nthis.temperature = fields?.temperature ?? this.temperature;\nthis.model = fields?.model ?? this.model;\n}\n\n_llmType() {\nreturn \"cohere\";\n}\n\n/** @ignore */\nasync _call(prompt: string, _stop?: string[]): Promise<string> {\nconst { cohere } = await Cohere.imports();\n\ncohere.init(this.apiKey);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/cohere.ts","loc":{"lines":{"from":91,"to":129}}}}],["633",{"pageContent":"// Hit the `generate` endpoint on the `large` model\nconst generateResponse = await this.caller.call(\ncohere.generate.bind(cohere),\n{\nprompt,\nmodel: this.model,\nmax_tokens: this.maxTokens,\ntemperature: this.temperature,\n}\n);\ntry {\nreturn generateResponse.body.generations[0].text;\n} catch {\nconsole.log(generateResponse);\nthrow new Error(\"Could not parse response.\");\n}\n}\n\n/** @ignore */\nstatic async imports(): Promise<{\ncohere: typeof import(\"cohere-ai\");\n}> {\ntry {\nconst { default: cohere } = await import(\"cohere-ai\");\nreturn { cohere };\n} catch (e) {\nthrow new Error(\n\"Please install cohere-ai as a dependency with, e.g. `yarn add cohere-ai`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/cohere.ts","loc":{"lines":{"from":180,"to":211}}}}],["634",{"pageContent":"import { LLM, BaseLLMParams } from \"./base.js\";\n\nexport interface HFInput {\n/** Model to use */\nmodel: string;\n\n/** Sampling temperature to use */\ntemperature?: number;\n\n/**\n* Maximum number of tokens to generate in the completion.\n*/\nmaxTokens?: number;\n\n/** Total probability mass of tokens to consider at each step */\ntopP?: number;\n\n/** Integer to define the top tokens considered within the sample operation to create new text. */\ntopK?: number;\n\n/** Penalizes repeated tokens according to frequency */\nfrequencyPenalty?: number;\n\n/** API key to use. */\napiKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/hf.ts","loc":{"lines":{"from":1,"to":26}}}}],["635",{"pageContent":"class HuggingFaceInference extends LLM implements HFInput {\nmodel = \"gpt2\";\n\ntemperature: number | undefined = undefined;\n\nmaxTokens: number | undefined = undefined;\n\ntopP: number | undefined = undefined;\n\ntopK: number | undefined = undefined;\n\nfrequencyPenalty: number | undefined = undefined;\n\napiKey: string | undefined = undefined;\n\nconstructor(fields?: Partial<HFInput> & BaseLLMParams) {\nsuper(fields ?? {});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/hf.ts","loc":{"lines":{"from":104,"to":120}}}}],["636",{"pageContent":"this.model = fields?.model ?? this.model;\nthis.temperature = fields?.temperature ?? this.temperature;\nthis.maxTokens = fields?.maxTokens ?? this.maxTokens;\nthis.topP = fields?.topP ?? this.topP;\nthis.topK = fields?.topK ?? this.topK;\nthis.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\nthis.apiKey =\nfields?.apiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.HUGGINGFACEHUB_API_KEY\n: undefined);\nif (!this.apiKey) {\nthrow new Error(\n\"Please set an API key for HuggingFace Hub in the environment variable HUGGINGFACEHUB_API_KEY or in the apiKey field of the HuggingFaceInference constructor.\"\n);\n}\n}\n\n_llmType() {\nreturn \"huggingface_hub\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/hf.ts","loc":{"lines":{"from":204,"to":225}}}}],["637",{"pageContent":"/** @ignore */\nasync _call(prompt: string, _stop?: string[]): Promise<string> {\nconst { HfInference } = await HuggingFaceInference.imports();\nconst hf = new HfInference(this.apiKey);\nconst res = await this.caller.call(hf.textGeneration.bind(hf), {\nmodel: this.model,\nparameters: {\n// make it behave similar to openai, returning only the generated text\nreturn_full_text: false,\ntemperature: this.temperature,\nmax_new_tokens: this.maxTokens,\ntop_p: this.topP,\ntop_k: this.topK,\nrepetition_penalty: this.frequencyPenalty,\n},\ninputs: prompt,\n});\nreturn res.generated_text;\n}\n\n/** @ignore */\nstatic async imports(): Promise<{\nHfInference: typeof import(\"@huggingface/inference\").HfInference;\n}> {\ntry {\nconst { HfInference } = await import(\"@huggingface/inference\");\nreturn { HfInference };\n} catch (e) {\nthrow new Error(\n\"Please install huggingface as a dependency with, e.g. `yarn add @huggingface/inference`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/hf.ts","loc":{"lines":{"from":296,"to":329}}}}],["638",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/llms' is deprecated. Import from eg. 'langchain/llms/openai' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport { BaseLLM, BaseLLMParams, LLM, SerializedLLM } from \"./base.js\";\nexport { OpenAI, PromptLayerOpenAI } from \"./openai.js\";\nexport { OpenAIChat } from \"./openai-chat.js\";\nexport { Cohere } from \"./cohere.js\";\nexport { HuggingFaceInference } from \"./hf.js\";\nexport { Replicate } from \"./replicate.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/index.ts","loc":{"lines":{"from":1,"to":10}}}}],["639",{"pageContent":"import { FileLoader, loadFromFile } from \"../util/load.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { parseFileConfig } from \"../util/parse.js\";\n\n/**\n* Load an LLM from a local file.\n*\n* @example\n* ```ts\n* import { loadLLM } from \"langchain/llms/load\";\n* const model = await loadLLM(\"/path/to/llm.json\");\n* ```\n*/\nconst loader: FileLoader<BaseLanguageModel> = (file: string, path: string) =>\nBaseLanguageModel.deserialize(parseFileConfig(file, path));\n\nexport const loadLLM = (uri: string): Promise<BaseLanguageModel> =>\nloadFromFile(uri, loader);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/load.ts","loc":{"lines":{"from":1,"to":18}}}}],["640",{"pageContent":"import {\nConfiguration,\nOpenAIApi,\nChatCompletionRequestMessage,\nCreateChatCompletionRequest,\nConfigurationParameters,\nChatCompletionResponseMessageRoleEnum,\nCreateChatCompletionResponse,\n} from \"openai\";\nimport {\nAzureOpenAIInput,\nOpenAICallOptions,\nOpenAIChatInput,\n} from \"../types/open-ai-types.js\";\nimport type { StreamingAxiosConfiguration } from \"../util/axios-types.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { BaseLLMParams, LLM } from \"./base.js\";\nimport { CallbackManagerForLLMRun } from \"../callbacks/manager.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":1,"to":18}}}}],["641",{"pageContent":"{ OpenAICallOptions, OpenAIChatInput, AzureOpenAIInput };\n\n/**\n* Wrapper around OpenAI large language models that use the Chat endpoint.\n*\n* To use you should have the `openai` package installed, with the\n* `OPENAI_API_KEY` environment variable set.\n*\n* To use with Azure you should have the `openai` package installed, with the\n* `AZURE_OPENAI_API_KEY`,\n* `AZURE_OPENAI_API_INSTANCE_NAME`,\n* `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n* and `AZURE_OPENAI_API_VERSION` environment variable set.\n*\n* @remarks\n* Any parameters that are valid to be passed to {@link\n* https://platform.openai.com/docs/api-reference/chat/create |\n* `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n* if not explicitly available on this class.\n*\n* @augments BaseLLM\n* @augments OpenAIInput\n* @augments AzureOpenAIChatInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":443,"to":466}}}}],["642",{"pageContent":"class OpenAIChat\nextends LLM\nimplements OpenAIChatInput, AzureOpenAIInput\n{\ndeclare CallOptions: OpenAICallOptions;\n\ntemperature = 1;\n\ntopP = 1;\n\nfrequencyPenalty = 0;\n\npresencePenalty = 0;\n\nn = 1;\n\nlogitBias?: Record<string, number>;\n\nmaxTokens?: number;\n\nmodelName = \"gpt-3.5-turbo\";\n\nprefixMessages?: ChatCompletionRequestMessage[];\n\nmodelKwargs?: OpenAIChatInput[\"modelKwargs\"];\n\ntimeout?: number;\n\nstop?: string[];\n\nstreaming = false;\n\nazureOpenAIApiVersion?: string;\n\nazureOpenAIApiKey?: string;\n\nazureOpenAIApiInstanceName?: string;\n\nazureOpenAIApiDeploymentName?: string;\n\nprivate client: OpenAIApi;\n\nprivate clientConfig: ConfigurationParameters;\n\nconstructor(\nfields?: Partial<OpenAIChatInput> &\nPartial<AzureOpenAIInput> &\nBaseLLMParams & {\nopenAIApiKey?: string;\n},\nconfiguration?: ConfigurationParameters\n) {\nsuper(fields ?? {});\n\nconst apiKey =\nfields?.openAIApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.OPENAI_API_KEY\n: undefined);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":883,"to":942}}}}],["643",{"pageContent":"const azureApiKey =\nfields?.azureOpenAIApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_KEY\n: undefined);\nif (!azureApiKey && !apiKey) {\nthrow new Error(\"(Azure) OpenAI API key not found\");\n}\n\nconst azureApiInstanceName =\nfields?.azureOpenAIApiInstanceName ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_INSTANCE_NAME\n: undefined);\n\nconst azureApiDeploymentName =\nfields?.azureOpenAIApiDeploymentName ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_DEPLOYMENT_NAME\n: undefined);\n\nconst azureApiVersion =\nfields?.azureOpenAIApiVersion ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_VERSION\n: undefined);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":1354,"to":1383}}}}],["644",{"pageContent":"this.modelName = fields?.modelName ?? this.modelName;\nthis.prefixMessages = fields?.prefixMessages ?? this.prefixMessages;\nthis.modelKwargs = fields?.modelKwargs ?? {};\nthis.timeout = fields?.timeout;\n\nthis.temperature = fields?.temperature ?? this.temperature;\nthis.topP = fields?.topP ?? this.topP;\nthis.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\nthis.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\nthis.n = fields?.n ?? this.n;\nthis.logitBias = fields?.logitBias;\nthis.maxTokens = fields?.maxTokens;\nthis.stop = fields?.stop;\n\nthis.streaming = fields?.streaming ?? false;\n\nthis.azureOpenAIApiVersion = azureApiVersion;\nthis.azureOpenAIApiKey = azureApiKey;\nthis.azureOpenAIApiInstanceName = azureApiInstanceName;\nthis.azureOpenAIApiDeploymentName = azureApiDeploymentName;\n\nif (this.streaming && this.n > 1) {\nthrow new Error(\"Cannot stream results when n > 1\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":1798,"to":1821}}}}],["645",{"pageContent":"if (this.azureOpenAIApiKey) {\nif (!this.azureOpenAIApiInstanceName) {\nthrow new Error(\"Azure OpenAI API instance name not found\");\n}\nif (!this.azureOpenAIApiDeploymentName) {\nthrow new Error(\"Azure OpenAI API deployment name not found\");\n}\nif (!this.azureOpenAIApiVersion) {\nthrow new Error(\"Azure OpenAI API version not found\");\n}\n}\n\nthis.clientConfig = {\napiKey,\n...configuration,\n};\n}\n\n/**\n* Get the parameters used to invoke the model\n*/\ninvocationParams(): Omit<CreateChatCompletionRequest, \"messages\"> {\nreturn {\nmodel: this.modelName,\ntemperature: this.temperature,\ntop_p: this.topP,\nfrequency_penalty: this.frequencyPenalty,\npresence_penalty: this.presencePenalty,\nn: this.n,\nlogit_bias: this.logitBias,\nmax_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\nstop: this.stop,\nstream: this.streaming,\n...this.modelKwargs,\n};\n}\n\n/** @ignore */\n_identifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n...this.clientConfig,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":2236,"to":2280}}}}],["646",{"pageContent":"/**\n* Get the identifying parameters for the model\n*/\nidentifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n...this.clientConfig,\n};\n}\n\nprivate formatMessages(prompt: string): ChatCompletionRequestMessage[] {\nconst message: ChatCompletionRequestMessage = {\nrole: \"user\",\ncontent: prompt,\n};\nreturn this.prefixMessages ? [...this.prefixMessages, message] : [message];\n}\n\n/** @ignore */\nasync _call(\nprompt: string,\nstopOrOptions?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<string> {\nconst stop = Array.isArray(stopOrOptions)\n? stopOrOptions\n: stopOrOptions?.stop;\nconst options = Array.isArray(stopOrOptions)\n? {}\n: stopOrOptions?.options ?? {};\n\nif (this.stop && stop) {\nthrow new Error(\"Stop found in input and default params\");\n}\n\nconst params = this.invocationParams();\nparams.stop = stop ?? params.stop;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":2693,"to":2730}}}}],["647",{"pageContent":"const data = params.stream\n? await new Promise<CreateChatCompletionResponse>((resolve, reject) => {\nlet response: CreateChatCompletionResponse;\nlet rejected = false;\nthis.completionWithRetry(\n{\n...params,\nmessages: this.formatMessages(prompt),\n},\n{\n...options,\nresponseType: \"stream\",\nonmessage: (event) => {\nif (event.data?.trim?.() === \"[DONE]\") {\nresolve(response);\n} else {\nconst message = JSON.parse(event.data) as {\nid: string;\nobject: string;\ncreated: number;\nmodel: string;\nchoices: Array<{\nindex: number;\nfinish_reason: string | null;\ndelta: { content?: string; role?: string };\n}>;\n};\n\n// on the first message set the response properties\nif (!response) {\nresponse = {\nid: message.id,\nobject: message.object,\ncreated: message.created,\nmodel: message.model,\nchoices: [],\n};\n}\n\n// on all messages, update choice\nconst part = message.choices[0];\nif (part != null) {\nlet choice = response.choices.find(\n(c) => c.index === part.index\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":3145,"to":3189}}}}],["648",{"pageContent":"if (!choice) {\nchoice = {\nindex: part.index,\nfinish_reason: part.finish_reason ?? undefined,\n};\nresponse.choices.push(choice);\n}\n\nif (!choice.message) {\nchoice.message = {\nrole: part.delta\n?.role as ChatCompletionResponseMessageRoleEnum,\ncontent: part.delta?.content ?? \"\",\n};\n}\n\nchoice.message.content += part.delta?.content ?? \"\";\n// eslint-disable-next-line no-void\nvoid runManager?.handleLLMNewToken(\npart.delta?.content ?? \"\"\n);\n}\n}\n},\n}\n).catch((error) => {\nif (!rejected) {\nrejected = true;\nreject(error);\n}\n});\n})\n: await this.completionWithRetry(\n{\n...params,\nmessages: this.formatMessages(prompt),\n},\noptions\n);\n\nreturn data.choices[0].message?.content ?? \"\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":3603,"to":3644}}}}],["649",{"pageContent":"/** @ignore */\nasync completionWithRetry(\nrequest: CreateChatCompletionRequest,\noptions?: StreamingAxiosConfiguration\n) {\nif (!this.client) {\nconst endpoint = this.azureOpenAIApiKey\n? `https://${this.azureOpenAIApiInstanceName}.openai.azure.com/openai/deployments/${this.azureOpenAIApiDeploymentName}`\n: this.clientConfig.basePath;\nconst clientConfig = new Configuration({\n...this.clientConfig,\nbasePath: endpoint,\nbaseOptions: {\ntimeout: this.timeout,\nadapter: fetchAdapter,\n...this.clientConfig.baseOptions,\n},\n});\nthis.client = new OpenAIApi(clientConfig);\n}\nconst axiosOptions = (options ?? {}) as StreamingAxiosConfiguration &\nOpenAICallOptions;\nif (this.azureOpenAIApiKey) {\naxiosOptions.headers = {\n\"api-key\": this.azureOpenAIApiKey,\n...axiosOptions.headers,\n};\naxiosOptions.params = {\n\"api-version\": this.azureOpenAIApiVersion,\n...axiosOptions.params,\n};\n}\nreturn this.caller\n.call(\nthis.client.createChatCompletion.bind(this.client),\nrequest,\naxiosOptions\n)\n.then((res) => res.data);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":4064,"to":4103}}}}],["650",{"pageContent":"_llmType() {\nreturn \"openai\";\n}\n}\n\n/**\n* PromptLayer wrapper to OpenAIChat\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":4515,"to":4522}}}}],["651",{"pageContent":"class PromptLayerOpenAIChat extends OpenAIChat {\npromptLayerApiKey?: string;\n\nplTags?: string[];\n\nconstructor(\nfields?: ConstructorParameters<typeof OpenAIChat>[0] & {\npromptLayerApiKey?: string;\nplTags?: string[];\n}\n) {\nsuper(fields);\n\nthis.plTags = fields?.plTags ?? [];\nthis.promptLayerApiKey =\nfields?.promptLayerApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.PROMPTLAYER_API_KEY\n: undefined);\n\nif (!this.promptLayerApiKey) {\nthrow new Error(\"Missing PromptLayer API key\");\n}\n}\n\nasync completionWithRetry(\nrequest: CreateChatCompletionRequest,\noptions?: StreamingAxiosConfiguration\n) {\nif (request.stream) {\nreturn super.completionWithRetry(request, options);\n}\n\nconst requestStartTime = Date.now();\nconst response = await super.completionWithRetry(request);\nconst requestEndTime = Date.now();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":4960,"to":4996}}}}],["652",{"pageContent":"// https://github.com/MagnivOrg/promptlayer-js-helper\nawait this.caller.call(fetch, \"https://api.promptlayer.com/track-request\", {\nmethod: \"POST\",\nheaders: {\n\"Content-Type\": \"application/json\",\nAccept: \"application/json\",\n},\nbody: JSON.stringify({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":5411,"to":5418}}}}],["653",{"pageContent":"_name: \"openai.ChatCompletion.create\",\nargs: [],\nkwargs: { engine: request.model, messages: request.messages },\ntags: this.plTags ?? [],\nrequest_response: response,\nrequest_start_time: Math.floor(requestStartTime / 1000),\nrequest_end_time: Math.floor(requestEndTime / 1000),\napi_key: this.promptLayerApiKey,\n}),\n});\n\nreturn response;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":5850,"to":5863}}}}],["654",{"pageContent":"import { TiktokenModel } from \"@dqbd/tiktoken\";\nimport {\nConfiguration,\nConfigurationParameters,\nCreateCompletionRequest,\nCreateCompletionResponse,\nCreateCompletionResponseChoicesInner,\nOpenAIApi,\n} from \"openai\";\nimport {\nAzureOpenAIInput,\nOpenAICallOptions,\nOpenAIInput,\n} from \"types/open-ai-types.js\";\nimport type { StreamingAxiosConfiguration } from \"../util/axios-types.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { chunkArray } from \"../util/chunk.js\";\nimport { BaseLLM, BaseLLMParams } from \"./base.js\";\nimport { calculateMaxTokens } from \"../base_language/count_tokens.js\";\nimport { OpenAIChat } from \"./openai-chat.js\";\nimport { LLMResult } from \"../schema/index.js\";\nimport { CallbackManagerForLLMRun } from \"../callbacks/manager.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":1,"to":22}}}}],["655",{"pageContent":"{ OpenAICallOptions, AzureOpenAIInput, OpenAIInput };\n\ninterface TokenUsage {\ncompletionTokens?: number;\npromptTokens?: number;\ntotalTokens?: number;\n}\n\n/**\n* Wrapper around OpenAI large language models.\n*\n* To use you should have the `openai` package installed, with the\n* `OPENAI_API_KEY` environment variable set.\n*\n* To use with Azure you should have the `openai` package installed, with the\n* `AZURE_OPENAI_API_KEY`,\n* `AZURE_OPENAI_API_INSTANCE_NAME`,\n* `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n* and `AZURE_OPENAI_API_VERSION` environment variable set.\n*\n* @remarks\n* Any parameters that are valid to be passed to {@link\n* https://platform.openai.com/docs/api-reference/completions/create |\n* `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n* if not explicitly available on this class.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":497,"to":522}}}}],["656",{"pageContent":"class OpenAI extends BaseLLM implements OpenAIInput, AzureOpenAIInput {\ndeclare CallOptions: OpenAICallOptions;\n\ntemperature = 0.7;\n\nmaxTokens = 256;\n\ntopP = 1;\n\nfrequencyPenalty = 0;\n\npresencePenalty = 0;\n\nn = 1;\n\nbestOf = 1;\n\nlogitBias?: Record<string, number>;\n\nmodelName = \"text-davinci-003\";\n\nmodelKwargs?: OpenAIInput[\"modelKwargs\"];\n\nbatchSize = 20;\n\ntimeout?: number;\n\nstop?: string[];\n\nstreaming = false;\n\nazureOpenAIApiVersion?: string;\n\nazureOpenAIApiKey?: string;\n\nazureOpenAIApiInstanceName?: string;\n\nazureOpenAIApiDeploymentName?: string;\n\nprivate client: OpenAIApi;\n\nprivate clientConfig: ConfigurationParameters;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":995,"to":1036}}}}],["657",{"pageContent":"constructor(\nfields?: Partial<OpenAIInput> &\nPartial<AzureOpenAIInput> &\nBaseLLMParams & {\nopenAIApiKey?: string;\n},\nconfiguration?: ConfigurationParameters\n) {\nif (\nfields?.modelName?.startsWith(\"gpt-3.5-turbo\") ||\nfields?.modelName?.startsWith(\"gpt-4\") ||\nfields?.modelName?.startsWith(\"gpt-4-32k\")\n) {\n// eslint-disable-next-line no-constructor-return, @typescript-eslint/no-explicit-any\nreturn new OpenAIChat(fields, configuration) as any as OpenAI;\n}\nsuper(fields ?? {});\n\nconst apiKey =\nfields?.openAIApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.OPENAI_API_KEY\n: undefined);\n\nconst azureApiKey =\nfields?.azureOpenAIApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_KEY\n: undefined);\nif (!azureApiKey && !apiKey) {\nthrow new Error(\"(Azure) OpenAI API key not found\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":1513,"to":1546}}}}],["658",{"pageContent":"const azureApiInstanceName =\nfields?.azureOpenAIApiInstanceName ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_INSTANCE_NAME\n: undefined);\n\nconst azureApiDeploymentName =\n(fields?.azureOpenAIApiCompletionsDeploymentName ||\nfields?.azureOpenAIApiDeploymentName) ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME ||\n// eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_DEPLOYMENT_NAME\n: undefined);\n\nconst azureApiVersion =\nfields?.azureOpenAIApiVersion ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.AZURE_OPENAI_API_VERSION\n: undefined);\n\nthis.modelName = fields?.modelName ?? this.modelName;\nthis.modelKwargs = fields?.modelKwargs ?? {};\nthis.batchSize = fields?.batchSize ?? this.batchSize;\nthis.timeout = fields?.timeout;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":2016,"to":2043}}}}],["659",{"pageContent":"this.temperature = fields?.temperature ?? this.temperature;\nthis.maxTokens = fields?.maxTokens ?? this.maxTokens;\nthis.topP = fields?.topP ?? this.topP;\nthis.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\nthis.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\nthis.n = fields?.n ?? this.n;\nthis.bestOf = fields?.bestOf ?? this.bestOf;\nthis.logitBias = fields?.logitBias;\nthis.stop = fields?.stop;\n\nthis.streaming = fields?.streaming ?? false;\n\nthis.azureOpenAIApiVersion = azureApiVersion;\nthis.azureOpenAIApiKey = azureApiKey;\nthis.azureOpenAIApiInstanceName = azureApiInstanceName;\nthis.azureOpenAIApiDeploymentName = azureApiDeploymentName;\n\nif (this.streaming && this.n > 1) {\nthrow new Error(\"Cannot stream results when n > 1\");\n}\n\nif (this.streaming && this.bestOf > 1) {\nthrow new Error(\"Cannot stream results when bestOf > 1\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":2511,"to":2534}}}}],["660",{"pageContent":"if (this.azureOpenAIApiKey) {\nif (!this.azureOpenAIApiInstanceName) {\nthrow new Error(\"Azure OpenAI API instance name not found\");\n}\nif (!this.azureOpenAIApiDeploymentName) {\nthrow new Error(\"Azure OpenAI API deployment name not found\");\n}\nif (!this.azureOpenAIApiVersion) {\nthrow new Error(\"Azure OpenAI API version not found\");\n}\n}\n\nthis.clientConfig = {\napiKey,\n...configuration,\n};\n}\n\n/**\n* Get the parameters used to invoke the model\n*/\ninvocationParams(): CreateCompletionRequest {\nreturn {\nmodel: this.modelName,\ntemperature: this.temperature,\nmax_tokens: this.maxTokens,\ntop_p: this.topP,\nfrequency_penalty: this.frequencyPenalty,\npresence_penalty: this.presencePenalty,\nn: this.n,\nbest_of: this.bestOf,\nlogit_bias: this.logitBias,\nstop: this.stop,\nstream: this.streaming,\n...this.modelKwargs,\n};\n}\n\n_identifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n...this.clientConfig,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":3005,"to":3049}}}}],["661",{"pageContent":"/**\n* Get the identifying parameters for the model\n*/\nidentifyingParams() {\nreturn this._identifyingParams();\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":3518,"to":3523}}}}],["662",{"pageContent":"/**\n* Call out to OpenAI's endpoint with k unique prompts\n*\n* @param prompts - The prompts to pass into the model.\n* @param [stop] - Optional list of stop words to use when generating.\n* @param [runManager] - Optional callback manager to use when generating.\n*\n* @returns The full LLM output.\n*\n* @example\n* ```ts\n* import { OpenAI } from \"langchain/llms/openai\";\n* const openai = new OpenAI();\n* const response = await openai.generate([\"Tell me a joke.\"]);\n* ```\n*/\nasync _generate(\nprompts: string[],\nstopOrOptions?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<LLMResult> {\nconst stop = Array.isArray(stopOrOptions)\n? stopOrOptions\n: stopOrOptions?.stop;\nconst options = Array.isArray(stopOrOptions)\n? {}\n: stopOrOptions?.options ?? {};\nconst subPrompts = chunkArray(prompts, this.batchSize);\nconst choices: CreateCompletionResponseChoicesInner[] = [];\nconst tokenUsage: TokenUsage = {};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":4015,"to":4044}}}}],["663",{"pageContent":"if (this.stop && stop) {\nthrow new Error(\"Stop found in input and default params\");\n}\n\nconst params = this.invocationParams();\nparams.stop = stop ?? params.stop;\n\nif (params.max_tokens === -1) {\nif (prompts.length !== 1) {\nthrow new Error(\n\"max_tokens set to -1 not supported for multiple inputs\"\n);\n}\nparams.max_tokens = await calculateMaxTokens({\nprompt: prompts[0],\n// Cast here to allow for other models that may not fit the union\nmodelName: this.modelName as TiktokenModel,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":4513,"to":4531}}}}],["664",{"pageContent":"for (let i = 0; i < subPrompts.length; i += 1) {\nconst data = params.stream\n? await new Promise<CreateCompletionResponse>((resolve, reject) => {\nconst choice: CreateCompletionResponseChoicesInner = {};\nlet response: Omit<CreateCompletionResponse, \"choices\">;\nlet rejected = false;\nthis.completionWithRetry(\n{\n...params,\nprompt: subPrompts[i],\n},\n{\n...options,\nresponseType: \"stream\",\nonmessage: (event) => {\nif (event.data?.trim?.() === \"[DONE]\") {\nresolve({\n...response,\nchoices: [choice],\n});\n} else {\nconst message = JSON.parse(event.data) as Omit<\nCreateCompletionResponse,\n\"usage\"\n>;\n\n// on the first message set the response properties\nif (!response) {\nresponse = {\nid: message.id,\nobject: message.object,\ncreated: message.created,\nmodel: message.model,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":5011,"to":5045}}}}],["665",{"pageContent":"// on all messages, update choice\nconst part = message.choices[0];\nif (part != null) {\nchoice.text = (choice.text ?? \"\") + (part.text ?? \"\");\nchoice.finish_reason = part.finish_reason;\nchoice.logprobs = part.logprobs;\n// eslint-disable-next-line no-void\nvoid runManager?.handleLLMNewToken(part.text ?? \"\");\n}\n}\n},\n}\n).catch((error) => {\nif (!rejected) {\nrejected = true;\nreject(error);\n}\n});\n})\n: await this.completionWithRetry(\n{\n...params,\nprompt: subPrompts[i],\n},\noptions\n);\n\nchoices.push(...data.choices);\n\nconst {\ncompletion_tokens: completionTokens,\nprompt_tokens: promptTokens,\ntotal_tokens: totalTokens,\n} = data.usage ?? {};\n\nif (completionTokens) {\ntokenUsage.completionTokens =\n(tokenUsage.completionTokens ?? 0) + completionTokens;\n}\n\nif (promptTokens) {\ntokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n}\n\nif (totalTokens) {\ntokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":5520,"to":5567}}}}],["666",{"pageContent":"const generations = chunkArray(choices, this.n).map((promptChoices) =>\npromptChoices.map((choice) => ({\ntext: choice.text ?? \"\",\ngenerationInfo: {\nfinishReason: choice.finish_reason,\nlogprobs: choice.logprobs,\n},\n}))\n);\nreturn {\ngenerations,\nllmOutput: { tokenUsage },\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":6035,"to":6048}}}}],["667",{"pageContent":"/** @ignore */\nasync completionWithRetry(\nrequest: CreateCompletionRequest,\noptions?: StreamingAxiosConfiguration\n) {\nif (!this.client) {\nconst endpoint = this.azureOpenAIApiKey\n? `https://${this.azureOpenAIApiInstanceName}.openai.azure.com/openai/deployments/${this.azureOpenAIApiDeploymentName}`\n: this.clientConfig.basePath;\nconst clientConfig = new Configuration({\n...this.clientConfig,\nbasePath: endpoint,\nbaseOptions: {\ntimeout: this.timeout,\nadapter: fetchAdapter,\n...this.clientConfig.baseOptions,\n},\n});\nthis.client = new OpenAIApi(clientConfig);\n}\nconst axiosOptions = (options ?? {}) as StreamingAxiosConfiguration &\nOpenAICallOptions;\nif (this.azureOpenAIApiKey) {\naxiosOptions.headers = {\n\"api-key\": this.azureOpenAIApiKey,\n...axiosOptions.headers,\n};\naxiosOptions.params = {\n\"api-version\": this.azureOpenAIApiVersion,\n...axiosOptions.params,\n};\n}\nreturn this.caller\n.call(\nthis.client.createCompletion.bind(this.client),\nrequest,\naxiosOptions\n)\n.then((res) => res.data);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":6533,"to":6572}}}}],["668",{"pageContent":"_llmType() {\nreturn \"openai\";\n}\n}\n\n/**\n* PromptLayer wrapper to OpenAI\n* @augments OpenAI\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":7036,"to":7044}}}}],["669",{"pageContent":"class PromptLayerOpenAI extends OpenAI {\npromptLayerApiKey?: string;\n\nplTags?: string[];\n\nconstructor(\nfields?: ConstructorParameters<typeof OpenAI>[0] & {\npromptLayerApiKey?: string;\nplTags?: string[];\n}\n) {\nsuper(fields);\n\nthis.plTags = fields?.plTags ?? [];\nthis.promptLayerApiKey =\nfields?.promptLayerApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.PROMPTLAYER_API_KEY\n: undefined);\n\nif (!this.promptLayerApiKey) {\nthrow new Error(\"Missing PromptLayer API key\");\n}\n}\n\nasync completionWithRetry(\nrequest: CreateCompletionRequest,\noptions?: StreamingAxiosConfiguration\n) {\nif (request.stream) {\nreturn super.completionWithRetry(request, options);\n}\n\nconst requestStartTime = Date.now();\nconst response = await super.completionWithRetry(request);\nconst requestEndTime = Date.now();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":7537,"to":7573}}}}],["670",{"pageContent":"// https://github.com/MagnivOrg/promptlayer-js-helper\nawait this.caller.call(fetch, \"https://api.promptlayer.com/track-request\", {\nmethod: \"POST\",\nheaders: {\n\"Content-Type\": \"application/json\",\nAccept: \"application/json\",\n},\nbody: JSON.stringify({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":8046,"to":8053}}}}],["671",{"pageContent":"_name: \"openai.Completion.create\",\nargs: [],\nkwargs: { engine: request.model, prompt: request.prompt },\ntags: this.plTags ?? [],\nrequest_response: response,\nrequest_start_time: Math.floor(requestStartTime / 1000),\nrequest_end_time: Math.floor(requestEndTime / 1000),\napi_key: this.promptLayerApiKey,\n}),\n});\n\nreturn response;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":8539,"to":8552}}}}],["672",{"pageContent":"{ OpenAIChat, PromptLayerOpenAIChat } from \"./openai-chat.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":9034,"to":9034}}}}],["673",{"pageContent":"import { LLM, BaseLLMParams } from \"./base.js\";\n\nexport interface ReplicateInput {\n// owner/model_name:version\nmodel: `${string}/${string}:${string}`;\n\ninput?: {\n// different models accept different inputs\n[key: string]: string | number | boolean;\n};\n\napiKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/replicate.ts","loc":{"lines":{"from":1,"to":13}}}}],["674",{"pageContent":"class Replicate extends LLM implements ReplicateInput {\nmodel: ReplicateInput[\"model\"];\n\ninput: ReplicateInput[\"input\"];\n\napiKey: string;\n\nconstructor(fields: ReplicateInput & BaseLLMParams) {\nsuper(fields);\n\nconst apiKey =\nfields?.apiKey ??\n// eslint-disable-next-line no-process-env\n(typeof process !== \"undefined\" && process.env?.REPLICATE_API_KEY);\n\nif (!apiKey) {\nthrow new Error(\"Please set the REPLICATE_API_KEY environment variable\");\n}\n\nthis.apiKey = apiKey;\nthis.model = fields.model;\nthis.input = fields.input ?? {};\n}\n\n_llmType() {\nreturn \"replicate\";\n}\n\n/** @ignore */\nasync _call(prompt: string, _stop?: string[]): Promise<string> {\nconst imports = await Replicate.imports();\n\nconst replicate = new imports.Replicate({\nuserAgent: \"langchain\",\nauth: this.apiKey,\n});\n\nconst output = await this.caller.call(() =>\nreplicate.run(this.model, {\nwait: true,\ninput: {\n...this.input,\nprompt,\n},\n})\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/replicate.ts","loc":{"lines":{"from":81,"to":126}}}}],["675",{"pageContent":"// Note this is a little odd, but the output format is not consistent\n// across models, so it makes some amount of sense.\nreturn String(output);\n}\n\n/** @ignore */\nstatic async imports(): Promise<{\nReplicate: typeof import(\"replicate\").default;\n}> {\ntry {\nconst { default: Replicate } = await import(\"replicate\");\nreturn { Replicate };\n} catch (e) {\nthrow new Error(\n\"Please install replicate as a dependency with, e.g. `yarn add replicate`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/replicate.ts","loc":{"lines":{"from":167,"to":185}}}}],["676",{"pageContent":"import { test } from \"@jest/globals\";\nimport { Cohere } from \"../cohere.js\";\n\ntest(\"Test Cohere\", async () => {\nconst model = new Cohere({ maxTokens: 20 });\nconst res = await model.call(\"1 + 1 =\");\nconsole.log(res);\n}, 50000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/cohere.int.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["677",{"pageContent":"import { test } from \"@jest/globals\";\nimport { HuggingFaceInference } from \"../hf.js\";\n\ntest(\"Test HuggingFace\", async () => {\nconst model = new HuggingFaceInference({ temperature: 0.1, topP: 0.5 });\nconst res = await model.call(\"1 + 1 =\");\nconsole.log(res);\n}, 50000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/huggingface_hub.int.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["678",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { OpenAIChat } from \"../openai-chat.js\";\nimport { CallbackManager } from \"../../callbacks/index.js\";\n\ntest(\"Test OpenAI\", async () => {\nconst model = new OpenAIChat({ modelName: \"gpt-3.5-turbo\", maxTokens: 10 });\nconst res = await model.call(\"Print hello world\");\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with prefix messages\", async () => {\nconst model = new OpenAIChat({\nprefixMessages: [\n{ role: \"user\", content: \"My name is John\" },\n{ role: \"assistant\", content: \"Hi there\" },\n],\nmaxTokens: 10,\n});\nconst res = await model.call(\"What is my name\");\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI in streaming mode\", async () => {\nlet nrNewTokens = 0;\nlet streamedCompletion = \"\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai-chat.int.test.ts","loc":{"lines":{"from":1,"to":25}}}}],["679",{"pageContent":"const model = new OpenAIChat({\nmaxTokens: 10,\nmodelName: \"gpt-3.5-turbo\",\nstreaming: true,\ncallbackManager: CallbackManager.fromHandlers({\nasync handleLLMNewToken(token: string) {\nnrNewTokens += 1;\nstreamedCompletion += token;\n},\n}),\n});\nconst res = await model.call(\"Print hello world\");\nconsole.log({ res });\n\nexpect(nrNewTokens > 0).toBe(true);\nexpect(res).toBe(streamedCompletion);\n}, 30000);\n\ntest(\"Test OpenAI with stop\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nconst res = await model.call(\"Print hello world\", [\"world\"]);\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with stop in object\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nconst res = await model.call(\"Print hello world\", { stop: [\"world\"] });\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with timeout in call options\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nawait expect(() =>\nmodel.call(\"Print hello world\", {\noptions: { timeout: 10 },\n})\n).rejects.toThrow();\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai-chat.int.test.ts","loc":{"lines":{"from":102,"to":139}}}}],["680",{"pageContent":"test(\"Test OpenAI with timeout in call options and node adapter\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nawait expect(() =>\nmodel.call(\"Print hello world\", {\noptions: { timeout: 10, adapter: undefined },\n})\n).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with signal in call options\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call(\"Print hello world\", {\noptions: { signal: controller.signal },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with signal in call options and node adapter\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call(\"Print hello world\", {\noptions: { signal: controller.signal, adapter: undefined },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai-chat.int.test.ts","loc":{"lines":{"from":207,"to":242}}}}],["681",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { LLMResult } from \"../../schema/index.js\";\nimport { OpenAIChat } from \"../openai-chat.js\";\nimport { OpenAI } from \"../openai.js\";\nimport { StringPromptValue } from \"../../prompts/index.js\";\nimport { CallbackManager } from \"../../callbacks/index.js\";\n\ntest(\"Test OpenAI\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst res = await model.call(\"Print hello world\");\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with stop\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst res = await model.call(\"Print hello world\", [\"world\"]);\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with stop in object\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst res = await model.call(\"Print hello world\", { stop: [\"world\"] });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai.int.test.ts","loc":{"lines":{"from":1,"to":24}}}}],["682",{"pageContent":"test(\"Test OpenAI with timeout in call options\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nawait expect(() =>\nmodel.call(\"Print hello world\", {\noptions: { timeout: 10 },\n})\n).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with timeout in call options and node adapter\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nawait expect(() =>\nmodel.call(\"Print hello world\", {\noptions: { timeout: 10, adapter: undefined },\n})\n).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with signal in call options\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call(\"Print hello world\", {\noptions: { signal: controller.signal },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai.int.test.ts","loc":{"lines":{"from":157,"to":187}}}}],["683",{"pageContent":"test(\"Test OpenAI with signal in call options and node adapter\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call(\"Print hello world\", {\noptions: { signal: controller.signal, adapter: undefined },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with concurrency == 1\", async () => {\nconst model = new OpenAI({\nmaxTokens: 5,\nmodelName: \"text-ada-001\",\nmaxConcurrency: 1,\n});\nconst res = await Promise.all([\nmodel.call(\"Print hello world\"),\nmodel.call(\"Print hello world\"),\n]);\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with maxTokens -1\", async () => {\nconst model = new OpenAI({ maxTokens: -1, modelName: \"text-ada-001\" });\nconst res = await model.call(\"Print hello world\", [\"world\"]);\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai.int.test.ts","loc":{"lines":{"from":321,"to":352}}}}],["684",{"pageContent":"test(\"Test OpenAI with chat model returns OpenAIChat\", async () => {\nconst model = new OpenAI({ modelName: \"gpt-3.5-turbo\" });\nexpect(model).toBeInstanceOf(OpenAIChat);\nconst res = await model.call(\"Print hello world\");\nconsole.log({ res });\nexpect(typeof res).toBe(\"string\");\n});\n\ntest(\"Test ChatOpenAI tokenUsage\", async () => {\nlet tokenUsage = {\ncompletionTokens: 0,\npromptTokens: 0,\ntotalTokens: 0,\n};\n\nconst model = new OpenAI({\nmaxTokens: 5,\nmodelName: \"text-ada-001\",\ncallbackManager: CallbackManager.fromHandlers({\nasync handleLLMEnd(output: LLMResult) {\ntokenUsage = output.llmOutput?.tokenUsage;\n},\n}),\n});\nconst res = await model.call(\"Hello\");\nconsole.log({ res });\n\nexpect(tokenUsage.promptTokens).toBe(1);\n});\n\ntest(\"Test OpenAI in streaming mode\", async () => {\nlet nrNewTokens = 0;\nlet streamedCompletion = \"\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai.int.test.ts","loc":{"lines":{"from":486,"to":518}}}}],["685",{"pageContent":"const model = new OpenAI({\nmaxTokens: 5,\nmodelName: \"text-ada-001\",\nstreaming: true,\ncallbacks: CallbackManager.fromHandlers({\nasync handleLLMNewToken(token: string) {\nnrNewTokens += 1;\nstreamedCompletion += token;\n},\n}),\n});\nconst res = await model.call(\"Print hello world\");\nconsole.log({ res });\n\nexpect(nrNewTokens > 0).toBe(true);\nexpect(res).toBe(streamedCompletion);\n});\n\ntest(\"Test OpenAI prompt value\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst res = await model.generatePrompt([\nnew StringPromptValue(\"Print hello world\"),\n]);\nexpect(res.generations.length).toBe(1);\nfor (const generation of res.generations) {\nexpect(generation.length).toBe(1);\nfor (const g of generation) {\nconsole.log(g.text);\n}\n}\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai.int.test.ts","loc":{"lines":{"from":653,"to":684}}}}],["686",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { Replicate } from \"../replicate.js\";\n\n// Test skipped because Replicate appears to be timing out often when called\ntest.skip(\"Test Replicate\", async () => {\nconst model = new Replicate({\nmodel:\n\"daanelson/flan-t5:04e422a9b85baed86a4f24981d7f9953e20c5fd82f6103b74ebc431588e1cec8\",\ninput: {\nmax_length: 10,\n},\n});\n\nconst res = await model.call(\"Hello, my name is \");\n\nexpect(typeof res).toBe(\"string\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/replicate.int.test.ts","loc":{"lines":{"from":1,"to":17}}}}],["687",{"pageContent":"import { BaseChatMessage, ChatMessage } from \"../schema/index.js\";\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type InputValues = Record<string, any>;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type OutputValues = Record<string, any>;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type MemoryVariables = Record<string, any>;\n\nexport abstract class BaseMemory {\nabstract get memoryKeys(): string[];\n\nabstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n\nabstract saveContext(\ninputValues: InputValues,\noutputValues: OutputValues\n): Promise<void>;\n}\n\n/**\n* This function is used by memory classes to select the input value\n* to use for the memory. If there is only one input value, it is used.\n* If there are multiple input values, the inputKey must be specified.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/base.ts","loc":{"lines":{"from":1,"to":24}}}}],["688",{"pageContent":"const getInputValue = (inputValues: InputValues, inputKey?: string) => {\nif (inputKey !== undefined) {\nreturn inputValues[inputKey];\n}\nconst keys = Object.keys(inputValues);\nif (keys.length === 1) {\nreturn inputValues[keys[0]];\n}\nthrow new Error(\n`input values have ${keys.length} keys, you must specify an input key or pass only 1 key as input`\n);\n};\n\n/**\n* This function is used by memory classes to get a string representation\n* of the chat message history, based on the message content and role.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/base.ts","loc":{"lines":{"from":65,"to":81}}}}],["689",{"pageContent":"function getBufferString(\nmessages: BaseChatMessage[],\nhumanPrefix = \"Human\",\naiPrefix = \"AI\"\n): string {\nconst string_messages: string[] = [];\nfor (const m of messages) {\nlet role: string;\nif (m._getType() === \"human\") {\nrole = humanPrefix;\n} else if (m._getType() === \"ai\") {\nrole = aiPrefix;\n} else if (m._getType() === \"system\") {\nrole = \"System\";\n} else if (m._getType() === \"generic\") {\nrole = (m as ChatMessage).role;\n} else {\nthrow new Error(`Got unsupported message type: ${m}`);\n}\nstring_messages.push(`${role}: ${m.text}`);\n}\nreturn string_messages.join(\"\\n\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/base.ts","loc":{"lines":{"from":133,"to":155}}}}],["690",{"pageContent":"import { InputValues, MemoryVariables, getBufferString } from \"./base.js\";\nimport { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\n\nexport interface BufferMemoryInput extends BaseChatMemoryInput {\nhumanPrefix?: string;\naiPrefix?: string;\nmemoryKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/buffer_memory.ts","loc":{"lines":{"from":1,"to":8}}}}],["691",{"pageContent":"class BufferMemory extends BaseChatMemory implements BufferMemoryInput {\nhumanPrefix = \"Human\";\n\naiPrefix = \"AI\";\n\nmemoryKey = \"history\";\n\nconstructor(fields?: BufferMemoryInput) {\nsuper({\nchatHistory: fields?.chatHistory,\nreturnMessages: fields?.returnMessages ?? false,\ninputKey: fields?.inputKey,\noutputKey: fields?.outputKey,\n});\nthis.humanPrefix = fields?.humanPrefix ?? this.humanPrefix;\nthis.aiPrefix = fields?.aiPrefix ?? this.aiPrefix;\nthis.memoryKey = fields?.memoryKey ?? this.memoryKey;\n}\n\nget memoryKeys() {\nreturn [this.memoryKey];\n}\n\nasync loadMemoryVariables(_values: InputValues): Promise<MemoryVariables> {\nconst messages = await this.chatHistory.getMessages();\nif (this.returnMessages) {\nconst result = {\n[this.memoryKey]: messages,\n};\nreturn result;\n}\nconst result = {\n[this.memoryKey]: getBufferString(messages),\n};\nreturn result;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/buffer_memory.ts","loc":{"lines":{"from":47,"to":83}}}}],["692",{"pageContent":"import { InputValues, MemoryVariables, getBufferString } from \"./base.js\";\n\nimport { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\n\nexport interface BufferWindowMemoryInput extends BaseChatMemoryInput {\nhumanPrefix?: string;\naiPrefix?: string;\nmemoryKey?: string;\nk?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/buffer_window_memory.ts","loc":{"lines":{"from":1,"to":10}}}}],["693",{"pageContent":"class BufferWindowMemory\nextends BaseChatMemory\nimplements BufferWindowMemoryInput\n{\nhumanPrefix = \"Human\";\n\naiPrefix = \"AI\";\n\nmemoryKey = \"history\";\n\nk = 5;\n\nconstructor(fields?: BufferWindowMemoryInput) {\nsuper({\nreturnMessages: fields?.returnMessages ?? false,\nchatHistory: fields?.chatHistory,\ninputKey: fields?.inputKey,\noutputKey: fields?.outputKey,\n});\nthis.humanPrefix = fields?.humanPrefix ?? this.humanPrefix;\nthis.aiPrefix = fields?.aiPrefix ?? this.aiPrefix;\nthis.memoryKey = fields?.memoryKey ?? this.memoryKey;\nthis.k = fields?.k ?? this.k;\n}\n\nget memoryKeys() {\nreturn [this.memoryKey];\n}\n\nasync loadMemoryVariables(_values: InputValues): Promise<MemoryVariables> {\nconst messages = await this.chatHistory.getMessages();\nif (this.returnMessages) {\nconst result = {\n[this.memoryKey]: messages.slice(-this.k * 2),\n};\nreturn result;\n}\nconst result = {\n[this.memoryKey]: getBufferString(messages.slice(-this.k * 2)),\n};\nreturn result;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/buffer_window_memory.ts","loc":{"lines":{"from":55,"to":97}}}}],["694",{"pageContent":"import { BaseChatMessageHistory } from \"../schema/index.js\";\nimport {\nBaseMemory,\nInputValues,\nOutputValues,\ngetInputValue,\n} from \"./base.js\";\nimport { ChatMessageHistory } from \"../stores/message/in_memory.js\";\n\nexport interface BaseChatMemoryInput {\nchatHistory?: BaseChatMessageHistory;\nreturnMessages?: boolean;\ninputKey?: string;\noutputKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/chat_memory.ts","loc":{"lines":{"from":1,"to":15}}}}],["695",{"pageContent":"abstract class BaseChatMemory extends BaseMemory {\nchatHistory: BaseChatMessageHistory;\n\nreturnMessages = false;\n\ninputKey?: string;\n\noutputKey?: string;\n\nconstructor(fields?: BaseChatMemoryInput) {\nsuper();\nthis.chatHistory = fields?.chatHistory ?? new ChatMessageHistory();\nthis.returnMessages = fields?.returnMessages ?? this.returnMessages;\nthis.inputKey = fields?.inputKey ?? this.inputKey;\nthis.outputKey = fields?.outputKey ?? this.outputKey;\n}\n\nasync saveContext(\ninputValues: InputValues,\noutputValues: OutputValues\n): Promise<void> {\n// this is purposefully done in sequence so they're saved in order\nawait this.chatHistory.addUserMessage(\ngetInputValue(inputValues, this.inputKey)\n);\nawait this.chatHistory.addAIChatMessage(\ngetInputValue(outputValues, this.outputKey)\n);\n}\n\nasync clear(): Promise<void> {\nawait this.chatHistory.clear();\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/chat_memory.ts","loc":{"lines":{"from":51,"to":84}}}}],["696",{"pageContent":"export { BufferMemory, BufferMemoryInput } from \"./buffer_memory.js\";\nexport { BaseMemory, getInputValue, getBufferString } from \"./base.js\";\nexport {\nConversationSummaryMemory,\nConversationSummaryMemoryInput,\n} from \"./summary.js\";\nexport {\nBufferWindowMemory,\nBufferWindowMemoryInput,\n} from \"./buffer_window_memory.js\";\nexport { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\nexport { ChatMessageHistory } from \"../stores/message/in_memory.js\";\nexport { MotorheadMemory, MotorheadMemoryInput } from \"./motorhead_memory.js\";\nexport {\nVectorStoreRetrieverMemory,\nVectorStoreRetrieverMemoryParams,\n} from \"./vector_store.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/index.ts","loc":{"lines":{"from":1,"to":17}}}}],["697",{"pageContent":"import { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\nimport {\nInputValues,\nOutputValues,\nMemoryVariables,\ngetBufferString,\ngetInputValue,\n} from \"./base.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../util/async_caller.js\";\n\nexport interface MotorheadMemoryMessage {\nrole: string;\ncontent: string;\n}\n\n/**\n* @interface\n*/\nexport type MotorheadMemoryInput = BaseChatMemoryInput &\nAsyncCallerParams & {\nsessionId: string;\nmotorheadURL?: string;\nmemoryKey?: string;\ntimeout?: number;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/motorhead_memory.ts","loc":{"lines":{"from":1,"to":25}}}}],["698",{"pageContent":"class MotorheadMemory extends BaseChatMemory {\nmotorheadURL = \"localhost:8080\";\n\ntimeout = 3000;\n\nmemoryKey = \"history\";\n\nsessionId: string;\n\ncontext?: string;\n\ncaller: AsyncCaller;\n\nconstructor(fields: MotorheadMemoryInput) {\nconst {\nsessionId,\nmotorheadURL,\nmemoryKey,\ntimeout,\nreturnMessages,\ninputKey,\noutputKey,\nchatHistory,\n...rest\n} = fields;\nsuper({ returnMessages, inputKey, outputKey, chatHistory });\n\nthis.caller = new AsyncCaller(rest);\nthis.sessionId = sessionId;\nthis.motorheadURL = motorheadURL ?? this.motorheadURL;\nthis.memoryKey = memoryKey ?? this.memoryKey;\nthis.timeout = timeout ?? this.timeout;\n}\n\nget memoryKeys() {\nreturn [this.memoryKey];\n}\n\nasync init(): Promise<void> {\nconst res = await this.caller.call(\nfetch,\n`${this.motorheadURL}/sessions/${this.sessionId}/memory`,\n{\nsignal: this.timeout ? AbortSignal.timeout(this.timeout) : undefined,\nheaders: {\n\"Content-Type\": \"application/json\",\n},\n}\n);\n\nconst { messages = [], context = \"NONE\" } = await res.json();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/motorhead_memory.ts","loc":{"lines":{"from":137,"to":187}}}}],["699",{"pageContent":"await Promise.all(\nmessages.reverse().map(async (message: MotorheadMemoryMessage) => {\nif (message.role === \"AI\") {\nawait this.chatHistory.addAIChatMessage(message.content);\n} else {\nawait this.chatHistory.addUserMessage(message.content);\n}\n})\n);\n\nif (context && context !== \"NONE\") {\nthis.context = context;\n}\n}\n\nasync loadMemoryVariables(_values: InputValues): Promise<MemoryVariables> {\nconst messages = await this.chatHistory.getMessages();\nif (this.returnMessages) {\nconst result = {\n[this.memoryKey]: messages,\n};\nreturn result;\n}\nconst result = {\n[this.memoryKey]: getBufferString(messages),\n};\nreturn result;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/motorhead_memory.ts","loc":{"lines":{"from":270,"to":297}}}}],["700",{"pageContent":"async saveContext(\ninputValues: InputValues,\noutputValues: OutputValues\n): Promise<void> {\nconst input = getInputValue(inputValues, this.inputKey);\nconst output = getInputValue(outputValues, this.outputKey);\nawait Promise.all([\nthis.caller.call(\nfetch,\n`${this.motorheadURL}/sessions/${this.sessionId}/memory`,\n{\nsignal: this.timeout ? AbortSignal.timeout(this.timeout) : undefined,\nmethod: \"POST\",\nbody: JSON.stringify({\nmessages: [\n{ role: \"Human\", content: `${input}` },\n{ role: \"AI\", content: `${output}` },\n],\n}),\nheaders: {\n\"Content-Type\": \"application/json\",\n},\n}\n),\nsuper.saveContext(inputValues, outputValues),\n]);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/motorhead_memory.ts","loc":{"lines":{"from":404,"to":431}}}}],["701",{"pageContent":"import { PromptTemplate } from \"../prompts/prompt.js\";\n\nconst _DEFAULT_SUMMARIZER_TEMPLATE = `Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n\nEXAMPLE\nCurrent summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n\nNew lines of conversation:\nHuman: Why do you think artificial intelligence is a force for good?\nAI: Because artificial intelligence will help humans reach their full potential.\n\nNew summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\nEND OF EXAMPLE\n\nCurrent summary:\n{summary}\n\nNew lines of conversation:\n{new_lines}\n\nNew summary:`;\n\n// eslint-disable-next-line spaced-comment","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/prompt.ts","loc":{"lines":{"from":1,"to":25}}}}],["702",{"pageContent":"const SUMMARY_PROMPT = /*#__PURE__*/ new PromptTemplate({\ninputVariables: [\"summary\", \"new_lines\"],\ntemplate: _DEFAULT_SUMMARIZER_TEMPLATE,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/prompt.ts","loc":{"lines":{"from":29,"to":32}}}}],["703",{"pageContent":"import { BaseLanguageModel } from \"../base_language/index.js\";\nimport { LLMChain } from \"../chains/llm_chain.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { BaseChatMessage, SystemChatMessage } from \"../schema/index.js\";\nimport {\ngetBufferString,\nInputValues,\nMemoryVariables,\nOutputValues,\n} from \"./base.js\";\nimport { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\nimport { SUMMARY_PROMPT } from \"./prompt.js\";\n\nexport interface ConversationSummaryMemoryInput extends BaseChatMemoryInput {\nllm: BaseLanguageModel;\nmemoryKey?: string;\nhumanPrefix?: string;\naiPrefix?: string;\nprompt?: BasePromptTemplate;\nsummaryChatMessageClass?: new (content: string) => BaseChatMessage;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/summary.ts","loc":{"lines":{"from":1,"to":21}}}}],["704",{"pageContent":"class ConversationSummaryMemory extends BaseChatMemory {\nbuffer = \"\";\n\nmemoryKey = \"history\";\n\nhumanPrefix = \"Human\";\n\naiPrefix = \"AI\";\n\nllm: BaseLanguageModel;\n\nprompt: BasePromptTemplate = SUMMARY_PROMPT;\n\nsummaryChatMessageClass: new (content: string) => BaseChatMessage =\nSystemChatMessage;\n\nconstructor(fields: ConversationSummaryMemoryInput) {\nconst {\nreturnMessages,\ninputKey,\noutputKey,\nchatHistory,\nhumanPrefix,\naiPrefix,\nllm,\nprompt,\nsummaryChatMessageClass,\n} = fields;\n\nsuper({ returnMessages, inputKey, outputKey, chatHistory });\n\nthis.memoryKey = fields?.memoryKey ?? this.memoryKey;\nthis.humanPrefix = humanPrefix ?? this.humanPrefix;\nthis.aiPrefix = aiPrefix ?? this.aiPrefix;\nthis.llm = llm;\nthis.prompt = prompt ?? this.prompt;\nthis.summaryChatMessageClass =\nsummaryChatMessageClass ?? this.summaryChatMessageClass;\n}\n\nget memoryKeys() {\nreturn [this.memoryKey];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/summary.ts","loc":{"lines":{"from":104,"to":146}}}}],["705",{"pageContent":"async predictNewSummary(\nmessages: BaseChatMessage[],\nexistingSummary: string\n): Promise<string> {\nconst newLines = getBufferString(messages, this.humanPrefix, this.aiPrefix);\nconst chain = new LLMChain({ llm: this.llm, prompt: this.prompt });\nreturn await chain.predict({\nsummary: existingSummary,\nnew_lines: newLines,\n});\n}\n\nasync loadMemoryVariables(_: InputValues): Promise<MemoryVariables> {\nif (this.returnMessages) {\nconst result = {\n[this.memoryKey]: [new this.summaryChatMessageClass(this.buffer)],\n};\nreturn result;\n}\nconst result = { [this.memoryKey]: this.buffer };\nreturn result;\n}\n\nasync saveContext(\ninputValues: InputValues,\noutputValues: OutputValues\n): Promise<void> {\nawait super.saveContext(inputValues, outputValues);\nconst messages = await this.chatHistory.getMessages();\nthis.buffer = await this.predictNewSummary(messages.slice(-2), this.buffer);\n}\n\nasync clear() {\nawait super.clear();\nthis.buffer = \"\";\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/summary.ts","loc":{"lines":{"from":219,"to":255}}}}],["706",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { BufferMemory } from \"../buffer_memory.js\";\nimport { ChatMessageHistory } from \"../../stores/message/in_memory.js\";\nimport { HumanChatMessage, AIChatMessage } from \"../../schema/index.js\";\n\ntest(\"Test buffer memory\", async () => {\nconst memory = new BufferMemory();\nconst result1 = await memory.loadMemoryVariables({});\nexpect(result1).toStrictEqual({ history: \"\" });\n\nawait memory.saveContext({ foo: \"bar\" }, { bar: \"foo\" });\nconst expectedString = \"Human: bar\\nAI: foo\";\nconst result2 = await memory.loadMemoryVariables({});\nexpect(result2).toStrictEqual({ history: expectedString });\n});\n\ntest(\"Test buffer memory return messages\", async () => {\nconst memory = new BufferMemory({ returnMessages: true });\nconst result1 = await memory.loadMemoryVariables({});\nexpect(result1).toStrictEqual({ history: [] });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/buffer_memory.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["707",{"pageContent":"await memory.saveContext({ foo: \"bar\" }, { bar: \"foo\" });\nconst expectedResult = [\nnew HumanChatMessage(\"bar\"),\nnew AIChatMessage(\"foo\"),\n];\nconst result2 = await memory.loadMemoryVariables({});\nexpect(result2).toStrictEqual({ history: expectedResult });\n});\n\ntest(\"Test buffer memory with pre-loaded history\", async () => {\nconst pastMessages = [\nnew HumanChatMessage(\"My name's Jonas\"),\nnew AIChatMessage(\"Nice to meet you, Jonas!\"),\n];\nconst memory = new BufferMemory({\nreturnMessages: true,\nchatHistory: new ChatMessageHistory(pastMessages),\n});\nconst result = await memory.loadMemoryVariables({});\nexpect(result).toStrictEqual({ history: pastMessages });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/buffer_memory.test.ts","loc":{"lines":{"from":42,"to":62}}}}],["708",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { BufferWindowMemory } from \"../buffer_window_memory.js\";\nimport { ChatMessageHistory } from \"../../stores/message/in_memory.js\";\nimport { HumanChatMessage, AIChatMessage } from \"../../schema/index.js\";\n\ntest(\"Test buffer window memory\", async () => {\nconst memory = new BufferWindowMemory({ k: 1 });\nconst result1 = await memory.loadMemoryVariables({});\nexpect(result1).toStrictEqual({ history: \"\" });\n\nawait memory.saveContext({ foo: \"bar\" }, { bar: \"foo\" });\nconst expectedString = \"Human: bar\\nAI: foo\";\nconst result2 = await memory.loadMemoryVariables({});\nexpect(result2).toStrictEqual({ history: expectedString });\n\nawait memory.saveContext({ foo: \"bar1\" }, { bar: \"foo\" });\nconst expectedString3 = \"Human: bar1\\nAI: foo\";\nconst result3 = await memory.loadMemoryVariables({});\nexpect(result3).toStrictEqual({ history: expectedString3 });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/buffer_window_memory.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["709",{"pageContent":"test(\"Test buffer window memory return messages\", async () => {\nconst memory = new BufferWindowMemory({ k: 1, returnMessages: true });\nconst result1 = await memory.loadMemoryVariables({});\nexpect(result1).toStrictEqual({ history: [] });\n\nawait memory.saveContext({ foo: \"bar\" }, { bar: \"foo\" });\nconst expectedResult = [\nnew HumanChatMessage(\"bar\"),\nnew AIChatMessage(\"foo\"),\n];\nconst result2 = await memory.loadMemoryVariables({});\nexpect(result2).toStrictEqual({ history: expectedResult });\n\nawait memory.saveContext({ foo: \"bar1\" }, { bar: \"foo\" });\nconst expectedResult2 = [\nnew HumanChatMessage(\"bar1\"),\nnew AIChatMessage(\"foo\"),\n];\nconst result3 = await memory.loadMemoryVariables({});\nexpect(result3).toStrictEqual({ history: expectedResult2 });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/buffer_window_memory.test.ts","loc":{"lines":{"from":56,"to":76}}}}],["710",{"pageContent":"test(\"Test buffer window memory with pre-loaded history\", async () => {\nconst pastMessages = [\nnew HumanChatMessage(\"My name's Jonas\"),\nnew AIChatMessage(\"Nice to meet you, Jonas!\"),\n];\nconst memory = new BufferWindowMemory({\nreturnMessages: true,\nchatHistory: new ChatMessageHistory(pastMessages),\n});\nconst result = await memory.loadMemoryVariables({});\nexpect(result).toStrictEqual({ history: pastMessages });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/buffer_window_memory.test.ts","loc":{"lines":{"from":114,"to":125}}}}],["711",{"pageContent":"import { test, expect, jest } from \"@jest/globals\";\nimport { MotorheadMemory } from \"../motorhead_memory.js\";\nimport { HumanChatMessage, AIChatMessage } from \"../../schema/index.js\";\n\ntest(\"Test motrhead memory\", async () => {\nglobal.fetch = jest.fn(() =>\nPromise.resolve({\njson: () =>\nPromise.resolve({\nmessages: [\n{ role: \"Human\", content: \"Who is the best vocalist?\" },\n{ role: \"AI\", content: \"Ozzy Osbourne\" },\n],\n}),\n} as Response)\n);\n\nconst memory = new MotorheadMemory({ sessionId: \"1\" });\nconst result1 = await memory.loadMemoryVariables({});\nexpect(result1).toStrictEqual({ history: \"\" });\n\nawait memory.saveContext(\n{ input: \"Who is the best vocalist?\" },\n{ response: \"Ozzy Osbourne\" }\n);\nconst expectedString = \"Human: Who is the best vocalist?\\nAI: Ozzy Osbourne\";\nconst result2 = await memory.loadMemoryVariables({});\nexpect(result2).toStrictEqual({ history: expectedString });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/motorhead_memory.test.ts","loc":{"lines":{"from":1,"to":29}}}}],["712",{"pageContent":"test(\"Test motrhead memory with pre-loaded history\", async () => {\nconst pastMessages = [\nnew HumanChatMessage(\"My name is Ozzy\"),\nnew AIChatMessage(\"Nice to meet you, Ozzy!\"),\n];\n\nglobal.fetch = jest.fn(() =>\nPromise.resolve({\njson: () =>\nPromise.resolve({\nmessages: [\n{ role: \"AI\", content: \"Nice to meet you, Ozzy!\" },\n{ role: \"Human\", content: \"My name is Ozzy\" },\n],\n}),\n} as Response)\n);\nconst memory = new MotorheadMemory({\nreturnMessages: true,\nsessionId: \"2\",\n});\nawait memory.init();\nconst result = await memory.loadMemoryVariables({});\nexpect(result).toStrictEqual({ history: pastMessages });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/motorhead_memory.test.ts","loc":{"lines":{"from":57,"to":81}}}}],["713",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { ConversationSummaryMemory } from \"../summary.js\";\nimport { OpenAIChat } from \"../../llms/openai-chat.js\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\nimport { SystemChatMessage } from \"../../schema/index.js\";\n\ntest(\"Test summary memory\", async () => {\nconst memory = new ConversationSummaryMemory({\nllm: new OpenAIChat({ modelName: \"gpt-3.5-turbo\", temperature: 0 }),\n});\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: \"\",\n});\n\nawait memory.saveContext(\n{ input: \"How's it going?\" },\n{ response: \"Hello! I'm doing fine. and you?\" }\n);\nconst result2 = await memory.loadMemoryVariables({});\nconsole.log(\"result2\", result2);\n\nawait memory.clear();\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: \"\",\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/summary.int.test.ts","loc":{"lines":{"from":1,"to":26}}}}],["714",{"pageContent":"test(\"Test summary memory with chat model\", async () => {\nconst memory = new ConversationSummaryMemory({\nllm: new ChatOpenAI({ temperature: 0 }),\n});\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: \"\",\n});\n\nawait memory.saveContext(\n{ input: \"How's it going?\" },\n{ response: \"Hello! I'm doing fine. and you?\" }\n);\nconst result2 = await memory.loadMemoryVariables({});\nconsole.log(\"result2\", result2);\n\nawait memory.clear();\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: \"\",\n});\n});\n\ntest(\"Test summary memory return messages\", async () => {\nconst memory = new ConversationSummaryMemory({\nllm: new OpenAIChat({ modelName: \"gpt-3.5-turbo\", temperature: 0 }),\nreturnMessages: true,\n});\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: [new SystemChatMessage(\"\")],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/summary.int.test.ts","loc":{"lines":{"from":72,"to":100}}}}],["715",{"pageContent":"await memory.saveContext(\n{ input: \"How's it going?\" },\n{ response: \"Hello! I'm doing fine. and you?\" }\n);\nconst result2 = await memory.loadMemoryVariables({});\nconsole.log(\"result2\", result2);\n\nawait memory.clear();\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: [new SystemChatMessage(\"\")],\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/summary.int.test.ts","loc":{"lines":{"from":146,"to":157}}}}],["716",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport \"@tensorflow/tfjs-backend-cpu\";\nimport { VectorStoreRetrieverMemory } from \"../vector_store.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\nimport { TensorFlowEmbeddings } from \"../../embeddings/tensorflow.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test vector store memory\", async () => {\nconst vectorStore = new MemoryVectorStore(new TensorFlowEmbeddings());\nconst memory = new VectorStoreRetrieverMemory({\nvectorStoreRetriever: vectorStore.asRetriever(),\n});\nconst result1 = await memory.loadMemoryVariables({ input: \"foo\" });\nexpect(result1).toStrictEqual({ memory: \"\" });\n\nawait memory.saveContext({ foo: \"bar\" }, { bar: \"foo\" });\nconst expectedString = \"foo: bar\\nbar: foo\";\nconst result2 = await memory.loadMemoryVariables({ input: \"foo\" });\nexpect(result2).toStrictEqual({ memory: expectedString });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/vector_store_memory.int.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["717",{"pageContent":"test(\"Test vector store memory return docs\", async () => {\nconst vectorStore = new MemoryVectorStore(new TensorFlowEmbeddings());\nconst memory = new VectorStoreRetrieverMemory({\nvectorStoreRetriever: vectorStore.asRetriever(),\nreturnDocs: true,\n});\nconst result1 = await memory.loadMemoryVariables({ input: \"foo\" });\nexpect(result1).toStrictEqual({ memory: [] });\n\nawait memory.saveContext({ foo: \"bar\" }, { bar: \"foo\" });\nconst expectedResult = [new Document({ pageContent: \"foo: bar\\nbar: foo\" })];\nconst result2 = await memory.loadMemoryVariables({ input: \"foo\" });\nexpect(result2).toStrictEqual({ memory: expectedResult });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/vector_store_memory.int.test.ts","loc":{"lines":{"from":36,"to":49}}}}],["718",{"pageContent":"import { Document } from \"../document.js\";\nimport { VectorStoreRetriever } from \"../vectorstores/base.js\";\nimport {\nBaseMemory,\ngetInputValue,\nInputValues,\nMemoryVariables,\nOutputValues,\n} from \"./base.js\";\n\nexport interface VectorStoreRetrieverMemoryParams {\nvectorStoreRetriever: VectorStoreRetriever;\ninputKey?: string;\noutputKey?: string;\nmemoryKey?: string;\nreturnDocs?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/vector_store.ts","loc":{"lines":{"from":1,"to":17}}}}],["719",{"pageContent":"class VectorStoreRetrieverMemory\nextends BaseMemory\nimplements VectorStoreRetrieverMemoryParams\n{\nvectorStoreRetriever: VectorStoreRetriever;\n\ninputKey?: string;\n\nmemoryKey: string;\n\nreturnDocs: boolean;\n\nconstructor(fields: VectorStoreRetrieverMemoryParams) {\nsuper();\nthis.vectorStoreRetriever = fields.vectorStoreRetriever;\nthis.inputKey = fields.inputKey;\nthis.memoryKey = fields.memoryKey ?? \"memory\";\nthis.returnDocs = fields.returnDocs ?? false;\n}\n\nget memoryKeys(): string[] {\nreturn [this.memoryKey];\n}\n\nasync loadMemoryVariables(values: InputValues): Promise<MemoryVariables> {\nconst query = getInputValue(values, this.inputKey);\nconst results = await this.vectorStoreRetriever.getRelevantDocuments(query);\nreturn {\n[this.memoryKey]: this.returnDocs\n? results\n: results.map((r) => r.pageContent).join(\"\\n\"),\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/vector_store.ts","loc":{"lines":{"from":67,"to":99}}}}],["720",{"pageContent":"async saveContext(\ninputValues: InputValues,\noutputValues: OutputValues\n): Promise<void> {\nconst text = Object.entries(inputValues)\n.filter(([k]) => k !== this.memoryKey)\n.concat(Object.entries(outputValues))\n.map(([k, v]) => `${k}: ${v}`)\n.join(\"\\n\");\nawait this.vectorStoreRetriever.addDocuments([\nnew Document({ pageContent: text }),\n]);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/vector_store.ts","loc":{"lines":{"from":130,"to":143}}}}],["721",{"pageContent":"import { Callbacks } from \"../callbacks/manager.js\";\nimport { BaseOutputParser } from \"../schema/output_parser.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type CombinedOutput = Record<string, any>;\n\n/**\n* Class to combine multiple output parsers\n* @augments BaseOutputParser\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/combining.ts","loc":{"lines":{"from":1,"to":10}}}}],["722",{"pageContent":"class CombiningOutputParser extends BaseOutputParser {\nparsers: BaseOutputParser[];\n\noutputDelimiter = \"-----\";\n\nconstructor(...parsers: BaseOutputParser[]) {\nsuper();\nthis.parsers = parsers;\n}\n\nasync parse(input: string, callbacks?: Callbacks): Promise<CombinedOutput> {\nconst inputs = input\n.trim()\n.split(\nnew RegExp(`${this.outputDelimiter}Output \\\\d+${this.outputDelimiter}`)\n)\n.slice(1);\nconst ret: CombinedOutput = {};\nfor (const [i, p] of this.parsers.entries()) {\nlet parsed;\ntry {\nlet extracted = inputs[i].includes(\"```\")\n? inputs[i].trim().split(/```/)[1]\n: inputs[i].trim();\nif (extracted.endsWith(this.outputDelimiter)) {\nextracted = extracted.slice(0, -this.outputDelimiter.length);\n}\nparsed = await p.parse(extracted, callbacks);\n} catch (e) {\nparsed = await p.parse(input.trim(), callbacks);\n}\nObject.assign(ret, parsed);\n}\nreturn ret;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/combining.ts","loc":{"lines":{"from":59,"to":93}}}}],["723",{"pageContent":"getFormatInstructions(): string {\nreturn `${[\n`Return the following ${this.parsers.length} outputs, each formatted as described below. Include the delimiter characters \"${this.outputDelimiter}\" in your response:`,\n...this.parsers.map(\n(p, i) =>\n`${this.outputDelimiter}Output ${i + 1}${this.outputDelimiter}\\n${p\n.getFormatInstructions()\n.trim()}\\n${this.outputDelimiter}`\n),\n].join(\"\\n\\n\")}\\n`;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/combining.ts","loc":{"lines":{"from":122,"to":133}}}}],["724",{"pageContent":"import {\nBaseOutputParser,\nOutputParserException,\n} from \"../schema/output_parser.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { LLMChain } from \"../chains/llm_chain.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { Callbacks } from \"../callbacks/manager.js\";\nimport { NAIVE_FIX_PROMPT } from \"./prompts.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/fix.ts","loc":{"lines":{"from":1,"to":9}}}}],["725",{"pageContent":"class OutputFixingParser<T> extends BaseOutputParser<T> {\nparser: BaseOutputParser<T>;\n\nretryChain: LLMChain;\n\nstatic fromLLM<T>(\nllm: BaseLanguageModel,\nparser: BaseOutputParser<T>,\nfields?: {\nprompt?: BasePromptTemplate;\n}\n) {\nconst prompt = fields?.prompt ?? NAIVE_FIX_PROMPT;\nconst chain = new LLMChain({ llm, prompt });\nreturn new OutputFixingParser<T>({ parser, retryChain: chain });\n}\n\nconstructor({\nparser,\nretryChain,\n}: {\nparser: BaseOutputParser<T>;\nretryChain: LLMChain;\n}) {\nsuper();\nthis.parser = parser;\nthis.retryChain = retryChain;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/fix.ts","loc":{"lines":{"from":64,"to":91}}}}],["726",{"pageContent":"async parse(completion: string, callbacks?: Callbacks) {\ntry {\nreturn await this.parser.parse(completion, callbacks);\n} catch (e) {\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (e instanceof OutputParserException) {\nconst result = await this.retryChain.call(\n{\ninstructions: this.parser.getFormatInstructions(),\ncompletion,\nerror: e,\n},\ncallbacks\n);\nconst newCompletion: string = result[this.retryChain.outputKey];\nreturn this.parser.parse(newCompletion);\n}\nthrow e;\n}\n}\n\ngetFormatInstructions() {\nreturn this.parser.getFormatInstructions();\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/fix.ts","loc":{"lines":{"from":137,"to":161}}}}],["727",{"pageContent":"export { ListOutputParser, CommaSeparatedListOutputParser } from \"./list.js\";\nexport { RegexParser } from \"./regex.js\";\nexport { StructuredOutputParser } from \"./structured.js\";\nexport { OutputFixingParser } from \"./fix.js\";\nexport { CombiningOutputParser } from \"./combining.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/index.ts","loc":{"lines":{"from":1,"to":5}}}}],["728",{"pageContent":"import {\nBaseOutputParser,\nOutputParserException,\n} from \"../schema/output_parser.js\";\n\n/**\n* Class to parse the output of an LLM call to a list.\n* @augments BaseOutputParser\n*/\nexport abstract class ListOutputParser extends BaseOutputParser<string[]> {}\n\n/**\n* Class to parse the output of an LLM call as a comma-separated list.\n* @augments ListOutputParser\n*/\nexport class CommaSeparatedListOutputParser extends ListOutputParser {\nasync parse(text: string): Promise<string[]> {\ntry {\nreturn text\n.trim()\n.split(\",\")\n.map((s) => s.trim());\n} catch (e) {\nthrow new OutputParserException(`Could not parse output: ${text}`, text);\n}\n}\n\ngetFormatInstructions(): string {\nreturn `Your response should be a list of comma separated values, eg: \\`foo, bar, baz\\``;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/list.ts","loc":{"lines":{"from":1,"to":31}}}}],["729",{"pageContent":"import { PromptTemplate } from \"../prompts/prompt.js\";\n\nexport const NAIVE_FIX_TEMPLATE = `Instructions:\n--------------\n{instructions}\n--------------\nCompletion:\n--------------\n{completion}\n--------------\n\nAbove, the Completion did not satisfy the constraints given in the Instructions.\nError:\n--------------\n{error}\n--------------\n\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:`;\n\nexport const NAIVE_FIX_PROMPT =\n/* #__PURE__ */ PromptTemplate.fromTemplate(NAIVE_FIX_TEMPLATE);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/prompts.ts","loc":{"lines":{"from":1,"to":21}}}}],["730",{"pageContent":"import {\nBaseOutputParser,\nOutputParserException,\n} from \"../schema/output_parser.js\";\n\n/**\n* Class to parse the output of an LLM call into a dictionary.\n* @augments BaseOutputParser\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/regex.ts","loc":{"lines":{"from":1,"to":9}}}}],["731",{"pageContent":"class RegexParser extends BaseOutputParser<Record<string, string>> {\nregex: string | RegExp;\n\noutputKeys: string[];\n\ndefaultOutputKey?: string;\n\nconstructor(\nregex: string | RegExp,\noutputKeys: string[],","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/regex.ts","loc":{"lines":{"from":55,"to":64}}}}],["732",{"pageContent":"OutputKey?: string\n) {\nsuper();\nthis.regex = typeof regex === \"string\" ? new RegExp(regex) : regex;\nthis.outputKeys = outputKeys;\nthis.defaultOutputKey = defaultOutputKey;\n}\n\n_type() {\nreturn \"regex_parser\";\n}\n\nasync parse(text: string): Promise<Record<string, string>> {\nconst match = text.match(this.regex);\nif (match) {\nreturn this.outputKeys.reduce((acc, key, index) => {\nacc[key] = match[index + 1];\nreturn acc;\n}, {} as Record<string, string>);\n}\n\nif (this.defaultOutputKey === undefined) {\nthrow new OutputParserException(`Could not parse output: ${text}`, text);\n}\n\nreturn this.outputKeys.reduce((acc, key) => {\nacc[key] = key === this.defaultOutputKey ? text : \"\";\nreturn acc;\n}, {} as Record<string, string>);\n}\n\ngetFormatInstructions(): string {\nreturn `Your response should match the following regex: ${this.regex}`;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/regex.ts","loc":{"lines":{"from":108,"to":142}}}}],["733",{"pageContent":"import { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\nimport {\nBaseOutputParser,\nOutputParserException,\n} from \"../schema/output_parser.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/structured.ts","loc":{"lines":{"from":1,"to":6}}}}],["734",{"pageContent":"class StructuredOutputParser<\nT extends z.ZodTypeAny\n> extends BaseOutputParser<z.infer<T>> {\nconstructor(public schema: T) {\nsuper();\n}\n\nstatic fromZodSchema<T extends z.ZodTypeAny>(schema: T) {\nreturn new this(schema);\n}\n\nstatic fromNamesAndDescriptions<S extends { [key: string]: string }>(\nschemas: S\n) {\nconst zodSchema = z.object(\nObject.fromEntries(\nObject.entries(schemas).map(\n([name, description]) =>\n[name, z.string().describe(description)] as const\n)\n)\n);\n\nreturn new this(zodSchema);\n}\n\ngetFormatInstructions(): string {\nreturn `You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n\n\"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/structured.ts","loc":{"lines":{"from":65,"to":94}}}}],["735",{"pageContent":"For example, the example \"JSON Schema\" instance {{\"properties\": {{\"foo\": {{\"description\": \"a list of test words\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nwould match an object with one required property, \"foo\". The \"type\" property specifies \"foo\" must be an \"array\", and the \"description\" property semantically describes it as \"a list of test words\". The items within \"foo\" must be strings.\nThus, the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of this example \"JSON Schema\". The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nYour output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n\nHere is the JSON Schema instance your output must adhere to:\n\\`\\`\\`json\n${JSON.stringify(zodToJsonSchema(this.schema))}\n\\`\\`\\`\n`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/structured.ts","loc":{"lines":{"from":129,"to":140}}}}],["736",{"pageContent":"async parse(text: string): Promise<z.infer<T>> {\ntry {\nconst json = text.includes(\"```\")\n? text.trim().split(/```(?:json)?/)[1]\n: text.trim();\nreturn this.schema.parseAsync(JSON.parse(json));\n} catch (e) {\nthrow new OutputParserException(\n`Failed to parse. Text: \"${text}\". Error: ${e}`,\ntext\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/structured.ts","loc":{"lines":{"from":170,"to":183}}}}],["737",{"pageContent":"import { test } from \"@jest/globals\";\n\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport {\nStructuredOutputParser,\nRegexParser,\nCombiningOutputParser,\n} from \"../index.js\";\n\ntest(\"CombiningOutputParser\", async () => {\nconst answerParser = StructuredOutputParser.fromNamesAndDescriptions({\nanswer: \"answer to the user's question\",\nsource: \"source used to answer the user's question, should be a website.\",\n});\n\nconst confidenceParser = new RegexParser(\n/Confidence: (A|B|C), Explanation: (.*)/,\n[\"confidence\", \"explanation\"],\n\"noConfidence\"\n);\n\nconst parser = new CombiningOutputParser(answerParser, confidenceParser);\nconst formatInstructions = parser.getFormatInstructions();\n\nconst prompt = new PromptTemplate({\ntemplate:\n\"Answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\ninputVariables: [\"question\"],\npartialVariables: { format_instructions: formatInstructions },\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/combining.int.test.ts","loc":{"lines":{"from":1,"to":31}}}}],["738",{"pageContent":"const model = new OpenAI({ temperature: 0 });\n\nconst input = await prompt.format({\nquestion: \"What is the capital of France?\",\n});\n\nconsole.log(input);\n\nconst response = await model.call(input);\n\nconsole.log(response);\n\nconsole.log(await parser.parse(response));\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/combining.int.test.ts","loc":{"lines":{"from":47,"to":60}}}}],["739",{"pageContent":"import { expect, test } from \"@jest/globals\";\n\nimport { CombiningOutputParser } from \"../combining.js\";\nimport { StructuredOutputParser } from \"../structured.js\";\nimport { RegexParser } from \"../regex.js\";\n\ntest(\"CombiningOutputParser\", async () => {\nconst parser = new CombiningOutputParser(\nStructuredOutputParser.fromNamesAndDescriptions({\nurl: \"A link to the resource\",\n}),\nnew RegexParser(\n/Confidence: (A|B|C), Explanation: (.*)/,\n[\"confidence\", \"explanation\"],\n\"noConfidence\"\n)\n);\n\nexpect(parser.getFormatInstructions()).toMatchInlineSnapshot(`\n\"Return the following 2 outputs, each formatted as described below. Include the delimiter characters \"-----\" in your response:\n\n-----Output 1-----\nYou must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n\n\"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/combining.test.ts","loc":{"lines":{"from":1,"to":25}}}}],["740",{"pageContent":"For example, the example \"JSON Schema\" instance {{\"properties\": {{\"foo\": {{\"description\": \"a list of test words\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nwould match an object with one required property, \"foo\". The \"type\" property specifies \"foo\" must be an \"array\", and the \"description\" property semantically describes it as \"a list of test words\". The items within \"foo\" must be strings.\nThus, the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of this example \"JSON Schema\". The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nYour output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/combining.test.ts","loc":{"lines":{"from":74,"to":78}}}}],["741",{"pageContent":"Here is the JSON Schema instance your output must adhere to:\n\\`\\`\\`json\n{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\",\"description\":\"A link to the resource\"}},\"required\":[\"url\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n\\`\\`\\`\n-----\n\n-----Output 2-----\nYour response should match the following regex: /Confidence: (A|B|C), Explanation: (.*)/\n-----\n\"\n`);\n\nexpect(\nawait parser.parse(\n`-----Output 0-----\n{\"url\": \"https://en.wikipedia.org/wiki/Paris\"}\n-----\n\n-----Output 1-----\nConfidence: A, Explanation: Because it is the capital of France.\n-----`\n)\n).toMatchInlineSnapshot(`\n{\n\"confidence\": \"A\",\n\"explanation\": \"Because it is the capital of France.\",\n\"url\": \"https://en.wikipedia.org/wiki/Paris\",\n}\n`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/combining.test.ts","loc":{"lines":{"from":130,"to":158}}}}],["742",{"pageContent":"expect(\nawait parser.parse(\n'```\\n{\"url\": \"https://en.wikipedia.org/wiki/Paris\"}\\n```\\nConfidence: A, Explanation: Because it is the capital of France.'\n)\n).toMatchInlineSnapshot(`\n{\n\"confidence\": \"A\",\n\"explanation\": \"Because it is the capital of France.\",\n\"url\": \"https://en.wikipedia.org/wiki/Paris\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/combining.test.ts","loc":{"lines":{"from":210,"to":221}}}}],["743",{"pageContent":"import { test, expect } from \"@jest/globals\";\n\nimport { CommaSeparatedListOutputParser } from \"../list.js\";\n\ntest(\"CommaSeparatedListOutputParser\", async () => {\nconst parser = new CommaSeparatedListOutputParser();\n\nexpect(await parser.parse(\"hello, bye\")).toEqual([\"hello\", \"bye\"]);\n\nexpect(await parser.parse(\"hello,bye\")).toEqual([\"hello\", \"bye\"]);\n\nexpect(await parser.parse(\"hello\")).toEqual([\"hello\"]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/list.test.ts","loc":{"lines":{"from":1,"to":13}}}}],["744",{"pageContent":"import { expect, test } from \"@jest/globals\";\n\nimport { StructuredOutputParser } from \"../structured.js\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\nimport { LLMChain } from \"../../chains/index.js\";\nimport {\nChatPromptTemplate,\nPromptTemplate,\nSystemMessagePromptTemplate,\n} from \"../../prompts/index.js\";\n\ntest(\"StructuredOutputParser deals special chars in prompt with llm model\", async () => {\nconst model = new OpenAI({\ntemperature: 0,\n});\n\nconst parser = StructuredOutputParser.fromNamesAndDescriptions({\nquestion1: \"a very on-topic question\",\nquestion2: \"a super weird question\",\nquestion3: \"an on-topic, but slightly creative\",\n});\n\nconst prompt = new PromptTemplate({\ntemplate: \"context:\\n{context}\\n---{format_instructions}\",\ninputVariables: [\"context\"],\npartialVariables: {\nformat_instructions: parser.getFormatInstructions(),\n},\n});\n\nconst chain = new LLMChain({\nllm: model,\nprompt,\noutputParser: parser,\noutputKey: \"questions\",\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.int.test.ts","loc":{"lines":{"from":1,"to":37}}}}],["745",{"pageContent":"const result = await chain.call({\ncontext: `The U2 ur-myth begins in 1976, when drummer Larry Mullen wanted to form a band.\nHe picked four school friends from Mount Temple Comprehensive School in Dublin.\nLarry formed U2, says Paul McGuinness, U2s manager from the beginning. He\nauditioned the other three and he chose them. The first name of U2 was the Larry\nMullen band, McGuinness laughs. And he never lets us forget it. `,\n});\n\nconsole.log(\"response\", result);\n\nexpect(result.questions).toHaveProperty(\"question1\");\nexpect(result.questions).toHaveProperty(\"question2\");\nexpect(result.questions).toHaveProperty(\"question3\");\n});\n\ntest(\"StructuredOutputParser deals special chars in prompt with chat model\", async () => {\nconst model = new ChatOpenAI({\ntemperature: 0,\n});\n\nconst parser = StructuredOutputParser.fromNamesAndDescriptions({\nquestion1: \"a very on-topic question\",\nquestion2: \"a super weird question\",\nquestion3: \"an on-topic, but slightly creative\",\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.int.test.ts","loc":{"lines":{"from":144,"to":168}}}}],["746",{"pageContent":"const prompt = new ChatPromptTemplate({\npromptMessages: [\nSystemMessagePromptTemplate.fromTemplate(\"context:\\n{context}\\n---\"),\nSystemMessagePromptTemplate.fromTemplate(`{format_instructions}`),\n],\ninputVariables: [\"context\"],\npartialVariables: {\nformat_instructions: parser.getFormatInstructions(),\n},\n});\n\nconst chain = new LLMChain({\nllm: model,\nprompt,\noutputParser: parser,\noutputKey: \"questions\",\n});\n\nconst result = await chain.call({\ncontext: `The U2 ur-myth begins in 1976, when drummer Larry Mullen wanted to form a band.\nHe picked four school friends from Mount Temple Comprehensive School in Dublin.\nLarry formed U2, says Paul McGuinness, U2s manager from the beginning. He\nauditioned the other three and he chose them. The first name of U2 was the Larry\nMullen band, McGuinness laughs. And he never lets us forget it. `,\n});\n\nconsole.log(\"response\", result);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.int.test.ts","loc":{"lines":{"from":276,"to":302}}}}],["747",{"pageContent":"expect(result.questions).toHaveProperty(\"question1\");\nexpect(result.questions).toHaveProperty(\"question2\");\nexpect(result.questions).toHaveProperty(\"question3\");\n});\n\ntest(\"StructuredOutputParser deals special chars in prompt with chat model 2\", async () => {\nconst model = new ChatOpenAI({\ntemperature: 0,\n});\n\nconst parser = StructuredOutputParser.fromNamesAndDescriptions({\nquestion1: \"a very on-topic question\",\nquestion2: \"a super weird question\",\nquestion3: \"an on-topic, but slightly creative\",\n});\n\nconst prompt = new ChatPromptTemplate({\npromptMessages: [\nSystemMessagePromptTemplate.fromTemplate(\"context:\\n{context}\\n---\"),\nSystemMessagePromptTemplate.fromTemplate(`{format_instructions}`),\n],\ninputVariables: [\"context\"],\npartialVariables: {\nformat_instructions: parser.getFormatInstructions(),\n},\n});\n\nconst chain = new LLMChain({\nllm: model,\nprompt,\noutputKey: \"questions\",\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.int.test.ts","loc":{"lines":{"from":415,"to":446}}}}],["748",{"pageContent":"const result = await chain.call({\ncontext: `The U2 ur-myth begins in 1976, when drummer Larry Mullen wanted to form a band.\nHe picked four school friends from Mount Temple Comprehensive School in Dublin.\nLarry formed U2, says Paul McGuinness, U2s manager from the beginning. He\nauditioned the other three and he chose them. The first name of U2 was the Larry\nMullen band, McGuinness laughs. And he never lets us forget it. `,\n});\n\nconsole.log(\"response\", result);\nconst parsed = await parser.parse(result.questions);\n\nexpect(parsed).toHaveProperty(\"question1\");\nexpect(parsed).toHaveProperty(\"question2\");\nexpect(parsed).toHaveProperty(\"question3\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.int.test.ts","loc":{"lines":{"from":558,"to":572}}}}],["749",{"pageContent":"import { z } from \"zod\";\n\nimport { expect, test } from \"@jest/globals\";\n\nimport { StructuredOutputParser } from \"../structured.js\";\n\ntest(\"StructuredOutputParser.fromNamesAndDescriptions\", async () => {\nconst parser = StructuredOutputParser.fromNamesAndDescriptions({\nurl: \"A link to the resource\",\n});\n\nexpect(await parser.parse('```\\n{\"url\": \"value\"}```')).toEqual({\nurl: \"value\",\n});\n\nexpect(parser.getFormatInstructions()).toMatchInlineSnapshot(`\n\"You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n\n\"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":1,"to":19}}}}],["750",{"pageContent":"For example, the example \"JSON Schema\" instance {{\"properties\": {{\"foo\": {{\"description\": \"a list of test words\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nwould match an object with one required property, \"foo\". The \"type\" property specifies \"foo\" must be an \"array\", and the \"description\" property semantically describes it as \"a list of test words\". The items within \"foo\" must be strings.\nThus, the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of this example \"JSON Schema\". The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nYour output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":21,"to":25}}}}],["751",{"pageContent":"Here is the JSON Schema instance your output must adhere to:\n\\`\\`\\`json\n{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\",\"description\":\"A link to the resource\"}},\"required\":[\"url\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n\\`\\`\\`\n\"\n`);\n});\n\nenum StateProvinceEnum {\nAlabama = \"AL\",\nAlaska = \"AK\",\nArizona = \"AZ\",\n}\n\ntest(\"StructuredOutputParser.fromZodSchema\", async () => {\nconst parser = StructuredOutputParser.fromZodSchema(\nz.object({ url: z.string().describe(\"A link to the resource\") })\n);\n\nexpect(await parser.parse('```\\n{\"url\": \"value\"}```')).toEqual({\nurl: \"value\",\n});\n\nexpect(parser.getFormatInstructions()).toMatchInlineSnapshot(`\n\"You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n\n\"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":189,"to":215}}}}],["752",{"pageContent":"For example, the example \"JSON Schema\" instance {{\"properties\": {{\"foo\": {{\"description\": \"a list of test words\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nwould match an object with one required property, \"foo\". The \"type\" property specifies \"foo\" must be an \"array\", and the \"description\" property semantically describes it as \"a list of test words\". The items within \"foo\" must be strings.\nThus, the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of this example \"JSON Schema\". The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nYour output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":215,"to":219}}}}],["753",{"pageContent":"Here is the JSON Schema instance your output must adhere to:\n\\`\\`\\`json\n{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\",\"description\":\"A link to the resource\"}},\"required\":[\"url\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n\\`\\`\\`\n\"\n`);\n});\n\ntest(\"StructuredOutputParser.fromZodSchema\", async () => {\nconst parser = StructuredOutputParser.fromZodSchema(\nz.object({\nanswer: z.string().describe(\"answer to the user's question\"),\nsources: z\n.array(z.string())\n.describe(\"sources used to answer the question, should be websites.\"),\n})\n);\n\nexpect(\nawait parser.parse(\n'```\\n{\"answer\": \"value\", \"sources\": [\"this-source\"]}```'\n)\n).toEqual({\nanswer: \"value\",\nsources: [\"this-source\"],\n});\n\nexpect(\nawait parser.parse(\n'```json\\n{\"answer\": \"value\", \"sources\": [\"this-source\"]}```'\n)\n).toEqual({\nanswer: \"value\",\nsources: [\"this-source\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":383,"to":417}}}}],["754",{"pageContent":"expect(\nawait parser.parse(\n'some other stuff```json\\n{\"answer\": \"value\", \"sources\": [\"this-source\"]}```some other stuff at the end'\n)\n).toEqual({\nanswer: \"value\",\nsources: [\"this-source\"],\n});\n\nexpect(parser.getFormatInstructions()).toMatchInlineSnapshot(`\n\"You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n\n\"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":584,"to":596}}}}],["755",{"pageContent":"For example, the example \"JSON Schema\" instance {{\"properties\": {{\"foo\": {{\"description\": \"a list of test words\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nwould match an object with one required property, \"foo\". The \"type\" property specifies \"foo\" must be an \"array\", and the \"description\" property semantically describes it as \"a list of test words\". The items within \"foo\" must be strings.\nThus, the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of this example \"JSON Schema\". The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nYour output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":601,"to":605}}}}],["756",{"pageContent":"Here is the JSON Schema instance your output must adhere to:\n\\`\\`\\`json\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer to the user's question\"},\"sources\":{\"type\":\"array\",\"items\":{\"type\":\"string\"},\"description\":\"sources used to answer the question, should be websites.\"}},\"required\":[\"answer\",\"sources\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n\\`\\`\\`\n\"\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":697,"to":703}}}}],["757",{"pageContent":"test(\"StructuredOutputParser.fromZodSchema\", async () => {\nconst parser = StructuredOutputParser.fromZodSchema(\nz\n.object({\nurl: z.string().describe(\"A link to the resource\"),\ntitle: z.string().describe(\"A title for the resource\"),\nyear: z.number().describe(\"The year the resource was created\"),\ncreatedAt: z\n.string()\n.datetime()\n.describe(\"The date and time the resource was created\"),\ncreatedAtDate: z.coerce\n.date()\n.describe(\"The date the resource was created\")\n.optional(),\nauthors: z.array(\nz.object({\nname: z.string().describe(\"The name of the author\"),\nemail: z.string().describe(\"The email of the author\"),","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":769,"to":787}}}}],["758",{"pageContent":": z.enum([\"author\", \"editor\"]).optional(),\naddress: z\n.string()\n.optional()\n.describe(\"The address of the author\"),\nstateProvince: z\n.nativeEnum(StateProvinceEnum)\n.optional()\n.describe(\"The state or province of the author\"),\n})\n),\n})\n.describe(\"Only One object\")\n);\n\nexpect(\nawait parser.parse(\n'```\\n{\"url\": \"value\", \"title\": \"value\", \"year\": 2011, \"createdAt\": \"2023-03-29T16:07:09.600Z\", \"createdAtDate\": \"2023-03-29\", \"authors\": [{\"name\": \"value\", \"email\": \"value\", \"stateProvince\": \"AZ\"}]}```'\n)\n).toEqual({\nurl: \"value\",\ntitle: \"value\",\nyear: 2011,\ncreatedAt: \"2023-03-29T16:07:09.600Z\",\ncreatedAtDate: new Date(\"2023-03-29T00:00:00.000Z\"),\nauthors: [{ name: \"value\", email: \"value\", stateProvince: \"AZ\" }],\n});\n\nexpect(parser.getFormatInstructions()).toMatchInlineSnapshot(`\n\"You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n\n\"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":957,"to":988}}}}],["759",{"pageContent":"For example, the example \"JSON Schema\" instance {{\"properties\": {{\"foo\": {{\"description\": \"a list of test words\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nwould match an object with one required property, \"foo\". The \"type\" property specifies \"foo\" must be an \"array\", and the \"description\" property semantically describes it as \"a list of test words\". The items within \"foo\" must be strings.\nThus, the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of this example \"JSON Schema\". The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nYour output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":988,"to":992}}}}],["760",{"pageContent":"Here is the JSON Schema instance your output must adhere to:\n\\`\\`\\`json","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":994,"to":995}}}}],["761",{"pageContent":"{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\",\"description\":\"A link to the resource\"},\"title\":{\"type\":\"string\",\"description\":\"A title for the resource\"},\"year\":{\"type\":\"number\",\"description\":\"The year the resource was created\"},\"createdAt\":{\"type\":\"string\",\"format\":\"date-time\",\"description\":\"The date and time the resource was created\"},\"createdAtDate\":{\"type\":\"string\",\"format\":\"date-time\",\"description\":\"The date the resource was created\"},\"authors\":{\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\",\"description\":\"The name of the author\"},\"email\":{\"type\":\"string\",\"description\":\"The email of the author\"},\"type\":{\"type\":\"string\",\"enum\":[\"author\",\"editor\"]},\"address\":{\"type\":\"string\",\"description\":\"The address of the author\"},\"stateProvince\":{\"type\":\"string\",\"enum\":[\"AL\",\"AK\",\"AZ\"],\"description\":\"The state or province of the","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":1152,"to":1152}}}}],["762",{"pageContent":"author\"}},\"required\":[\"name\",\"email\"],\"additionalProperties\":false}}},\"required\":[\"url\",\"title\",\"year\",\"createdAt\",\"authors\"],\"additionalProperties\":false,\"description\":\"Only One object\",\"$schema\":\"http://json-schema.org/draft-07/schema#\"}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":1152,"to":1152}}}}],["763",{"pageContent":"\\`\\`\\`\n\"\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":1152,"to":1155}}}}],["764",{"pageContent":"import {\nBasePromptValue,\nExample,\nHumanChatMessage,\nInputValues,\nPartialValues,\n} from \"../schema/index.js\";\nimport { BaseOutputParser } from \"../schema/output_parser.js\";\nimport { SerializedBasePromptTemplate } from \"./serde.js\";\n\nexport class StringPromptValue extends BasePromptValue {\nvalue: string;\n\nconstructor(value: string) {\nsuper();\nthis.value = value;\n}\n\ntoString() {\nreturn this.value;\n}\n\ntoChatMessages() {\nreturn [new HumanChatMessage(this.value)];\n}\n}\n\n/**\n* Input common to all prompt templates.\n*/\nexport interface BasePromptTemplateInput {\n/**\n* A list of variable names the prompt template expects\n*/\ninputVariables: string[];\n\n/**\n* How to parse the output of calling an LLM on this formatted prompt\n*/\noutputParser?: BaseOutputParser;\n\n/** Partial variables */\npartialVariables?: PartialValues;\n}\n\n/**\n* Base class for prompt templates. Exposes a format method that returns a\n* string prompt given a set of input values.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":1,"to":49}}}}],["765",{"pageContent":"abstract class BasePromptTemplate implements BasePromptTemplateInput {\ninputVariables: string[];\n\noutputParser?: BaseOutputParser;\n\npartialVariables?: InputValues;\n\nconstructor(input: BasePromptTemplateInput) {\nconst { inputVariables } = input;\nif (inputVariables.includes(\"stop\")) {\nthrow new Error(\n\"Cannot have an input variable named 'stop', as it is used internally, please rename.\"\n);\n}\nObject.assign(this, input);\n}\n\nabstract partial(values: PartialValues): Promise<BasePromptTemplate>;\n\nasync mergePartialAndUserVariables(\nuserVariables: InputValues\n): Promise<InputValues> {\nconst partialVariables = this.partialVariables ?? {};\nconst partialValues: InputValues = {};\n\nfor (const [key, value] of Object.entries(partialVariables)) {\nif (typeof value === \"string\") {\npartialValues[key] = value;\n} else {\npartialValues[key] = await value();\n}\n}\n\nconst allKwargs = { ...partialValues, ...userVariables };\nreturn allKwargs;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":167,"to":202}}}}],["766",{"pageContent":"/**\n* Format the prompt given the input values.\n*\n* @param values - A dictionary of arguments to be passed to the prompt template.\n* @returns A formatted prompt string.\n*\n* @example\n* ```ts\n* prompt.format({ foo: \"bar\" });\n* ```\n*/\nabstract format(values: InputValues): Promise<string>;\n\n/**\n* Format the prompt given the input values and return a formatted prompt value.\n* @param values\n* @returns A formatted PromptValue.\n*/\nabstract formatPromptValue(values: InputValues): Promise<BasePromptValue>;\n\n/**\n* Return the string type key uniquely identifying this class of prompt template.\n*/\nabstract _getPromptType(): string;\n\n/**\n* Return a json-like object representing this prompt template.\n*/\nabstract serialize(): SerializedBasePromptTemplate;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":320,"to":348}}}}],["767",{"pageContent":"/**\n* Load a prompt template from a json-like object describing it.\n*\n* @remarks\n* Deserializing needs to be async because templates (e.g. {@link FewShotPromptTemplate}) can\n* reference remote resources that we read asynchronously with a web\n* request.\n*/\nstatic async deserialize(\ndata: SerializedBasePromptTemplate\n): Promise<BasePromptTemplate> {\nswitch (data._type) {\ncase \"prompt\": {\nconst { PromptTemplate } = await import(\"./prompt.js\");\nreturn PromptTemplate.deserialize(data);\n}\ncase undefined: {\nconst { PromptTemplate } = await import(\"./prompt.js\");\nreturn PromptTemplate.deserialize({ ...data, _type: \"prompt\" });\n}\ncase \"few_shot\": {\nconst { FewShotPromptTemplate } = await import(\"./few_shot.js\");\nreturn FewShotPromptTemplate.deserialize(data);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":475,"to":498}}}}],["768",{"pageContent":":\nthrow new Error(\n`Invalid prompt type in config: ${\n(data as SerializedBasePromptTemplate)._type\n}`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":625,"to":633}}}}],["769",{"pageContent":"abstract class BaseStringPromptTemplate extends BasePromptTemplate {\nasync formatPromptValue(values: InputValues): Promise<BasePromptValue> {\nconst formattedPrompt = await this.format(values);\nreturn new StringPromptValue(formattedPrompt);\n}\n}\n\n/**\n* Base class for example selectors.\n*/\nexport abstract class BaseExampleSelector {\nabstract addExample(example: Example): Promise<void | string>;\n\nabstract selectExamples(input_variables: Example): Promise<Example[]>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":791,"to":805}}}}],["770",{"pageContent":"import {\nAIChatMessage,\nBaseChatMessage,\nBasePromptValue,\nChatMessage,\nHumanChatMessage,\nInputValues,\nPartialValues,\nSystemChatMessage,\n} from \"../schema/index.js\";\nimport {\nBasePromptTemplate,\nBasePromptTemplateInput,\nBaseStringPromptTemplate,\n} from \"./base.js\";\nimport { PromptTemplate } from \"./prompt.js\";\nimport {\nSerializedChatPromptTemplate,\nSerializedMessagePromptTemplate,\n} from \"./serde.js\";\n\nexport abstract class BaseMessagePromptTemplate {\nabstract inputVariables: string[];\n\nabstract formatMessages(values: InputValues): Promise<BaseChatMessage[]>;\n\nserialize(): SerializedMessagePromptTemplate {\nreturn {\n_type: this.constructor.name,\n...JSON.parse(JSON.stringify(this)),\n};\n}\n}\n\nexport class ChatPromptValue extends BasePromptValue {\nmessages: BaseChatMessage[];\n\nconstructor(messages: BaseChatMessage[]) {\nsuper();\nthis.messages = messages;\n}\n\ntoString() {\nreturn JSON.stringify(this.messages);\n}\n\ntoChatMessages() {\nreturn this.messages;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":1,"to":50}}}}],["771",{"pageContent":"class MessagesPlaceholder extends BaseMessagePromptTemplate {\nvariableName: string;\n\nconstructor(variableName: string) {\nsuper();\nthis.variableName = variableName;\n}\n\nget inputVariables() {\nreturn [this.variableName];\n}\n\nformatMessages(values: InputValues): Promise<BaseChatMessage[]> {\nreturn Promise.resolve(values[this.variableName] as BaseChatMessage[]);\n}\n}\n\nexport abstract class BaseMessageStringPromptTemplate extends BaseMessagePromptTemplate {\nprompt: BaseStringPromptTemplate;\n\nprotected constructor(prompt: BaseStringPromptTemplate) {\nsuper();\nthis.prompt = prompt;\n}\n\nget inputVariables() {\nreturn this.prompt.inputVariables;\n}\n\nabstract format(values: InputValues): Promise<BaseChatMessage>;\n\nasync formatMessages(values: InputValues): Promise<BaseChatMessage[]> {\nreturn [await this.format(values)];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":326,"to":360}}}}],["772",{"pageContent":"abstract class BaseChatPromptTemplate extends BasePromptTemplate {\nconstructor(input: BasePromptTemplateInput) {\nsuper(input);\n}\n\nabstract formatMessages(values: InputValues): Promise<BaseChatMessage[]>;\n\nasync format(values: InputValues): Promise<string> {\nreturn (await this.formatPromptValue(values)).toString();\n}\n\nasync formatPromptValue(values: InputValues): Promise<BasePromptValue> {\nconst resultMessages = await this.formatMessages(values);\nreturn new ChatPromptValue(resultMessages);\n}\n}\n\nexport class ChatMessagePromptTemplate extends BaseMessageStringPromptTemplate {\nrole: string;\n\nasync format(values: InputValues): Promise<BaseChatMessage> {\nreturn new ChatMessage(await this.prompt.format(values), this.role);\n}\n\nconstructor(prompt: BaseStringPromptTemplate, role: string) {\nsuper(prompt);\nthis.role = role;\n}\n\nstatic fromTemplate(template: string, role: string) {\nreturn new this(PromptTemplate.fromTemplate(template), role);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":644,"to":676}}}}],["773",{"pageContent":"class HumanMessagePromptTemplate extends BaseMessageStringPromptTemplate {\nasync format(values: InputValues): Promise<BaseChatMessage> {\nreturn new HumanChatMessage(await this.prompt.format(values));\n}\n\nconstructor(prompt: BaseStringPromptTemplate) {\nsuper(prompt);\n}\n\nstatic fromTemplate(template: string) {\nreturn new this(PromptTemplate.fromTemplate(template));\n}\n}\n\nexport class AIMessagePromptTemplate extends BaseMessageStringPromptTemplate {\nasync format(values: InputValues): Promise<BaseChatMessage> {\nreturn new AIChatMessage(await this.prompt.format(values));\n}\n\nconstructor(prompt: BaseStringPromptTemplate) {\nsuper(prompt);\n}\n\nstatic fromTemplate(template: string) {\nreturn new this(PromptTemplate.fromTemplate(template));\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":953,"to":979}}}}],["774",{"pageContent":"class SystemMessagePromptTemplate extends BaseMessageStringPromptTemplate {\nasync format(values: InputValues): Promise<BaseChatMessage> {\nreturn new SystemChatMessage(await this.prompt.format(values));\n}\n\nconstructor(prompt: BaseStringPromptTemplate) {\nsuper(prompt);\n}\n\nstatic fromTemplate(template: string) {\nreturn new this(PromptTemplate.fromTemplate(template));\n}\n}\n\nexport interface ChatPromptTemplateInput extends BasePromptTemplateInput {\n/**\n* The prompt messages\n*/\npromptMessages: BaseMessagePromptTemplate[];\n\n/**\n* Whether to try validating the template on initialization\n*\n* @defaultValue `true`\n*/\nvalidateTemplate?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":1268,"to":1294}}}}],["775",{"pageContent":"class ChatPromptTemplate\nextends BaseChatPromptTemplate\nimplements ChatPromptTemplateInput\n{\npromptMessages: BaseMessagePromptTemplate[];\n\nvalidateTemplate = true;\n\nconstructor(input: ChatPromptTemplateInput) {\nsuper(input);\nObject.assign(this, input);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":1587,"to":1597}}}}],["776",{"pageContent":"if (this.validateTemplate) {\nconst inputVariablesMessages = new Set<string>();\nfor (const promptMessage of this.promptMessages) {\nfor (const inputVariable of promptMessage.inputVariables) {\ninputVariablesMessages.add(inputVariable);\n}\n}\nconst inputVariablesInstance = new Set(\nthis.partialVariables\n? this.inputVariables.concat(Object.keys(this.partialVariables))\n: this.inputVariables\n);\nconst difference = new Set(\n[...inputVariablesInstance].filter(\n(x) => !inputVariablesMessages.has(x)\n)\n);\nif (difference.size > 0) {\nthrow new Error(\n`Input variables \\`${[\n...difference,\n]}\\` are not used in any of the prompt messages.`\n);\n}\nconst otherDifference = new Set(\n[...inputVariablesMessages].filter(\n(x) => !inputVariablesInstance.has(x)\n)\n);\nif (otherDifference.size > 0) {\nthrow new Error(\n`Input variables \\`${[\n...otherDifference,\n]}\\` are used in prompt messages but not in the prompt template.`\n);\n}\n}\n}\n\n_getPromptType(): \"chat\" {\nreturn \"chat\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":1903,"to":1944}}}}],["777",{"pageContent":"async formatMessages(values: InputValues): Promise<BaseChatMessage[]> {\nconst allValues = await this.mergePartialAndUserVariables(values);\n\nlet resultMessages: BaseChatMessage[] = [];\n\nfor (const promptMessage of this.promptMessages) {\nconst inputValues = promptMessage.inputVariables.reduce(\n(acc, inputVariable) => {\nif (!(inputVariable in allValues)) {\nthrow new Error(\n`Missing value for input variable \\`${inputVariable}\\``\n);\n}\nacc[inputVariable] = allValues[inputVariable];\nreturn acc;\n},\n{} as InputValues\n);\nconst message = await promptMessage.formatMessages(inputValues);\nresultMessages = resultMessages.concat(message);\n}\nreturn resultMessages;\n}\n\nserialize(): SerializedChatPromptTemplate {\nif (this.outputParser !== undefined) {\nthrow new Error(\n\"ChatPromptTemplate cannot be serialized if outputParser is set\"\n);\n}\nreturn {\ninput_variables: this.inputVariables,\nprompt_messages: this.promptMessages.map((m) => m.serialize()),\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":2220,"to":2254}}}}],["778",{"pageContent":"async partial(values: PartialValues): Promise<ChatPromptTemplate> {\n// This is implemented in a way it doesn't require making\n// BaseMessagePromptTemplate aware of .partial()\nconst promptDict: ChatPromptTemplateInput = { ...this };\npromptDict.inputVariables = this.inputVariables.filter(\n(iv) => !(iv in values)\n);\npromptDict.partialVariables = {\n...(this.partialVariables ?? {}),\n...values,\n};\nreturn new ChatPromptTemplate(promptDict);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":2531,"to":2543}}}}],["779",{"pageContent":"static fromPromptMessages(\npromptMessages: (BaseMessagePromptTemplate | ChatPromptTemplate)[]\n): ChatPromptTemplate {\nconst flattenedMessages = promptMessages.reduce(\n(acc, promptMessage) =>\nacc.concat(\n// eslint-disable-next-line no-instanceof/no-instanceof\npromptMessage instanceof ChatPromptTemplate\n? promptMessage.promptMessages\n: [promptMessage]\n),\n[] as BaseMessagePromptTemplate[]\n);\nconst flattenedPartialVariables = promptMessages.reduce(\n(acc, promptMessage) =>\n// eslint-disable-next-line no-instanceof/no-instanceof\npromptMessage instanceof ChatPromptTemplate\n? Object.assign(acc, promptMessage.partialVariables)\n: acc,\nObject.create(null) as PartialValues\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":2841,"to":2861}}}}],["780",{"pageContent":"const inputVariables = new Set<string>();\nfor (const promptMessage of flattenedMessages) {\nfor (const inputVariable of promptMessage.inputVariables) {\nif (inputVariable in flattenedPartialVariables) {\ncontinue;\n}\ninputVariables.add(inputVariable);\n}\n}\nreturn new ChatPromptTemplate({\ninputVariables: [...inputVariables],\npromptMessages: flattenedMessages,\npartialVariables: flattenedPartialVariables,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":3152,"to":3167}}}}],["781",{"pageContent":"import {\nBaseStringPromptTemplate,\nBasePromptTemplateInput,\nBaseExampleSelector,\n} from \"./base.js\";\nimport {\nTemplateFormat,\ncheckValidTemplate,\nrenderTemplate,\n} from \"./template.js\";\nimport { PromptTemplate } from \"./prompt.js\";\nimport { SerializedFewShotTemplate } from \"./serde.js\";\nimport { Example, InputValues, PartialValues } from \"../schema/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":1,"to":13}}}}],["782",{"pageContent":"interface FewShotPromptTemplateInput extends BasePromptTemplateInput {\n/**\n* Examples to format into the prompt. Exactly one of this or\n* {@link exampleSelector} must be\n* provided.\n*/\nexamples?: Example[];\n\n/**\n* An {@link BaseExampleSelector} Examples to format into the prompt. Exactly one of this or\n* {@link examples} must be\n* provided.\n*/\nexampleSelector?: BaseExampleSelector;\n\n/**\n* An {@link PromptTemplate} used to format a single example.\n*/\nexamplePrompt: PromptTemplate;\n\n/**\n* String separator used to join the prefix, the examples, and suffix.\n*/\nexampleSeparator?: string;\n\n/**\n* A prompt template string to put before the examples.\n*\n* @defaultValue `\"\"`\n*/\nprefix?: string;\n\n/**\n* A prompt template string to put after the examples.\n*/\nsuffix?: string;\n\n/**\n* The format of the prompt template. Options are: 'f-string', 'jinja-2'\n*/\ntemplateFormat?: TemplateFormat;\n\n/**\n* Whether or not to try validating the template on initialization.\n*/\nvalidateTemplate?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":215,"to":261}}}}],["783",{"pageContent":"/**\n* Prompt template that contains few-shot examples.\n* @augments BasePromptTemplate\n* @augments FewShotPromptTemplateInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":440,"to":444}}}}],["784",{"pageContent":"class FewShotPromptTemplate\nextends BaseStringPromptTemplate\nimplements FewShotPromptTemplateInput\n{\nexamples?: InputValues[];\n\nexampleSelector?: BaseExampleSelector | undefined;\n\nexamplePrompt: PromptTemplate;\n\nsuffix = \"\";\n\nexampleSeparator = \"\\n\\n\";\n\nprefix = \"\";\n\ntemplateFormat: TemplateFormat = \"f-string\";\n\nvalidateTemplate = true;\n\nconstructor(input: FewShotPromptTemplateInput) {\nsuper(input);\nObject.assign(this, input);\n\nif (this.examples !== undefined && this.exampleSelector !== undefined) {\nthrow new Error(\n\"Only one of 'examples' and 'example_selector' should be provided\"\n);\n}\n\nif (this.examples === undefined && this.exampleSelector === undefined) {\nthrow new Error(\n\"One of 'examples' and 'example_selector' should be provided\"\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":652,"to":686}}}}],["785",{"pageContent":"if (this.validateTemplate) {\nlet totalInputVariables = this.inputVariables;\nif (this.partialVariables) {\ntotalInputVariables = totalInputVariables.concat(\nObject.keys(this.partialVariables)\n);\n}\ncheckValidTemplate(\nthis.prefix + this.suffix,\nthis.templateFormat,\ntotalInputVariables\n);\n}\n}\n\n_getPromptType(): \"few_shot\" {\nreturn \"few_shot\";\n}\n\nprivate async getExamples(\ninputVariables: InputValues\n): Promise<InputValues[]> {\nif (this.examples !== undefined) {\nreturn this.examples;\n}\nif (this.exampleSelector !== undefined) {\nreturn this.exampleSelector.selectExamples(inputVariables);\n}\n\nthrow new Error(\n\"One of 'examples' and 'example_selector' should be provided\"\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":875,"to":907}}}}],["786",{"pageContent":"async partial(values: PartialValues): Promise<FewShotPromptTemplate> {\nconst promptDict: FewShotPromptTemplate = { ...this };\npromptDict.inputVariables = this.inputVariables.filter(\n(iv) => !(iv in values)\n);\npromptDict.partialVariables = {\n...(this.partialVariables ?? {}),\n...values,\n};\nreturn new FewShotPromptTemplate(promptDict);\n}\n\nasync format(values: InputValues): Promise<string> {\nconst allValues = await this.mergePartialAndUserVariables(values);\nconst examples = await this.getExamples(allValues);\n\nconst exampleStrings = await Promise.all(\nexamples.map((example) => this.examplePrompt.format(example))\n);\nconst template = [this.prefix, ...exampleStrings, this.suffix].join(\nthis.exampleSeparator\n);\nreturn renderTemplate(template, this.templateFormat, allValues);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":1098,"to":1121}}}}],["787",{"pageContent":"serialize(): SerializedFewShotTemplate {\nif (this.exampleSelector || !this.examples) {\nthrow new Error(\n\"Serializing an example selector is not currently supported\"\n);\n}\nif (this.outputParser !== undefined) {\nthrow new Error(\n\"Serializing an output parser is not currently supported\"\n);\n}\nreturn {\n_type: this._getPromptType(),\ninput_variables: this.inputVariables,\nexample_prompt: this.examplePrompt.serialize(),\nexample_separator: this.exampleSeparator,\nsuffix: this.suffix,\nprefix: this.prefix,\ntemplate_format: this.templateFormat,\nexamples: this.examples,\n};\n}\n\nstatic async deserialize(\ndata: SerializedFewShotTemplate\n): Promise<FewShotPromptTemplate> {\nconst { example_prompt } = data;\nif (!example_prompt) {\nthrow new Error(\"Missing example prompt\");\n}\nconst examplePrompt = await PromptTemplate.deserialize(example_prompt);\n\nlet examples: Example[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":1308,"to":1340}}}}],["788",{"pageContent":"if (Array.isArray(data.examples)) {\nexamples = data.examples;\n} else {\nthrow new Error(\n\"Invalid examples format. Only list or string are supported.\"\n);\n}\n\nreturn new FewShotPromptTemplate({\ninputVariables: data.input_variables,\nexamplePrompt,\nexamples,\nexampleSeparator: data.example_separator,\nprefix: data.prefix,\nsuffix: data.suffix,\ntemplateFormat: data.template_format,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":1524,"to":1542}}}}],["789",{"pageContent":"export {\nBaseExampleSelector,\nBasePromptTemplate,\nBasePromptTemplateInput,\nStringPromptValue,\nBaseStringPromptTemplate,\n} from \"./base.js\";\nexport { PromptTemplate, PromptTemplateInput } from \"./prompt.js\";\nexport {\nLengthBasedExampleSelector,\nLengthBasedExampleSelectorInput,\n} from \"./selectors/LengthBasedExampleSelector.js\";\nexport {\nSemanticSimilarityExampleSelector,\nSemanticSimilarityExampleSelectorInput,\n} from \"./selectors/SemanticSimilarityExampleSelector.js\";\nexport {\nFewShotPromptTemplate,\nFewShotPromptTemplateInput,\n} from \"./few_shot.js\";\nexport {\nChatPromptTemplate,\nHumanMessagePromptTemplate,\nAIMessagePromptTemplate,\nSystemMessagePromptTemplate,\nChatMessagePromptTemplate,\nMessagesPlaceholder,\nBaseChatPromptTemplate,\n} from \"./chat.js\";\nexport {\nSerializedPromptTemplate,\nSerializedBasePromptTemplate,\nSerializedFewShotTemplate,\nSerializedMessagePromptTemplate,\nSerializedChatPromptTemplate,\n} from \"./serde.js\";\nexport {\nparseTemplate,\nrenderTemplate,\ncheckValidTemplate,\nTemplateFormat,\n} from \"./template.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/index.ts","loc":{"lines":{"from":1,"to":42}}}}],["790",{"pageContent":"import { BasePromptTemplate } from \"./base.js\";\nimport { loadFromHub } from \"../util/hub.js\";\nimport { FileLoader, loadFromFile } from \"../util/load.js\";\nimport { parseFileConfig } from \"../util/parse.js\";\n\nconst loadPromptFromFile: FileLoader<BasePromptTemplate> = (text, path) =>\nBasePromptTemplate.deserialize(parseFileConfig(text, path));\n\n/**\n* Load a prompt from {@link https://github.com/hwchase17/langchain-hub | LangchainHub} or local filesystem.\n*\n* @example\n* Loading from LangchainHub:\n* ```ts\n* import { loadPrompt } from \"langchain/prompts/load\";\n* const prompt = await loadPrompt(\"lc://prompts/hello-world/prompt.yaml\");\n* ```\n*\n* @example\n* Loading from local filesystem:\n* ```ts\n* import { loadPrompt } from \"langchain/prompts/load\";\n* const prompt = await loadPrompt(\"/path/to/prompt.json\");\n* ```\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/load.ts","loc":{"lines":{"from":1,"to":25}}}}],["791",{"pageContent":"const loadPrompt = async (uri: string): Promise<BasePromptTemplate> => {\nconst hubResult = await loadFromHub(\nuri,\nloadPromptFromFile,\n\"prompts\",\nnew Set([\"py\", \"json\", \"yaml\"])\n);\nif (hubResult) {\nreturn hubResult;\n}\n\nreturn loadFromFile(uri, loadPromptFromFile);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/load.ts","loc":{"lines":{"from":40,"to":52}}}}],["792",{"pageContent":"import { BaseStringPromptTemplate, BasePromptTemplateInput } from \"./base.js\";\nimport {\ncheckValidTemplate,\nparseTemplate,\nrenderTemplate,\nTemplateFormat,\n} from \"./template.js\";\nimport { SerializedPromptTemplate } from \"./serde.js\";\nimport { InputValues, PartialValues } from \"../schema/index.js\";\n\n/**\n* Inputs to create a {@link PromptTemplate}\n* @augments BasePromptTemplateInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":1,"to":14}}}}],["793",{"pageContent":"interface PromptTemplateInput extends BasePromptTemplateInput {\n/**\n* The prompt template\n*/\ntemplate: string;\n\n/**\n* The format of the prompt template. Options are 'f-string', 'jinja-2'\n*\n* @defaultValue 'f-string'\n*/\ntemplateFormat?: TemplateFormat;\n\n/**\n* Whether or not to try validating the template on initialization\n*\n* @defaultValue `true`\n*/\nvalidateTemplate?: boolean;\n}\n\n/**\n* Schema to represent a basic prompt for an LLM.\n* @augments BasePromptTemplate\n* @augments PromptTemplateInput\n*\n* @example\n* ```ts\n* import { PromptTemplate } from \"langchain/prompts\";\n*\n* const prompt = new PromptTemplate({\n*   inputVariables: [\"foo\"],\n*   template: \"Say {foo}\",\n* });\n* ```\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":183,"to":218}}}}],["794",{"pageContent":"class PromptTemplate\nextends BaseStringPromptTemplate\nimplements PromptTemplateInput\n{\ntemplate: string;\n\ntemplateFormat: TemplateFormat = \"f-string\";\n\nvalidateTemplate = true;\n\nconstructor(input: PromptTemplateInput) {\nsuper(input);\nObject.assign(this, input);\n\nif (this.validateTemplate) {\nlet totalInputVariables = this.inputVariables;\nif (this.partialVariables) {\ntotalInputVariables = totalInputVariables.concat(\nObject.keys(this.partialVariables)\n);\n}\ncheckValidTemplate(\nthis.template,\nthis.templateFormat,\ntotalInputVariables\n);\n}\n}\n\n_getPromptType(): \"prompt\" {\nreturn \"prompt\";\n}\n\nasync format(values: InputValues): Promise<string> {\nconst allValues = await this.mergePartialAndUserVariables(values);\nreturn renderTemplate(this.template, this.templateFormat, allValues);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":374,"to":410}}}}],["795",{"pageContent":"/**\n* Take examples in list format with prefix and suffix to create a prompt.\n*\n* Intendend to be used a a way to dynamically create a prompt from examples.\n*\n* @param examples - List of examples to use in the prompt.\n* @param suffix - String to go after the list of examples. Should generally set up the user's input.\n* @param inputVariables - A list of variable names the final prompt template will expect\n* @param exampleSeparator - The separator to use in between examples\n* @param prefix - String that should go before any examples. Generally includes examples.\n*\n* @returns The final prompt template generated.\n*/\nstatic fromExamples(\nexamples: string[],\nsuffix: string,\ninputVariables: string[],\nexampleSeparator = \"\\n\\n\",\nprefix = \"\"\n) {\nconst template = [prefix, ...examples, suffix].join(exampleSeparator);\nreturn new PromptTemplate({\ninputVariables,\ntemplate,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":561,"to":586}}}}],["796",{"pageContent":"/**\n* Load prompt template from a template f-string\n*/\nstatic fromTemplate(\ntemplate: string,\n{\ntemplateFormat = \"f-string\",\n...rest\n}: Omit<PromptTemplateInput, \"template\" | \"inputVariables\"> = {}\n) {\nconst names = new Set<string>();\nparseTemplate(template, templateFormat).forEach((node) => {\nif (node.type === \"variable\") {\nnames.add(node.name);\n}\n});\n\nreturn new PromptTemplate({\ninputVariables: [...names],\ntemplateFormat,\ntemplate,\n...rest,\n});\n}\n\nasync partial(values: PartialValues): Promise<PromptTemplate> {\nconst promptDict: PromptTemplateInput = { ...this };\npromptDict.inputVariables = this.inputVariables.filter(\n(iv) => !(iv in values)\n);\npromptDict.partialVariables = {\n...(this.partialVariables ?? {}),\n...values,\n};\nreturn new PromptTemplate(promptDict);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":731,"to":766}}}}],["797",{"pageContent":"serialize(): SerializedPromptTemplate {\nif (this.outputParser !== undefined) {\nthrow new Error(\n\"Cannot serialize a prompt template with an output parser\"\n);\n}\nreturn {\n_type: this._getPromptType(),\ninput_variables: this.inputVariables,\ntemplate: this.template,\ntemplate_format: this.templateFormat,\n};\n}\n\nstatic async deserialize(\ndata: SerializedPromptTemplate\n): Promise<PromptTemplate> {\nif (!data.template) {\nthrow new Error(\"Prompt template must have a template\");\n}\nconst res = new PromptTemplate({\ninputVariables: data.input_variables,\ntemplate: data.template,\ntemplateFormat: data.template_format,\n});\nreturn res;\n}\n\n// TODO(from file)\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":917,"to":946}}}}],["798",{"pageContent":"import { Example } from \"../../schema/index.js\";\nimport type { BaseExampleSelector } from \"../base.js\";\nimport { PromptTemplate } from \"../prompt.js\";\n\nfunction getLengthBased(text: string): number {\nreturn text.split(/\\n| /).length;\n}\n\nexport interface LengthBasedExampleSelectorInput {\nexamplePrompt: PromptTemplate;\nmaxLength?: number;\ngetTextLength?: (text: string) => number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/LengthBasedExampleSelector.ts","loc":{"lines":{"from":1,"to":13}}}}],["799",{"pageContent":"class LengthBasedExampleSelector implements BaseExampleSelector {\nprotected examples: Example[] = [];\n\nexamplePrompt!: PromptTemplate;\n\ngetTextLength: (text: string) => number = getLengthBased;\n\nmaxLength = 2048;\n\nexampleTextLengths: number[] = [];\n\nconstructor(data: LengthBasedExampleSelectorInput) {\nthis.examplePrompt = data.examplePrompt;\nthis.maxLength = data.maxLength ?? 2048;\nthis.getTextLength = data.getTextLength ?? getLengthBased;\n}\n\nasync addExample(example: Example): Promise<void> {\nthis.examples.push(example);\nconst stringExample = await this.examplePrompt.format(example);\nthis.exampleTextLengths.push(this.getTextLength(stringExample));\n}\n\nasync calculateExampleTextLengths(\nv: number[],\nvalues: LengthBasedExampleSelector\n): Promise<number[]> {\nif (v.length > 0) {\nreturn v;\n}\n\nconst { examples, examplePrompt } = values;\nconst stringExamples = await Promise.all(\nexamples.map((eg: Example) => examplePrompt.format(eg))\n);\nreturn stringExamples.map((eg: string) => this.getTextLength(eg));\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/LengthBasedExampleSelector.ts","loc":{"lines":{"from":82,"to":118}}}}],["800",{"pageContent":"async selectExamples(inputVariables: Example): Promise<Example[]> {\nconst inputs = Object.values(inputVariables).join(\" \");\nlet remainingLength = this.maxLength - this.getTextLength(inputs);\nlet i = 0;\nconst examples: Example[] = [];\n\nwhile (remainingLength > 0 && i < this.examples.length) {\nconst newLength = remainingLength - this.exampleTextLengths[i];\nif (newLength < 0) {\nbreak;\n} else {\nexamples.push(this.examples[i]);\nremainingLength = newLength;\n}\ni += 1;\n}\n\nreturn examples;\n}\n\nstatic async fromExamples(\nexamples: Example[],\nargs: LengthBasedExampleSelectorInput\n) {\nconst selector = new LengthBasedExampleSelector(args);\nawait Promise.all(examples.map((eg) => selector.addExample(eg)));\nreturn selector;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/LengthBasedExampleSelector.ts","loc":{"lines":{"from":165,"to":193}}}}],["801",{"pageContent":"import { Embeddings } from \"../../embeddings/base.js\";\nimport { VectorStore } from \"../../vectorstores/base.js\";\nimport { Document } from \"../../document.js\";\nimport { Example } from \"../../schema/index.js\";\nimport type { BaseExampleSelector } from \"../base.js\";\n\nfunction sortedValues<T>(values: Record<string, T>): T[] {\nreturn Object.keys(values)\n.sort()\n.map((key) => values[key]);\n}\n\nexport interface SemanticSimilarityExampleSelectorInput {\nvectorStore: VectorStore;\nk?: number;\nexampleKeys?: string[];\ninputKeys?: string[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/SemanticSimilarityExampleSelector.ts","loc":{"lines":{"from":1,"to":18}}}}],["802",{"pageContent":"class SemanticSimilarityExampleSelector implements BaseExampleSelector {\nvectorStore: VectorStore;\n\nk = 4;\n\nexampleKeys?: string[];\n\ninputKeys?: string[];\n\nconstructor(data: SemanticSimilarityExampleSelectorInput) {\nthis.vectorStore = data.vectorStore;\nthis.k = data.k ?? 4;\nthis.exampleKeys = data.exampleKeys;\nthis.inputKeys = data.inputKeys;\n}\n\nasync addExample(example: Example): Promise<void> {\nconst inputKeys = this.inputKeys ?? Object.keys(example);\nconst stringExample = sortedValues(\ninputKeys.reduce(\n(acc, key) => ({ ...acc, [key]: example[key] }),\n{} as Example\n)\n).join(\" \");\n\nawait this.vectorStore.addDocuments([\nnew Document({\npageContent: stringExample,\nmetadata: { example },\n}),\n]);\n}\n\nasync selectExamples<T>(\ninputVariables: Record<string, T>\n): Promise<Example[]> {\nconst inputKeys = this.inputKeys ?? Object.keys(inputVariables);\nconst query = sortedValues(\ninputKeys.reduce(\n(acc, key) => ({ ...acc, [key]: inputVariables[key] }),\n{} as Record<string, T>\n)\n).join(\" \");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/SemanticSimilarityExampleSelector.ts","loc":{"lines":{"from":115,"to":157}}}}],["803",{"pageContent":"const exampleDocs = await this.vectorStore.similaritySearch(query, this.k);\n\nconst examples = exampleDocs.map((doc) => doc.metadata);\nif (this.exampleKeys) {\n// If example keys are provided, filter examples to those keys.\nreturn examples.map((example) =>\n(this.exampleKeys as string[]).reduce(\n(acc, key) => ({ ...acc, [key]: example[key] }),\n{}\n)\n);\n}\nreturn examples;\n}\n\nstatic async fromExamples<C extends typeof VectorStore>(\nexamples: Record<string, string>[],\nembeddings: Embeddings,\nvectorStoreCls: C,\noptions: {\nk?: number;\ninputKeys?: string[];\n} & Parameters<C[\"fromTexts\"]>[3] = {}\n): Promise<SemanticSimilarityExampleSelector> {\nconst inputKeys = options.inputKeys ?? null;\nconst stringExamples = examples.map((example) =>\nsortedValues(\ninputKeys\n? inputKeys.reduce(\n(acc, key) => ({ ...acc, [key]: example[key] }),\n{} as Record<string, string>\n)\n: example\n).join(\" \")\n);\n\nconst vectorStore = await vectorStoreCls.fromTexts(\nstringExamples,\nexamples, // metadatas\nembeddings,\noptions\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/SemanticSimilarityExampleSelector.ts","loc":{"lines":{"from":234,"to":275}}}}],["804",{"pageContent":"return new SemanticSimilarityExampleSelector({\nvectorStore,\nk: options.k ?? 4,\nexampleKeys: options.exampleKeys,\ninputKeys: options.inputKeys,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/SemanticSimilarityExampleSelector.ts","loc":{"lines":{"from":352,"to":359}}}}],["805",{"pageContent":"import type { Example } from \"../schema/index.js\";\nimport type { TemplateFormat } from \"./template.js\";\n\nexport type SerializedPromptTemplate = {\n_type?: \"prompt\";\ninput_variables: string[];\ntemplate_format?: TemplateFormat;\ntemplate?: string;\n};\n\nexport type SerializedFewShotTemplate = {\n_type: \"few_shot\";\ninput_variables: string[];\nexamples: string | Example[];\nexample_prompt?: SerializedPromptTemplate;\nexample_separator: string;\nprefix?: string;\nsuffix?: string;\ntemplate_format: TemplateFormat;\n};\n\nexport type SerializedMessagePromptTemplate = {\n_type: \"message\";\ninput_variables: string[];\n[key: string]: unknown;\n};\n\n/** Serialized Chat prompt template */\nexport type SerializedChatPromptTemplate = {\n_type?: \"chat_prompt\";\ninput_variables: string[];\ntemplate_format?: TemplateFormat;\nprompt_messages: SerializedMessagePromptTemplate[];\n};\n\nexport type SerializedBasePromptTemplate =\n| SerializedFewShotTemplate\n| SerializedPromptTemplate\n| SerializedChatPromptTemplate;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/serde.ts","loc":{"lines":{"from":1,"to":39}}}}],["806",{"pageContent":"import { InputValues } from \"../schema/index.js\";\n\nexport type TemplateFormat = \"f-string\" | \"jinja2\";\n\ntype ParsedFStringNode =\n| { type: \"literal\"; text: string }\n| { type: \"variable\"; name: string };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/template.ts","loc":{"lines":{"from":1,"to":7}}}}],["807",{"pageContent":"const parseFString = (template: string): ParsedFStringNode[] => {\n// Core logic replicated from internals of pythons built in Formatter class.\n// https://github.com/python/cpython/blob/135ec7cefbaffd516b77362ad2b2ad1025af462e/Objects/stringlib/unicode_format.h#L700-L706\nconst chars = template.split(\"\");\nconst nodes: ParsedFStringNode[] = [];\n\nconst nextBracket = (bracket: \"}\" | \"{\" | \"{}\", start: number) => {\nfor (let i = start; i < chars.length; i += 1) {\nif (bracket.includes(chars[i])) {\nreturn i;\n}\n}\nreturn -1;\n};\n\nlet i = 0;\nwhile (i < chars.length) {\nif (chars[i] === \"{\" && i + 1 < chars.length && chars[i + 1] === \"{\") {\nnodes.push({ type: \"literal\", text: \"{\" });\ni += 2;\n} else if (\nchars[i] === \"}\" &&\ni + 1 < chars.length &&\nchars[i + 1] === \"}\"\n) {\nnodes.push({ type: \"literal\", text: \"}\" });\ni += 2;\n} else if (chars[i] === \"{\") {\nconst j = nextBracket(\"}\", i);\nif (j < 0) {\nthrow new Error(\"Unclosed '{' in template.\");\n}\n\nnodes.push({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/template.ts","loc":{"lines":{"from":115,"to":148}}}}],["808",{"pageContent":": \"variable\",\nname: chars.slice(i + 1, j).join(\"\"),\n});\ni = j + 1;\n} else if (chars[i] === \"}\") {\nthrow new Error(\"Single '}' in template.\");\n} else {\nconst next = nextBracket(\"{}\", i);\nconst text = (next < 0 ? chars.slice(i) : chars.slice(i, next)).join(\"\");\nnodes.push({ type: \"literal\", text });\ni = next < 0 ? chars.length : next;\n}\n}\nreturn nodes;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/template.ts","loc":{"lines":{"from":235,"to":249}}}}],["809",{"pageContent":"const interpolateFString = (template: string, values: InputValues) =>\nparseFString(template).reduce((res, node) => {\nif (node.type === \"variable\") {\nif (node.name in values) {\nreturn res + values[node.name];\n}\nthrow new Error(`Missing value for input ${node.name}`);\n}\n\nreturn res + node.text;\n}, \"\");\n\ntype Interpolator = (template: string, values: InputValues) => string;\n\ntype Parser = (template: string) => ParsedFStringNode[];\n\nexport const DEFAULT_FORMATTER_MAPPING: Record<TemplateFormat, Interpolator> = {\n\"f-string\": interpolateFString,\njinja2: (_: string, __: InputValues) => \"\",\n};\n\nexport const DEFAULT_PARSER_MAPPING: Record<TemplateFormat, Parser> = {\n\"f-string\": parseFString,\njinja2: (_: string) => [],\n};\n\nexport const renderTemplate = (\ntemplate: string,\ntemplateFormat: TemplateFormat,\ninputValues: InputValues\n) => DEFAULT_FORMATTER_MAPPING[templateFormat](template, inputValues);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/template.ts","loc":{"lines":{"from":354,"to":384}}}}],["810",{"pageContent":"const parseTemplate = (\ntemplate: string,\ntemplateFormat: TemplateFormat\n) => DEFAULT_PARSER_MAPPING[templateFormat](template);\n\nexport const checkValidTemplate = (\ntemplate: string,\ntemplateFormat: TemplateFormat,\ninputVariables: string[]\n) => {\nif (!(templateFormat in DEFAULT_FORMATTER_MAPPING)) {\nconst validFormats = Object.keys(DEFAULT_FORMATTER_MAPPING);\nthrow new Error(`Invalid template format. Got \\`${templateFormat}\\`;\nshould be one of ${validFormats}`);\n}\ntry {\nconst dummyInputs: InputValues = inputVariables.reduce((acc, v) => {\nacc[v] = \"foo\";\nreturn acc;\n}, {} as Record<string, string>);\nrenderTemplate(template, templateFormat, dummyInputs);\n} catch {\nthrow new Error(\"Invalid prompt schema.\");\n}\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/template.ts","loc":{"lines":{"from":473,"to":497}}}}],["811",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport {\nAIMessagePromptTemplate,\nChatPromptTemplate,\nChatMessagePromptTemplate,\nHumanMessagePromptTemplate,\nSystemMessagePromptTemplate,\nMessagesPlaceholder,\n} from \"../chat.js\";\nimport { PromptTemplate } from \"../prompt.js\";\nimport {\nAIChatMessage,\nChatMessage,\nHumanChatMessage,\nSystemChatMessage,\n} from \"../../schema/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["812",{"pageContent":"createChatPromptTemplate(): ChatPromptTemplate {\nconst systemPrompt = new PromptTemplate({\ntemplate: \"Here's some context: {context}\",\ninputVariables: [\"context\"],\n});\nconst userPrompt = new PromptTemplate({\ntemplate: \"Hello {foo}, I'm {bar}. Thanks for the {context}\",\ninputVariables: [\"foo\", \"bar\", \"context\"],\n});\nconst aiPrompt = new PromptTemplate({\ntemplate: \"I'm an AI. I'm {foo}. I'm {bar}.\",\ninputVariables: [\"foo\", \"bar\"],\n});\nconst genericPrompt = new PromptTemplate({\ntemplate: \"I'm a generic message. I'm {foo}. I'm {bar}.\",\ninputVariables: [\"foo\", \"bar\"],\n});\nreturn new ChatPromptTemplate({\npromptMessages: [\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\nnew AIMessagePromptTemplate(aiPrompt),\nnew ChatMessagePromptTemplate(genericPrompt, \"test\"),\n],\ninputVariables: [\"context\", \"foo\", \"bar\"],\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":228,"to":254}}}}],["813",{"pageContent":"test(\"Test format\", async () => {\nconst chatPrompt = createChatPromptTemplate();\nconst messages = await chatPrompt.formatPromptValue({\ncontext: \"This is a context\",\nfoo: \"Foo\",\nbar: \"Bar\",\n});\nexpect(messages.toChatMessages()).toEqual([\nnew SystemChatMessage(\"Here's some context: This is a context\"),\nnew HumanChatMessage(\n\"Hello Foo, I'm Bar. Thanks for the This is a context\"\n),\nnew AIChatMessage(\"I'm an AI. I'm Foo. I'm Bar.\"),\nnew ChatMessage(\"I'm a generic message. I'm Foo. I'm Bar.\", \"test\"),\n]);\n});\n\ntest(\"Test serialize\", async () => {\nconst chatPrompt = createChatPromptTemplate();\nexpect(chatPrompt.serialize()).toMatchSnapshot();\n});\n\ntest(\"Test format with invalid input values\", async () => {\nconst chatPrompt = createChatPromptTemplate();\nawait expect(\nchatPrompt.formatPromptValue({\ncontext: \"This is a context\",\nfoo: \"Foo\",\n})\n).rejects.toThrow(\"Missing value for input variable `bar`\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":453,"to":483}}}}],["814",{"pageContent":"test(\"Test format with invalid input variables\", async () => {\nconst systemPrompt = new PromptTemplate({\ntemplate: \"Here's some context: {context}\",\ninputVariables: [\"context\"],\n});\nconst userPrompt = new PromptTemplate({\ntemplate: \"Hello {foo}, I'm {bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nexpect(\n() =>\nnew ChatPromptTemplate({\npromptMessages: [\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\n],\ninputVariables: [\"context\", \"foo\", \"bar\", \"baz\"],\n})\n).toThrow(\n\"Input variables `baz` are not used in any of the prompt messages.\"\n);\n\nexpect(\n() =>\nnew ChatPromptTemplate({\npromptMessages: [\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\n],\ninputVariables: [\"context\", \"foo\"],\n})\n).toThrow(\n\"Input variables `bar` are used in prompt messages but not in the prompt template.\"\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":680,"to":714}}}}],["815",{"pageContent":"test(\"Test fromPromptMessages\", async () => {\nconst systemPrompt = new PromptTemplate({\ntemplate: \"Here's some context: {context}\",\ninputVariables: [\"context\"],\n});\nconst userPrompt = new PromptTemplate({\ntemplate: \"Hello {foo}, I'm {bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\n]);\nexpect(chatPrompt.inputVariables).toEqual([\"context\", \"foo\", \"bar\"]);\nconst messages = await chatPrompt.formatPromptValue({\ncontext: \"This is a context\",\nfoo: \"Foo\",\nbar: \"Bar\",\n});\nexpect(messages.toChatMessages()).toEqual([\nnew SystemChatMessage(\"Here's some context: This is a context\"),\nnew HumanChatMessage(\"Hello Foo, I'm Bar\"),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":913,"to":936}}}}],["816",{"pageContent":"test(\"Test fromPromptMessages is composable\", async () => {\nconst systemPrompt = new PromptTemplate({\ntemplate: \"Here's some context: {context}\",\ninputVariables: [\"context\"],\n});\nconst userPrompt = new PromptTemplate({\ntemplate: \"Hello {foo}, I'm {bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nconst chatPromptInner = ChatPromptTemplate.fromPromptMessages([\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\n]);\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nchatPromptInner,\nAIMessagePromptTemplate.fromTemplate(\"I'm an AI. I'm {foo}. I'm {bar}.\"),\n]);\nexpect(chatPrompt.inputVariables).toEqual([\"context\", \"foo\", \"bar\"]);\nconst messages = await chatPrompt.formatPromptValue({\ncontext: \"This is a context\",\nfoo: \"Foo\",\nbar: \"Bar\",\n});\nexpect(messages.toChatMessages()).toEqual([\nnew SystemChatMessage(\"Here's some context: This is a context\"),\nnew HumanChatMessage(\"Hello Foo, I'm Bar\"),\nnew AIChatMessage(\"I'm an AI. I'm Foo. I'm Bar.\"),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":1137,"to":1165}}}}],["817",{"pageContent":"test(\"Test fromPromptMessages is composable with partial vars\", async () => {\nconst systemPrompt = new PromptTemplate({\ntemplate: \"Here's some context: {context}\",\ninputVariables: [\"context\"],\n});\nconst userPrompt = new PromptTemplate({\ntemplate: \"Hello {foo}, I'm {bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nconst chatPromptInner = ChatPromptTemplate.fromPromptMessages([\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\n]);\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nawait chatPromptInner.partial({\ncontext: \"This is a context\",\nfoo: \"Foo\",\n}),\nAIMessagePromptTemplate.fromTemplate(\"I'm an AI. I'm {foo}. I'm {bar}.\"),\n]);\nexpect(chatPrompt.inputVariables).toEqual([\"bar\"]);\nconst messages = await chatPrompt.formatPromptValue({\nbar: \"Bar\",\n});\nexpect(messages.toChatMessages()).toEqual([\nnew SystemChatMessage(\"Here's some context: This is a context\"),\nnew HumanChatMessage(\"Hello Foo, I'm Bar\"),\nnew AIChatMessage(\"I'm an AI. I'm Foo. I'm Bar.\"),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":1360,"to":1389}}}}],["818",{"pageContent":"test(\"Test SimpleMessagePromptTemplate\", async () => {\nconst prompt = new MessagesPlaceholder(\"foo\");\nconst values = { foo: [new HumanChatMessage(\"Hello Foo, I'm Bar\")] };\nconst messages = await prompt.formatMessages(values);\nexpect(messages).toEqual([new HumanChatMessage(\"Hello Foo, I'm Bar\")]);\n});\n\ntest(\"Test using partial\", async () => {\nconst userPrompt = new PromptTemplate({\ntemplate: \"{foo}{bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst prompt = new ChatPromptTemplate({\npromptMessages: [new HumanMessagePromptTemplate(userPrompt)],\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst partialPrompt = await prompt.partial({ foo: \"foo\" });\n\n// original prompt is not modified\nexpect(prompt.inputVariables).toEqual([\"foo\", \"bar\"]);\n// partial prompt has only remaining variables\nexpect(partialPrompt.inputVariables).toEqual([\"bar\"]);\n\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\n'[{\"text\":\"foobaz\"}]'\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":1584,"to":1612}}}}],["819",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { FewShotPromptTemplate } from \"../few_shot.js\";\nimport { LengthBasedExampleSelector } from \"../index.js\";\nimport { PromptTemplate } from \"../prompt.js\";\n\ntest(\"Test using partial\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"{foo}{bar}\");\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexamples: [],\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [\"foo\"],\npartialVariables: { bar: \"baz\" },\n});\nexpect(await prompt.format({ foo: \"foo\" })).toBe(\"foobaz\\n\");\n});\n\ntest(\"Test using full partial\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"{foo}{bar}\");\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexamples: [],\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [],\npartialVariables: { bar: \"baz\", foo: \"boo\" },\n});\nexpect(await prompt.format({})).toBe(\"boobaz\\n\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/few_shot.test.ts","loc":{"lines":{"from":1,"to":34}}}}],["820",{"pageContent":"test(\"Test partial with string\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"{foo}{bar}\");\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexamples: [],\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst partialPrompt = await prompt.partial({ foo: \"foo\" });\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\"foobaz\\n\");\nexpect(prompt.inputVariables).toEqual([\"foo\", \"bar\"]);\n});\n\ntest(\"Test partial with function\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"{foo}{bar}\");\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexamples: [],\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst partialPrompt = await prompt.partial({\nfoo: () => Promise.resolve(\"boo\"),\n});\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\"boobaz\\n\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/few_shot.test.ts","loc":{"lines":{"from":122,"to":155}}}}],["821",{"pageContent":"test(\"Test partial with function and examples\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"An example about {x}\");\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexamples: [{ x: \"foo\" }, { x: \"bar\" }],\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst partialPrompt = await prompt.partial({\nfoo: () => Promise.resolve(\"boo\"),\n});\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\n`boobaz\nAn example about foo\nAn example about bar\n`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/few_shot.test.ts","loc":{"lines":{"from":244,"to":265}}}}],["822",{"pageContent":"test(\"Test partial with function and example selector\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"An example about {x}\");\nconst exampleSelector = await LengthBasedExampleSelector.fromExamples(\n[{ x: \"foo\" }, { x: \"bar\" }],\n{ examplePrompt, maxLength: 200 }\n);\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexampleSelector,\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst partialPrompt = await prompt.partial({\nfoo: () => Promise.resolve(\"boo\"),\n});\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\n`boobaz\nAn example about foo\nAn example about bar\n`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/few_shot.test.ts","loc":{"lines":{"from":368,"to":393}}}}],["823",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport * as path from \"node:path\";\nimport { fileURLToPath } from \"node:url\";\nimport { loadPrompt } from \"../load.js\";\n\ntest(\"Load Hello World Prompt\", async () => {\nconst helloWorld = path.join(\npath.join(path.dirname(fileURLToPath(import.meta.url)), \"prompts\"),\n\"hello_world.yaml\"\n);\nconst prompt = await loadPrompt(helloWorld);\nexpect(prompt._getPromptType()).toBe(\"prompt\");\nexpect(await prompt.format({})).toBe(\"Say hello world.\");\n});\n\ntest(\"Load hub prompt\", async () => {\nconst prompt = await loadPrompt(\n\"lc@abb92d8://prompts/hello-world/prompt.yaml\"\n);\nexpect(prompt._getPromptType()).toBe(\"prompt\");\nexpect(await prompt.format({})).toBe(\"Say hello world.\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/load.int.test.ts","loc":{"lines":{"from":1,"to":22}}}}],["824",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { PromptTemplate } from \"../prompt.js\";\n\ntest(\"Test using partial\", async () => {\nconst prompt = new PromptTemplate({\ntemplate: \"{foo}{bar}\",\ninputVariables: [\"foo\"],\npartialVariables: { bar: \"baz\" },\n});\nexpect(await prompt.format({ foo: \"foo\" })).toBe(\"foobaz\");\n});\n\ntest(\"Test using full partial\", async () => {\nconst prompt = new PromptTemplate({\ntemplate: \"{foo}{bar}\",\ninputVariables: [],\npartialVariables: { bar: \"baz\", foo: \"boo\" },\n});\nexpect(await prompt.format({})).toBe(\"boobaz\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/prompt.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["825",{"pageContent":"test(\"Test partial\", async () => {\nconst prompt = new PromptTemplate({\ntemplate: \"{foo}{bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nexpect(prompt.inputVariables).toEqual([\"foo\", \"bar\"]);\nconst partialPrompt = await prompt.partial({ foo: \"foo\" });\n// original prompt is not modified\nexpect(prompt.inputVariables).toEqual([\"foo\", \"bar\"]);\n// partial prompt has only remaining variables\nexpect(partialPrompt.inputVariables).toEqual([\"bar\"]);\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\"foobaz\");\n});\n\ntest(\"Test partial with function\", async () => {\nconst prompt = new PromptTemplate({\ntemplate: \"{foo}{bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nconst partialPrompt = await prompt.partial({\nfoo: () => Promise.resolve(\"boo\"),\n});\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\"boobaz\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/prompt.test.ts","loc":{"lines":{"from":46,"to":69}}}}],["826",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";\nimport { LengthBasedExampleSelector } from \"../selectors/LengthBasedExampleSelector.js\";\nimport { SemanticSimilarityExampleSelector } from \"../selectors/SemanticSimilarityExampleSelector.js\";\nimport { HNSWLib } from \"../../vectorstores/hnswlib.js\";\nimport { PromptTemplate } from \"../prompt.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/selectors.test.ts","loc":{"lines":{"from":1,"to":6}}}}],["827",{"pageContent":"test(\"Test using LengthBasedExampleSelector\", async () => {\nconst prompt = new PromptTemplate({\ntemplate: \"{foo} {bar}\",\ninputVariables: [\"foo\"],\npartialVariables: { bar: \"baz\" },\n});\nconst selector = await LengthBasedExampleSelector.fromExamples(\n[{ foo: \"one one one\" }],\n{\nexamplePrompt: prompt,\nmaxLength: 10,\n}\n);\nawait selector.addExample({ foo: \"one two three\" });\nawait selector.addExample({ foo: \"four five six\" });\nawait selector.addExample({ foo: \"seven eight nine\" });\nawait selector.addExample({ foo: \"ten eleven twelve\" });\nconst chosen = await selector.selectExamples({ foo: \"hello\", bar: \"world\" });\nexpect(chosen).toStrictEqual([\n{ foo: \"one one one\" },\n{ foo: \"one two three\" },\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/selectors.test.ts","loc":{"lines":{"from":43,"to":65}}}}],["828",{"pageContent":"test(\"Test using SemanticSimilarityExampleSelector\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew FakeEmbeddings() // not using  OpenAIEmbeddings() because would be extra dependency\n);\nconst selector = new SemanticSimilarityExampleSelector({\nvectorStore,\n});\nconst chosen = await selector.selectExamples({ id: 1 });\nexpect(chosen).toEqual([{ id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/selectors.test.ts","loc":{"lines":{"from":92,"to":103}}}}],["829",{"pageContent":"import { expect, test, describe } from \"@jest/globals\";\nimport { interpolateFString } from \"../template.js\";\n\ndescribe.each([\n[\"{foo}\", { foo: \"bar\" }, \"bar\"],\n[\"pre{foo}post\", { foo: \"bar\" }, \"prebarpost\"],\n[\"{{pre{foo}post}}\", { foo: \"bar\" }, \"{prebarpost}\"],\n[\"text\", {}, \"text\"],\n[\"}}{{\", {}, \"}{\"],\n[\"{first}_{second}\", { first: \"foo\", second: \"bar\" }, \"foo_bar\"],\n])(\"Valid f-string\", (template, variables, result) => {\ntest(`Interpolation works: ${template}`, () => {\nexpect(interpolateFString(template, variables)).toBe(result);\n});\n});\n\ndescribe.each([\n[\"{\", {}],\n[\"}\", {}],\n[\"{foo\", {}],\n[\"foo}\", {}],\n])(\"Invalid f-string\", (template, variables) => {\ntest(`Interpolation throws: ${template}`, () => {\nexpect(() => interpolateFString(template, variables)).toThrow();\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/template.test.ts","loc":{"lines":{"from":1,"to":26}}}}],["830",{"pageContent":"import { BaseDocumentCompressor } from \"./document_compressors/index.js\";\nimport { Document } from \"../document.js\";\nimport { BaseRetriever } from \"../schema/index.js\";\n\nexport interface ContextualCompressionRetrieverArgs {\nbaseCompressor: BaseDocumentCompressor;\nbaseRetriever: BaseRetriever;\n}\n\nexport class ContextualCompressionRetriever extends BaseRetriever {\nbaseCompressor: BaseDocumentCompressor;\n\nbaseRetriever: BaseRetriever;\n\nconstructor({\nbaseCompressor,\nbaseRetriever,\n}: ContextualCompressionRetrieverArgs) {\nsuper();\n\nthis.baseCompressor = baseCompressor;\nthis.baseRetriever = baseRetriever;\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nconst docs = await this.baseRetriever.getRelevantDocuments(query);\nconst compressedDocs = await this.baseCompressor.compressDocuments(\ndocs,\nquery\n);\nreturn compressedDocs;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/contextual_compression.ts","loc":{"lines":{"from":1,"to":33}}}}],["831",{"pageContent":"import { BaseRetriever } from \"../schema/index.js\";\nimport { Document } from \"../document.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../util/async_caller.js\";\n\nexport interface DataberryRetrieverArgs extends AsyncCallerParams {\ndatastoreUrl: string;\ntopK?: number;\napiKey?: string;\n}\n\ninterface Berry {\ntext: string;\nscore: number;\nsource?: string;\n[key: string]: unknown;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/databerry.ts","loc":{"lines":{"from":1,"to":16}}}}],["832",{"pageContent":"class DataberryRetriever extends BaseRetriever {\ncaller: AsyncCaller;\n\ndatastoreUrl: string;\n\ntopK?: number;\n\napiKey?: string;\n\nconstructor({ datastoreUrl, apiKey, topK, ...rest }: DataberryRetrieverArgs) {\nsuper();\n\nthis.caller = new AsyncCaller(rest);\nthis.datastoreUrl = datastoreUrl;\nthis.apiKey = apiKey;\nthis.topK = topK;\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nconst r = await this.caller.call(fetch, this.datastoreUrl, {\nmethod: \"POST\",\nbody: JSON.stringify({\nquery,\n...(this.topK ? { topK: this.topK } : {}),\n}),\nheaders: {\n\"Content-Type\": \"application/json\",\n...(this.apiKey ? { Authorization: `Bearer ${this.apiKey}` } : {}),\n},\n});\n\nconst { results } = (await r.json()) as { results: Berry[] };\n\nreturn results.map(\n({ text, score, source, ...rest }) =>\nnew Document({\npageContent: text,\nmetadata: {\nscore,\nsource,\n...rest,\n},\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/databerry.ts","loc":{"lines":{"from":64,"to":109}}}}],["833",{"pageContent":"import { Document } from \"../../document.js\";\nimport { LLMChain } from \"../../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { BaseOutputParser } from \"../../schema/output_parser.js\";\nimport { BaseDocumentCompressor } from \"./index.js\";\nimport { PROMPT_TEMPLATE } from \"./chain_extract_prompt.js\";\n\nfunction defaultGetInput(\nquery: string,\ndoc: Document\n): Record<string, unknown> {\nreturn { question: query, context: doc.pageContent };\n}\n\nclass NoOutputParser extends BaseOutputParser<string> {\nnoOutputStr = \"NO_OUTPUT\";\n\nparse(text: string): Promise<string> {\nconst cleanedText = text.trim();\nif (cleanedText === this.noOutputStr) {\nreturn Promise.resolve(\"\");\n}\nreturn Promise.resolve(cleanedText);\n}\n\ngetFormatInstructions(): string {\nthrow new Error(\"Method not implemented.\");\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/document_compressors/chain_extract.ts","loc":{"lines":{"from":1,"to":30}}}}],["834",{"pageContent":"getDefaultChainPrompt(): PromptTemplate {\nconst outputParser = new NoOutputParser();\nconst template = PROMPT_TEMPLATE(outputParser.noOutputStr);\nreturn new PromptTemplate({\ntemplate,\ninputVariables: [\"question\", \"context\"],\noutputParser,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/document_compressors/chain_extract.ts","loc":{"lines":{"from":92,"to":100}}}}],["835",{"pageContent":"interface LLMChainExtractorArgs {\nllmChain: LLMChain;\ngetInput: (query: string, doc: Document) => Record<string, unknown>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/document_compressors/chain_extract.ts","loc":{"lines":{"from":185,"to":188}}}}],["836",{"pageContent":"class LLMChainExtractor extends BaseDocumentCompressor {\nllmChain: LLMChain;\n\ngetInput: (query: string, doc: Document) => Record<string, unknown> =","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/document_compressors/chain_extract.ts","loc":{"lines":{"from":275,"to":278}}}}],["837",{"pageContent":"GetInput;\n\nconstructor({ llmChain, getInput }: LLMChainExtractorArgs) {\nsuper();\nthis.llmChain = llmChain;\nthis.getInput = getInput;\n}\n\nasync compressDocuments(\ndocuments: Document[],\nquery: string\n): Promise<Document[]> {\nconst compressedDocs: Document[] = [];\nfor (const doc of documents) {\nconst input = this.getInput(query, doc);\nconst output = await this.llmChain.predict(input);\nif (output.length === 0) {\ncontinue;\n}\ncompressedDocs.push(\nnew Document({\npageContent: output,\nmetadata: doc.metadata,\n})\n);\n}\nreturn compressedDocs;\n}\n\nstatic fromLLM(\nllm: BaseLanguageModel,\nprompt?: PromptTemplate,\ngetInput?: (query: string, doc: Document) => Record<string, unknown>\n): LLMChainExtractor {\nconst _prompt = prompt || getDefaultChainPrompt();\nconst _getInput = getInput || defaultGetInput;\nconst llmChain = new LLMChain({ llm, prompt: _prompt });\nreturn new LLMChainExtractor({ llmChain, getInput: _getInput });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/document_compressors/chain_extract.ts","loc":{"lines":{"from":365,"to":404}}}}],["838",{"pageContent":"export const PROMPT_TEMPLATE = (\nnoOutputStr: string\n) => `Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return ${noOutputStr}.\n\nRemember, *DO NOT* edit the extracted parts of the context.\n\n> Question: {question}\n> Context:\n>>>\n{context}\n>>>\nExtracted relevant parts:`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/document_compressors/chain_extract_prompt.ts","loc":{"lines":{"from":1,"to":12}}}}],["839",{"pageContent":"import { Document } from \"../../document.js\";\n\n/**\n* Base Document Compression class. All compressors should extend this class.\n*/\nexport abstract class BaseDocumentCompressor {\nabstract compressDocuments(\ndocuments: Document[],\nquery: string\n): Promise<Document[]>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/document_compressors/index.ts","loc":{"lines":{"from":1,"to":11}}}}],["840",{"pageContent":"import { Document } from \"../document.js\";\nimport { BasePromptTemplate, StringPromptValue } from \"../prompts/base.js\";\nimport { PromptTemplate } from \"../prompts/prompt.js\";\nimport {\nVectorStore,\nVectorStoreRetriever,\nVectorStoreRetrieverInput,\n} from \"../vectorstores/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { BasePromptValue } from \"../schema/index.js\";\n\nexport type PromptKey =\n| \"websearch\"\n| \"scifact\"\n| \"arguana\"\n| \"trec-covid\"\n| \"fiqa\"\n| \"dbpedia-entity\"\n| \"trec-news\"\n| \"mr-tydi\";\n\nexport interface HydeRetrieverOptions<V extends VectorStore>\nextends VectorStoreRetrieverInput<V> {\nllm: BaseLanguageModel;\npromptTemplate?: BasePromptTemplate | PromptKey;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":1,"to":26}}}}],["841",{"pageContent":"class HydeRetriever<\nV extends VectorStore = VectorStore\n> extends VectorStoreRetriever<V> {\nllm: BaseLanguageModel;\n\npromptTemplate?: BasePromptTemplate;\n\nconstructor(fields: HydeRetrieverOptions<V>) {\nsuper(fields);\nthis.llm = fields.llm;\nthis.promptTemplate =","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":125,"to":135}}}}],["842",{"pageContent":"of fields.promptTemplate === \"string\"\n? getPromptTemplateFromKey(fields.promptTemplate)\n: fields.promptTemplate;\nif (this.promptTemplate) {\nconst { inputVariables } = this.promptTemplate;\nif (inputVariables.length !== 1 && inputVariables[0] !== \"question\") {\nthrow new Error(\n`Prompt template must accept a single input variable 'question'. Invalid input variables for prompt template: ${inputVariables}`\n);\n}\n}\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nlet value: BasePromptValue = new StringPromptValue(query);\n\n// Use a custom template if provided\nif (this.promptTemplate) {\nvalue = await this.promptTemplate.formatPromptValue({ question: query });\n}\n\n// Get a hypothetical answer from the LLM\nconst res = await this.llm.generatePrompt([value]);\nconst answer = res.generations[0][0].text;\n\n// Retrieve relevant documents based on the hypothetical answer\nconst results = await this.vectorStore.similaritySearch(\nanswer,\nthis.k,\nthis.filter\n);\n\nreturn results;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":251,"to":285}}}}],["843",{"pageContent":"function getPromptTemplateFromKey(key: PromptKey): BasePromptTemplate {\nlet template: string;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":372,"to":373}}}}],["844",{"pageContent":"switch (key) {\ncase \"websearch\":\ntemplate = `Please write a passage to answer the question \nQuestion: {question}\nPassage:`;\nbreak;\ncase \"scifact\":\ntemplate = `Please write a scientific paper passage to support/refute the claim \nClaim: {question}\nPassage:`;\nbreak;\ncase \"arguana\":\ntemplate = `Please write a counter argument for the passage \nPassage: {question}\nCounter Argument:`;\nbreak;\ncase \"trec-covid\":\ntemplate = `Please write a scientific paper passage to answer the question\nQuestion: {question}\nPassage:`;\nbreak;\ncase \"fiqa\":\ntemplate = `Please write a financial article passage to answer the question\nQuestion: {question}\nPassage:`;\nbreak;\ncase \"dbpedia-entity\":\ntemplate = `Please write a passage to answer the question.\nQuestion: {question}\nPassage:`;\nbreak;\ncase \"trec-news\":\ntemplate = `Please write a news passage about the topic.\nTopic: {question}\nPassage:`;\nbreak;\ncase \"mr-tydi\":\ntemplate = `Please write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail.\nQuestion: {question}\nPassage:`;\nbreak;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":495,"to":535}}}}],["845",{"pageContent":":\nthrow new Error(`Invalid prompt key: ${key}`);\n}\n\nreturn PromptTemplate.fromTemplate(template);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":620,"to":625}}}}],["846",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/retrievers' is deprecated. Import from eg. 'langchain/retrievers/remote' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport { RemoteRetriever } from \"./remote/base.js\";\nexport { ChatGPTPluginRetriever } from \"./remote/chatgpt-plugin.js\";\nexport {\nSupabaseHybridSearch,\nSupabaseHybridSearchParams,\n} from \"./supabase.js\";\nexport { RemoteLangChainRetriever } from \"./remote/remote-retriever.js\";\nexport { MetalRetriever } from \"./metal.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/index.ts","loc":{"lines":{"from":1,"to":12}}}}],["847",{"pageContent":"import { BaseRetriever } from \"../schema/index.js\";\nimport { Document } from \"../document.js\";\n\nexport interface MetalRetrieverFields {\nclient: import(\"@getmetal/metal-sdk\").default;\n}\n\ninterface ResponseItem {\ntext: string;\n[key: string]: unknown;\n}\n\nexport class MetalRetriever extends BaseRetriever {\nprivate client: import(\"@getmetal/metal-sdk\").default;\n\nconstructor(fields: MetalRetrieverFields) {\nsuper();\n\nthis.client = fields.client;\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nconst res = await this.client.search({ text: query });\n\nconst items = (\"data\" in res ? res.data : res) as ResponseItem[];\nreturn items.map(\n({ text, metadata }) =>\nnew Document({\npageContent: text,\nmetadata: metadata as Record<string, unknown>,\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/metal.ts","loc":{"lines":{"from":1,"to":34}}}}],["848",{"pageContent":"import { BaseRetriever } from \"../../schema/index.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../../util/async_caller.js\";\nimport { Document } from \"../../document.js\";\n\nexport type RemoteRetrieverAuth = false | { bearer: string };\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type RemoteRetrieverValues = Record<string, any>;\n\nexport interface RemoteRetrieverParams extends AsyncCallerParams {\n/**\n* The URL of the remote retriever server\n*/\nurl: string;\n\n/**\n* The authentication method to use, currently implemented is\n* - false: no authentication\n* - { bearer: string }: Bearer token authentication\n*/\nauth: RemoteRetrieverAuth;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/base.ts","loc":{"lines":{"from":1,"to":21}}}}],["849",{"pageContent":"abstract class RemoteRetriever\nextends BaseRetriever\nimplements RemoteRetrieverParams\n{\nurl: string;\n\nauth: RemoteRetrieverAuth;\n\nheaders: Record<string, string>;\n\nasyncCaller: AsyncCaller;\n\nconstructor({ url, auth, ...rest }: RemoteRetrieverParams) {\nsuper();\nthis.url = url;\nthis.auth = auth;\nthis.headers = {\nAccept: \"application/json\",\n\"Content-Type\": \"application/json\",\n...(this.auth && this.auth.bearer\n? { Authorization: `Bearer ${this.auth.bearer}` }\n: {}),\n};\nthis.asyncCaller = new AsyncCaller(rest);\n}\n\nabstract createJsonBody(query: string): RemoteRetrieverValues;\n\nabstract processJsonResponse(json: RemoteRetrieverValues): Document[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/base.ts","loc":{"lines":{"from":71,"to":99}}}}],["850",{"pageContent":"async getRelevantDocuments(query: string): Promise<Document[]> {\nconst body = this.createJsonBody(query);\nconst response = await this.asyncCaller.call(() =>\nfetch(this.url, {\nmethod: \"POST\",\nheaders: this.headers,\nbody: JSON.stringify(body),\n})\n);\nif (!response.ok) {\nthrow new Error(\n`Failed to retrieve documents from ${this.url}: ${response.status} ${response.statusText}`\n);\n}\nconst json = await response.json();\nreturn this.processJsonResponse(json);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/base.ts","loc":{"lines":{"from":151,"to":168}}}}],["851",{"pageContent":"import { Document } from \"../../document.js\";\nimport {\nRemoteRetriever,\nRemoteRetrieverParams,\nRemoteRetrieverValues,\n} from \"./base.js\";\n\nexport interface ChatGPTPluginRetrieverFilter {\ndocument_id?: string;\nsource?: string;\nsource_id?: string;\nauthor?: string;\nstart_date?: string;\nend_date?: string;\n}\n\nexport interface ChatGPTPluginRetrieverParams extends RemoteRetrieverParams {\n/**\n* The number of results to request from the ChatGPTRetrievalPlugin server\n*/\ntopK?: number;\n\n/**\n* The filter to use when querying the ChatGPTRetrievalPlugin server\n*/\nfilter?: ChatGPTPluginRetrieverFilter;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/chatgpt-plugin.ts","loc":{"lines":{"from":1,"to":27}}}}],["852",{"pageContent":"class ChatGPTPluginRetriever\nextends RemoteRetriever\nimplements ChatGPTPluginRetrieverParams\n{\ntopK: number;\n\nfilter?: ChatGPTPluginRetrieverFilter;\n\nconstructor({ topK = 4, filter, ...rest }: ChatGPTPluginRetrieverParams) {\nsuper(rest);\nthis.topK = topK;\nthis.filter = filter;\n}\n\ncreateJsonBody(query: string): RemoteRetrieverValues {\nreturn {\nqueries: [\n{\nquery,\ntop_k: this.topK,\nfilter: this.filter,\n},\n],\n};\n}\n\nprocessJsonResponse(json: RemoteRetrieverValues): Document[] {\nconst results = json?.results?.[0]?.results;\n\nif (!results) {\n// Note an empty array of results would not fall into this case\nthrow new Error(\"No results returned from ChatGPTPluginRetriever\");\n}\n\nreturn results.map(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n(result: any) =>\nnew Document({\npageContent: result.text,\nmetadata: result.metadata,\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/chatgpt-plugin.ts","loc":{"lines":{"from":73,"to":116}}}}],["853",{"pageContent":"export {\nRemoteRetriever,\nRemoteRetrieverParams,\nRemoteRetrieverAuth,\nRemoteRetrieverValues,\n} from \"./base.js\";\nexport {\nChatGPTPluginRetriever,\nChatGPTPluginRetrieverFilter,\nChatGPTPluginRetrieverParams,\n} from \"./chatgpt-plugin.js\";\nexport {\nRemoteLangChainRetriever,\nRemoteLangChainRetrieverParams,\n} from \"./remote-retriever.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/index.ts","loc":{"lines":{"from":1,"to":15}}}}],["854",{"pageContent":"import { Document } from \"../../document.js\";\nimport {\nRemoteRetriever,\nRemoteRetrieverParams,\nRemoteRetrieverValues,\n} from \"./base.js\";\n\nexport interface RemoteLangChainRetrieverParams extends RemoteRetrieverParams {\n/**\n* The key in the JSON body to put the query in\n*/\ninputKey?: string;\n/**\n* The key in the JSON response to get the response from\n*/\nresponseKey?: string;\n/**\n* The key in the JSON response to get the page content from\n*/\npageContentKey?: string;\n/**\n* The key in the JSON response to get the metadata from\n*/\nmetadataKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/remote-retriever.ts","loc":{"lines":{"from":1,"to":25}}}}],["855",{"pageContent":"class RemoteLangChainRetriever\nextends RemoteRetriever\nimplements RemoteLangChainRetrieverParams\n{\ninputKey: string;\n\nresponseKey: string;\n\npageContentKey: string;\n\nmetadataKey: string;\n\nconstructor({\ninputKey = \"message\",\nresponseKey = \"response\",\npageContentKey = \"page_content\",\nmetadataKey = \"metadata\",\n...rest\n}: RemoteLangChainRetrieverParams) {\nsuper(rest);\nthis.inputKey = inputKey;\nthis.responseKey = responseKey;\nthis.pageContentKey = pageContentKey;\nthis.metadataKey = metadataKey;\n}\n\ncreateJsonBody(query: string): RemoteRetrieverValues {\nreturn {\n[this.inputKey]: query,\n};\n}\n\nprocessJsonResponse(json: RemoteRetrieverValues): Document[] {\nreturn json[this.responseKey].map(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n(r: any) =>\nnew Document({\npageContent: r[this.pageContentKey],\nmetadata: r[this.metadataKey],\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/remote-retriever.ts","loc":{"lines":{"from":72,"to":114}}}}],["856",{"pageContent":"import type { SupabaseClient } from \"@supabase/supabase-js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\nimport { BaseRetriever } from \"../schema/index.js\";\n\ninterface SearchEmbeddingsParams {\nquery_embedding: number[];\nmatch_count: number; // int\n}\n\ninterface SearchKeywordParams {\nquery_text: string;\nmatch_count: number; // int\n}\n\ninterface SearchResponseRow {\nid: number;\ncontent: string;\nmetadata: object;\nsimilarity: number;\n}\n\ntype SearchResult = [Document, number, number];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/supabase.ts","loc":{"lines":{"from":1,"to":23}}}}],["857",{"pageContent":"interface SupabaseLibArgs {\nclient: SupabaseClient;\n/**\n* The table name on Supabase. Defaults to \"documents\".\n*/\ntableName?: string;\n/**\n* The name of the Similarity search function on Supabase. Defaults to \"match_documents\".\n*/\nsimilarityQueryName?: string;\n/**\n* The name of the Keyword search function on Supabase. Defaults to \"kw_match_documents\".\n*/\nkeywordQueryName?: string;\n/**\n* The number of documents to return from the similarity search. Defaults to 2.\n*/\nsimilarityK?: number;\n/**\n* The number of documents to return from the keyword search. Defaults to 2.\n*/\nkeywordK?: number;\n}\n\nexport interface SupabaseHybridSearchParams {\nquery: string;\nsimilarityK: number;\nkeywordK: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/supabase.ts","loc":{"lines":{"from":183,"to":211}}}}],["858",{"pageContent":"class SupabaseHybridSearch extends BaseRetriever {\nsimilarityK: number;\n\nquery: string;\n\nkeywordK: number;\n\nsimilarityQueryName: string;\n\nclient: SupabaseClient;\n\ntableName: string;\n\nkeywordQueryName: string;\n\nembeddings: Embeddings;\n\nconstructor(embeddings: Embeddings, args: SupabaseLibArgs) {\nsuper();\nthis.embeddings = embeddings;\nthis.client = args.client;\nthis.tableName = args.tableName || \"documents\";\nthis.similarityQueryName = args.similarityQueryName || \"match_documents\";\nthis.keywordQueryName = args.keywordQueryName || \"kw_match_documents\";\nthis.similarityK = args.similarityK || 2;\nthis.keywordK = args.keywordK || 2;\n}\n\nprotected async similaritySearch(\nquery: string,\nk: number\n): Promise<SearchResult[]> {\nconst embeddedQuery = await this.embeddings.embedQuery(query);\n\nconst matchDocumentsParams: SearchEmbeddingsParams = {\nquery_embedding: embeddedQuery,\nmatch_count: k,\n};\n\nconst { data: searches, error } = await this.client.rpc(\nthis.similarityQueryName,\nmatchDocumentsParams\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/supabase.ts","loc":{"lines":{"from":364,"to":406}}}}],["859",{"pageContent":"if (error) {\nthrow new Error(\n`Error searching for documents: ${error.code} ${error.message} ${error.details}`\n);\n}\n\nreturn (searches as SearchResponseRow[]).map((resp) => [\nnew Document({\nmetadata: resp.metadata,\npageContent: resp.content,\n}),\nresp.similarity,\nresp.id,\n]);\n}\n\nprotected async keywordSearch(\nquery: string,\nk: number\n): Promise<SearchResult[]> {\nconst kwMatchDocumentsParams: SearchKeywordParams = {\nquery_text: query,\nmatch_count: k,\n};\n\nconst { data: searches, error } = await this.client.rpc(\nthis.keywordQueryName,\nkwMatchDocumentsParams\n);\n\nif (error) {\nthrow new Error(\n`Error searching for documents: ${error.code} ${error.message} ${error.details}`\n);\n}\n\nreturn (searches as SearchResponseRow[]).map((resp) => [\nnew Document({\nmetadata: resp.metadata,\npageContent: resp.content,\n}),\nresp.similarity * 10,\nresp.id,\n]);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/supabase.ts","loc":{"lines":{"from":549,"to":593}}}}],["860",{"pageContent":"protected async hybridSearch(\nquery: string,\nsimilarityK: number,\nkeywordK: number\n): Promise<SearchResult[]> {\nconst similarity_search = this.similaritySearch(query, similarityK);\n\nconst keyword_search = this.keywordSearch(query, keywordK);\n\nreturn Promise.all([similarity_search, keyword_search])\n.then((results) => results.flat())\n.then((results) => {\nconst picks = new Map<number, SearchResult>();\n\nresults.forEach((result) => {\nconst id = result[2];\nconst nextScore = result[1];\nconst prevScore = picks.get(id)?.[1];\n\nif (prevScore === undefined || nextScore > prevScore) {\npicks.set(id, result);\n}\n});\n\nreturn Array.from(picks.values());\n})\n.then((results) => results.sort((a, b) => b[1] - a[1]));\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nconst searchResults = await this.hybridSearch(\nquery,\nthis.similarityK,\nthis.keywordK\n);\n\nreturn searchResults.map(([doc]) => doc);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/supabase.ts","loc":{"lines":{"from":741,"to":779}}}}],["861",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { LLMChain } from \"../../chains/llm_chain.js\";\nimport { StuffDocumentsChain } from \"../../chains/combine_docs_chain.js\";\nimport { ConversationalRetrievalQAChain } from \"../../chains/conversational_retrieval_chain.js\";\nimport { HNSWLib } from \"../../vectorstores/hnswlib.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { ContextualCompressionRetriever } from \"../contextual_compression.js\";\nimport { LLMChainExtractor } from \"../document_compressors/chain_extract.js\";\n\ntest(\"Test LLMChainExtractor\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = PromptTemplate.fromTemplate(\n\"Print {question}, and ignore {chat_history}\"\n);\nconst baseCompressor = LLMChainExtractor.fromLLM(model);\nexpect(baseCompressor).toBeDefined();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/chain_extract.int.test.ts","loc":{"lines":{"from":1,"to":18}}}}],["862",{"pageContent":"const retriever = new ContextualCompressionRetriever({\nbaseCompressor,\nbaseRetriever: await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n).then((vectorStore) => vectorStore.asRetriever()),\n});\n\nconst llmChain = new LLMChain({ prompt, llm: model });\nconst combineDocsChain = new StuffDocumentsChain({\nllmChain,\ndocumentVariableName: \"foo\",\n});\nconst chain = new ConversationalRetrievalQAChain({\nretriever,\ncombineDocumentsChain: combineDocsChain,\nquestionGeneratorChain: llmChain,\n});\nconst res = await chain.call({ question: \"foo\", chat_history: \"bar\" });\n\nexpect(res.text.length).toBeGreaterThan(0);\n\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/chain_extract.int.test.ts","loc":{"lines":{"from":44,"to":68}}}}],["863",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { HydeRetriever } from \"../hyde.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Hyde retriever\", async () => {\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = new MemoryVectorStore(embeddings);\nconst llm = new OpenAI();\nconst retriever = new HydeRetriever({\nvectorStore,\nllm,\nk: 1,\n});\n\nawait vectorStore.addDocuments(\n[\n\"My name is John.\",\n\"My name is Bob.\",\n\"My favourite food is pizza.\",\n\"My favourite food is pasta.\",\n].map((pageContent) => new Document({ pageContent }))\n);\n\nconst results = await retriever.getRelevantDocuments(\n\"What is my favourite food?\"\n);\n\nexpect(results.length).toBe(1);\nconsole.log(results);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/hyde.int.test.ts","loc":{"lines":{"from":1,"to":33}}}}],["864",{"pageContent":"test(\"Hyde retriever with default prompt template\", async () => {\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = new MemoryVectorStore(embeddings);\nconst llm = new OpenAI();\nconst retriever = new HydeRetriever({\nvectorStore,\nllm,\nk: 1,\npromptTemplate: \"websearch\",\n});\n\nawait vectorStore.addDocuments(\n[\n\"My name is John.\",\n\"My name is Bob.\",\n\"My favourite food is pizza.\",\n\"My favourite food is pasta.\",\n].map((pageContent) => new Document({ pageContent }))\n);\n\nconst results = await retriever.getRelevantDocuments(\n\"What is my favourite food?\"\n);\n\nexpect(results.length).toBe(1);\nconsole.log(results);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/hyde.int.test.ts","loc":{"lines":{"from":66,"to":92}}}}],["865",{"pageContent":"/* eslint-disable no-process-env */\n/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport { test, expect } from \"@jest/globals\";\nimport Metal from \"@getmetal/metal-sdk\";\n\nimport { MetalRetriever } from \"../metal.js\";\n\ntest(\"MetalRetriever\", async () => {\nconst MetalSDK = Metal.default;\nconst client = new MetalSDK(\nprocess.env.METAL_API_KEY!,\nprocess.env.METAL_CLIENT_ID!,\nprocess.env.METAL_INDEX_ID\n);\nconst retriever = new MetalRetriever({ client });\n\nconst docs = await retriever.getRelevantDocuments(\"hello\");\n\nexpect(docs.length).toBeGreaterThan(0);\n\nconsole.log(docs);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/metal.int.test.ts","loc":{"lines":{"from":1,"to":22}}}}],["866",{"pageContent":"/* eslint-disable no-process-env */\n/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport { test, expect } from \"@jest/globals\";\nimport { createClient } from \"@supabase/supabase-js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { SupabaseHybridSearch } from \"../supabase.js\";\n\ntest(\"Supabase hybrid keyword search\", async () => {\nconst client = createClient(\nprocess.env.SUPABASE_URL!,\nprocess.env.SUPABASE_PRIVATE_KEY!\n);\n\nconst embeddings = new OpenAIEmbeddings();\n\nconst retriever = new SupabaseHybridSearch(embeddings, {\nclient,\nsimilarityK: 2,\nkeywordK: 2,\n});\n\nexpect(retriever).toBeDefined();\n\nconst results = await retriever.getRelevantDocuments(\"hello bye\");\n\nexpect(results.length).toBeGreaterThan(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/supabase.int.test.ts","loc":{"lines":{"from":1,"to":27}}}}],["867",{"pageContent":"import { describe, expect, jest, test } from \"@jest/globals\";\nimport { Document } from \"../../document.js\";\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\nimport {\nBUFFER_IDX,\nLAST_ACCESSED_AT_KEY,\nTimeWeightedVectorStoreRetriever,\n} from \"../time_weighted.js\";\n\njest.useFakeTimers();\nconst mockNow = new Date(\"2023-04-18 15:30\");\njest.setSystemTime(mockNow);\n\nconst getSec = (date: Date) => Math.floor(date.getTime() / 1000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":1,"to":15}}}}],["868",{"pageContent":"const getMemoryStream = (): Document[] => [\n{\npageContent: \"foo\",\nmetadata: {\n[BUFFER_IDX]: 0,\n[LAST_ACCESSED_AT_KEY]: getSec(new Date(\"2023-04-18 12:00\")),\ncreated_at: getSec(new Date(\"2023-04-18 12:00\")),\n},\n},\n{\npageContent: \"bar\",\nmetadata: {\n[BUFFER_IDX]: 1,\n[LAST_ACCESSED_AT_KEY]: getSec(new Date(\"2023-04-18 13:00\")),\ncreated_at: getSec(new Date(\"2023-04-18 13:00\")),\n},\n},\n{\npageContent: \"baz\",\nmetadata: {\n[BUFFER_IDX]: 2,\n[LAST_ACCESSED_AT_KEY]: getSec(new Date(\"2023-04-18 11:00\")),\ncreated_at: getSec(new Date(\"2023-04-18 11:00\")),\n},\n},\n];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":337,"to":362}}}}],["869",{"pageContent":"describe(\"Test getRelevantDocuments\", () => {\ntest(\"Should fail on a vector store with documents that have not been added through the addDocuments method on the retriever\", async () => {\nconst vectorStore = new MemoryVectorStore(new FakeEmbeddings());\nconst retriever = new TimeWeightedVectorStoreRetriever({\nvectorStore,\nmemoryStream: [],\nsearchKwargs: 2,\n});\nawait vectorStore.addDocuments([\n{ pageContent: \"aaa\", metadata: {} },\n{ pageContent: \"aaaa\", metadata: {} },\n{ pageContent: \"bbb\", metadata: {} },\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":682,"to":694}}}}],["870",{"pageContent":"const query = \"aaa\";\nawait expect(() => retriever.getRelevantDocuments(query)).rejects.toThrow();\n});\ntest(\"For different pageContent with the same lastAccessedAt, return in descending order of similar words.\", async () => {\nconst retriever = new TimeWeightedVectorStoreRetriever({\nvectorStore: new MemoryVectorStore(new FakeEmbeddings()),\nmemoryStream: [],\nsearchKwargs: 2,\n});\nawait retriever.addDocuments([\n{ pageContent: \"aaa\", metadata: {} },\n{ pageContent: \"aaaa\", metadata: {} },\n{ pageContent: \"bbb\", metadata: {} },\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":1015,"to":1028}}}}],["871",{"pageContent":"const query = \"aaa\";\nconst resultsDocs = await retriever.getRelevantDocuments(query);\nconst expected = [\n{\npageContent: \"aaa\",\nmetadata: {\n[BUFFER_IDX]: 0,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(mockNow),\n},\n},\n{\npageContent: \"aaaa\",\nmetadata: {\n[BUFFER_IDX]: 1,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(mockNow),\n},\n},\n{\npageContent: \"bbb\",\nmetadata: {\n[BUFFER_IDX]: 2,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(mockNow),\n},\n},\n];\nexpect(resultsDocs).toStrictEqual(expected);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":1348,"to":1377}}}}],["872",{"pageContent":"test(\"Return in descending order of lastAccessedAt when memoryStream of the same pageContent\", async () => {\nconst samePageContent = \"Test query\";\nconst samePageContentMemoryStream = getMemoryStream().map((doc) => ({\n...doc,\npageContent: samePageContent,\n}));\nconst retriever = new TimeWeightedVectorStoreRetriever({\nvectorStore: new MemoryVectorStore(new FakeEmbeddings()),\nmemoryStream: samePageContentMemoryStream,\n});\nawait retriever.addDocuments([\n{ pageContent: samePageContent, metadata: {} },\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":1697,"to":1709}}}}],["873",{"pageContent":"const query = \"Test query\";\nconst resultsDocs = await retriever.getRelevantDocuments(query);\nconst expected = [\n{\npageContent: samePageContent,\nmetadata: {\n[BUFFER_IDX]: 3,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(mockNow),\n},\n},\n{\npageContent: samePageContent,\nmetadata: {\n[BUFFER_IDX]: 1,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 13:00\")),\n},\n},\n{\npageContent: samePageContent,\nmetadata: {\n[BUFFER_IDX]: 0,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 12:00\")),\n},\n},\n{\npageContent: samePageContent,\nmetadata: {\n[BUFFER_IDX]: 2,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 11:00\")),\n},\n},\n];\nexpect(resultsDocs).toStrictEqual(expected);\n});\ntest(\"Return in descending order of lastAccessedAt when memoryStream of different pageContent\", async () => {\nconst retriever = new TimeWeightedVectorStoreRetriever({\nvectorStore: new MemoryVectorStore(new FakeEmbeddings()),\nmemoryStream: getMemoryStream(),\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":2031,"to":2073}}}}],["874",{"pageContent":"await retriever.addDocuments([{ pageContent: \"qux\", metadata: {} }]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":2196,"to":2196}}}}],["875",{"pageContent":"const query = \"Test query\";\nconst resultsDocs = await retriever.getRelevantDocuments(query);\nconst expected = [\n{\npageContent: \"qux\",\nmetadata: {\n[BUFFER_IDX]: 3,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(mockNow),\n},\n},\n{\npageContent: \"bar\",\nmetadata: {\n[BUFFER_IDX]: 1,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 13:00\")),\n},\n},\n{\npageContent: \"foo\",\nmetadata: {\n[BUFFER_IDX]: 0,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 12:00\")),\n},\n},\n{\npageContent: \"baz\",\nmetadata: {\n[BUFFER_IDX]: 2,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 11:00\")),\n},\n},\n];\nexpect(resultsDocs).toStrictEqual(expected);\n});\ntest(\"Return in descending order of lastAccessedAt when memoryStream of different pageContent and decayRate\", async () => {\nconst decayRate = 0.5;\nconst retriever = new TimeWeightedVectorStoreRetriever({\nvectorStore: new MemoryVectorStore(new FakeEmbeddings()),\nmemoryStream: getMemoryStream(),\ndecayRate,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":2372,"to":2415}}}}],["876",{"pageContent":"});\nawait retriever.addDocuments([{ pageContent: \"qux\", metadata: {} }]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":2714,"to":2715}}}}],["877",{"pageContent":"const query = \"Test query\";\nconst resultsDocs = await retriever.getRelevantDocuments(query);\nconst expected = [\n{\npageContent: \"qux\",\nmetadata: {\n[BUFFER_IDX]: 3,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(mockNow),\n},\n},\n{\npageContent: \"bar\",\nmetadata: {\n[BUFFER_IDX]: 1,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 13:00\")),\n},\n},\n{\npageContent: \"foo\",\nmetadata: {\n[BUFFER_IDX]: 0,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 12:00\")),\n},\n},\n{\npageContent: \"baz\",\nmetadata: {\n[BUFFER_IDX]: 2,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 11:00\")),\n},\n},\n];\nexpect(resultsDocs).toStrictEqual(expected);\n});\ntest(\"Return in descending order of lastAccessedAt when memoryStream of different pageContent and k = 3\", async () => {\nconst retriever = new TimeWeightedVectorStoreRetriever({\nvectorStore: new MemoryVectorStore(new FakeEmbeddings()),\nmemoryStream: getMemoryStream(),\nk: 3,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":3050,"to":3093}}}}],["878",{"pageContent":"await retriever.addDocuments([{ pageContent: \"qux\", metadata: {} }]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":3217,"to":3217}}}}],["879",{"pageContent":"const query = \"Test query\";\nconst resultsDocs = await retriever.getRelevantDocuments(query);\nconst expected = [\n{\npageContent: \"qux\",\nmetadata: {\n[BUFFER_IDX]: 3,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(mockNow),\n},\n},\n{\npageContent: \"bar\",\nmetadata: {\n[BUFFER_IDX]: 1,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 13:00\")),\n},\n},\n{\npageContent: \"baz\",\nmetadata: {\n[BUFFER_IDX]: 2,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 11:00\")),\n},\n},\n];\nexpect(resultsDocs).toStrictEqual(expected);\n});\ntest(\"Return in descending order of lastAccessedAt when memoryStream of different pageContent and searchKwargs = 2\", async () => {\nconst retriever = new TimeWeightedVectorStoreRetriever({\nvectorStore: new MemoryVectorStore(new FakeEmbeddings()),\nmemoryStream: getMemoryStream(),\nsearchKwargs: 2,\n});\nawait retriever.addDocuments([\n{ pageContent: \"qux\", metadata: {} },\n{ pageContent: \"quux\", metadata: {} },\n{ pageContent: \"corge\", metadata: {} },\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":3393,"to":3433}}}}],["880",{"pageContent":"const query = \"Test query\";\nconst resultsDocs = await retriever.getRelevantDocuments(query);\nconst expected = [\n{\npageContent: \"qux\",\nmetadata: {\n[BUFFER_IDX]: 3,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(mockNow),\n},\n},\n{\npageContent: \"quux\",\nmetadata: {\n[BUFFER_IDX]: 4,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(mockNow),\n},\n},\n{\npageContent: \"corge\",\nmetadata: {\n[BUFFER_IDX]: 5,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(mockNow),\n},\n},\n{\npageContent: \"baz\",\nmetadata: {\n[BUFFER_IDX]: 2,\n[LAST_ACCESSED_AT_KEY]: getSec(mockNow),\ncreated_at: getSec(new Date(\"2023-04-18 11:00\")),\n},\n},\n];\nconsole.log(resultsDocs);\nexpect(resultsDocs).toStrictEqual(expected);\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/time_weighted.test.ts","loc":{"lines":{"from":3732,"to":3771}}}}],["881",{"pageContent":"import { VectorStore } from \"vectorstores/base.js\";\nimport { Document } from \"../document.js\";\nimport { BaseRetriever } from \"../schema/index.js\";\n\nexport interface TimeWeightedVectorStoreRetrieverFields {\nvectorStore: VectorStore;\nsearchKwargs?: number;\nmemoryStream?: Document[];\ndecayRate?: number;\nk?: number;\notherScoreKeys?: string[];\ndefaultSalience?: number;\n}\n\nexport const LAST_ACCESSED_AT_KEY = \"last_accessed_at\";\nexport const BUFFER_IDX = \"buffer_idx\";\n\n/**\n* TimeWeightedVectorStoreRetriever retrieves documents based on their time-weighted relevance.\n* ref: https://github.com/hwchase17/langchain/blob/master/langchain/retrievers/time_weighted_retriever.py\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/time_weighted.ts","loc":{"lines":{"from":1,"to":21}}}}],["882",{"pageContent":"class TimeWeightedVectorStoreRetriever extends BaseRetriever {\n/**\n* The vectorstore to store documents and determine salience.\n*/\nprivate vectorStore: VectorStore;\n\n/**\n* The number of top K most relevant documents to consider when searching.\n*/\nprivate searchKwargs: number;\n\n/**\n* The memory_stream of documents to search through.\n*/\nprivate memoryStream: Document[];\n\n/**\n* The exponential decay factor used as (1.0-decay_rate)**(hrs_passed).\n*/\nprivate decayRate: number;\n\n/**\n* The maximum number of documents to retrieve in a given call.\n*/\nprivate k: number;\n\n/**\n* Other keys in the metadata to factor into the score, e.g. 'importance'.\n*/\nprivate otherScoreKeys: string[];\n\n/**\n* The salience to assign memories not retrieved from the vector store.\n*/\nprivate defaultSalience: number | null;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/time_weighted.ts","loc":{"lines":{"from":239,"to":273}}}}],["883",{"pageContent":"/**\n* Constructor to initialize the required fields\n* @param fields - The fields required for initializing the TimeWeightedVectorStoreRetriever\n*/\nconstructor(fields: TimeWeightedVectorStoreRetrieverFields) {\nsuper();\nthis.vectorStore = fields.vectorStore;\nthis.searchKwargs = fields.searchKwargs ?? 100;\nthis.memoryStream = fields.memoryStream ?? [];\nthis.decayRate = fields.decayRate ?? 0.01;\nthis.k = fields.k ?? 4;\nthis.otherScoreKeys = fields.otherScoreKeys ?? [];\nthis.defaultSalience = fields.defaultSalience ?? null;\n}\n\n/**\n* Get relevant documents based on time-weighted relevance\n* @param query - The query to search for\n* @returns The relevant documents\n*/\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nconst now = Math.floor(Date.now() / 1000);\nconst memoryDocsAndScores = this.getMemoryDocsAndScores();\n\nconst salientDocsAndScores = await this.getSalientDocuments(query);\nconst docsAndScores = { ...memoryDocsAndScores, ...salientDocsAndScores };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/time_weighted.ts","loc":{"lines":{"from":487,"to":512}}}}],["884",{"pageContent":"return this.computeResults(docsAndScores, now);\n}\n\n/**\n* NOTE: When adding documents to a vector store, use addDocuments\n* via retriever instead of directly to the vector store.\n* This is because it is necessary to process the document\n* in prepareDocuments.\n*\n* @param docs - The documents to add to vector store in the retriever\n*/\nasync addDocuments(docs: Document[]): Promise<void> {\nconst now = Math.floor(Date.now() / 1000);\nconst savedDocs = this.prepareDocuments(docs, now);\n\nthis.memoryStream.push(...savedDocs);\nawait this.vectorStore.addDocuments(savedDocs);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/time_weighted.ts","loc":{"lines":{"from":719,"to":736}}}}],["885",{"pageContent":"/**\n* Get memory documents and their scores\n* @returns An object containing memory documents and their scores\n*/\nprivate getMemoryDocsAndScores(): Record<\nnumber,\n{ doc: Document; score: number }\n> {\nconst memoryDocsAndScores: Record<\nnumber,\n{ doc: Document; score: number }\n> = {};\nfor (const doc of this.memoryStream.slice(-this.k)) {\nconst bufferIdx = doc.metadata[BUFFER_IDX];\nif (bufferIdx === undefined) {\nthrow new Error(\n`Found a document in the vector store that is missing required metadata. This retriever only supports vector stores with documents that have been added through the \"addDocuments\" method on a TimeWeightedVectorStoreRetriever, not directly added or loaded into the backing vector store.`\n);\n}\nmemoryDocsAndScores[bufferIdx] = {\ndoc,\nscore: this.defaultSalience ?? 0,\n};\n}\nreturn memoryDocsAndScores;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/time_weighted.ts","loc":{"lines":{"from":955,"to":980}}}}],["886",{"pageContent":"/**\n* Get salient documents and their scores based on the query\n* @param query - The query to search for\n* @returns An object containing salient documents and their scores\n*/\nprivate async getSalientDocuments(\nquery: string\n): Promise<Record<number, { doc: Document; score: number }>> {\nconst docAndScores: [Document, number][] =\nawait this.vectorStore.similaritySearchWithScore(\nquery,\nthis.searchKwargs\n);\nconst results: Record<number, { doc: Document; score: number }> = {};\nfor (const [fetchedDoc, score] of docAndScores) {\nconst bufferIdx = fetchedDoc.metadata[BUFFER_IDX];\nif (bufferIdx === undefined) {\nthrow new Error(\n`Found a document in the vector store that is missing required metadata. This retriever only supports vector stores with documents that have been added through the \"addDocuments\" method on a TimeWeightedVectorStoreRetriever, not directly added or loaded into the backing vector store.`\n);\n}\nconst doc = this.memoryStream[bufferIdx];\nresults[bufferIdx] = { doc, score };\n}\nreturn results;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/time_weighted.ts","loc":{"lines":{"from":1194,"to":1219}}}}],["887",{"pageContent":"/**\n* Compute the final result set of documents based on the combined scores\n* @param docsAndScores - An object containing documents and their scores\n* @param now - The current timestamp\n* @returns The final set of documents\n*/\nprivate computeResults(\ndocsAndScores: Record<number, { doc: Document; score: number }>,\nnow: number\n): Document[] {\nconst recordedDocs = Object.values(docsAndScores)\n.map(({ doc, score }) => ({\ndoc,\nscore: this.getCombinedScore(doc, score, now),\n}))\n.sort((a, b) => b.score - a.score);\n\nconst results: Document[] = [];\nfor (const { doc } of recordedDocs) {\nconst bufferedDoc = this.memoryStream[doc.metadata[BUFFER_IDX]];\nbufferedDoc.metadata[LAST_ACCESSED_AT_KEY] = now;\nresults.push(bufferedDoc);\n}\nreturn results;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/time_weighted.ts","loc":{"lines":{"from":1423,"to":1447}}}}],["888",{"pageContent":"/**\n* Prepare documents with necessary metadata before saving\n* @param docs - The documents to prepare\n* @param now - The current timestamp\n* @returns The prepared documents\n*/\nprivate prepareDocuments(docs: Document[], now: number): Document[] {\nreturn docs.map((doc, i) => ({\n...doc,\nmetadata: {\n[LAST_ACCESSED_AT_KEY]: doc.metadata[LAST_ACCESSED_AT_KEY] ?? now,\ncreated_at: doc.metadata.created_at ?? now,\n[BUFFER_IDX]: this.memoryStream.length + i,\n},\n}));\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/time_weighted.ts","loc":{"lines":{"from":1663,"to":1678}}}}],["889",{"pageContent":"/**\n* Calculate the combined score based on vector relevance and other factors\n* @param doc - The document to calculate the score for\n* @param vectorRelevance - The relevance score\n* from the vector store\n@param nowMsec - The current timestamp in milliseconds\n@returns The combined score for the document\n*/\nprivate getCombinedScore(\ndoc: Document,\nvectorRelevance: number | null,\nnowMsec: number\n): number {\nconst hoursPassed = this.getHoursPassed(\nnowMsec,\ndoc.metadata[LAST_ACCESSED_AT_KEY]\n);\nlet score = (1.0 - this.decayRate) ** hoursPassed;\nfor (const key of this.otherScoreKeys) {\nscore += doc.metadata[key];\n}\nif (vectorRelevance !== null) {\nscore += vectorRelevance;\n}\nreturn score;\n}\n\n/**","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/time_weighted.ts","loc":{"lines":{"from":1900,"to":1927}}}}],["890",{"pageContent":"Calculate the hours passed between two time points\n@param time - The current time in seconds\n@param refTime - The reference time in seconds\n@returns The number of hours passed between the two time points\n*/\nprivate getHoursPassed(time: number, refTime: number): number {\nreturn (time - refTime) / 3600;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/time_weighted.ts","loc":{"lines":{"from":2143,"to":2151}}}}],["891",{"pageContent":"import { Document } from \"../document.js\";\n\nexport const RUN_KEY = \"__run\";\n\nexport type Example = Record<string, string>;\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type InputValues = Record<string, any>;\n\nexport type PartialValues = Record<\nstring,\nstring | (() => Promise<string>) | (() => string)\n>;\n\n/**\n* Output of a single generation.\n*/\nexport interface Generation {\n/**\n* Generated text output\n*/\ntext: string;\n/**\n* Raw generation info response from the provider.\n* May include things like reason for finishing (e.g. in {@link OpenAI})\n*/\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ngenerationInfo?: Record<string, any>;\n}\n\n/**\n* Contains all relevant information returned by an LLM.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/schema/index.ts","loc":{"lines":{"from":1,"to":33}}}}],["892",{"pageContent":"type LLMResult = {\n/**\n* List of the things generated. Each input could have multiple {@link Generation | generations}, hence this is a list of lists.\n*/\ngenerations: Generation[][];\n/**\n* Dictionary of arbitrary LLM-provider specific output.\n*/\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nllmOutput?: Record<string, any>;\n/**\n* Dictionary of run metadata\n*/\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n[RUN_KEY]?: Record<string, any>;\n};\nexport type MessageType = \"human\" | \"ai\" | \"generic\" | \"system\";\n\nexport abstract class BaseChatMessage {\n/** The text of the message. */\ntext: string;\n\n/** The name of the message sender in a multi-user chat. */\nname?: string;\n\n/** The type of the message. */\nabstract _getType(): MessageType;\n\nconstructor(text: string) {\nthis.text = text;\n}\n}\n\nexport class HumanChatMessage extends BaseChatMessage {\n_getType(): MessageType {\nreturn \"human\";\n}\n}\n\nexport class AIChatMessage extends BaseChatMessage {\n_getType(): MessageType {\nreturn \"ai\";\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/schema/index.ts","loc":{"lines":{"from":165,"to":208}}}}],["893",{"pageContent":"class SystemChatMessage extends BaseChatMessage {\n_getType(): MessageType {\nreturn \"system\";\n}\n}\n\nexport class ChatMessage extends BaseChatMessage {\nrole: string;\n\nconstructor(text: string, role: string) {\nsuper(text);\nthis.role = role;\n}\n\n_getType(): MessageType {\nreturn \"generic\";\n}\n}\n\nexport interface ChatGeneration extends Generation {\nmessage: BaseChatMessage;\n}\n\nexport interface ChatResult {\ngenerations: ChatGeneration[];\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nllmOutput?: Record<string, any>;\n}\n\n/**\n* Base PromptValue class. All prompt values should extend this class.\n*/\nexport abstract class BasePromptValue {\nabstract toString(): string;\n\nabstract toChatMessages(): BaseChatMessage[];\n}\n\nexport type AgentAction = {\ntool: string;\ntoolInput: string;\nlog: string;\n};\n\nexport type AgentFinish = {\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nreturnValues: Record<string, any>;\nlog: string;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/schema/index.ts","loc":{"lines":{"from":332,"to":381}}}}],["894",{"pageContent":"type AgentStep = {\naction: AgentAction;\nobservation: string;\n};\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type ChainValues = Record<string, any>;\n\n/**\n* Base Index class. All indexes should extend this class.\n*/\nexport abstract class BaseRetriever {\nabstract getRelevantDocuments(query: string): Promise<Document[]>;\n}\n\nexport abstract class BaseChatMessageHistory {\npublic abstract getMessages(): Promise<BaseChatMessage[]>;\n\npublic abstract addUserMessage(message: string): Promise<void>;\n\npublic abstract addAIChatMessage(message: string): Promise<void>;\n\npublic abstract clear(): Promise<void>;\n}\n\nexport abstract class BaseCache<T = Generation[]> {\nabstract lookup(prompt: string, llmKey: string): Promise<T | null>;\n\nabstract update(prompt: string, llmKey: string, value: T): Promise<void>;\n}\n\nexport abstract class BaseFileStore {\nabstract readFile(path: string): Promise<string>;\n\nabstract writeFile(path: string, contents: string): Promise<void>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/schema/index.ts","loc":{"lines":{"from":507,"to":542}}}}],["895",{"pageContent":"import { Callbacks } from \"../callbacks/manager.js\";\nimport { BasePromptValue } from \"./index.js\";\n\n/** Class to parse the output of an LLM call.\n*/\nexport abstract class BaseOutputParser<T = unknown> {\n/**\n* Parse the output of an LLM call.\n*\n* @param text - LLM output to parse.\n* @returns Parsed output.\n*/\nabstract parse(text: string, callbacks?: Callbacks): Promise<T>;\n\nasync parseWithPrompt(\ntext: string,\n_prompt: BasePromptValue,\ncallbacks?: Callbacks\n): Promise<T> {\nreturn this.parse(text, callbacks);\n}\n\n/**\n* Return a string describing the format of the output.\n* @returns Format instructions.\n* @example\n* ```json\n* {\n*  \"foo\": \"bar\"\n* }\n* ```\n*/\nabstract getFormatInstructions(): string;\n\n/**\n* Return the string type key uniquely identifying this class of parser\n*/\n_type(): string {\nthrow new Error(\"_type not implemented\");\n}\n}\n\nexport class OutputParserException extends Error {\noutput?: string;\n\nconstructor(message: string, output?: string) {\nsuper(message);\nthis.output = output;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/schema/output_parser.ts","loc":{"lines":{"from":1,"to":50}}}}],["896",{"pageContent":"import type { DataSource as DataSourceT, DataSourceOptions } from \"typeorm\";\nimport {\ngenerateTableInfoFromTables,\ngetTableAndColumnsName,\nSerializedSqlDatabase,\nSqlDatabaseDataSourceParams,\nSqlDatabaseOptionsParams,\nSqlTable,\nverifyIgnoreTablesExistInDatabase,\nverifyIncludeTablesExistInDatabase,\nverifyListTablesExistInDatabase,\n} from \"./util/sql_utils.js\";\n\nexport { SqlDatabaseDataSourceParams, SqlDatabaseOptionsParams };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":1,"to":14}}}}],["897",{"pageContent":"class SqlDatabase\nimplements SqlDatabaseOptionsParams, SqlDatabaseDataSourceParams\n{\nappDataSourceOptions: DataSourceOptions;\n\nappDataSource: DataSourceT;\n\nallTables: Array<SqlTable> = [];\n\nincludesTables: Array<string> = [];\n\nignoreTables: Array<string> = [];\n\nsampleRowsInTableInfo = 3;\n\nprotected constructor(fields: SqlDatabaseDataSourceParams) {\nthis.appDataSource = fields.appDataSource;\nthis.appDataSourceOptions = fields.appDataSource.options;\nif (fields?.includesTables && fields?.ignoreTables) {\nthrow new Error(\"Cannot specify both include_tables and ignoreTables\");\n}\nthis.includesTables = fields?.includesTables ?? [];\nthis.ignoreTables = fields?.ignoreTables ?? [];\nthis.sampleRowsInTableInfo =\nfields?.sampleRowsInTableInfo ?? this.sampleRowsInTableInfo;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":159,"to":184}}}}],["898",{"pageContent":"static async fromDataSourceParams(\nfields: SqlDatabaseDataSourceParams\n): Promise<SqlDatabase> {\nconst sqlDatabase = new SqlDatabase(fields);\nif (!sqlDatabase.appDataSource.isInitialized) {\nawait sqlDatabase.appDataSource.initialize();\n}\nsqlDatabase.allTables = await getTableAndColumnsName(\nsqlDatabase.appDataSource\n);\nverifyIncludeTablesExistInDatabase(\nsqlDatabase.allTables,\nsqlDatabase.includesTables\n);\nverifyIgnoreTablesExistInDatabase(\nsqlDatabase.allTables,\nsqlDatabase.ignoreTables\n);\nreturn sqlDatabase;\n}\n\nstatic async fromOptionsParams(\nfields: SqlDatabaseOptionsParams\n): Promise<SqlDatabase> {\nconst { DataSource } = await import(\"typeorm\");\nconst dataSource = new DataSource(fields.appDataSourceOptions);\nreturn SqlDatabase.fromDataSourceParams({\n...fields,\nappDataSource: dataSource,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":312,"to":342}}}}],["899",{"pageContent":"/**\n* Get information about specified tables.\n*\n* Follows best practices as specified in: Rajkumar et al, 2022\n* (https://arxiv.org/abs/2204.00498)\n*\n* If `sample_rows_in_table_info`, the specified number of sample rows will be\n* appended to each table description. This can increase performance as\n* demonstrated in the paper.\n*/\nasync getTableInfo(targetTables?: Array<string>): Promise<string> {\nlet selectedTables =\nthis.includesTables.length > 0\n? this.allTables.filter((currentTable) =>\nthis.includesTables.includes(currentTable.tableName)\n)\n: this.allTables;\n\nif (this.ignoreTables.length > 0) {\nselectedTables = selectedTables.filter(\n(currentTable) => !this.ignoreTables.includes(currentTable.tableName)\n);\n}\n\nif (targetTables && targetTables.length > 0) {\nverifyListTablesExistInDatabase(\nthis.allTables,\ntargetTables,\n\"Wrong target table name:\"\n);\nselectedTables = this.allTables.filter((currentTable) =>\ntargetTables.includes(currentTable.tableName)\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":470,"to":503}}}}],["900",{"pageContent":"return generateTableInfoFromTables(\nselectedTables,\nthis.appDataSource,\nthis.sampleRowsInTableInfo\n);\n}\n\n/**\n* Execute a SQL command and return a string representing the results.\n* If the statement returns rows, a string of the results is returned.\n* If the statement returns no rows, an empty string is returned.\n*/\nasync run(command: string, fetch: \"all\" | \"one\" = \"all\"): Promise<string> {\n// TODO: Potential security issue here\nconst res = await this.appDataSource.query(command);\n\nif (fetch === \"all\") {\nreturn JSON.stringify(res);\n}\n\nif (res?.length > 0) {\nreturn JSON.stringify(res[0]);\n}\n\nreturn \"\";\n}\n\nserialize(): SerializedSqlDatabase {\nreturn {\n_type: \"sql_database\",\nappDataSourceOptions: this.appDataSourceOptions,\nincludesTables: this.includesTables,\nignoreTables: this.ignoreTables,\nsampleRowsInTableInfo: this.sampleRowsInTableInfo,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":628,"to":663}}}}],["901",{"pageContent":"/** @ignore */\nstatic async imports() {\ntry {\nconst { DataSource } = await import(\"typeorm\");\nreturn { DataSource };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Failed to load typeorm. Please install it with eg. `yarn add typeorm`.\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":790,"to":802}}}}],["902",{"pageContent":"import { BaseFileStore } from \"../../schema/index.js\";\n\nexport class InMemoryFileStore extends BaseFileStore {\nprivate files: Map<string, string> = new Map();\n\nasync readFile(path: string): Promise<string> {\nconst contents = this.files.get(path);\nif (contents === undefined) {\nthrow new Error(`File not found: ${path}`);\n}\nreturn contents;\n}\n\nasync writeFile(path: string, contents: string): Promise<void> {\nthis.files.set(path, contents);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/stores/file/in_memory.ts","loc":{"lines":{"from":1,"to":17}}}}],["903",{"pageContent":"import * as fs from \"node:fs/promises\";\nimport { mkdtempSync } from \"node:fs\";\nimport { join } from \"node:path\";\n\nimport { BaseFileStore } from \"../../schema/index.js\";\n\nexport class NodeFileStore extends BaseFileStore {\nconstructor(public basePath: string = mkdtempSync(\"langchain-\")) {\nsuper();\n}\n\nasync readFile(path: string): Promise<string> {\nreturn await fs.readFile(join(this.basePath, path), \"utf8\");\n}\n\nasync writeFile(path: string, contents: string): Promise<void> {\nawait fs.writeFile(join(this.basePath, path), contents, \"utf8\");\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/stores/file/node.ts","loc":{"lines":{"from":1,"to":19}}}}],["904",{"pageContent":"import {\nHumanChatMessage,\nAIChatMessage,\nBaseChatMessage,\nBaseChatMessageHistory,\n} from \"../../schema/index.js\";\n\nexport class ChatMessageHistory extends BaseChatMessageHistory {\nprivate messages: BaseChatMessage[] = [];\n\nconstructor(messages?: BaseChatMessage[]) {\nsuper();\nthis.messages = messages ?? [];\n}\n\nasync getMessages(): Promise<BaseChatMessage[]> {\nreturn this.messages;\n}\n\nasync addUserMessage(message: string) {\nthis.messages.push(new HumanChatMessage(message));\n}\n\nasync addAIChatMessage(message: string) {\nthis.messages.push(new AIChatMessage(message));\n}\n\nasync clear() {\nthis.messages = [];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/stores/message/in_memory.ts","loc":{"lines":{"from":1,"to":31}}}}],["905",{"pageContent":"import { test, expect, beforeEach, afterEach } from \"@jest/globals\";\nimport { DataSource } from \"typeorm\";\nimport { SqlDatabase } from \"../sql_db.js\";\n\nlet datasource: DataSource;\n\nbeforeEach(async () => {\ndatasource = new DataSource({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/sql_database.int.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["906",{"pageContent":": \"sqlite\",\ndatabase: \":memory:\",\nsynchronize: true,\n});\nawait datasource.initialize();\n\nawait datasource.query(`\nCREATE TABLE products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Apple', 100);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Banana', 200);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Orange', 300);\n`);\nawait datasource.query(`\nCREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, age INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Alice', 20);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Bob', 21);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Charlie', 22);\n`);\n});\n\nafterEach(async () => {\nawait datasource.destroy();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/sql_database.int.test.ts","loc":{"lines":{"from":145,"to":179}}}}],["907",{"pageContent":"test(\"Test getTableInfo\", async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\nconst result = await db.getTableInfo([\"users\", \"products\"]);\nconst expectStr = `\nCREATE TABLE products (\nid INTEGER , name TEXT , price INTEGER ) \nSELECT * FROM \"products\" LIMIT 3;\nid name price\n1 Apple 100\n2 Banana 200\n3 Orange 300\nCREATE TABLE users (\nid INTEGER , name TEXT , age INTEGER ) \nSELECT * FROM \"users\" LIMIT 3;\nid name age\n1 Alice 20\n2 Bob 21\n3 Charlie 22`;\nexpect(result.trim()).toBe(expectStr.trim());\n});\n\ntest(\"Test getTableInfo with less tables than in db\", async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\nconst result = await db.getTableInfo([\"products\"]);\nconst expectStr = `\nCREATE TABLE products (\nid INTEGER , name TEXT , price INTEGER ) \nSELECT * FROM \"products\" LIMIT 3;\nid name price\n1 Apple 100\n2 Banana 200\n3 Orange 300`;\nexpect(result.trim()).toBe(expectStr.trim());\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/sql_database.int.test.ts","loc":{"lines":{"from":296,"to":333}}}}],["908",{"pageContent":"test(\"Test getTableInfo with includes tables\", async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\nincludesTables: [\"products\"],\n});\nconst result = await db.getTableInfo();\nconst expectStr = `\nCREATE TABLE products (\nid INTEGER , name TEXT , price INTEGER ) \nSELECT * FROM \"products\" LIMIT 3;\nid name price\n1 Apple 100\n2 Banana 200\n3 Orange 300`;\nexpect(result.trim()).toBe(expectStr.trim());\n});\n\ntest(\"Test getTableInfo with ignoreTables\", async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\nignoreTables: [\"users\"],\n});\nconst result = await db.getTableInfo();\nconst expectStr = `\nCREATE TABLE products (\nid INTEGER , name TEXT , price INTEGER ) \nSELECT * FROM \"products\" LIMIT 3;\nid name price\n1 Apple 100\n2 Banana 200\n3 Orange 300`;\nexpect(result.trim()).toBe(expectStr.trim());\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/sql_database.int.test.ts","loc":{"lines":{"from":447,"to":479}}}}],["909",{"pageContent":"test(\"Test getTableInfo with error\", async () => {\nawait expect(async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\nawait db.getTableInfo([\"users\", \"productss\"]);\n}).rejects.toThrow(\n\"Wrong target table name: the table productss was not found in the database\"\n);\n});\n\ntest(\"Test run\", async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\nconst result = await db.run(\"SELECT * FROM users\");\nconst expectStr = `[{\"id\":1,\"name\":\"Alice\",\"age\":20},{\"id\":2,\"name\":\"Bob\",\"age\":21},{\"id\":3,\"name\":\"Charlie\",\"age\":22}]`;\nexpect(result.trim()).toBe(expectStr.trim());\n});\n\ntest(\"Test run with error\", async () => {\nawait expect(async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\nawait db.run(\"SELECT * FROM userss\");\n}).rejects.toThrow(\"SQLITE_ERROR: no such table: userss\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/sql_database.int.test.ts","loc":{"lines":{"from":596,"to":623}}}}],["910",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { Document } from \"../document.js\";\nimport {\nCharacterTextSplitter,\nMarkdownTextSplitter,\nRecursiveCharacterTextSplitter,\nTokenTextSplitter,\n} from \"../text_splitter.js\";\n\ntest(\"Test splitting by character count.\", async () => {\nconst text = \"foo bar baz 123\";\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 7,\nchunkOverlap: 3,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"foo bar\", \"bar baz\", \"baz 123\"];\nexpect(output).toEqual(expectedOutput);\n});\n\ntest(\"Test splitting by character count doesn't create empty documents.\", async () => {\nconst text = \"foo  bar\";\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 2,\nchunkOverlap: 0,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"foo\", \"bar\"];\nexpect(output).toEqual(expectedOutput);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["911",{"pageContent":"test(\"Test splitting by character count on long words.\", async () => {\nconst text = \"foo bar baz a a\";\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 3,\nchunkOverlap: 1,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"foo\", \"bar\", \"baz\", \"a a\"];\nexpect(output).toEqual(expectedOutput);\n});\n\ntest(\"Test splitting by character count when shorter words are first.\", async () => {\nconst text = \"a a foo bar baz\";\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 3,\nchunkOverlap: 1,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"a a\", \"foo\", \"bar\", \"baz\"];\nexpect(output).toEqual(expectedOutput);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":210,"to":232}}}}],["912",{"pageContent":"test(\"Test splitting by characters when splits not found easily.\", async () => {\nconst text = \"foo bar baz 123\";\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 1,\nchunkOverlap: 0,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"foo\", \"bar\", \"baz\", \"123\"];\nexpect(output).toEqual(expectedOutput);\n});\n\ntest(\"Test invalid arguments.\", () => {\nexpect(() => {\nconst res = new CharacterTextSplitter({ chunkSize: 2, chunkOverlap: 4 });\nconsole.log(res);\n}).toThrow();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":417,"to":434}}}}],["913",{"pageContent":"test(\"Test create documents method.\", async () => {\nconst texts = [\"foo bar\", \"baz\"];\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 3,\nchunkOverlap: 0,\n});\nconst docs = await splitter.createDocuments(texts);\nconst metadata = { loc: { lines: { from: 1, to: 1 } } };\nconst expectedDocs = [\nnew Document({ pageContent: \"foo\", metadata }),\nnew Document({ pageContent: \"bar\", metadata }),\nnew Document({ pageContent: \"baz\", metadata }),\n];\nexpect(docs).toEqual(expectedDocs);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":624,"to":639}}}}],["914",{"pageContent":"test(\"Test create documents with metadata method.\", async () => {\nconst texts = [\"foo bar\", \"baz\"];\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 3,\nchunkOverlap: 0,\n});\nconst docs = await splitter.createDocuments(texts, [\n{ source: \"1\" },\n{ source: \"2\" },\n]);\nconst loc = { lines: { from: 1, to: 1 } };\nconst expectedDocs = [\nnew Document({ pageContent: \"foo\", metadata: { source: \"1\", loc } }),\nnew Document({\npageContent: \"bar\",\nmetadata: { source: \"1\", loc },\n}),\nnew Document({ pageContent: \"baz\", metadata: { source: \"2\", loc } }),\n];\nexpect(docs).toEqual(expectedDocs);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":829,"to":850}}}}],["915",{"pageContent":"test(\"Test iterative text splitter.\", async () => {\nconst text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\nThis is a weird text to write, but gotta test the splittingggg some how.\\n\\n\nBye!\\n\\n-H.`;\nconst splitter = new RecursiveCharacterTextSplitter({\nchunkSize: 10,\nchunkOverlap: 1,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\n\"Hi.\",\n\"I'm\",\n\"Harrison.\",\n\"How? Are?\",\n\"You?\",\n\"Okay then f\",\n\"f f f f.\",\n\"This is a\",\n\"a weird\",\n\"text to\",\n\"write, but\",\n\"gotta test\",\n\"the\",\n\"splitting\",\n\"gggg\",\n\"some how.\",\n\"Bye!\\n\\n-H.\",\n];\nexpect(output).toEqual(expectedOutput);\n});\n\ntest(\"Token text splitter\", async () => {\nconst text = \"foo bar baz a a\";\nconst splitter = new TokenTextSplitter({\nencodingName: \"r50k_base\",\nchunkSize: 3,\nchunkOverlap: 0,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"foo bar b\", \"az a a\"];\n\nexpect(output).toEqual(expectedOutput);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":1036,"to":1078}}}}],["916",{"pageContent":"test(\"Test markdown text splitter.\", async () => {\nconst text =\n\"#  LangChain\\n\" +\n\"\\n\" +\n\" Building applications with LLMs through composability \\n\" +\n\"\\n\" +\n\"## Quick Install\\n\" +\n\"\\n\" +\n\"```bash\\n\" +\n\"# Hopefully this code block isn't split\\n\" +\n\"pip install langchain\\n\" +\n\"```\\n\" +\n\"\\n\" +\n\"As an open source project in a rapidly developing field, we are extremely open to contributions.\";\nconst splitter = new MarkdownTextSplitter({\nchunkSize: 100,\nchunkOverlap: 0,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\n\"#  LangChain\\n\\n Building applications with LLMs through composability \",\n\"Quick Install\\n\\n```bash\\n# Hopefully this code block isn't split\\npip install langchain\",\n\"As an open source project in a rapidly developing field, we are extremely open to contributions.\",\n];\nexpect(output).toEqual(expectedOutput);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":1255,"to":1280}}}}],["917",{"pageContent":"test(\"Test lines loc on iterative text splitter.\", async () => {\nconst text = `Hi.\\nI'm Harrison.\\n\\nHow?\\na\\nb`;\nconst splitter = new RecursiveCharacterTextSplitter({\nchunkSize: 20,\nchunkOverlap: 1,\n});\nconst docs = await splitter.createDocuments([text]);\n\nconst expectedDocs = [\nnew Document({\npageContent: \"Hi.\\nI'm Harrison.\",\nmetadata: { loc: { lines: { from: 1, to: 2 } } },\n}),\nnew Document({\npageContent: \"How?\\na\\nb\",\nmetadata: { loc: { lines: { from: 4, to: 6 } } },\n}),\n];\n\nexpect(docs).toEqual(expectedDocs);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":1458,"to":1478}}}}],["918",{"pageContent":"import type * as tiktoken from \"@dqbd/tiktoken\";\nimport { Document } from \"./document.js\";\n\nexport interface TextSplitterParams {\nchunkSize: number;\n\nchunkOverlap: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":1,"to":8}}}}],["919",{"pageContent":"abstract class TextSplitter implements TextSplitterParams {\nchunkSize = 1000;\n\nchunkOverlap = 200;\n\nconstructor(fields?: Partial<TextSplitterParams>) {\nthis.chunkSize = fields?.chunkSize ?? this.chunkSize;\nthis.chunkOverlap = fields?.chunkOverlap ?? this.chunkOverlap;\nif (this.chunkOverlap >= this.chunkSize) {\nthrow new Error(\"Cannot have chunkOverlap >= chunkSize\");\n}\n}\n\nabstract splitText(text: string): Promise<string[]>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":336,"to":349}}}}],["920",{"pageContent":"async createDocuments(\ntexts: string[],\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nmetadatas: Record<string, any>[] = []\n): Promise<Document[]> {\nconst _metadatas =\nmetadatas.length > 0 ? metadatas : new Array(texts.length).fill({});\nconst documents = new Array<Document>();\nfor (let i = 0; i < texts.length; i += 1) {\nconst text = texts[i];\nlet lineCounterIndex = 1;\nlet prevChunk = null;\nfor (const chunk of await this.splitText(text)) {\n// we need to count the \\n that are in the text before getting removed by the splitting\nlet numberOfIntermediateNewLines = 0;\nif (prevChunk) {\nconst indexChunk = text.indexOf(chunk);\nconst indexEndPrevChunk = text.indexOf(prevChunk) + prevChunk.length;\nconst removedNewlinesFromSplittingText = text.slice(\nindexEndPrevChunk,\nindexChunk\n);\nnumberOfIntermediateNewLines = (\nremovedNewlinesFromSplittingText.match(/\\n/g) || []\n).length;\n}\nlineCounterIndex += numberOfIntermediateNewLines;\nconst newLinesCount = (chunk.match(/\\n/g) || []).length;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":667,"to":694}}}}],["921",{"pageContent":"const loc =\n_metadatas[i].loc && typeof _metadatas[i].loc === \"object\"\n? { ..._metadatas[i].loc }\n: {};\nloc.lines = {\nfrom: lineCounterIndex,\nto: lineCounterIndex + newLinesCount,\n};\nconst metadataWithLinesNumber = {\n..._metadatas[i],\nloc,\n};\ndocuments.push(\nnew Document({\npageContent: chunk,\nmetadata: metadataWithLinesNumber,\n})\n);\nlineCounterIndex += newLinesCount;\nprevChunk = chunk;\n}\n}\nreturn documents;\n}\n\nasync splitDocuments(documents: Document[]): Promise<Document[]> {\nconst selectedDocuments = documents.filter(\n(doc) => doc.pageContent !== undefined\n);\nconst texts = selectedDocuments.map((doc) => doc.pageContent);\nconst metadatas = selectedDocuments.map((doc) => doc.metadata);\nreturn this.createDocuments(texts, metadatas);\n}\n\nprivate joinDocs(docs: string[], separator: string): string | null {\nconst text = docs.join(separator).trim();\nreturn text === \"\" ? null : text;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":996,"to":1033}}}}],["922",{"pageContent":"mergeSplits(splits: string[], separator: string): string[] {\nconst docs: string[] = [];\nconst currentDoc: string[] = [];\nlet total = 0;\nfor (const d of splits) {\nconst _len = d.length;\nif (total + _len >= this.chunkSize) {\nif (total > this.chunkSize) {\nconsole.warn(\n`Created a chunk of size ${total}, +\nwhich is longer than the specified ${this.chunkSize}`\n);\n}\nif (currentDoc.length > 0) {\nconst doc = this.joinDocs(currentDoc, separator);\nif (doc !== null) {\ndocs.push(doc);\n}\n// Keep on popping if:\n// - we have a larger chunk than in the chunk overlap\n// - or if we still have any chunks and the length is long\nwhile (\ntotal > this.chunkOverlap ||\n(total + _len > this.chunkSize && total > 0)\n) {\ntotal -= currentDoc[0].length;\ncurrentDoc.shift();\n}\n}\n}\ncurrentDoc.push(d);\ntotal += _len;\n}\nconst doc = this.joinDocs(currentDoc, separator);\nif (doc !== null) {\ndocs.push(doc);\n}\nreturn docs;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":1337,"to":1376}}}}],["923",{"pageContent":"interface CharacterTextSplitterParams extends TextSplitterParams {\nseparator: string;\n}\n\nexport class CharacterTextSplitter\nextends TextSplitter\nimplements CharacterTextSplitterParams\n{\nseparator = \"\\n\\n\";\n\nconstructor(fields?: Partial<CharacterTextSplitterParams>) {\nsuper(fields);\nthis.separator = fields?.separator ?? this.separator;\n}\n\nasync splitText(text: string): Promise<string[]> {\n// First we naively split the large input into a bunch of smaller ones.\nlet splits: string[];\nif (this.separator) {\nsplits = text.split(this.separator);\n} else {\nsplits = text.split(\"\");\n}\nreturn this.mergeSplits(splits, this.separator);\n}\n}\n\nexport interface RecursiveCharacterTextSplitterParams\nextends TextSplitterParams {\nseparators: string[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":1680,"to":1710}}}}],["924",{"pageContent":"class RecursiveCharacterTextSplitter\nextends TextSplitter\nimplements RecursiveCharacterTextSplitterParams\n{\nseparators: string[] = [\"\\n\\n\", \"\\n\", \" \", \"\"];\n\nconstructor(fields?: Partial<RecursiveCharacterTextSplitterParams>) {\nsuper(fields);\nthis.separators = fields?.separators ?? this.separators;\n}\n\nasync splitText(text: string): Promise<string[]> {\nconst finalChunks: string[] = [];\n\n// Get appropriate separator to use\nlet separator: string = this.separators[this.separators.length - 1];\nfor (const s of this.separators) {\nif (s === \"\") {\nseparator = s;\nbreak;\n}\nif (text.includes(s)) {\nseparator = s;\nbreak;\n}\n}\n\n// Now that we have the separator, split the text\nlet splits: string[];\nif (separator) {\nsplits = text.split(separator);\n} else {\nsplits = text.split(\"\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":2018,"to":2051}}}}],["925",{"pageContent":"// Now go merging things, recursively splitting longer texts.\nlet goodSplits: string[] = [];\nfor (const s of splits) {\nif (s.length < this.chunkSize) {\ngoodSplits.push(s);\n} else {\nif (goodSplits.length) {\nconst mergedText = this.mergeSplits(goodSplits, separator);\nfinalChunks.push(...mergedText);\ngoodSplits = [];\n}\nconst otherInfo = await this.splitText(s);\nfinalChunks.push(...otherInfo);\n}\n}\nif (goodSplits.length) {\nconst mergedText = this.mergeSplits(goodSplits, separator);\nfinalChunks.push(...mergedText);\n}\nreturn finalChunks;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":2358,"to":2379}}}}],["926",{"pageContent":"interface TokenTextSplitterParams extends TextSplitterParams {\nencodingName: tiktoken.TiktokenEncoding;\nallowedSpecial: \"all\" | Array<string>;\ndisallowedSpecial: \"all\" | Array<string>;\n}\n\n/**\n* Implementation of splitter which looks at tokens.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":2695,"to":2703}}}}],["927",{"pageContent":"class TokenTextSplitter\nextends TextSplitter\nimplements TokenTextSplitterParams\n{\nencodingName: tiktoken.TiktokenEncoding;\n\nallowedSpecial: \"all\" | Array<string>;\n\ndisallowedSpecial: \"all\" | Array<string>;\n\nprivate tokenizer: tiktoken.Tiktoken;\n\nprivate registry: FinalizationRegistry<tiktoken.Tiktoken>;\n\nconstructor(fields?: Partial<TokenTextSplitterParams>) {\nsuper(fields);\n\nthis.encodingName = fields?.encodingName ?? \"gpt2\";\nthis.allowedSpecial = fields?.allowedSpecial ?? [];\nthis.disallowedSpecial = fields?.disallowedSpecial ?? \"all\";\n}\n\nasync splitText(text: string): Promise<string[]> {\nif (!this.tokenizer) {\nconst tiktoken = await TokenTextSplitter.imports();\nthis.tokenizer = tiktoken.get_encoding(this.encodingName);\n// We need to register a finalizer to free the tokenizer when the\n// splitter is garbage collected.\nthis.registry = new FinalizationRegistry((t) => t.free());\nthis.registry.register(this, this.tokenizer);\n}\n\nconst splits: string[] = [];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":3028,"to":3060}}}}],["928",{"pageContent":"const input_ids = this.tokenizer.encode(\ntext,\nthis.allowedSpecial,\nthis.disallowedSpecial\n);\n\nlet start_idx = 0;\nlet cur_idx = Math.min(start_idx + this.chunkSize, input_ids.length);\nlet chunk_ids = input_ids.slice(start_idx, cur_idx);\n\nconst decoder = new TextDecoder();\n\nwhile (start_idx < input_ids.length) {\nsplits.push(decoder.decode(this.tokenizer.decode(chunk_ids)));\n\nstart_idx += this.chunkSize - this.chunkOverlap;\ncur_idx = Math.min(start_idx + this.chunkSize, input_ids.length);\nchunk_ids = input_ids.slice(start_idx, cur_idx);\n}\n\nreturn splits;\n}\n\nstatic async imports(): Promise<typeof tiktoken> {\ntry {\nreturn await import(\"@dqbd/tiktoken\");\n} catch (err) {\nconsole.error(err);\nthrow new Error(\n\"Please install @dqbd/tiktoken as a dependency with, e.g. `npm install -S @dqbd/tiktoken`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":3363,"to":3396}}}}],["929",{"pageContent":"type MarkdownTextSplitterParams = TextSplitterParams;\n\nexport class MarkdownTextSplitter\nextends RecursiveCharacterTextSplitter\nimplements MarkdownTextSplitterParams\n{\nseparators: string[] = [\n// First, try to split along Markdown headings (starting with level 2)\n\"\\n## \",\n\"\\n### \",\n\"\\n#### \",\n\"\\n##### \",\n\"\\n###### \",\n// Note the alternative syntax for headings (below) is not handled here\n// Heading level 2\n// ---------------\n// End of code block\n\"```\\n\\n\",\n// Horizontal lines\n\"\\n\\n***\\n\\n\",\n\"\\n\\n---\\n\\n\",\n\"\\n\\n___\\n\\n\",\n// Note that this splitter doesn't handle horizontal lines defined\n// by *three or more* of ***, ---, or ___, but this is not handled\n\"\\n\\n\",\n\"\\n\",\n\" \",\n\"\",\n];\n\nconstructor(fields?: Partial<MarkdownTextSplitterParams>) {\nsuper(fields);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":3702,"to":3735}}}}],["930",{"pageContent":"import { Tool } from \"./base.js\";\n\nexport interface AIPluginToolParams {\nname: string;\ndescription: string;\napiSpec: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aiplugin.ts","loc":{"lines":{"from":1,"to":7}}}}],["931",{"pageContent":"class AIPluginTool extends Tool implements AIPluginToolParams {\nprivate _name: string;\n\nprivate _description: string;\n\napiSpec: string;\n\nget name() {\nreturn this._name;\n}\n\nget description() {\nreturn this._description;\n}\n\nconstructor(params: AIPluginToolParams) {\nsuper();\nthis._name = params.name;\nthis._description = params.description;\nthis.apiSpec = params.apiSpec;\n}\n\n/** @ignore */\nasync _call(_input: string) {\nreturn this.apiSpec;\n}\n\nstatic async fromPluginUrl(url: string) {\nconst aiPluginRes = await fetch(url);\nif (!aiPluginRes.ok) {\nthrow new Error(\n`Failed to fetch plugin from ${url} with status ${aiPluginRes.status}`\n);\n}\nconst aiPluginJson = await aiPluginRes.json();\n\nconst apiUrlRes = await fetch(aiPluginJson.api.url);\nif (!apiUrlRes.ok) {\nthrow new Error(\n`Failed to fetch API spec from ${aiPluginJson.api.url} with status ${apiUrlRes.status}`\n);\n}\nconst apiUrlJson = await apiUrlRes.text();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aiplugin.ts","loc":{"lines":{"from":62,"to":104}}}}],["932",{"pageContent":"return new AIPluginTool({\nname: aiPluginJson.name_for_model,\ndescription: `Call this tool to get the OpenAPI spec (and usage guide) for interacting with the ${aiPluginJson.name_for_human} API. You should only call this ONCE! What is the ${aiPluginJson.name_for_human} API useful for? ${aiPluginJson.description_for_human}`,\napiSpec: `Usage Guide: ${aiPluginJson.description_for_model}\n\nOpenAPI Spec in JSON or YAML format:\\n${apiUrlJson}`,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aiplugin.ts","loc":{"lines":{"from":122,"to":130}}}}],["933",{"pageContent":"import { DynamicTool, DynamicToolInput } from \"./dynamic.js\";\n\ninterface LambdaConfig {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":1,"to":3}}}}],["934",{"pageContent":"Name: string;\nregion?: string;\naccessKeyId?: string;\nsecretAccessKey?: string;\n}\n\ninterface LambdaClientConstructorArgs {\nregion?: string;\ncredentials?: {\naccessKeyId: string;\nsecretAccessKey: string;\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":98,"to":110}}}}],["935",{"pageContent":"AWSLambda extends DynamicTool {\nprivate lambdaConfig: LambdaConfig;\n\nconstructor({\nname,\ndescription,\n...rest\n}: LambdaConfig & Omit<DynamicToolInput, \"func\">) {\nsuper({\nname,\ndescription,\nfunc: async (input: string) => this._func(input),\n});\n\nthis.lambdaConfig = rest;\n}\n\n/** @ignore */\nasync _func(input: string): Promise<string> {\nconst { Client, Invoker } = await LambdaImports();\n\nconst clientConstructorArgs: LambdaClientConstructorArgs = {};\n\nif (this.lambdaConfig.region) {\nclientConstructorArgs.region = this.lambdaConfig.region;\n}\n\nif (this.lambdaConfig.accessKeyId && this.lambdaConfig.secretAccessKey) {\nclientConstructorArgs.credentials = {\naccessKeyId: this.lambdaConfig.accessKeyId,\nsecretAccessKey: this.lambdaConfig.secretAccessKey,\n};\n}\n\nconst lambdaClient = new Client(clientConstructorArgs);\n\nreturn new Promise((resolve) => {\nconst payloadUint8Array = new TextEncoder().encode(JSON.stringify(input));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":198,"to":235}}}}],["936",{"pageContent":"const command = new Invoker({\nFunctionName: this.lambdaConfig.functionName,\nInvocationType: \"RequestResponse\",\nPayload: payloadUint8Array,\n});\n\nlambdaClient\n.send(command)\n.then((response) => {\nconst responseData = JSON.parse(\nnew TextDecoder().decode(response.Payload)\n);\n\nresolve(responseData.body ? responseData.body : \"request completed.\");\n})\n.catch((error: Error) => {\nconsole.error(\"Error invoking Lambda function:\", error);\nresolve(\"failed to complete request\");\n});\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":291,"to":312}}}}],["937",{"pageContent":"LambdaImports() {\ntry {\nconst { LambdaClient, InvokeCommand } = await import(\n\"@aws-sdk/client-lambda\"\n);\n\nreturn {\nClient: LambdaClient as typeof LambdaClient,\nInvoker: InvokeCommand as typeof InvokeCommand,\n};\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Failed to load @aws-sdk/client-lambda'. Please install it eg. `yarn add @aws-sdk/client-lambda`.\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":385,"to":401}}}}],["938",{"pageContent":"{ AWSLambda };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":480,"to":480}}}}],["939",{"pageContent":"import { z } from \"zod\";\nimport {\nCallbackManager,\nCallbackManagerForToolRun,\nCallbacks,\n} from \"../callbacks/manager.js\";\nimport { BaseLangChain, BaseLangChainParams } from \"../base_language/index.js\";\n\nexport interface ToolParams extends BaseLangChainParams {}\n\n/**\n* Base class for Tools that accept input of any shape defined by a Zod schema.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/base.ts","loc":{"lines":{"from":1,"to":13}}}}],["940",{"pageContent":"abstract class StructuredTool<\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nT extends z.ZodObject<any, any, any, any> = z.ZodObject<any, any, any, any>\n> extends BaseLangChain {\nabstract schema: T | z.ZodEffects<T>;\n\nconstructor(fields?: ToolParams) {\nsuper(fields ?? {});\n}\n\nprotected abstract _call(\narg: z.output<T>,\nrunManager?: CallbackManagerForToolRun\n): Promise<string>;\n\nasync call(\narg: (z.output<T> extends string ? string : never) | z.input<T>,\ncallbacks?: Callbacks\n): Promise<string> {\nconst parsed = await this.schema.parseAsync(arg);\nconst callbackManager_ = await CallbackManager.configure(\ncallbacks,\nthis.callbacks,\n{ verbose: this.verbose }\n);\nconst runManager = await callbackManager_?.handleToolStart(\n{ name: this.name },","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/base.ts","loc":{"lines":{"from":83,"to":109}}}}],["941",{"pageContent":"of parsed === \"string\" ? parsed : JSON.stringify(parsed)\n);\nlet result;\ntry {\nresult = await this._call(parsed, runManager);\n} catch (e) {\nawait runManager?.handleToolError(e);\nthrow e;\n}\nawait runManager?.handleToolEnd(result);\nreturn result;\n}\n\nabstract name: string;\n\nabstract description: string;\n\nreturnDirect = false;\n}\n\n/**\n* Base class for Tools that accept input as a string.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/base.ts","loc":{"lines":{"from":165,"to":187}}}}],["942",{"pageContent":"abstract class Tool extends StructuredTool {\nschema = z\n.object({ input: z.string().optional() })\n.transform((obj) => obj.input);\n\nconstructor(verbose?: boolean, callbacks?: Callbacks) {\nsuper({ verbose, callbacks });\n}\n\ncall(\narg: string | undefined | z.input<this[\"schema\"]>,\ncallbacks?: Callbacks\n): Promise<string> {\nreturn super.call(\ntypeof arg === \"string\" || !arg ? { input: arg } : arg,\ncallbacks\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/base.ts","loc":{"lines":{"from":255,"to":273}}}}],["943",{"pageContent":"import { Tool } from \"./base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/bingserpapi.ts","loc":{"lines":{"from":1,"to":1}}}}],["944",{"pageContent":"BingSerpAPI extends Tool {\nname = \"bing-search\";\n\ndescription =\n\"a search engine. useful for when you need to answer questions about current events. input should be a search query.\";\n\nkey: string;\n\nparams: Record<string, string>;\n\nconstructor(\napiKey: string | undefined = typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.BingApiKey\n: undefined,\nparams: Record<string, string> = {}\n) {\nsuper();\n\nif (!apiKey) {\nthrow new Error(\n\"BingSerpAPI API key not set. You can set it as BingApiKey in your .env file.\"\n);\n}\n\nthis.key = apiKey;\nthis.params = params;\n}\n\n/** @ignore */\nasync _call(input: string): Promise<string> {\nconst headers = { \"Ocp-Apim-Subscription-Key\": this.key };\nconst params = { q: input, textDecorations: \"true\", textFormat: \"HTML\" };\nconst searchUrl = new URL(\"https://api.bing.microsoft.com/v7.0/search\");\n\nObject.entries(params).forEach(([key, value]) => {\nsearchUrl.searchParams.append(key, value);\n});\n\nconst response = await fetch(searchUrl, { headers });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/bingserpapi.ts","loc":{"lines":{"from":62,"to":101}}}}],["945",{"pageContent":"if (!response.ok) {\nthrow new Error(`HTTP error ${response.status}`);\n}\n\nconst res = await response.json();\nconst results: [] = res.webPages.value;\n\nif (results.length === 0) {\nreturn \"No good results found.\";\n}\nconst snippets = results\n.map((result: { snippet: string }) => result.snippet)\n.join(\" \");\n\nreturn snippets;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/bingserpapi.ts","loc":{"lines":{"from":125,"to":141}}}}],["946",{"pageContent":"{ BingSerpAPI };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/bingserpapi.ts","loc":{"lines":{"from":189,"to":189}}}}],["947",{"pageContent":"import { Parser } from \"expr-eval\";\n\nimport { Tool } from \"./base.js\";\n\nexport class Calculator extends Tool {\nname = \"calculator\";\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nreturn Parser.evaluate(input).toString();\n} catch (error) {\nreturn \"I don't know how to do that.\";\n}\n}\n\ndescription = `Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/calculator.ts","loc":{"lines":{"from":1,"to":18}}}}],["948",{"pageContent":"import { DynamicTool, DynamicToolInput } from \"./dynamic.js\";\nimport { BaseChain } from \"../chains/base.js\";\n\nexport interface ChainToolInput extends Omit<DynamicToolInput, \"func\"> {\nchain: BaseChain;\n}\n\nexport class ChainTool extends DynamicTool {\nchain: BaseChain;\n\nconstructor({ chain, ...rest }: ChainToolInput) {\nsuper({\n...rest,\nfunc: async (input, runManager) =>\nchain.run(input, runManager?.getChild()),\n});\nthis.chain = chain;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/chain.ts","loc":{"lines":{"from":1,"to":19}}}}],["949",{"pageContent":"import { Tool } from \"./base.js\";\n\nclass DadJokeAPI extends Tool {\nname: string;\n\ndescription: string;\n\nconstructor() {\nsuper();\nthis.name = \"dadjoke\";\nthis.description =\n\"a dad joke generator. get a dad joke about a specific topic. input should be a search term.\";\n}\n\n/** @ignore */\nasync _call(input: string): Promise<string> {\nconst headers = { Accept: \"application/json\" };\nconst searchUrl = `https://icanhazdadjoke.com/search?term=${input}`;\n\nconst response = await fetch(searchUrl, { headers });\n\nif (!response.ok) {\nthrow new Error(`HTTP error ${response.status}`);\n}\n\nconst data = await response.json();\nconst jokes = data.results;\n\nif (jokes.length === 0) {\nreturn `No dad jokes found about ${input}`;\n}\n\nconst randomIndex = Math.floor(Math.random() * jokes.length);\nconst randomJoke = jokes[randomIndex].joke;\n\nreturn randomJoke;\n}\n}\n\nexport { DadJokeAPI };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/dadjokeapi.ts","loc":{"lines":{"from":1,"to":40}}}}],["950",{"pageContent":"import { CallbackManagerForToolRun, Callbacks } from \"../callbacks/manager.js\";\nimport { Tool } from \"./base.js\";\n\nexport interface DynamicToolInput {\nname: string;\ndescription: string;\nfunc: (\ninput: string,\nrunManager?: CallbackManagerForToolRun\n) => Promise<string>;\nreturnDirect?: boolean;\nverbose?: boolean;\ncallbacks?: Callbacks;\n}\n\n/**\n* A tool that can be created dynamically from a function, name, and description.\n*/\nexport class DynamicTool extends Tool {\nname: string;\n\ndescription: string;\n\nfunc: DynamicToolInput[\"func\"];\n\nconstructor(fields: DynamicToolInput) {\nsuper(fields.verbose, fields.callbacks);\nthis.name = fields.name;\nthis.description = fields.description;\nthis.func = fields.func;\nthis.returnDirect = fields.returnDirect ?? this.returnDirect;\n}\n\n/** @ignore */\nasync _call(\ninput: string,\nrunManager?: CallbackManagerForToolRun\n): Promise<string> {\nreturn this.func(input, runManager);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/dynamic.ts","loc":{"lines":{"from":1,"to":41}}}}],["951",{"pageContent":"import { z } from \"zod\";\nimport { BaseFileStore } from \"../schema/index.js\";\nimport { StructuredTool, ToolParams } from \"./base.js\";\n\ninterface ReadFileParams extends ToolParams {\nstore: BaseFileStore;\n}\n\nexport class ReadFileTool extends StructuredTool {\nschema = z.object({\nfile_path: z.string().describe(\"name of file\"),\n});\n\nname = \"read_file\";\n\ndescription = \"Read file from disk\";\n\nstore: BaseFileStore;\n\nconstructor({ store, ...rest }: ReadFileParams) {\nsuper(rest);\n\nthis.store = store;\n}\n\nasync _call({ file_path }: z.infer<typeof this.schema>) {\nreturn await this.store.readFile(file_path);\n}\n}\n\ninterface WriteFileParams extends ToolParams {\nstore: BaseFileStore;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/fs.ts","loc":{"lines":{"from":1,"to":33}}}}],["952",{"pageContent":"class WriteFileTool extends StructuredTool {\nschema = z.object({\nfile_path: z.string().describe(\"name of file\"),\ntext: z.string().describe(\"text to write to file\"),\n});\n\nname = \"write_file\";\n\ndescription = \"Write file from disk\";\n\nstore: BaseFileStore;\n\nconstructor({ store, ...rest }: WriteFileParams) {\nsuper(rest);\n\nthis.store = store;\n}\n\nasync _call({ file_path, text }: z.infer<typeof this.schema>) {\nawait this.store.writeFile(file_path, text);\nreturn \"File written to successfully.\";\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/fs.ts","loc":{"lines":{"from":59,"to":81}}}}],["953",{"pageContent":"/** From https://github.com/SidU/teams-langchain-js/wiki/Connecting-IFTTT-Services.\n\n# Creating a webhook\n- Go to https://ifttt.com/create\n\n# Configuring the \"If This\"\n- Click on the \"If This\" button in the IFTTT interface.\n- Search for \"Webhooks\" in the search bar.\n- Choose the first option for \"Receive a web request with a JSON payload.\"\n- Choose an Event Name that is specific to the service you plan to connect to.\nThis will make it easier for you to manage the webhook URL.\nFor example, if you're connecting to Spotify, you could use \"Spotify\" as your\nEvent Name.\n- Click the \"Create Trigger\" button to save your settings and create your webhook.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/IFTTTWebhook.ts","loc":{"lines":{"from":1,"to":14}}}}],["954",{"pageContent":"# Configuring the \"Then That\"\n- Tap on the \"Then That\" button in the IFTTT interface.\n- Search for the service you want to connect, such as Spotify.\n- Choose an action from the service, such as \"Add track to a playlist\".\n- Configure the action by specifying the necessary details, such as the playlist name,\ne.g., \"Songs from AI\".\n- Reference the JSON Payload received by the Webhook in your action. For the Spotify\nscenario, choose \"{{JsonPayload}}\" as your search query.\n- Tap the \"Create Action\" button to save your action settings.\n- Once you have finished configuring your action, click the \"Finish\" button to\ncomplete the setup.\n- Congratulations! You have successfully connected the Webhook to the desired\nservice, and you're ready to start receiving data and triggering actions ","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/IFTTTWebhook.ts","loc":{"lines":{"from":16,"to":28}}}}],["955",{"pageContent":"# Finishing up\n- To get your webhook URL go to https://ifttt.com/maker_webhooks/settings\n- Copy the IFTTT key value from there. The URL is of the form\nhttps://maker.ifttt.com/use/YOUR_IFTTT_KEY. Grab the YOUR_IFTTT_KEY value.\n*/\nimport { Tool } from \"./base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/IFTTTWebhook.ts","loc":{"lines":{"from":69,"to":74}}}}],["956",{"pageContent":"class IFTTTWebhook extends Tool {\nprivate url: string;\n\nname: string;\n\ndescription: string;\n\nconstructor(url: string, name: string, description: string) {\nsuper();\nthis.url = url;\nthis.name = name;\nthis.description = description;\n}\n\n/** @ignore */\nasync _call(input: string): Promise<string> {\nconst headers = { \"Content-Type\": \"application/json\" };\nconst body = JSON.stringify({ this: input });\n\nconst response = await fetch(this.url, {\nmethod: \"POST\",\nheaders,\nbody,\n});\n\nif (!response.ok) {\nthrow new Error(`HTTP error ${response.status}`);\n}\n\nconst result = await response.text();\nreturn result;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/IFTTTWebhook.ts","loc":{"lines":{"from":135,"to":167}}}}],["957",{"pageContent":"export { SerpAPI, SerpAPIParameters } from \"./serpapi.js\";\nexport { DadJokeAPI } from \"./dadjokeapi.js\";\nexport { BingSerpAPI } from \"./bingserpapi.js\";\nexport { Tool, ToolParams, StructuredTool } from \"./base.js\";\nexport { DynamicTool, DynamicToolInput } from \"./dynamic.js\";\nexport { IFTTTWebhook } from \"./IFTTTWebhook.js\";\nexport { ChainTool, ChainToolInput } from \"./chain.js\";\nexport {\nQuerySqlTool,\nInfoSqlTool,\nListTablesSqlTool,\nQueryCheckerTool,\n} from \"./sql.js\";\nexport {\nJsonSpec,\nJsonListKeysTool,\nJsonGetValueTool,\nJsonObject,\nJson,\n} from \"./json.js\";\nexport { RequestsGetTool, RequestsPostTool } from \"./requests.js\";\nexport { VectorStoreQATool } from \"./vectorstore.js\";\nexport {\nZapierNLARunAction,\nZapierNLAWrapper,\nZapiterNLAWrapperParams,\n} from \"./zapier.js\";\nexport { Serper, SerperParameters } from \"./serper.js\";\nexport { AIPluginTool } from \"./aiplugin.js\";\nexport { ReadFileTool, WriteFileTool } from \"./fs.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/index.ts","loc":{"lines":{"from":1,"to":30}}}}],["958",{"pageContent":"import jsonpointer from \"jsonpointer\";\nimport { Tool } from \"./base.js\";\n\nexport type Json =\n| string\n| number\n| boolean\n| null\n| { [key: string]: Json }\n| Json[];\n\nexport type JsonObject = { [key: string]: Json };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/json.ts","loc":{"lines":{"from":1,"to":12}}}}],["959",{"pageContent":"class JsonSpec {\nobj: JsonObject;\n\nmaxValueLength = 4000;\n\nconstructor(obj: JsonObject, max_value_length = 4000) {\nthis.obj = obj;\nthis.maxValueLength = max_value_length;\n}\n\npublic getKeys(input: string): string {\nconst pointer = jsonpointer.compile(input);\nconst res = pointer.get(this.obj) as Json;\nif (typeof res === \"object\" && !Array.isArray(res) && res !== null) {\nreturn Object.keys(res).join(\", \");\n}\n\nthrow new Error(\n`Value at ${input} is not a dictionary, get the value directly instead.`\n);\n}\n\npublic getValue(input: string): string {\nconst pointer = jsonpointer.compile(input);\nconst res = pointer.get(this.obj) as Json;\n\nif (res === null || res === undefined) {\nthrow new Error(`Value at ${input} is null or undefined.`);\n}\n\nconst str = typeof res === \"object\" ? JSON.stringify(res) : res.toString();\nif (","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/json.ts","loc":{"lines":{"from":100,"to":131}}}}],["960",{"pageContent":"of res === \"object\" &&\n!Array.isArray(res) &&\nstr.length > this.maxValueLength\n) {\nreturn `Value is a large dictionary, should explore its keys directly.`;\n}\n\nif (str.length > this.maxValueLength) {\nreturn `${str.slice(0, this.maxValueLength)}...`;\n}\nreturn str;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/json.ts","loc":{"lines":{"from":195,"to":207}}}}],["961",{"pageContent":"class JsonListKeysTool extends Tool {\nname = \"json_list_keys\";\n\nconstructor(public jsonSpec: JsonSpec) {\nsuper();\n}\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nreturn this.jsonSpec.getKeys(input);\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Can be used to list all keys at a given path. \nBefore calling this you should be SURE that the path to this exists.\nThe input is a text representation of the path to the json as json pointer syntax (e.g. /key1/0/key2).`;\n}\n\nexport class JsonGetValueTool extends Tool {\nname = \"json_get_value\";\n\nconstructor(public jsonSpec: JsonSpec) {\nsuper();\n}\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nreturn this.jsonSpec.getValue(input);\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Can be used to see value in string format at a given path.\nBefore calling this you should be SURE that the path to this exists.\nThe input is a text representation of the path to the json as json pointer syntax (e.g. /key1/0/key2).`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/json.ts","loc":{"lines":{"from":292,"to":332}}}}],["962",{"pageContent":"import { Tool } from \"./base.js\";\n\nexport interface Headers {\n[key: string]: string;\n}\n\nexport interface RequestTool {\nheaders: Headers;\nmaxOutputLength: number;\n}\n\nexport class RequestsGetTool extends Tool implements RequestTool {\nname = \"requests_get\";\n\nmaxOutputLength = 2000;\n\nconstructor(\npublic headers: Headers = {},\n{ maxOutputLength }: { maxOutputLength?: number } = {}\n) {\nsuper();\n\nthis.maxOutputLength = maxOutputLength ?? this.maxOutputLength;\n}\n\n/** @ignore */\nasync _call(input: string) {\nconst res = await fetch(input, {\nheaders: this.headers,\n});\nconst text = await res.text();\nreturn text.slice(0, this.maxOutputLength);\n}\n\ndescription = `A portal to the internet. Use this when you need to get specific content from a website. \nInput should be a  url (i.e. https://www.google.com). The output will be the text response of the GET request.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/requests.ts","loc":{"lines":{"from":1,"to":37}}}}],["963",{"pageContent":"class RequestsPostTool extends Tool implements RequestTool {\nname = \"requests_post\";\n\nmaxOutputLength = Infinity;\n\nconstructor(\npublic headers: Headers = {},\n{ maxOutputLength }: { maxOutputLength?: number } = {}\n) {\nsuper();\n\nthis.maxOutputLength = maxOutputLength ?? this.maxOutputLength;\n}\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nconst { url, data } = JSON.parse(input);\nconst res = await fetch(url, {\nmethod: \"POST\",\nheaders: this.headers,\nbody: JSON.stringify(data),\n});\nconst text = await res.text();\nreturn text.slice(0, this.maxOutputLength);\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Use this when you want to POST to a website.\nInput should be a json string with two keys: \"url\" and \"data\".\nThe value of \"url\" should be a string, and the value of \"data\" should be a dictionary of \nkey-value pairs you want to POST to the url as a JSON body.\nBe careful to always use double quotes for strings in the json string\nThe output will be the text response of the POST request.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/requests.ts","loc":{"lines":{"from":76,"to":112}}}}],["964",{"pageContent":"import { Tool } from \"./base.js\";\n\n/**\n* This does not use the `serpapi` package because it appears to cause issues\n* when used in `jest` tests. Part of the issue seems to be that the `serpapi`\n* package imports a wasm module to use instead of native `fetch`, which we\n* don't want anyway.\n*\n* NOTE: you must provide location, gl and hl or your region and language will\n* may not match your location, and will not be deterministic.\n*/\n\n// Copied over from `serpapi` package","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":1,"to":13}}}}],["965",{"pageContent":"BaseParameters {\n/**\n* Parameter defines the device to use to get the results. It can be set to\n* `desktop` (default) to use a regular browser, `tablet` to use a tablet browser\n* (currently using iPads), or `mobile` to use a mobile browser (currently\n* using iPhones).\n*/\ndevice?: \"desktop\" | \"tablet\" | \"mobile\";\n/**\n* Parameter will force SerpApi to fetch the Google results even if a cached\n* version is already present. A cache is served only if the query and all\n* parameters are exactly the same. Cache expires after 1h. Cached searches\n* are free, and are not counted towards your searches per month. It can be set\n* to `false` (default) to allow results from the cache, or `true` to disallow\n* results from the cache. `no_cache` and `async` parameters should not be used together.\n*/\nno_cache?: boolean;\n/**\n* Specify the client-side timeout of the request. In milliseconds.\n*/\ntimeout?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":386,"to":407}}}}],["966",{"pageContent":"interface SerpAPIParameters extends BaseParameters {\n/**\n* Search Query\n* Parameter defines the query you want to search. You can use anything that you\n* would use in a regular Google search. e.g. `inurl:`, `site:`, `intitle:`. We\n* also support advanced search query parameters such as as_dt and as_eq. See the\n* [full list](https://serpapi.com/advanced-google-query-parameters) of supported\n* advanced search query parameters.\n*/\nq: string;\n/**\n* Location\n* Parameter defines from where you want the search to originate. If several\n* locations match the location requested, we'll pick the most popular one. Head to\n* [/locations.json API](https://serpapi.com/locations-api) if you need more\n* precise control. location and uule parameters can't be used together. Avoid\n* utilizing location when setting the location outside the U.S. when using Google\n* Shopping and/or Google Product API.\n*/\nlocation?: string;\n/**\n* Encoded Location\n* Parameter is the Google encoded location you want to use for the search. uule","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":770,"to":792}}}}],["967",{"pageContent":"* and location parameters can't be used together.\n*/\nuule?: string;\n/**\n* Google Place ID\n* Parameter defines the id (`CID`) of the Google My Business listing you want to\n* scrape. Also known as Google Place ID.\n*/\nludocid?: string;\n/**\n* Additional Google Place ID\n* Parameter that you might have to use to force the knowledge graph map view to\n* show up. You can find the lsig ID by using our [Local Pack\n* API](https://serpapi.com/local-pack) or [Places Results\n* API](https://serpapi.com/places-results).\n* lsig ID is also available via a redirect Google uses within [Google My\n* Business](https://www.google.com/business/).\n*/\nlsig?: string;\n/**\n* Google Knowledge Graph ID\n* Parameter defines the id (`KGMID`) of the Google Knowledge Graph listing you\n* want to scrape. Also known as Google Knowledge Graph ID. Searches with kgmid\n* parameter will return results for the originally encrypted search parameters.\n* For some searches, kgmid may override all other parameters except start, and num\n* parameters.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":1153,"to":1179}}}}],["968",{"pageContent":"kgmid?: string;\n/**\n* Google Cached Search Parameters ID\n* Parameter defines the cached search parameters of the Google Search you want to\n* scrape. Searches with si parameter will return results for the originally\n* encrypted search parameters. For some searches, si may override all other\n* parameters except start, and num parameters. si can be used to scrape Google\n* Knowledge Graph Tabs.\n*/\nsi?: string;\n/**\n* Domain\n* Parameter defines the Google domain to use. It defaults to `google.com`. Head to\n* the [Google domains page](https://serpapi.com/google-domains) for a full list of\n* supported Google domains.\n*/\ngoogle_domain?: string;\n/**\n* Country\n* Parameter defines the country to use for the Google search. It's a two-letter\n* country code. (e.g., `us` for the United States, `uk` for United Kingdom, or\n* `fr` for France). Head to the [Google countries\n* page](https://serpapi.com/google-countries) for a full list of supported Google\n* countries.\n*/\ngl?: string;\n/**\n* Language","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":1540,"to":1567}}}}],["969",{"pageContent":"* Parameter defines the language to use for the Google search. It's a two-letter\n* language code. (e.g., `en` for English, `es` for Spanish, or `fr` for French).\n* Head to the [Google languages page](https://serpapi.com/google-languages) for a\n* full list of supported Google languages.\n*/\nhl?: string;\n/**\n* Set Multiple Languages\n* Parameter defines one or multiple languages to limit the search to. It uses\n* `lang_{two-letter language code}` to specify languages and `|` as a delimiter.\n* (e.g., `lang_fr|lang_de` will only search French and German pages). Head to the\n* [Google lr languages page](https://serpapi.com/google-lr-languages) for a full\n* list of supported languages.\n*/\nlr?: string;\n/**\n* as_dt\n* Parameter controls whether to include or exclude results from the site named in\n* the as_sitesearch parameter.\n*/\nas_dt?: string;\n/**\n* as_epq\n* Parameter identifies a phrase that all documents in the search results must\n* contain. You can also use the [phrase","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":1928,"to":1952}}}}],["970",{"pageContent":"* search](https://developers.google.com/custom-search/docs/xml_results#PhraseSearchqt)\n* query term to search for a phrase.\n*/\nas_epq?: string;\n/**\n* as_eq\n* Parameter identifies a word or phrase that should not appear in any documents in\n* the search results. You can also use the [exclude\n* query](https://developers.google.com/custom-search/docs/xml_results#Excludeqt)\n* term to ensure that a particular word or phrase will not appear in the documents\n* in a set of search results.\n*/\nas_eq?: string;\n/**\n* as_lq\n* Parameter specifies that all search results should contain a link to a\n* particular URL. You can also use the\n* [link:](https://developers.google.com/custom-search/docs/xml_results#BackLinksqt)\n* query term for this type of query.\n*/\nas_lq?: string;\n/**\n* as_nlo\n* Parameter specifies the starting value for a search range. Use as_nlo and as_nhi\n* to append an inclusive search range.\n*/\nas_nlo?: string;\n/**\n* as_nhi\n* Parameter specifies the ending value for a search range. Use as_nlo and as_nhi","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":2314,"to":2343}}}}],["971",{"pageContent":"* to append an inclusive search range.\n*/\nas_nhi?: string;\n/**\n* as_oq\n* Parameter provides additional search terms to check for in a document, where\n* each document in the search results must contain at least one of the additional\n* search terms. You can also use the [Boolean\n* OR](https://developers.google.com/custom-search/docs/xml_results#BooleanOrqt)\n* query term for this type of query.\n*/\nas_oq?: string;\n/**\n* as_q\n* Parameter provides search terms to check for in a document. This parameter is\n* also commonly used to allow users to specify additional terms to search for\n* within a set of search results.\n*/\nas_q?: string;\n/**\n* as_qdr\n* Parameter requests search results from a specified time period (quick date\n* range). The following values are supported:\n* `d[number]`: requests results from the specified number of past days. Example\n* for the past 10 days: `as_qdr=d10`\n* `w[number]`: requests results from the specified number of past weeks.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":2704,"to":2729}}}}],["972",{"pageContent":"* `m[number]`: requests results from the specified number of past months.\n* `y[number]`: requests results from the specified number of past years. Example\n* for the past year: `as_qdr=y`\n*/\nas_qdr?: string;\n/**\n* as_rq\n* Parameter specifies that all search results should be pages that are related to\n* the specified URL. The parameter value should be a URL. You can also use the\n* [related:](https://developers.google.com/custom-search/docs/xml_results#RelatedLinksqt)\n* query term for this type of query.\n*/\nas_rq?: string;\n/**\n* as_sitesearch\n* Parameter allows you to specify that all search results should be pages from a\n* given site. By setting the as_dt parameter, you can also use it to exclude pages\n* from a given site from your search resutls.\n*/\nas_sitesearch?: string;\n/**\n* Advanced Search Parameters\n* (to be searched) parameter defines advanced search parameters that aren't\n* possible in the regular query field. (e.g., advanced search for patents, dates,\n* news, videos, images, apps, or text contents).","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":3091,"to":3115}}}}],["973",{"pageContent":"*/\ntbs?: string;\n/**\n* Adult Content Filtering\n* Parameter defines the level of filtering for adult content. It can be set to\n* `active`, or `off` (default).\n*/\nsafe?: string;\n/**\n* Exclude Auto-corrected Results\n* Parameter defines the exclusion of results from an auto-corrected query that is\n* spelled wrong. It can be set to `1` to exclude these results, or `0` to include\n* them (default).\n*/\nnfpr?: string;\n/**\n* Results Filtering\n* Parameter defines if the filters for 'Similar Results' and 'Omitted Results' are\n* on or off. It can be set to `1` (default) to enable these filters, or `0` to\n* disable these filters.\n*/\nfilter?: string;\n/**\n* Search Type\n* (to be matched) parameter defines the type of search you want to do.\n* It can be set to:\n* `(no tbm parameter)`: regular Google Search,\n* `isch`: [Google Images API](https://serpapi.com/images-results),\n* `lcl` - [Google Local API](https://serpapi.com/local-results)\n* `vid`: [Google Videos API](https://serpapi.com/videos-results),","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":3476,"to":3505}}}}],["974",{"pageContent":"* `nws`: [Google News API](https://serpapi.com/news-results),\n* `shop`: [Google Shopping API](https://serpapi.com/shopping-results),\n* or any other Google service.\n*/\ntbm?: string;\n/**\n* Result Offset\n* Parameter defines the result offset. It skips the given number of results. It's\n* used for pagination. (e.g., `0` (default) is the first page of results, `10` is\n* the 2nd page of results, `20` is the 3rd page of results, etc.).\n* Google Local Results only accepts multiples of `20`(e.g. `20` for the second\n* page results, `40` for the third page results, etc.) as the start value.\n*/\nstart?: number;\n/**\n* Number of Results\n* Parameter defines the maximum number of results to return. (e.g., `10` (default)\n* returns 10 results, `40` returns 40 results, and `100` returns 100 results).\n*/\nnum?: string;\n/**\n* Page Number (images)\n* Parameter defines the page number for [Google\n* Images](https://serpapi.com/images-results). There are 100 images per page. This","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":3866,"to":3889}}}}],["975",{"pageContent":"* parameter is equivalent to start (offset) = ijn * 100. This parameter works only\n* for [Google Images](https://serpapi.com/images-results) (set tbm to `isch`).\n*/\nijn?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":4251,"to":4255}}}}],["976",{"pageContent":"UrlParameters = Record<\nstring,\nstring | number | boolean | undefined | null\n>;\n\n/**\n* Wrapper around SerpAPI.\n*\n* To use, you should have the `serpapi` package installed and the SERPAPI_API_KEY environment variable set.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":4636,"to":4645}}}}],["977",{"pageContent":"class SerpAPI extends Tool {\nprotected key: string;\n\nprotected params: Partial<SerpAPIParameters>;\n\nprotected baseUrl: string;\n\nconstructor(\napiKey: string | undefined = typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.SERPAPI_API_KEY\n: undefined,\nparams: Partial<SerpAPIParameters> = {},\nbaseUrl = \"https://serpapi.com\"\n) {\nsuper();\n\nif (!apiKey) {\nthrow new Error(\n\"SerpAPI API key not set. You can set it as SERPAPI_API_KEY in your .env file, or pass it to SerpAPI.\"\n);\n}\n\nthis.key = apiKey;\nthis.params = params;\nthis.baseUrl = baseUrl;\n}\n\nname = \"search\";\n\nprotected buildUrl<P extends UrlParameters>(\npath: string,\nparameters: P,\nbaseUrl: string\n): string {\nconst nonUndefinedParams: [string, string][] = Object.entries(parameters)\n.filter(([_, value]) => value !== undefined)\n.map(([key, value]) => [key, `${value}`]);\nconst searchParams = new URLSearchParams(nonUndefinedParams);\nreturn `${baseUrl}/${path}?${searchParams}`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":5025,"to":5065}}}}],["978",{"pageContent":"/** @ignore */\nasync _call(input: string) {\nconst { timeout, ...params } = this.params;\nconst resp = await fetch(\nthis.buildUrl(\n\"search\",\n{\n...params,\napi_key: this.key,\nq: input,\n},\nthis.baseUrl\n),\n{\nsignal: timeout ? AbortSignal.timeout(timeout) : undefined,\n}\n);\n\nconst res = await resp.json();\n\nif (res.error) {\nthrow new Error(`Got error from serpAPI: ${res.error}`);\n}\n\nif (res.answer_box?.answer) {\nreturn res.answer_box.answer;\n}\n\nif (res.answer_box?.snippet) {\nreturn res.answer_box.snippet;\n}\n\nif (res.answer_box?.snippet_highlighted_words) {\nreturn res.answer_box.snippet_highlighted_words[0];\n}\n\nif (res.sports_results?.game_spotlight) {\nreturn res.sports_results.game_spotlight;\n}\n\nif (res.knowledge_graph?.description) {\nreturn res.knowledge_graph.description;\n}\n\nif (res.organic_results?.[0]?.snippet) {\nreturn res.organic_results[0].snippet;\n}\n\nreturn \"No good search result found\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":5427,"to":5476}}}}],["979",{"pageContent":"description =\n\"a search engine. useful for when you need to answer questions about current events. input should be a search query.\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":5839,"to":5841}}}}],["980",{"pageContent":"import { Tool } from \"./base.js\";\n\nexport type SerperParameters = {\ngl?: string;\nhl?: string;\n};\n\n/**\n* Wrapper around serper.\n*\n* You can create a free API key at https://serper.dev.\n*\n* To use, you should have the SERPER_API_KEY environment variable set.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serper.ts","loc":{"lines":{"from":1,"to":14}}}}],["981",{"pageContent":"class Serper extends Tool {\nprotected key: string;\n\nprotected params: Partial<SerperParameters>;\n\nconstructor(\napiKey: string | undefined = typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.SERPER_API_KEY\n: undefined,\nparams: Partial<SerperParameters> = {}\n) {\nsuper();\n\nif (!apiKey) {\nthrow new Error(\n\"Serper API key not set. You can set it as SERPER_API_KEY in your .env file, or pass it to Serper.\"\n);\n}\n\nthis.key = apiKey;\nthis.params = params;\n}\n\nname = \"search\";\n\n/** @ignore */\nasync _call(input: string) {\nconst options = {\nmethod: \"POST\",\nheaders: {\n\"X-API-KEY\": this.key,\n\"Content-Type\": \"application/json\",\n},\nbody: JSON.stringify({\nq: input,\n...this.params,\n}),\n};\n\nconst res = await fetch(\"https://google.serper.dev/search\", options);\n\nif (!res.ok) {\nthrow new Error(`Got ${res.status} error from serper: ${res.statusText}`);\n}\n\nconst json = await res.json();\n\nif (json.answerBox?.answer) {\nreturn json.answerBox.answer;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serper.ts","loc":{"lines":{"from":93,"to":143}}}}],["982",{"pageContent":"if (json.answerBox?.snippet) {\nreturn json.answerBox.snippet;\n}\n\nif (json.answerBox?.snippet_highlighted_words) {\nreturn json.answerBox.snippet_highlighted_words[0];\n}\n\nif (json.sportsResults?.game_spotlight) {\nreturn json.sportsResults.game_spotlight;\n}\n\nif (json.knowledgeGraph?.description) {\nreturn json.knowledgeGraph.description;\n}\n\nif (json.organic?.[0]?.snippet) {\nreturn json.organic[0].snippet;\n}\n\nreturn \"No good search result found\";\n}\n\ndescription =\n\"a search engine. useful for when you need to answer questions about current events. input should be a search query.\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serper.ts","loc":{"lines":{"from":190,"to":215}}}}],["983",{"pageContent":"import { Tool } from \"./base.js\";\nimport { OpenAI } from \"../llms/openai.js\";\nimport { LLMChain } from \"../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../prompts/prompt.js\";\nimport type { SqlDatabase } from \"../sql_db.js\";\nimport { SqlTable } from \"../util/sql_utils.js\";\n\ninterface SqlTool {\ndb: SqlDatabase;\n}\n\nexport class QuerySqlTool extends Tool implements SqlTool {\nname = \"query-sql\";\n\ndb: SqlDatabase;\n\nconstructor(db: SqlDatabase) {\nsuper();\nthis.db = db;\n}\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nreturn await this.db.run(input);\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Input to this tool is a detailed and correct SQL query, output is a result from the database.\nIf the query is not correct, an error message will be returned. \nIf an error is returned, rewrite the query, check the query, and try again.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/sql.ts","loc":{"lines":{"from":1,"to":34}}}}],["984",{"pageContent":"class InfoSqlTool extends Tool implements SqlTool {\nname = \"info-sql\";\n\ndb: SqlDatabase;\n\nconstructor(db: SqlDatabase) {\nsuper();\nthis.db = db;\n}\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nconst tables = input.split(\",\").map((table) => table.trim());\nreturn await this.db.getTableInfo(tables);\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables.\nBe sure that the tables actually exist by calling list-tables-sql first!\n\nExample Input: \"table1, table2, table3.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/sql.ts","loc":{"lines":{"from":128,"to":152}}}}],["985",{"pageContent":"class ListTablesSqlTool extends Tool implements SqlTool {\nname = \"list-tables-sql\";\n\ndb: SqlDatabase;\n\nconstructor(db: SqlDatabase) {\nsuper();\nthis.db = db;\n}\n\n/** @ignore */\nasync _call(_: string) {\ntry {\nconst tables = this.db.allTables.map(\n(table: SqlTable) => table.tableName\n);\nreturn tables.join(\", \");\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Input is an empty string, output is a comma separated list of tables in the database.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/sql.ts","loc":{"lines":{"from":254,"to":277}}}}],["986",{"pageContent":"class QueryCheckerTool extends Tool {\nname = \"query-checker\";\n\ntemplate = `\n{query}\nDouble check the sqlite query above for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\n\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.`;\n\nllmChain: LLMChain;\n\nconstructor(llmChain?: LLMChain) {\nsuper();\nif (llmChain) {\nthis.llmChain = llmChain;\n} else {\nconst model = new OpenAI({ temperature: 0 });\nconst prompt = new PromptTemplate({\ntemplate: this.template,\ninputVariables: [\"query\"],\n});\nthis.llmChain = new LLMChain({ llm: model, prompt });\n}\n}\n\n/** @ignore */\nasync _call(input: string) {\nreturn this.llmChain.predict({ query: input });\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/sql.ts","loc":{"lines":{"from":387,"to":423}}}}],["987",{"pageContent":"description = `Use this tool to double check if your query is correct before executing it.\nAlways use this tool before executing a query with query-sql!`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/sql.ts","loc":{"lines":{"from":512,"to":514}}}}],["988",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { AIPluginTool } from \"../aiplugin.js\";\n\ntest(\"AIPluginTool\", async () => {\nconst tool = await AIPluginTool.fromPluginUrl(\n\"https://www.klarna.com/.well-known/ai-plugin.json\"\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":1,"to":7}}}}],["989",{"pageContent":"expect(await tool.call(undefined)).toMatchInlineSnapshot(`","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":10,"to":10}}}}],["990",{"pageContent":"\"Usage Guide: Assistant uses the Klarna plugin to get relevant product suggestions for any shopping or product discovery purpose. Assistant will reply with the following 3 paragraphs 1) Search Results 2) Product Comparison of the Search Results 3) Followup Questions. The first paragraph contains a list of the products with their attributes listed clearly and concisely as bullet points under the product, together with a link to the product and an explanation. Links will always be returned and should be shown to the user. The second paragraph compares the results returned in a summary sentence starting with \"In summary\". Assistant comparisons consider only the most important features of the products that will help them fit the users request, and each product mention is brief, short and concise. In the third paragraph assistant always asks helpful follow-up questions and end with a question mark. When assistant is asking a follow-up question, it uses it's product expertise to provide information pertaining to the subject of the user's request that may guide them in their search for the right product.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":11,"to":11}}}}],["991",{"pageContent":"OpenAPI Spec in JSON or YAML format:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":13,"to":13}}}}],["992",{"pageContent":"{\"openapi\":\"3.0.1\",\"info\":{\"version\":\"v0\",\"title\":\"Open AI Klarna product Api\"},\"servers\":[{\"url\":\"https://www.klarna.com/us/shopping\"}],\"tags\":[{\"name\":\"open-ai-product-endpoint\",\"description\":\"Open AI Product Endpoint. Query for products.\"}],\"paths\":{\"/public/openai/v0/products\":{\"get\":{\"tags\":[\"open-ai-product-endpoint\"],\"summary\":\"API for fetching Klarna product information\",\"operationId\":\"productsUsingGET\",\"parameters\":[{\"name\":\"q\",\"in\":\"query\",\"description\":\"A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started.\",\"required\":true,\"schema\":{\"type\":\"string\"}},{\"name\":\"size\",\"in\":\"query\",\"description\":\"number of products","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":14,"to":14}}}}],["993",{"pageContent":"returned\",\"required\":false,\"schema\":{\"type\":\"integer\"}},{\"name\":\"min_price\",\"in\":\"query\",\"description\":\"(Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\",\"required\":false,\"schema\":{\"type\":\"integer\"}},{\"name\":\"max_price\",\"in\":\"query\",\"description\":\"(Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\",\"required\":false,\"schema\":{\"type\":\"integer\"}}],\"responses\":{\"200\":{\"description\":\"Products found\",\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/ProductResponse\"}}}},\"503\":{\"description\":\"one or more services are","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":14,"to":14}}}}],["994",{"pageContent":"unavailable\"}},\"deprecated\":false}}},\"components\":{\"schemas\":{\"Product\":{\"type\":\"object\",\"properties\":{\"attributes\":{\"type\":\"array\",\"items\":{\"type\":\"string\"}},\"name\":{\"type\":\"string\"},\"price\":{\"type\":\"string\"},\"url\":{\"type\":\"string\"}},\"title\":\"Product\"},\"ProductResponse\":{\"type\":\"object\",\"properties\":{\"products\":{\"type\":\"array\",\"items\":{\"$ref\":\"#/components/schemas/Product\"}}},\"title\":\"ProductResponse\"}}}}\"","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":14,"to":14}}}}],["995",{"pageContent":"`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":15,"to":15}}}}],["996",{"pageContent":"expect(await tool.call({})).toMatch(/Usage Guide/);\n\nexpect(await tool.call(\"\")).toMatch(/OpenAPI Spec/);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":20,"to":23}}}}],["997",{"pageContent":"import { test, expect, jest } from \"@jest/globals\";\n\nimport { ChainTool } from \"../chain.js\";\nimport { LLMChain } from \"../../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\nimport { LLM } from \"../../llms/base.js\";\nimport { VectorDBQAChain } from \"../../chains/vector_db_qa.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["998",{"pageContent":"FakeLLM extends LLM {\n_llmType() {\nreturn \"fake\";\n}\n\nasync _call(prompt: string): Promise<string> {\nreturn prompt;\n}\n}\n\ntest(\"chain tool with llm chain and local callback\", async () => {\nconst calls: string[] = [];\nconst handleToolStart = jest.fn(() => {\ncalls.push(\"tool start\");\n});\nconst handleToolEnd = jest.fn(() => {\ncalls.push(\"tool end\");\n});\nconst handleLLMStart = jest.fn(() => {\ncalls.push(\"llm start\");\n});\nconst handleLLMEnd = jest.fn(() => {\ncalls.push(\"llm end\");\n});\nconst handleChainStart = jest.fn(() => {\ncalls.push(\"chain start\");\n});\nconst handleChainEnd = jest.fn(() => {\ncalls.push(\"chain end\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":140,"to":169}}}}],["999",{"pageContent":"const chain = new LLMChain({\nllm: new FakeLLM({}),\nprompt: PromptTemplate.fromTemplate(\"hello world\"),\n});\nconst tool = new ChainTool({ chain, name: \"fake\", description: \"fake\" });\nconst result = await tool.call(\"hi\", [\n{\nhandleToolStart,\nhandleToolEnd,\nhandleLLMStart,\nhandleLLMEnd,\nhandleChainStart,\nhandleChainEnd,\n},\n]);\nexpect(result).toMatchInlineSnapshot(`\"hello world\"`);\nexpect(handleToolStart).toBeCalledTimes(1);\nexpect(handleToolEnd).toBeCalledTimes(1);\nexpect(handleLLMStart).toBeCalledTimes(1);\nexpect(handleLLMEnd).toBeCalledTimes(1);\nexpect(handleChainStart).toBeCalledTimes(1);\nexpect(handleChainEnd).toBeCalledTimes(1);\nexpect(calls).toMatchInlineSnapshot(`\n[\n\"tool start\",\n\"chain start\",\n\"llm start\",\n\"llm end\",\n\"chain end\",\n\"tool end\",\n]\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":288,"to":320}}}}],["1000",{"pageContent":"test(\"chain tool with vectordbqa chain\", async () => {\nconst calls: string[] = [];\nconst handleToolStart = jest.fn(() => {\ncalls.push(\"tool start\");\n});\nconst handleToolEnd = jest.fn(() => {\ncalls.push(\"tool end\");\n});\nconst handleLLMStart = jest.fn(() => {\ncalls.push(\"llm start\");\n});\nconst handleLLMEnd = jest.fn(() => {\ncalls.push(\"llm end\");\n});\nconst handleChainStart = jest.fn(() => {\ncalls.push(\"chain start\");\n});\nconst handleChainEnd = jest.fn(() => {\ncalls.push(\"chain end\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":434,"to":453}}}}],["1001",{"pageContent":"const chain = VectorDBQAChain.fromLLM(\nnew FakeLLM({}),\nawait MemoryVectorStore.fromExistingIndex(new FakeEmbeddings())\n);\nconst tool = new ChainTool({ chain, name: \"fake\", description: \"fake\" });\nconst result = await tool.call(\"hi\", [\n{\nhandleToolStart,\nhandleToolEnd,\nhandleLLMStart,\nhandleLLMEnd,\nhandleChainStart,\nhandleChainEnd,\n},\n]);\nexpect(result).toMatchInlineSnapshot(`\n\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":580,"to":596}}}}],["1002",{"pageContent":"Question: hi\nHelpful Answer:\"\n`);\nexpect(handleToolStart).toBeCalledTimes(1);\nexpect(handleToolEnd).toBeCalledTimes(1);\nexpect(handleLLMStart).toBeCalledTimes(1);\nexpect(handleLLMEnd).toBeCalledTimes(1);\nexpect(handleChainStart).toBeCalledTimes(3);\nexpect(handleChainEnd).toBeCalledTimes(3);\nexpect(calls).toMatchInlineSnapshot(`\n[\n\"tool start\",\n\"chain start\",\n\"chain start\",\n\"chain start\",\n\"llm start\",\n\"llm end\",\n\"chain end\",\n\"chain end\",\n\"chain end\",\n\"tool end\",\n]\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":720,"to":743}}}}],["1003",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { SerpAPI } from \"../../tools/serpapi.js\";\n\ndescribe(\"serp api test suite\", () => {\nclass SerpApiUrlTester extends SerpAPI {\ntestThisUrl(): string {\nreturn this.buildUrl(\"search\", this.params, this.baseUrl);\n}\n}\n\ntest(\"Test default url\", async () => {\nconst serpApi = new SerpApiUrlTester(\n\"Not a real key but constructor error if not set\",\n{\nhl: \"en\",\ngl: \"us\",\n}\n);\nexpect(serpApi.testThisUrl()).toEqual(\n\"https://serpapi.com/search?hl=en&gl=us\"\n);\n});\n\ntest(\"Test override url\", async () => {\nconst serpApiProxied = new SerpApiUrlTester(\n\"Not a real key but constructor error if not set\",\n{\ngl: \"us\",\n},\n\"https://totallyProxied.com\"\n);\n\nexpect(\nserpApiProxied.testThisUrl() === \"https://totallyProxied.com/search?gl=us\"\n);\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/serpapi.test.ts","loc":{"lines":{"from":1,"to":37}}}}],["1004",{"pageContent":"import { test, expect, describe } from \"@jest/globals\";\nimport { WebBrowser } from \"../webbrowser.js\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport fetchAdapter from \"../../util/axios-fetch-adapter.js\";\n\ndescribe(\"webbrowser Test suite\", () => {\ntest(\"get word of the day\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://www.merriam-webster.com/word-of-the-day\",\"word of the day\"`\n);\n\nexpect(result).toContain(\"Word of the Day:\");\n});\n\ntest(\"get a summary of the page when empty request with fetch adapter\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.int.test.ts","loc":{"lines":{"from":1,"to":22}}}}],["1005",{"pageContent":"const browser = new WebBrowser({\nmodel,\nembeddings,\naxiosConfig: {\nadapter: fetchAdapter,\n},\n});\nconst result = await browser.call(\n`\"https://www.merriam-webster.com/word-of-the-day\",\"\"`\n);\n\n// fuzzy, sometimes its capped and others not\nexpect(result).toMatch(/word of the day/i);\n});\n\ntest(\"error no url\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(`\"\",\"\"`);\n\nexpect(result).toEqual(\"TypeError [ERR_INVALID_URL]: Invalid URL\");\n});\n\ntest(\"error no protocol or malformed\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"www.merriam-webster.com/word-of-the-day\",\"word of the day\"`\n);\n\nexpect(result).toEqual(\"TypeError [ERR_INVALID_URL]: Invalid URL\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.int.test.ts","loc":{"lines":{"from":124,"to":159}}}}],["1006",{"pageContent":"test(\"error bad site\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://www.hDjRBKoAD0EIbF29TWM4rbXDGGM5Nhy4uzNEAdDS.com\",\"word of the day\"`\n);\n\nexpect(result).toEqual(\n\"Error: getaddrinfo ENOTFOUND www.hdjrbkoad0eibf29twm4rbxdggm5nhy4uzneadds.com\"\n);\n});\n\ntest(\"get a summary of a page that detects scraping\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://www.musicgateway.com/spotify-pre-save\",\"\"`\n);\n\nexpect(result).not.toEqual(\"Error: http response 403\");\n});\n\n// cant we figure the headers to fix this?\ntest.skip(\"get a summary of a page that detects scraping 2\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.int.test.ts","loc":{"lines":{"from":257,"to":286}}}}],["1007",{"pageContent":"const browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://parade.com/991228/marynliles/couples-goals\",\"\"`\n);\nexpect(result).not.toEqual(\"Error: http response 403\");\n});\n\ntest(\"get a summary of a page that rejects unauthorized\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://firstround.com/review/how-to-fix-the-co-founder-fights-youre-sick-of-having-lessons-from-couples-therapist-esther-perel\",\"\"`\n);\n\nexpect(result).toContain(\"Esther Perel\");\n});\n\n// other urls that have done this too\n// \"https://wsimag.com/economy-and-politics/15473-power-and-money\",\n// \"https://thriveglobal.com/stories/sleep-what-to-do-what-not-to-do\",\ntest(\"get a summary of a page that redirects too many times\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.int.test.ts","loc":{"lines":{"from":382,"to":406}}}}],["1008",{"pageContent":"const browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://www.healtheuropa.eu/why-mdma-must-be-reclassified-as-a-schedule-2-drug/95780\",\"\"`\n);\nexpect(result).toContain(\"Beckley Foundation\");\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.int.test.ts","loc":{"lines":{"from":502,"to":508}}}}],["1009",{"pageContent":"import { test, expect, describe } from \"@jest/globals\";\nimport { readFileSync } from \"fs\";\nimport { getText, parseInputs } from \"../webbrowser.js\";\n\ndescribe(\"webbrowser Test suite\", () => {\nconst html = readFileSync(\"./src/tools/fixtures/wordoftheday.html\", \"utf8\");\n\ntest(\"parse html to text and links\", async () => {\nconst baseUrl = \"https://www.merriam-webster.com/word-of-the-day\";\nconst text = getText(html, baseUrl, false);\nexpect(text).toContain(\"Word of the Day: Foible\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.test.ts","loc":{"lines":{"from":1,"to":12}}}}],["1010",{"pageContent":"test(\"parseInputs\", () => {\nexpect(\nparseInputs(`\"https://www.merriam-webster.com/word-of-the-day\",\"\"`)\n).toEqual([\"https://www.merriam-webster.com/word-of-the-day\", \"\"]);\nexpect(\nparseInputs(\n`\"https://www.merriam-webster.com/word-of-the-day\",\"word of the day\"`\n)\n).toEqual([\n\"https://www.merriam-webster.com/word-of-the-day\",\n\"word of the day\",\n]);\nexpect(\nparseInputs(`\"https://www.merriam-webster.com/word-of-the-day\",\"`)\n).toEqual([\"https://www.merriam-webster.com/word-of-the-day\", \"\"]);\nexpect(\nparseInputs(`\"https://www.merriam-webster.com/word-of-the-day\",`)\n).toEqual([\"https://www.merriam-webster.com/word-of-the-day\", \"\"]);\nexpect(\nparseInputs(`\"https://www.merriam-webster.com/word-of-the-day\"`)\n).toEqual([\"https://www.merriam-webster.com/word-of-the-day\", undefined]);\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.test.ts","loc":{"lines":{"from":37,"to":59}}}}],["1011",{"pageContent":"import { VectorStore } from \"../vectorstores/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { VectorDBQAChain } from \"../chains/vector_db_qa.js\";\nimport { Tool } from \"./base.js\";\n\ninterface VectorStoreTool {\nvectorStore: VectorStore;\nllm: BaseLanguageModel;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/vectorstore.ts","loc":{"lines":{"from":1,"to":9}}}}],["1012",{"pageContent":"class VectorStoreQATool extends Tool implements VectorStoreTool {\nvectorStore: VectorStore;\n\nllm: BaseLanguageModel;\n\nname: string;\n\ndescription: string;\n\nchain: VectorDBQAChain;\n\nconstructor(name: string, description: string, fields: VectorStoreTool) {\nsuper();\nthis.name = name;\nthis.description = description;\nthis.vectorStore = fields.vectorStore;\nthis.llm = fields.llm;\nthis.chain = VectorDBQAChain.fromLLM(this.llm, this.vectorStore);\n}\n\nstatic getDescription(name: string, description: string): string {\nreturn `Useful for when you need to answer questions about ${name}. Whenever you need information about ${description} you should ALWAYS use this. Input should be a fully formed question.`;\n}\n\n/** @ignore */\nasync _call(input: string) {\nreturn this.chain.run(input);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/vectorstore.ts","loc":{"lines":{"from":40,"to":68}}}}],["1013",{"pageContent":"import axiosMod, { AxiosRequestConfig, AxiosStatic } from \"axios\";\nimport { isNode } from \"browser-or-node\";\nimport * as cheerio from \"cheerio\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { RecursiveCharacterTextSplitter } from \"../text_splitter.js\";\nimport { MemoryVectorStore } from \"../vectorstores/memory.js\";\nimport { StringPromptValue } from \"../prompts/base.js\";\nimport { Document } from \"../document.js\";\nimport { Tool, ToolParams } from \"./base.js\";\nimport {\nCallbackManager,\nCallbackManagerForToolRun,\n} from \"../callbacks/manager.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1,"to":15}}}}],["1014",{"pageContent":"const parseInputs = (inputs: string): [string, string] => {\nconst [baseUrl, task] = inputs.split(\",\").map((input) => {\nlet t = input.trim();\nt = t.startsWith('\"') ? t.slice(1) : t;\nt = t.endsWith('\"') ? t.slice(0, -1) : t;\n// it likes to put / at the end of urls, wont matter for task\nt = t.endsWith(\"/\") ? t.slice(0, -1) : t;\nreturn t.trim();\n});\n\nreturn [baseUrl, task];\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":249,"to":260}}}}],["1015",{"pageContent":"const getText = (\nhtml: string,\nbaseUrl: string,\nsummary: boolean\n): string => {\n// scriptingEnabled so noscript elements are parsed\nconst $ = cheerio.load(html, { scriptingEnabled: true });\n\nlet text = \"\";\n\n// lets only get the body if its a summary, dont need to summarize header or footer etc\nconst rootElement = summary ? \"body \" : \"*\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n$(`${rootElement}:not(style):not(script):not(svg)`).each((_i, elem: any) => {\n// we dont want duplicated content as we drill down so remove children\nlet content = $(elem).clone().children().remove().end().text().trim();\nconst $el = $(elem);\n\n// if its an ahref, print the content and url\nlet href = $el.attr(\"href\");\nif ($el.prop(\"tagName\")?.toLowerCase() === \"a\" && href) {\nif (!href.startsWith(\"http\")) {\ntry {\nhref = new URL(href, baseUrl).toString();\n} catch {\n// if this fails thats fine, just no url for this\nhref = \"\";\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":502,"to":531}}}}],["1016",{"pageContent":"const imgAlt = $el.find(\"img[alt]\").attr(\"alt\")?.trim();\nif (imgAlt) {\ncontent += ` ${imgAlt}`;\n}\n\ntext += ` [${content}](${href})`;\n}\n// otherwise just print the content\nelse if (content !== \"\") {\ntext += ` ${content}`;\n}\n});\n\nreturn text.trim().replace(/\\n+/g, \" \");\n};\n\nconst getHtml = async (\nbaseUrl: string,\nh: Headers,\nconfig: AxiosRequestConfig\n) => {\nconst axios = (\n\"default\" in axiosMod ? axiosMod.default : axiosMod\n) as AxiosStatic;\n\nconst domain = new URL(baseUrl).hostname;\n\nconst headers = { ...h };\n// these appear to be positional, which means they have to exist in the headers passed in\nheaders.Host = domain;\nheaders[\"Alt-Used\"] = domain;\n\nlet htmlResponse;\ntry {\nhtmlResponse = await axios.get(baseUrl, {\n...config,\nheaders,\n});\n} catch (e) {\nif (axios.isAxiosError(e) && e.response && e.response.status) {\nthrow new Error(`http response ${e.response.status}`);\n}\nthrow e;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":758,"to":801}}}}],["1017",{"pageContent":"const allowedContentTypes = [\n\"text/html\",\n\"application/json\",\n\"application/xml\",\n\"application/javascript\",\n\"text/plain\",\n];\n\nconst contentType = htmlResponse.headers[\"content-type\"];\nconst contentTypeArray = contentType.split(\";\");\nif (\ncontentTypeArray[0] &&\n!allowedContentTypes.includes(contentTypeArray[0])\n) {\nthrow new Error(\"returned page was not utf8\");\n}\nreturn htmlResponse.data;\n};\n\nconst DEFAULT_HEADERS = {\nAccept:\n\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n\"Accept-Encoding\": \"gzip, deflate\",\n\"Accept-Language\": \"en-US,en;q=0.5\",\n\"Alt-Used\": \"LEAVE-THIS-KEY-SET-BY-TOOL\",\nConnection: \"keep-alive\",\nHost: \"LEAVE-THIS-KEY-SET-BY-TOOL\",\nReferer: \"https://www.google.com/\",\n\"Sec-Fetch-Dest\": \"document\",\n\"Sec-Fetch-Mode\": \"navigate\",\n\"Sec-Fetch-Site\": \"cross-site\",\n\"Upgrade-Insecure-Requests\": \"1\",\n\"User-Agent\":\n\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/111.0\",\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1029,"to":1063}}}}],["1018",{"pageContent":"// eslint-disable-next-line @typescript-eslint/no-explicit-any","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1084,"to":1084}}}}],["1019",{"pageContent":"Headers = Record<string, any>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1184,"to":1184}}}}],["1020",{"pageContent":"interface WebBrowserArgs extends ToolParams {\nmodel: BaseLanguageModel;\n\nembeddings: Embeddings;\n\nheaders?: Headers;\n\naxiosConfig?: Omit<AxiosRequestConfig, \"url\">;\n\n/** @deprecated */\ncallbackManager?: CallbackManager;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1290,"to":1301}}}}],["1021",{"pageContent":"class WebBrowser extends Tool {\nprivate model: BaseLanguageModel;\n\nprivate embeddings: Embeddings;\n\nprivate headers: Headers;\n\nprivate axiosConfig: Omit<AxiosRequestConfig, \"url\">;\n\nconstructor({\nmodel,\nheaders,\nembeddings,\nverbose,\ncallbacks,\ncallbackManager,\naxiosConfig,\n}: WebBrowserArgs) {\nsuper(verbose, callbacks ?? callbackManager);\n\nthis.model = model;\nthis.embeddings = embeddings;\nthis.headers = headers || DEFAULT_HEADERS;\nthis.axiosConfig = {\nwithCredentials: true,\nadapter: isNode ? undefined : fetchAdapter,\n...axiosConfig,\n};\n}\n\n/** @ignore */\nasync _call(inputs: string, runManager?: CallbackManagerForToolRun) {\nconst [baseUrl, task] = parseInputs(inputs);\nconst doSummary = !task;\n\nlet text;\ntry {\nconst html = await getHtml(baseUrl, this.headers, this.axiosConfig);\ntext = getText(html, baseUrl, doSummary);\n} catch (e) {\nif (e) {\nreturn e.toString();\n}\nreturn \"There was a problem connecting to the site\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1545,"to":1589}}}}],["1022",{"pageContent":"const textSplitter = new RecursiveCharacterTextSplitter({\nchunkSize: 2000,\nchunkOverlap: 200,\n});\nconst texts = await textSplitter.splitText(text);\n\nlet context;\n// if we want a summary grab first 4\nif (doSummary) {\ncontext = texts.slice(0, 4).join(\"\\n\");\n}\n// search term well embed and grab top 4\nelse {\nconst docs = texts.map(\n(pageContent) =>\nnew Document({\npageContent,\nmetadata: [],\n})\n);\n\nconst vectorStore = await MemoryVectorStore.fromDocuments(\ndocs,\nthis.embeddings\n);\nconst results = await vectorStore.similaritySearch(task, 4);\ncontext = results.map((res) => res.pageContent).join(\"\\n\");\n}\n\nconst input = `Text:${context}\\n\\nI need ${\ndoSummary ? \"a summary\" : task\n} from the above text, also provide up to 5 markdown links from within that would be of interest (always including URL and text). Links should be provided, if present, in markdown syntax as a list under the heading \"Relevant Links:\".`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1817,"to":1848}}}}],["1023",{"pageContent":"const res = await this.model.generatePrompt(\n[new StringPromptValue(input)],\nundefined,\nrunManager?.getChild()\n);\n\nreturn res.generations[0][0].text;\n}\n\nname = \"web-browser\";\n\ndescription = `useful for when you need to find something on or summarize a webpage. input should be a comma seperated list of \"ONE valid http URL including protocol\",\"what you want to find on the page or empty string for a summary\".`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":2076,"to":2088}}}}],["1024",{"pageContent":"import { Tool } from \"./base.js\";\nimport { renderTemplate } from \"../prompts/template.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../util/async_caller.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":1,"to":3}}}}],["1025",{"pageContent":"const zapierNLABaseDescription: string =\n\"A wrapper around Zapier NLA actions. \" +\n\"The input to this tool is a natural language instruction, \" +\n'for example \"get the latest email from my bank\" or ' +\n'\"send a slack message to the #general channel\". ' +\n\"Each tool will have params associated with it that are specified as a list. You MUST take into account the params when creating the instruction. \" +\n\"For example, if the params are ['Message_Text', 'Channel'], your instruction should be something like 'send a slack message to the #general channel with the text hello world'. \" +\n\"Another example: if the params are ['Calendar', 'Search_Term'], your instruction should be something like 'find the meeting in my personal calendar at 3pm'. \" +\n\"Do not make up params, they will be explicitly specified in the tool description. \" +\n\"If you do not have enough information to fill in the params, just say 'not enough information provided in the instruction, missing <param>'. \" +","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":228,"to":237}}}}],["1026",{"pageContent":"\"If you get a none or null response, STOP EXECUTION, do not try to another tool! \" +\n\"This tool specifically used for: {zapier_description}, \" +\n\"and has params: {params}\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":452,"to":454}}}}],["1027",{"pageContent":"// eslint-disable-next-line @typescript-eslint/no-explicit-any","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":468,"to":468}}}}],["1028",{"pageContent":"type ZapierValues = Record<string, any>;\n\nexport interface ZapiterNLAWrapperParams extends AsyncCallerParams {\napiKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":677,"to":681}}}}],["1029",{"pageContent":"class ZapierNLAWrapper {\nzapierNlaApiKey: string;\n\nzapierNlaApiBase = \"https://nla.zapier.com/api/v1/\";\n\ncaller: AsyncCaller;\n\nconstructor(params?: string | ZapiterNLAWrapperParams) {\nconst zapierNlaApiKey =\ntypeof params === \"string\" ? params : params?.apiKey;\nconst apiKey =\nzapierNlaApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.ZAPIER_NLA_API_KEY\n: undefined);\nif (!apiKey) {\nthrow new Error(\"ZAPIER_NLA_API_KEY not set\");\n}\nthis.zapierNlaApiKey = apiKey;\nthis.caller = new AsyncCaller(","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":906,"to":926}}}}],["1030",{"pageContent":"of params === \"string\" ? {} : params ?? {}\n);\n}\n\nprotected _getHeaders(): Record<string, string> {\nreturn {\n\"Content-Type\": \"application/json\",\nAccept: \"application/json\",\n\"x-api-key\": this.zapierNlaApiKey,\n};\n}\n\nprotected async _getActionRequest(\nactionId: string,\ninstructions: string,\nparams?: ZapierValues\n): Promise<ZapierValues> {\nconst data = params ?? {};\ndata.instructions = instructions;\nconst headers = this._getHeaders();\n\n// add api key to params\nconst resp = await this.caller.call(\nfetch,\n`${this.zapierNlaApiBase}exposed/${actionId}/execute/`,\n{\nmethod: \"POST\",\nheaders,\nbody: JSON.stringify(data),\n}\n);\n\nif (!resp.ok) {\nthrow new Error(\n`Failed to execute action ${actionId} with instructions ${instructions}`\n);\n}\n\nconst jsonResp = await resp.json();\n\nif (jsonResp.status === \"error\") {\nthrow new Error(`Error from Zapier: ${jsonResp.error}`);\n}\n\nreturn jsonResp;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":1144,"to":1189}}}}],["1031",{"pageContent":"/**\n* Executes an action that is identified by action_id, must be exposed\n* (enabled) by the current user (associated with the set api_key). Change\n* your exposed actions here: https://nla.zapier.com/demo/start/\n* @param actionId\n* @param instructions\n* @param params\n*/\nasync runAction(\nactionId: string,\ninstructions: string,\nparams?: ZapierValues\n): Promise<ZapierValues> {\nconst resp = await this._getActionRequest(actionId, instructions, params);\nreturn resp.status === \"error\" ? resp.error : resp.result;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":1405,"to":1420}}}}],["1032",{"pageContent":"/**\n* Same as run, but instead of actually executing the action, will\n* instead return a preview of params that have been guessed by the AI in\n* case you need to explicitly review before executing.\n* @param actionId\n* @param instructions\n* @param params\n*/\nasync previewAction(\nactionId: string,\ninstructions: string,\nparams?: ZapierValues\n): Promise<ZapierValues> {\nconst data = params ?? {};\ndata.preview_only = true;\nconst resp = await this._getActionRequest(actionId, instructions, data);\nreturn resp.input_params;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":1638,"to":1655}}}}],["1033",{"pageContent":"/**\n* Returns a list of all exposed (enabled) actions associated with\n* current user (associated with the set api_key). Change your exposed\n* actions here: https://nla.zapier.com/demo/start/\n*/\nasync listActions(): Promise<ZapierValues[]> {\nconst headers = this._getHeaders();\nconst resp = await this.caller.call(\nfetch,\n`${this.zapierNlaApiBase}exposed/`,\n{\nmethod: \"GET\",\nheaders,\n}\n);\nif (!resp.ok) {\nthrow new Error(\"Failed to list actions\");\n}\nreturn (await resp.json()).results;\n}\n\n/**\n* Same as run, but returns a stringified version of the result.\n* @param actionId\n* @param instructions\n* @param params\n*/\nasync runAsString(\nactionId: string,\ninstructions: string,\nparams?: ZapierValues\n): Promise<string> {\nconst result = await this.runAction(actionId, instructions, params);\nreturn JSON.stringify(result);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":1873,"to":1907}}}}],["1034",{"pageContent":"/**\n* Same as preview, but returns a stringified version of the result.\n* @param actionId\n* @param instructions\n* @param params\n*/\nasync previewAsString(\nactionId: string,\ninstructions: string,\nparams?: ZapierValues\n): Promise<string> {\nconst result = await this.previewAction(actionId, instructions, params);\nreturn JSON.stringify(result);\n}\n\n/**\n* Same as list, but returns a stringified version of the result.\n*/\nasync listActionsAsString(): Promise<string> {\nconst result = await this.listActions();\nreturn JSON.stringify(result);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":2123,"to":2145}}}}],["1035",{"pageContent":"class ZapierNLARunAction extends Tool {\napiWrapper: ZapierNLAWrapper;\n\nactionId: string;\n\nparams?: ZapierValues;\n\nname: string;\n\ndescription: string;\n\nconstructor(\napiWrapper: ZapierNLAWrapper,\nactionId: string,\nzapierDescription: string,\nparamsSchema: ZapierValues,\nparams?: ZapierValues\n) {\nsuper();\nthis.apiWrapper = apiWrapper;\nthis.actionId = actionId;\nthis.params = params;\nthis.name = zapierDescription;\nconst paramsSchemaWithoutInstructions = { ...paramsSchema };\ndelete paramsSchemaWithoutInstructions.instructions;\nconst paramsSchemaKeysString = JSON.stringify(\nObject.keys(paramsSchemaWithoutInstructions)\n);\nthis.description = renderTemplate(zapierNLABaseDescription, \"f-string\", {\nzapier_description: zapierDescription,\nparams: paramsSchemaKeysString,\n});\n}\n\n/** @ignore */\nasync _call(arg: string): Promise<string> {\nreturn this.apiWrapper.runAsString(this.actionId, arg, this.params);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":2363,"to":2401}}}}],["1036",{"pageContent":"import { BaseLanguageModelCallOptions } from \"../base_language/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/open-ai-types.d.ts","loc":{"lines":{"from":1,"to":1}}}}],["1037",{"pageContent":"declare interface OpenAIBaseInput {\n/** Sampling temperature to use */\ntemperature: number;\n\n/**\n* Maximum number of tokens to generate in the completion. -1 returns as many\n* tokens as possible given the prompt and the model's maximum context size.\n*/\nmaxTokens?: number;\n\n/** Total probability mass of tokens to consider at each step */\ntopP: number;\n\n/** Penalizes repeated tokens according to frequency */\nfrequencyPenalty: number;\n\n/** Penalizes repeated tokens */\npresencePenalty: number;\n\n/** Number of completions to generate for each prompt */\nn: number;\n\n/** Dictionary used to adjust the probability of specific tokens being generated */\nlogitBias?: Record<string, number>;\n\n/** Whether to stream the results or not. Enabling disables tokenUsage reporting */\nstreaming: boolean;\n\n/** Model name to use */\nmodelName: string;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/open-ai-types.d.ts","loc":{"lines":{"from":122,"to":151}}}}],["1038",{"pageContent":"/** Holds any additional parameters that are valid to pass to {@link\n* https://platform.openai.com/docs/api-reference/completions/create |\n* `openai.createCompletion`} that are not explicitly specified on this class.\n*/\nmodelKwargs?: Record<string, any>;\n\n/** List of stop words to use when generating */\nstop?: string[];\n\n/**\n* Timeout to use when making requests to OpenAI.\n*/\ntimeout?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/open-ai-types.d.ts","loc":{"lines":{"from":245,"to":258}}}}],["1039",{"pageContent":"interface OpenAICallOptions extends BaseLanguageModelCallOptions {\n/**\n* List of stop words to use when generating\n*/\nstop?: string[];\n\n/**\n* Additional options to pass to the underlying axios request.\n*/\noptions?: AxiosRequestConfig;\n}\n\n/**\n* Input to OpenAI class.\n*/\nexport declare interface OpenAIInput extends OpenAIBaseInput {\n/** Generates `bestOf` completions server side and returns the \"best\" */\nbestOf: number;\n\n/** Batch size to use when passing multiple documents to generate */\nbatchSize: number;\n}\n\nexport interface OpenAIChatInput extends OpenAIBaseInput {\n/** ChatGPT messages to pass as a prefix to the prompt */\nprefixMessages?: ChatCompletionRequestMessage[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/open-ai-types.d.ts","loc":{"lines":{"from":367,"to":393}}}}],["1040",{"pageContent":"declare interface AzureOpenAIInput {\n/**\n* API version to use when making requests to Azure OpenAI.\n*/\nazureOpenAIApiVersion?: string;\n\n/**\n* API key to use when making requests to Azure OpenAI.\n*/\nazureOpenAIApiKey?: string;\n\n/**\n* Azure OpenAI API instance name to use when making requests to Azure OpenAI.\n* this is the name of the instance you created in the Azure portal.\n* e.g. \"my-openai-instance\"\n* this will be used in the endpoint URL: https://my-openai-instance.openai.azure.com/openai/deployments/{DeploymentName}/\n*/\nazureOpenAIApiInstanceName?: string;\n\n/**\n* Azure OpenAI API deployment name to use for completions when making requests to Azure OpenAI.\n* This is the name of the deployment you created in the Azure portal.\n* e.g. \"my-openai-deployment\"\n* this will be used in the endpoint URL: https://{InstanceName}.openai.azure.com/openai/deployments/my-openai-deployment/\n*/\nazureOpenAIApiDeploymentName?: string;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/open-ai-types.d.ts","loc":{"lines":{"from":490,"to":515}}}}],["1041",{"pageContent":"/**\n* Azure OpenAI API deployment name to use for embedding when making requests to Azure OpenAI.\n* This is the name of the deployment you created in the Azure portal.\n* This will fallback to azureOpenAIApiDeploymentName if not provided.\n* e.g. \"my-openai-deployment\"\n* this will be used in the endpoint URL: https://{InstanceName}.openai.azure.com/openai/deployments/my-openai-deployment/\n*/\nazureOpenAIApiEmbeddingsDeploymentName?: string;\n\n/**\n* Azure OpenAI API deployment name to use for completions when making requests to Azure OpenAI.\n* Completions are only available for gpt-3.5-turbo and text-davinci-003 deployments.\n* This is the name of the deployment you created in the Azure portal.\n* This will fallback to azureOpenAIApiDeploymentName if not provided.\n* e.g. \"my-openai-deployment\"\n* this will be used in the endpoint URL: https://{InstanceName}.openai.azure.com/openai/deployments/my-openai-deployment/\n*/\nazureOpenAIApiCompletionsDeploymentName?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/open-ai-types.d.ts","loc":{"lines":{"from":606,"to":624}}}}],["1042",{"pageContent":"/**\n* Type definitions adapted from pdfjs-dist\n* https://github.com/mozilla/pdfjs-dist/blob/master/types/src/display/api.d.ts\n*/\n\ndeclare module \"pdf-parse/lib/pdf.js/v1.10.100/build/pdf.js\" {\nexport type TypedArray =\n| Int8Array\n| Uint8Array\n| Uint8ClampedArray\n| Int16Array\n| Uint16Array\n| Int32Array\n| Uint32Array\n| Float32Array\n| Float64Array;\nexport type BinaryData = TypedArray | ArrayBuffer | Array<number> | string;\nexport type RefProxy = {\nnum: number;\ngen: number;\n};\n/**\n* Document initialization / loading parameters object.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":1,"to":24}}}}],["1043",{"pageContent":"type DocumentInitParameters = {\n/**\n* - The URL of the PDF.\n*/\nurl?: string | URL | undefined;\n/**\n* - Binary PDF data.\n* Use TypedArrays (Uint8Array) to improve the memory usage. If PDF data is\n* BASE64-encoded, use `atob()` to convert it to a binary string first.\n*\n* NOTE: If TypedArrays are used they will generally be transferred to the\n* worker-thread. This will help reduce main-thread memory usage, however\n* it will take ownership of the TypedArrays.\n*/\ndata?: BinaryData | undefined;\n/**\n* - Basic authentication headers.\n*/\nhttpHeaders?: Object | undefined;\n/**\n* - Indicates whether or not\n* cross-site Access-Control requests should be made using credentials such\n* as cookies or authorization headers. The default is `false`.\n*/\nwithCredentials?: boolean | undefined;\n/**\n* - For decrypting password-protected PDFs.\n*/\npassword?: string | undefined;\n/**\n* - The PDF file length. It's used for progress\n* reports and range requests operations.\n*/\nlength?: number | undefined;\n/**\n* - Allows for using a custom range","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":1411,"to":1446}}}}],["1044",{"pageContent":"* transport implementation.\n*/\nrange?: PDFDataRangeTransport | undefined;\n/**\n* - Specify maximum number of bytes fetched\n* per range request. The default value is {@link DEFAULT_RANGE_CHUNK_SIZE }.\n*/\nrangeChunkSize?: number | undefined;\n/**\n* - The worker that will be used for loading and\n* parsing the PDF data.\n*/\nworker?: PDFWorker | undefined;\n/**\n* - Controls the logging level; the constants\n* from {@link VerbosityLevel } should be used.\n*/\nverbosity?: number | undefined;\n/**\n* - The base URL of the document, used when\n* attempting to recover valid absolute URLs for annotations, and outline\n* items, that (incorrectly) only specify relative URLs.\n*/\ndocBaseUrl?: string | undefined;\n/**\n* - The URL where the predefined Adobe CMaps are\n* located. Include the trailing slash.\n*/\ncMapUrl?: string | undefined;\n/**\n* - Specifies if the Adobe CMaps are binary\n* packed or not. The default value is `true`.\n*/\ncMapPacked?: boolean | undefined;\n/**\n* - The factory that will be used when","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":2819,"to":2854}}}}],["1045",{"pageContent":"* reading built-in CMap files. Providing a custom factory is useful for\n* environments without Fetch API or `XMLHttpRequest` support, such as\n* Node.js. The default value is {DOMCMapReaderFactory}.\n*/\nCMapReaderFactory?: Object | undefined;\n/**\n* - When `true`, fonts that aren't\n* embedded in the PDF document will fallback to a system font.\n* The default value is `true` in web environments and `false` in Node.js;\n* unless `disableFontFace === true` in which case this defaults to `false`\n* regardless of the environment (to prevent completely broken fonts).\n*/\nuseSystemFonts?: boolean | undefined;\n/**\n* - The URL where the standard font\n* files are located. Include the trailing slash.\n*/\nstandardFontDataUrl?: string | undefined;\n/**\n* - The factory that will be used\n* when reading the standard font files. Providing a custom factory is useful\n* for environments without Fetch API or `XMLHttpRequest` support, such as\n* Node.js. The default value is {DOMStandardFontDataFactory}.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":4228,"to":4251}}}}],["1046",{"pageContent":"StandardFontDataFactory?: Object | undefined;\n/**\n* - Enable using the Fetch API in the\n* worker-thread when reading CMap and standard font files. When `true`,\n* the `CMapReaderFactory` and `StandardFontDataFactory` options are ignored.\n* The default value is `true` in web environments and `false` in Node.js.\n*/\nuseWorkerFetch?: boolean | undefined;\n/**\n* - Reject certain promises, e.g.\n* `getOperatorList`, `getTextContent`, and `RenderTask`, when the associated\n* PDF data cannot be successfully parsed, instead of attempting to recover\n* whatever possible of the data. The default value is `false`.\n*/\nstopAtErrors?: boolean | undefined;\n/**\n* - The maximum allowed image size in total\n* pixels, i.e. width * height. Images above this value will not be rendered.\n* Use -1 for no limit, which is also the default value.\n*/\nmaxImageSize?: number | undefined;\n/**\n* - Determines if we can evaluate strings\n* as JavaScript. Primarily used to improve performance of font rendering, and","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":5625,"to":5648}}}}],["1047",{"pageContent":"* when parsing PDF functions. The default value is `true`.\n*/\nisEvalSupported?: boolean | undefined;\n/**\n* - Determines if we can use\n* `OffscreenCanvas` in the worker. Primarily used to improve performance of\n* image conversion/rendering.\n* The default value is `true` in web environments and `false` in Node.js.\n*/\nisOffscreenCanvasSupported?: boolean | undefined;\n/**\n* - The integer value is used to\n* know when an image must be resized (uses `OffscreenCanvas` in the worker).\n* If it's -1 then a possibly slow algorithm is used to guess the max value.\n*/\ncanvasMaxAreaInBytes?: boolean | undefined;\n/**\n* - By default fonts are converted to\n* OpenType fonts and loaded via the Font Loading API or `@font-face` rules.\n* If disabled, fonts will be rendered using a built-in font renderer that\n* constructs the glyphs with primitive path commands.\n* The default value is `false` in web environments and `true` in Node.js.\n*/\ndisableFontFace?: boolean | undefined;\n/**\n* - Include additional properties,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":7022,"to":7047}}}}],["1048",{"pageContent":"* which are unused during rendering of PDF documents, when exporting the\n* parsed font data from the worker-thread. This may be useful for debugging\n* purposes (and backwards compatibility), but note that it will lead to\n* increased memory usage. The default value is `false`.\n*/\nfontExtraProperties?: boolean | undefined;\n/**\n* - Render Xfa forms if any.\n* The default value is `false`.\n*/\nenableXfa?: boolean | undefined;\n/**\n* - Specify an explicit document\n* context to create elements with and to load resources, such as fonts,\n* into. Defaults to the current document.\n*/\nownerDocument?: HTMLDocument | undefined;\n/**\n* - Disable range request loading of PDF\n* files. When enabled, and if the server supports partial content requests,\n* then the PDF will be fetched in chunks. The default value is `false`.\n*/\ndisableRange?: boolean | undefined;\n/**\n* - Disable streaming of PDF file data.\n* By default PDF.js attempts to load PDF files in chunks. The default value\n* is `false`.\n*/\ndisableStream?: boolean | undefined;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":8420,"to":8448}}}}],["1049",{"pageContent":"/**\n* - Disable pre-fetching of PDF file\n* data. When range requests are enabled PDF.js will automatically keep\n* fetching more data even if it isn't needed to display the current page.\n* The default value is `false`.\n*\n* NOTE: It is also necessary to disable streaming, see above, in order for\n* disabling of pre-fetching to work correctly.\n*/\ndisableAutoFetch?: boolean | undefined;\n/**\n* - Enables special hooks for debugging PDF.js\n* (see `web/debugger.js`). The default value is `false`.\n*/\npdfBug?: boolean | undefined;\n/**\n* - The factory instance that will be used\n* when creating canvases. The default value is {new DOMCanvasFactory()}.\n*/\ncanvasFactory?: Object | undefined;\n/**\n* - A factory instance that will be used\n* to create SVG filters when rendering some images on the main canvas.\n*/\nfilterFactory?: Object | undefined;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":9821,"to":9846}}}}],["1050",{"pageContent":"type OnProgressParameters = {\n/**\n* - Currently loaded number of bytes.\n*/\nloaded: number;\n/**\n* - Total number of bytes in the PDF file.\n*/\ntotal: number;\n};\n/**\n* Page getViewport parameters.\n*/\nexport type GetViewportParameters = {\n/**\n* - The desired scale of the viewport.\n*/\nscale: number;\n/**\n* - The desired rotation, in degrees, of\n* the viewport. If omitted it defaults to the page rotation.\n*/\nrotation?: number | undefined;\n/**\n* - The horizontal, i.e. x-axis, offset.\n* The default value is `0`.\n*/\noffsetX?: number | undefined;\n/**\n* - The vertical, i.e. y-axis, offset.\n* The default value is `0`.\n*/\noffsetY?: number | undefined;\n/**\n* - If true, the y-axis will not be\n* flipped. The default value is `false`.\n*/\ndontFlip?: boolean | undefined;\n};\n/**\n* Page getTextContent parameters.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":11222,"to":11263}}}}],["1051",{"pageContent":"type getTextContentParameters = {\n/**\n* - When true include marked\n* content items in the items array of TextContent. The default is `false`.\n*/\nincludeMarkedContent?: boolean | undefined;\n};\n/**\n* Page text content.\n*/\nexport type TextContent = {\n/**\n* - Array of\n* {@link TextItem } and {@link TextMarkedContent } objects. TextMarkedContent\n* items are included when includeMarkedContent is true.\n*/\nitems: Array<TextItem | TextMarkedContent>;\n/**\n* - {@link TextStyle } objects,\n* indexed by font name.\n*/\nstyles: {\n[x: string]: TextStyle;\n};\n};\n/**\n* Page text content part.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":12640,"to":12667}}}}],["1052",{"pageContent":"type TextItem = {\n/**\n* - Text content.\n*/\nstr: string;\n/**\n* - Text direction: 'ttb', 'ltr' or 'rtl'.\n*/\ndir: string;\n/**\n* - Transformation matrix.\n*/\ntransform: Array<any>;\n/**\n* - Width in device space.\n*/\nwidth: number;\n/**\n* - Height in device space.\n*/\nheight: number;\n/**\n* - Font name used by PDF.js for converted font.\n*/\nfontName: string;\n/**\n* - Indicating if the text content is followed by a\n* line-break.\n*/\nhasEOL: boolean;\n};\n/**\n* Page text marked content part.\n*/\nexport type TextMarkedContent = {\n/**\n* - Either 'beginMarkedContent',\n* 'beginMarkedContentProps', or 'endMarkedContent'.\n*/\ntype: string;\n/**\n* - The marked content identifier. Only used for type\n* 'beginMarkedContentProps'.\n*/\nid: string;\n};\n/**\n* Text style.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":14053,"to":14101}}}}],["1053",{"pageContent":"type TextStyle = {\n/**\n* - Font ascent.\n*/\nascent: number;\n/**\n* - Font descent.\n*/\ndescent: number;\n/**\n* - Whether or not the text is in vertical mode.\n*/\nvertical: boolean;\n/**\n* - The possible font family.\n*/\nfontFamily: string;\n};\n/**\n* Page annotation parameters.\n*/\nexport type GetAnnotationsParameters = {\n/**\n* - Determines the annotations that are fetched,\n* can be 'display' (viewable annotations), 'print' (printable annotations),\n* or 'any' (all annotations). The default value is 'display'.\n*/\nintent?: string | undefined;\n};\n/**\n* Page render parameters.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":15479,"to":15510}}}}],["1054",{"pageContent":"type RenderParameters = {\n/**\n* - A 2D context of a DOM\n* Canvas object.\n*/\ncanvasContext: CanvasRenderingContext2D;\n/**\n* - Rendering viewport obtained by calling\n* the `PDFPageProxy.getViewport` method.\n*/\nviewport: PageViewport;\n/**\n* - Rendering intent, can be 'display', 'print',\n* or 'any'. The default value is 'display'.\n*/\nintent?: string | undefined;\n/**\n* Controls which annotations are rendered\n* onto the canvas, for annotations with appearance-data; the values from\n* {@link AnnotationMode } should be used. The following values are supported:\n* - `AnnotationMode.DISABLE`, which disables all annotations.\n* - `AnnotationMode.ENABLE`, which includes all possible annotations (thus\n* it also depends on the `intent`-option, see above).\n* - `AnnotationMode.ENABLE_FORMS`, which excludes annotations that contain\n* interactive form elements (those will be rendered in the display layer).\n* - `AnnotationMode.ENABLE_STORAGE`, which includes all possible annotations","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":16896,"to":16921}}}}],["1055",{"pageContent":"* (as above) but where interactive form elements are updated with data\n* from the {@link AnnotationStorage }-instance; useful e.g. for printing.\n* The default value is `AnnotationMode.ENABLE`.\n*/\nannotationMode?: number | undefined;\n/**\n* - Additional transform, applied just\n* before viewport transform.\n*/\ntransform?: any[] | undefined;\n/**\n* - Background\n* to use for the canvas.\n* Any valid `canvas.fillStyle` can be used: a `DOMString` parsed as CSS\n* <color> value, a `CanvasGradient` object (a linear or radial gradient) or\n* a `CanvasPattern` object (a repetitive image). The default value is\n* 'rgb(255,255,255)'.\n*\n* NOTE: This option may be partially, or completely, ignored when the\n* `pageColors`-option is used.\n*/\nbackground?: string | CanvasGradient | CanvasPattern | undefined;\n/**\n* - Overwrites background and foreground colors\n* with user defined ones in order to improve readability in high contrast\n* mode.\n*/\npageColors?: Object | undefined;\n/**\n* -","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":18295,"to":18324}}}}],["1056",{"pageContent":"* A promise that should resolve with an {@link OptionalContentConfig }created from `PDFDocumentProxy.getOptionalContentConfig`. If `null`,\n* the configuration will be fetched automatically with the default visibility\n* states set.\n*/\noptionalContentConfigPromise?: Promise<OptionalContentConfig> | undefined;\n/**\n* - Map some\n* annotation ids with canvases used to render them.\n*/\nannotationCanvasMap?: Map<string, HTMLCanvasElement> | undefined;\nprintAnnotationStorage?: PrintAnnotationStorage | undefined;\n};\n/**\n* Page getOperatorList parameters.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":19698,"to":19712}}}}],["1057",{"pageContent":"type GetOperatorListParameters = {\n/**\n* - Rendering intent, can be 'display', 'print',\n* or 'any'. The default value is 'display'.\n*/\nintent?: string | undefined;\n/**\n* Controls which annotations are included\n* in the operatorList, for annotations with appearance-data; the values from\n* {@link AnnotationMode } should be used. The following values are supported:\n* - `AnnotationMode.DISABLE`, which disables all annotations.\n* - `AnnotationMode.ENABLE`, which includes all possible annotations (thus\n* it also depends on the `intent`-option, see above).\n* - `AnnotationMode.ENABLE_FORMS`, which excludes annotations that contain\n* interactive form elements (those will be rendered in the display layer).\n* - `AnnotationMode.ENABLE_STORAGE`, which includes all possible annotations\n* (as above) but where interactive form elements are updated with data\n* from the {@link AnnotationStorage }-instance; useful e.g. for printing.\n* The default value is `AnnotationMode.ENABLE`.\n*/\nannotationMode?: number | undefined;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":21098,"to":21118}}}}],["1058",{"pageContent":"printAnnotationStorage?: PrintAnnotationStorage | undefined;\n};\n/**\n* Structure tree node. The root node will have a role \"Root\".\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":22491,"to":22495}}}}],["1059",{"pageContent":"type StructTreeNode = {\n/**\n* - Array of\n* {@link StructTreeNode } and {@link StructTreeContent } objects.\n*/\nchildren: Array<StructTreeNode | StructTreeContent>;\n/**\n* - element's role, already mapped if a role map exists\n* in the PDF.\n*/\nrole: string;\n};\n/**\n* Structure tree content.\n*/\nexport type StructTreeContent = {\n/**\n* - either \"content\" for page and stream structure\n* elements or \"object\" for object references.\n*/\ntype: string;\n/**\n* - unique id that will map to the text layer.\n*/\nid: string;\n};\n/**\n* PDF page operator list.\n*/\nexport type PDFOperatorList = {\n/**\n* - Array containing the operator functions.\n*/\nfnArray: Array<number>;\n/**\n* - Array containing the arguments of the\n* functions.\n*/\nargsArray: Array<any>;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":23900,"to":23939}}}}],["1060",{"pageContent":"type PDFWorkerParameters = {\n/**\n* - The name of the worker.\n*/\nname?: string | undefined;\n/**\n* - The `workerPort` object.\n*/\nport?: Worker | undefined;\n/**\n* - Controls the logging level;\n* the constants from {@link VerbosityLevel } should be used.\n*/\nverbosity?: number | undefined;\n};\n/** @type {string} */\nexport const build: string;\nexport let DefaultCanvasFactory: typeof DOMCanvasFactory;\nexport let DefaultCMapReaderFactory: typeof DOMCMapReaderFactory;\nexport let DefaultFilterFactory: typeof DOMFilterFactory;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":25317,"to":25336}}}}],["1061",{"pageContent":"let DefaultStandardFontDataFactory: typeof DOMStandardFontDataFactory;\n/**\n* @typedef { Int8Array | Uint8Array | Uint8ClampedArray |\n*            Int16Array | Uint16Array |\n*            Int32Array | Uint32Array | Float32Array |\n*            Float64Array\n* } TypedArray\n*/\n/**\n* @typedef { TypedArray | ArrayBuffer | Array<number> | string } BinaryData\n*/\n/**\n* @typedef {Object} RefProxy\n* @property {number} num\n* @property {number} gen\n*/\n/**\n* Document initialization / loading parameters object.\n*\n* @typedef {Object} DocumentInitParameters\n* @property {string | URL} [url] - The URL of the PDF.\n* @property {BinaryData} [data] - Binary PDF data.\n*   Use TypedArrays (Uint8Array) to improve the memory usage. If PDF data is\n*   BASE64-encoded, use `atob()` to convert it to a binary string first.\n*\n*   NOTE: If TypedArrays are used they will generally be transferred to the\n*   worker-thread. This will help reduce main-thread memory usage, however\n*   it will take ownership of the TypedArrays.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":26725,"to":26752}}}}],["1062",{"pageContent":"* @property {Object} [httpHeaders] - Basic authentication headers.\n* @property {boolean} [withCredentials] - Indicates whether or not\n*   cross-site Access-Control requests should be made using credentials such\n*   as cookies or authorization headers. The default is `false`.\n* @property {string} [password] - For decrypting password-protected PDFs.\n* @property {number} [length] - The PDF file length. It's used for progress\n*   reports and range requests operations.\n* @property {PDFDataRangeTransport} [range] - Allows for using a custom range\n*   transport implementation.\n* @property {number} [rangeChunkSize] - Specify maximum number of bytes fetched\n*   per range request. The default value is {@link DEFAULT_RANGE_CHUNK_SIZE}.\n* @property {PDFWorker} [worker] - The worker that will be used for loading and\n*   parsing the PDF data.\n* @property {number} [verbosity] - Controls the logging level; the constants\n*   from {@link VerbosityLevel} should be used.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":28126,"to":28140}}}}],["1063",{"pageContent":"* @property {string} [docBaseUrl] - The base URL of the document, used when\n*   attempting to recover valid absolute URLs for annotations, and outline\n*   items, that (incorrectly) only specify relative URLs.\n* @property {string} [cMapUrl] - The URL where the predefined Adobe CMaps are\n*   located. Include the trailing slash.\n* @property {boolean} [cMapPacked] - Specifies if the Adobe CMaps are binary\n*   packed or not. The default value is `true`.\n* @property {Object} [CMapReaderFactory] - The factory that will be used when\n*   reading built-in CMap files. Providing a custom factory is useful for\n*   environments without Fetch API or `XMLHttpRequest` support, such as\n*   Node.js. The default value is {DOMCMapReaderFactory}.\n* @property {boolean} [useSystemFonts] - When `true`, fonts that aren't\n*   embedded in the PDF document will fallback to a system font.\n*   The default value is `true` in web environments and `false` in Node.js;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":29514,"to":29527}}}}],["1064",{"pageContent":"*   unless `disableFontFace === true` in which case this defaults to `false`\n*   regardless of the environment (to prevent completely broken fonts).\n* @property {string} [standardFontDataUrl] - The URL where the standard font\n*   files are located. Include the trailing slash.\n* @property {Object} [StandardFontDataFactory] - The factory that will be used\n*   when reading the standard font files. Providing a custom factory is useful\n*   for environments without Fetch API or `XMLHttpRequest` support, such as\n*   Node.js. The default value is {DOMStandardFontDataFactory}.\n* @property {boolean} [useWorkerFetch] - Enable using the Fetch API in the\n*   worker-thread when reading CMap and standard font files. When `true`,\n*   the `CMapReaderFactory` and `StandardFontDataFactory` options are ignored.\n*   The default value is `true` in web environments and `false` in Node.js.\n* @property {boolean} [stopAtErrors] - Reject certain promises, e.g.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":30901,"to":30913}}}}],["1065",{"pageContent":"*   `getOperatorList`, `getTextContent`, and `RenderTask`, when the associated\n*   PDF data cannot be successfully parsed, instead of attempting to recover\n*   whatever possible of the data. The default value is `false`.\n* @property {number} [maxImageSize] - The maximum allowed image size in total\n*   pixels, i.e. width * height. Images above this value will not be rendered.\n*   Use -1 for no limit, which is also the default value.\n* @property {boolean} [isEvalSupported] - Determines if we can evaluate strings\n*   as JavaScript. Primarily used to improve performance of font rendering, and\n*   when parsing PDF functions. The default value is `true`.\n* @property {boolean} [isOffscreenCanvasSupported] - Determines if we can use\n*   `OffscreenCanvas` in the worker. Primarily used to improve performance of\n*   image conversion/rendering.\n*   The default value is `true` in web environments and `false` in Node.js.\n* @property {boolean} [canvasMaxAreaInBytes] - The integer value is used to","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":32287,"to":32300}}}}],["1066",{"pageContent":"*   know when an image must be resized (uses `OffscreenCanvas` in the worker).\n*   If it's -1 then a possibly slow algorithm is used to guess the max value.\n* @property {boolean} [disableFontFace] - By default fonts are converted to\n*   OpenType fonts and loaded via the Font Loading API or `@font-face` rules.\n*   If disabled, fonts will be rendered using a built-in font renderer that\n*   constructs the glyphs with primitive path commands.\n*   The default value is `false` in web environments and `true` in Node.js.\n* @property {boolean} [fontExtraProperties] - Include additional properties,\n*   which are unused during rendering of PDF documents, when exporting the\n*   parsed font data from the worker-thread. This may be useful for debugging\n*   purposes (and backwards compatibility), but note that it will lead to\n*   increased memory usage. The default value is `false`.\n* @property {boolean} [enableXfa] - Render Xfa forms if any.\n*   The default value is `false`.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":33674,"to":33687}}}}],["1067",{"pageContent":"* @property {HTMLDocument} [ownerDocument] - Specify an explicit document\n*   context to create elements with and to load resources, such as fonts,\n*   into. Defaults to the current document.\n* @property {boolean} [disableRange] - Disable range request loading of PDF\n*   files. When enabled, and if the server supports partial content requests,\n*   then the PDF will be fetched in chunks. The default value is `false`.\n* @property {boolean} [disableStream] - Disable streaming of PDF file data.\n*   By default PDF.js attempts to load PDF files in chunks. The default value\n*   is `false`.\n* @property {boolean} [disableAutoFetch] - Disable pre-fetching of PDF file\n*   data. When range requests are enabled PDF.js will automatically keep\n*   fetching more data even if it isn't needed to display the current page.\n*   The default value is `false`.\n*\n*   NOTE: It is also necessary to disable streaming, see above, in order for\n*   disabling of pre-fetching to work correctly.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":35061,"to":35076}}}}],["1068",{"pageContent":"* @property {boolean} [pdfBug] - Enables special hooks for debugging PDF.js\n*   (see `web/debugger.js`). The default value is `false`.\n* @property {Object} [canvasFactory] - The factory instance that will be used\n*   when creating canvases. The default value is {new DOMCanvasFactory()}.\n* @property {Object} [filterFactory] - A factory instance that will be used\n*   to create SVG filters when rendering some images on the main canvas.\n*/\n/**\n* This is the main entry point for loading a PDF and interacting with it.\n*\n* NOTE: If a URL is used to fetch the PDF data a standard Fetch API call (or\n* XHR as fallback) is used, which means it must follow same origin rules,\n* e.g. no cross-domain requests without CORS.\n*\n* @param {string | URL | TypedArray | ArrayBuffer | DocumentInitParameters}\n*   src - Can be a URL where a PDF file is located, a typed array (Uint8Array)\n*         already populated with data, or a parameter object.\n* @returns {PDFDocumentLoadingTask}\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":36450,"to":36468}}}}],["1069",{"pageContent":"function getDocument(\nsrc: string | URL | TypedArray | ArrayBuffer | DocumentInitParameters\n): PDFDocumentLoadingTask;\nexport class LoopbackPort {\npostMessage(obj: any, transfer: any): void;\naddEventListener(name: any, listener: any): void;\nremoveEventListener(name: any, listener: any): void;\nterminate(): void;\n#private;\n}\n/**\n* @typedef {Object} OnProgressParameters\n* @property {number} loaded - Currently loaded number of bytes.\n* @property {number} total - Total number of bytes in the PDF file.\n*/\n/**\n* The loading task controls the operations required to load a PDF document\n* (such as network requests) and provides a way to listen for completion,\n* after which individual pages can be rendered.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":37842,"to":37861}}}}],["1070",{"pageContent":"class PDFDocumentLoadingTask {\nstatic \"__#16@#docId\": number;\n_capability: import(\"../shared/util.js\").PromiseCapability;\n_transport: any;\n_worker: any;\n/**\n* Unique identifier for the document loading task.\n* @type {string}\n*/\ndocId: string;\n/**\n* Whether the loading task is destroyed or not.\n* @type {boolean}\n*/\ndestroyed: boolean;\n/**\n* Callback to request a password if a wrong or no password was provided.\n* The callback receives two parameters: a function that should be called\n* with the new password, and a reason (see {@link PasswordResponses}).\n* @type {function}\n*/\nonPassword: Function;\n/**\n* Callback to be able to monitor the loading progress of the PDF file\n* (necessary to implement e.g. a loading bar).\n* The callback receives an {@link OnProgressParameters} argument.\n* @type {function}\n*/\nonProgress: Function;\n/**\n* Promise for document loading task completion.\n* @type {Promise<PDFDocumentProxy>}\n*/\nget promise(): Promise<PDFDocumentProxy>;\n/**\n* Abort all network requests and destroy the worker.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":39241,"to":39276}}}}],["1071",{"pageContent":"* @returns {Promise<void>} A promise that is resolved when destruction is\n*   completed.\n*/\ndestroy(): Promise<void>;\n}\n/**\n* Proxy to a `PDFDocument` in the worker thread.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":40649,"to":40656}}}}],["1072",{"pageContent":"class PDFDocumentProxy {\nconstructor(pdfInfo: any, transport: any);\n_pdfInfo: any;\n_transport: any;\n/**\n* @type {AnnotationStorage} Storage for annotation data in forms.\n*/\nget annotationStorage(): AnnotationStorage;\n/**\n* @type {Object} The filter factory instance.\n*/\nget filterFactory(): Object;\n/**\n* @type {number} Total number of pages in the PDF file.\n*/\nget numPages(): number;\n/**\n* @type {Array<string, string|null>} A (not guaranteed to be) unique ID to\n*   identify the PDF document.\n*   NOTE: The first element will always be defined for all PDF documents,\n*   whereas the second element is only defined for *modified* PDF documents.\n*/\nget fingerprints(): string[];\n/**\n* @type {boolean} True if only XFA form.\n*/\nget isPureXfa(): boolean;\n/**\n* NOTE: This is (mostly) intended to support printing of XFA forms.\n*\n* @type {Object | null} An object representing a HTML tree structure\n*   to render the XFA, or `null` when no XFA form exists.\n*/\nget allXfaHtml(): Object | null;\n/**","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":42059,"to":42093}}}}],["1073",{"pageContent":"* @param {number} pageNumber - The page number to get. The first page is 1.\n* @returns {Promise<PDFPageProxy>} A promise that is resolved with\n*   a {@link PDFPageProxy} object.\n*/\ngetPage(pageNumber: number): Promise<PDFPageProxy>;\n/**\n* @param {RefProxy} ref - The page reference.\n* @returns {Promise<number>} A promise that is resolved with the page index,\n*   starting from zero, that is associated with the reference.\n*/\ngetPageIndex(ref: RefProxy): Promise<number>;\n/**\n* @returns {Promise<Object<string, Array<any>>>} A promise that is resolved\n*   with a mapping from named destinations to references.\n*\n* This can be slow for large documents. Use `getDestination` instead.\n*/\ngetDestinations(): Promise<{\n[x: string]: Array<any>;\n}>;\n/**\n* @param {string} id - The named destination to get.\n* @returns {Promise<Array<any> | null>} A promise that is resolved with all\n*   information of the given named destination, or `null` when the named\n*   destination is not present in the PDF file.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":43467,"to":43492}}}}],["1074",{"pageContent":"getDestination(id: string): Promise<Array<any> | null>;\n/**\n* @returns {Promise<Array<string> | null>} A promise that is resolved with\n*   an {Array} containing the page labels that correspond to the page\n*   indexes, or `null` when no page labels are present in the PDF file.\n*/\ngetPageLabels(): Promise<Array<string> | null>;\n/**\n* @returns {Promise<string>} A promise that is resolved with a {string}\n*   containing the page layout name.\n*/\ngetPageLayout(): Promise<string>;\n/**\n* @returns {Promise<string>} A promise that is resolved with a {string}\n*   containing the page mode name.\n*/\ngetPageMode(): Promise<string>;\n/**\n* @returns {Promise<Object | null>} A promise that is resolved with an\n*   {Object} containing the viewer preferences, or `null` when no viewer\n*   preferences are present in the PDF file.\n*/\ngetViewerPreferences(): Promise<Object | null>;\n/**\n* @returns {Promise<any | null>} A promise that is resolved with an {Array}\n*   containing the destination, or `null` when no open action is present","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":44866,"to":44891}}}}],["1075",{"pageContent":"*   in the PDF.\n*/\ngetOpenAction(): Promise<any | null>;\n/**\n* @returns {Promise<any>} A promise that is resolved with a lookup table\n*   for mapping named attachments to their content.\n*/\ngetAttachments(): Promise<any>;\n/**\n* @returns {Promise<Array<string> | null>} A promise that is resolved with\n*   an {Array} of all the JavaScript strings in the name tree, or `null`\n*   if no JavaScript exists.\n*/\ngetJavaScript(): Promise<Array<string> | null>;\n/**\n* @returns {Promise<Object | null>} A promise that is resolved with\n*   an {Object} with the JavaScript actions:\n*     - from the name tree (like getJavaScript);\n*     - from A or AA entries in the catalog dictionary.\n*   , or `null` if no JavaScript exists.\n*/\ngetJSActions(): Promise<Object | null>;\n/**\n* @typedef {Object} OutlineNode\n* @property {string} title\n* @property {boolean} bold\n* @property {boolean} italic\n* @property {Uint8ClampedArray} color - The color in RGB format to use for\n*   display purposes.\n* @property {string | Array<any> | null} dest","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":46264,"to":46293}}}}],["1076",{"pageContent":"* @property {string | null} url\n* @property {string | undefined} unsafeUrl\n* @property {boolean | undefined} newWindow\n* @property {number | undefined} count\n* @property {Array<OutlineNode>} items\n*/\n/**\n* @returns {Promise<Array<OutlineNode>>} A promise that is resolved with an\n*   {Array} that is a tree outline (if it has one) of the PDF file.\n*/\ngetOutline(): Promise<\n{\ntitle: string;\nbold: boolean;\nitalic: boolean;\n/**\n* - The color in RGB format to use for\n* display purposes.\n*/\ncolor: Uint8ClampedArray;\ndest: string | Array<any> | null;\nurl: string | null;\nunsafeUrl: string | undefined;\nnewWindow: boolean | undefined;\ncount: number | undefined;\nitems: any[];\n}[]\n>;\n/**\n* @returns {Promise<OptionalContentConfig>} A promise that is resolved with\n*   an {@link OptionalContentConfig} that contains all the optional content\n*   groups (assuming that the document has any).\n*/\ngetOptionalContentConfig(): Promise<OptionalContentConfig>;\n/**\n* @returns {Promise<Array<number> | null>} A promise that is resolved with","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":47666,"to":47701}}}}],["1077",{"pageContent":"*   an {Array} that contains the permission flags for the PDF document, or\n*   `null` when no permissions are present in the PDF file.\n*/\ngetPermissions(): Promise<Array<number> | null>;\n/**\n* @returns {Promise<{ info: Object, metadata: Metadata }>} A promise that is\n*   resolved with an {Object} that has `info` and `metadata` properties.\n*   `info` is an {Object} filled with anything available in the information\n*   dictionary and similarly `metadata` is a {Metadata} object with\n*   information from the metadata section of the PDF.\n*/\ngetMetadata(): Promise<{\ninfo: Object;\nmetadata: Metadata;\n}>;\n/**\n* @typedef {Object} MarkInfo\n* Properties correspond to Table 321 of the PDF 32000-1:2008 spec.\n* @property {boolean} Marked\n* @property {boolean} UserProperties\n* @property {boolean} Suspects\n*/\n/**\n* @returns {Promise<MarkInfo | null>} A promise that is resolved with\n*   a {MarkInfo} object that contains the MarkInfo flags for the PDF\n*   document, or `null` when no MarkInfo values are present in the PDF file.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":49074,"to":49099}}}}],["1078",{"pageContent":"*/\ngetMarkInfo(): Promise<{\nMarked: boolean;\nUserProperties: boolean;\nSuspects: boolean;\n} | null>;\n/**\n* @returns {Promise<Uint8Array>} A promise that is resolved with a\n*   {Uint8Array} containing the raw data of the PDF document.\n*/\ngetData(): Promise<Uint8Array>;\n/**\n* @returns {Promise<Uint8Array>} A promise that is resolved with a\n*   {Uint8Array} containing the full data of the saved document.\n*/\nsaveDocument(): Promise<Uint8Array>;\n/**\n* @returns {Promise<{ length: number }>} A promise that is resolved when the\n*   document's data is loaded. It is resolved with an {Object} that contains\n*   the `length` property that indicates size of the PDF data in bytes.\n*/\ngetDownloadInfo(): Promise<{\nlength: number;\n}>;\n/**\n* Cleans up resources allocated by the document on both the main and worker\n* threads.\n*\n* NOTE: Do not, under any circumstances, call this method when rendering is\n* currently ongoing since that may lead to rendering errors.\n*","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":50472,"to":50502}}}}],["1079",{"pageContent":"* @param {boolean} [keepLoadedFonts] - Let fonts remain attached to the DOM.\n*   NOTE: This will increase persistent memory usage, hence don't use this\n*   option unless absolutely necessary. The default value is `false`.\n* @returns {Promise} A promise that is resolved when clean-up has finished.\n*/\ncleanup(keepLoadedFonts?: boolean | undefined): Promise<any>;\n/**\n* Destroys the current document instance and terminates the worker.\n*/\ndestroy(): Promise<void>;\n/**\n* @type {DocumentInitParameters} A subset of the current\n*   {DocumentInitParameters}, which are needed in the viewer.\n*/\nget loadingParams(): DocumentInitParameters;\n/**\n* @type {PDFDocumentLoadingTask} The loadingTask for the current document.\n*/\nget loadingTask(): PDFDocumentLoadingTask;\n/**\n* @returns {Promise<Object<string, Array<Object>> | null>} A promise that is\n*   resolved with an {Object} containing /AcroForm field data for the JS\n*   sandbox, or `null` when no field data is present in the PDF file.\n*/\ngetFieldObjects(): Promise<{","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":51876,"to":51900}}}}],["1080",{"pageContent":"[x: string]: Array<Object>;\n} | null>;\n/**\n* @returns {Promise<boolean>} A promise that is resolved with `true`\n*   if some /AcroForm fields have JavaScript actions.\n*/\nhasJSActions(): Promise<boolean>;\n/**\n* @returns {Promise<Array<string> | null>} A promise that is resolved with an\n*   {Array<string>} containing IDs of annotations that have a calculation\n*   action, or `null` when no such annotations are present in the PDF file.\n*/\ngetCalculationOrderIds(): Promise<Array<string> | null>;\n}\n/**\n* Page getViewport parameters.\n*\n* @typedef {Object} GetViewportParameters\n* @property {number} scale - The desired scale of the viewport.\n* @property {number} [rotation] - The desired rotation, in degrees, of\n*   the viewport. If omitted it defaults to the page rotation.\n* @property {number} [offsetX] - The horizontal, i.e. x-axis, offset.\n*   The default value is `0`.\n* @property {number} [offsetY] - The vertical, i.e. y-axis, offset.\n*   The default value is `0`.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":53273,"to":53297}}}}],["1081",{"pageContent":"* @property {boolean} [dontFlip] - If true, the y-axis will not be\n*   flipped. The default value is `false`.\n*/\n/**\n* Page getTextContent parameters.\n*\n* @typedef {Object} getTextContentParameters\n* @property {boolean} [includeMarkedContent] - When true include marked\n*   content items in the items array of TextContent. The default is `false`.\n*/\n/**\n* Page text content.\n*\n* @typedef {Object} TextContent\n* @property {Array<TextItem | TextMarkedContent>} items - Array of\n*   {@link TextItem} and {@link TextMarkedContent} objects. TextMarkedContent\n*   items are included when includeMarkedContent is true.\n* @property {Object<string, TextStyle>} styles - {@link TextStyle} objects,\n*   indexed by font name.\n*/\n/**\n* Page text content part.\n*\n* @typedef {Object} TextItem\n* @property {string} str - Text content.\n* @property {string} dir - Text direction: 'ttb', 'ltr' or 'rtl'.\n* @property {Array<any>} transform - Transformation matrix.\n* @property {number} width - Width in device space.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":54671,"to":54698}}}}],["1082",{"pageContent":"* @property {number} height - Height in device space.\n* @property {string} fontName - Font name used by PDF.js for converted font.\n* @property {boolean} hasEOL - Indicating if the text content is followed by a\n*   line-break.\n*/\n/**\n* Page text marked content part.\n*\n* @typedef {Object} TextMarkedContent\n* @property {string} type - Either 'beginMarkedContent',\n*   'beginMarkedContentProps', or 'endMarkedContent'.\n* @property {string} id - The marked content identifier. Only used for type\n*   'beginMarkedContentProps'.\n*/\n/**\n* Text style.\n*\n* @typedef {Object} TextStyle\n* @property {number} ascent - Font ascent.\n* @property {number} descent - Font descent.\n* @property {boolean} vertical - Whether or not the text is in vertical mode.\n* @property {string} fontFamily - The possible font family.\n*/\n/**\n* Page annotation parameters.\n*\n* @typedef {Object} GetAnnotationsParameters\n* @property {string} [intent] - Determines the annotations that are fetched,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":56072,"to":56099}}}}],["1083",{"pageContent":"*   can be 'display' (viewable annotations), 'print' (printable annotations),\n*   or 'any' (all annotations). The default value is 'display'.\n*/\n/**\n* Page render parameters.\n*\n* @typedef {Object} RenderParameters\n* @property {CanvasRenderingContext2D} canvasContext - A 2D context of a DOM\n*   Canvas object.\n* @property {PageViewport} viewport - Rendering viewport obtained by calling\n*   the `PDFPageProxy.getViewport` method.\n* @property {string} [intent] - Rendering intent, can be 'display', 'print',\n*   or 'any'. The default value is 'display'.\n* @property {number} [annotationMode] Controls which annotations are rendered\n*   onto the canvas, for annotations with appearance-data; the values from\n*   {@link AnnotationMode} should be used. The following values are supported:\n*    - `AnnotationMode.DISABLE`, which disables all annotations.\n*    - `AnnotationMode.ENABLE`, which includes all possible annotations (thus\n*      it also depends on the `intent`-option, see above).","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":57473,"to":57491}}}}],["1084",{"pageContent":"*    - `AnnotationMode.ENABLE_FORMS`, which excludes annotations that contain\n*      interactive form elements (those will be rendered in the display layer).\n*    - `AnnotationMode.ENABLE_STORAGE`, which includes all possible annotations\n*      (as above) but where interactive form elements are updated with data\n*      from the {@link AnnotationStorage}-instance; useful e.g. for printing.\n*   The default value is `AnnotationMode.ENABLE`.\n* @property {Array<any>} [transform] - Additional transform, applied just\n*   before viewport transform.\n* @property {CanvasGradient | CanvasPattern | string} [background] - Background\n*   to use for the canvas.\n*   Any valid `canvas.fillStyle` can be used: a `DOMString` parsed as CSS\n*   <color> value, a `CanvasGradient` object (a linear or radial gradient) or\n*   a `CanvasPattern` object (a repetitive image). The default value is\n*   'rgb(255,255,255)'.\n*\n*   NOTE: This option may be partially, or completely, ignored when the\n*   `pageColors`-option is used.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":58865,"to":58881}}}}],["1085",{"pageContent":"* @property {Object} [pageColors] - Overwrites background and foreground colors\n*   with user defined ones in order to improve readability in high contrast\n*   mode.\n* @property {Promise<OptionalContentConfig>} [optionalContentConfigPromise] -\n*   A promise that should resolve with an {@link OptionalContentConfig}\n*   created from `PDFDocumentProxy.getOptionalContentConfig`. If `null`,\n*   the configuration will be fetched automatically with the default visibility\n*   states set.\n* @property {Map<string, HTMLCanvasElement>} [annotationCanvasMap] - Map some\n*   annotation ids with canvases used to render them.\n* @property {PrintAnnotationStorage} [printAnnotationStorage]\n*/\n/**\n* Page getOperatorList parameters.\n*\n* @typedef {Object} GetOperatorListParameters\n* @property {string} [intent] - Rendering intent, can be 'display', 'print',\n*   or 'any'. The default value is 'display'.\n* @property {number} [annotationMode] Controls which annotations are included","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":60254,"to":60272}}}}],["1086",{"pageContent":"*   in the operatorList, for annotations with appearance-data; the values from\n*   {@link AnnotationMode} should be used. The following values are supported:\n*    - `AnnotationMode.DISABLE`, which disables all annotations.\n*    - `AnnotationMode.ENABLE`, which includes all possible annotations (thus\n*      it also depends on the `intent`-option, see above).\n*    - `AnnotationMode.ENABLE_FORMS`, which excludes annotations that contain\n*      interactive form elements (those will be rendered in the display layer).\n*    - `AnnotationMode.ENABLE_STORAGE`, which includes all possible annotations\n*      (as above) but where interactive form elements are updated with data\n*      from the {@link AnnotationStorage}-instance; useful e.g. for printing.\n*   The default value is `AnnotationMode.ENABLE`.\n* @property {PrintAnnotationStorage} [printAnnotationStorage]\n*/\n/**\n* Structure tree node. The root node will have a role \"Root\".\n*\n* @typedef {Object} StructTreeNode","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":61646,"to":61662}}}}],["1087",{"pageContent":"* @property {Array<StructTreeNode | StructTreeContent>} children - Array of\n*   {@link StructTreeNode} and {@link StructTreeContent} objects.\n* @property {string} role - element's role, already mapped if a role map exists\n* in the PDF.\n*/\n/**\n* Structure tree content.\n*\n* @typedef {Object} StructTreeContent\n* @property {string} type - either \"content\" for page and stream structure\n*   elements or \"object\" for object references.\n* @property {string} id - unique id that will map to the text layer.\n*/\n/**\n* PDF page operator list.\n*\n* @typedef {Object} PDFOperatorList\n* @property {Array<number>} fnArray - Array containing the operator functions.\n* @property {Array<any>} argsArray - Array containing the arguments of the\n*   functions.\n*/\n/**\n* Proxy to a `PDFPage` in the worker thread.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":63036,"to":63059}}}}],["1088",{"pageContent":"class PDFPageProxy {\nconstructor(\npageIndex: any,\npageInfo: any,\ntransport: any,\npdfBug?: boolean\n);\n_pageIndex: any;\n_pageInfo: any;\n_transport: any;\n_stats: StatTimer | null;\n_pdfBug: boolean;\n/** @type {PDFObjects} */\ncommonObjs: PDFObjects;\nobjs: PDFObjects;\n_maybeCleanupAfterRender: boolean;\n_intentStates: Map<any, any>;\ndestroyed: boolean;\n/**\n* @type {number} Page number of the page. First page is 1.\n*/\nget pageNumber(): number;\n/**\n* @type {number} The number of degrees the page is rotated clockwise.\n*/\nget rotate(): number;\n/**\n* @type {RefProxy | null} The reference that points to this page.\n*/\nget ref(): RefProxy | null;\n/**\n* @type {number} The default size of units in 1/72nds of an inch.\n*/\nget userUnit(): number;\n/**\n* @type {Array<number>} An array of the visible portion of the PDF page in\n*   user space units [x1, y1, x2, y2].\n*/\nget view(): number[];\n/**\n* @param {GetViewportParameters} params - Viewport parameters.\n* @returns {PageViewport} Contains 'width' and 'height' properties","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":64436,"to":64477}}}}],["1089",{"pageContent":"*   along with transforms required for rendering.\n*/\ngetViewport({\nscale,\nrotation,\noffsetX,\noffsetY,\ndontFlip,\n}?: GetViewportParameters): PageViewport;\n/**\n* @param {GetAnnotationsParameters} params - Annotation parameters.\n* @returns {Promise<Array<any>>} A promise that is resolved with an\n*   {Array} of the annotation objects.\n*/\ngetAnnotations({ intent }?: GetAnnotationsParameters): Promise<Array<any>>;\n/**\n* @returns {Promise<Object>} A promise that is resolved with an\n*   {Object} with JS actions.\n*/\ngetJSActions(): Promise<Object>;\n/**\n* @type {boolean} True if only XFA form.\n*/\nget isPureXfa(): boolean;\n/**\n* @returns {Promise<Object | null>} A promise that is resolved with\n*   an {Object} with a fake DOM object (a tree structure where elements\n*   are {Object} with a name, attributes (class, style, ...), value and\n*   children, very similar to a HTML DOM tree), or `null` if no XFA exists.\n*/\ngetXfa(): Promise<Object | null>;\n/**\n* Begins the process of rendering a page to the desired context.\n*","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":65850,"to":65883}}}}],["1090",{"pageContent":"* @param {RenderParameters} params - Page render parameters.\n* @returns {RenderTask} An object that contains a promise that is\n*   resolved when the page finishes rendering.\n*/\nrender(\n{\ncanvasContext,\nviewport,\nintent,\nannotationMode,\ntransform,\nbackground,\noptionalContentConfigPromise,\nannotationCanvasMap,\npageColors,\nprintAnnotationStorage,\n}: RenderParameters,\n...args: any[]\n): RenderTask;\n/**\n* @param {GetOperatorListParameters} params - Page getOperatorList\n*   parameters.\n* @returns {Promise<PDFOperatorList>} A promise resolved with an\n*   {@link PDFOperatorList} object that represents the page's operator list.\n*/\ngetOperatorList({\nintent,\nannotationMode,\nprintAnnotationStorage,\n}?: GetOperatorListParameters): Promise<PDFOperatorList>;\n/**\n* NOTE: All occurrences of whitespace will be replaced by\n* standard spaces (0x20).\n*\n* @param {getTextContentParameters} params - getTextContent parameters.\n* @returns {ReadableStream} Stream for reading text content chunks.\n*/\nstreamTextContent({\nincludeMarkedContent,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":67256,"to":67294}}}}],["1091",{"pageContent":"}?: getTextContentParameters): ReadableStream;\n/**\n* NOTE: All occurrences of whitespace will be replaced by\n* standard spaces (0x20).\n*\n* @param {getTextContentParameters} params - getTextContent parameters.\n* @returns {Promise<TextContent>} A promise that is resolved with a\n*   {@link TextContent} object that represents the page's text content.\n*/\ngetTextContent(params?: getTextContentParameters): Promise<TextContent>;\n/**\n* @returns {Promise<StructTreeNode>} A promise that is resolved with a\n*   {@link StructTreeNode} object that represents the page's structure tree,\n*   or `null` when no structure tree is present for the current page.\n*/\ngetStructTree(): Promise<StructTreeNode>;\n/**\n* Destroys the page object.\n* @private\n*/\nprivate _destroy;\n/**\n* Cleans up resources allocated by the page.\n*\n* @param {boolean} [resetStats] - Reset page stats, if enabled.\n*   The default value is `false`.\n* @returns {boolean} Indicates if clean-up was successfully run.\n*/\ncleanup(resetStats?: boolean | undefined): boolean;\n/**","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":68667,"to":68696}}}}],["1092",{"pageContent":"* @private\n*/\nprivate _startRenderPage;\n/**\n* @private\n*/\nprivate _renderPageChunk;\n/**\n* @private\n*/\nprivate _pumpOperatorList;\n/**\n* @private\n*/\nprivate _abortOperatorList;\n/**\n* @type {StatTimer | null} Returns page stats, if enabled; returns `null`\n*   otherwise.\n*/\nget stats(): StatTimer | null;\n#private;\n}\n/**\n* PDF.js web worker abstraction that controls the instantiation of PDF\n* documents. Message handlers are used to pass information from the main\n* thread to the worker thread and vice versa. If the creation of a web\n* worker is not possible, a \"fake\" worker will be used instead.\n*\n* @param {PDFWorkerParameters} params - The worker initialization parameters.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":70069,"to":70098}}}}],["1093",{"pageContent":"class PDFWorker {\nstatic \"__#19@#workerPorts\": WeakMap<object, any>;\n/**\n* @param {PDFWorkerParameters} params - The worker initialization parameters.\n*/\nstatic fromPort(params: PDFWorkerParameters): any;\n/**\n* The current `workerSrc`, when it exists.\n* @type {string}\n*/\nstatic get workerSrc(): string;\nstatic get _mainThreadWorkerMessageHandler(): any;\nstatic get _setupFakeWorkerGlobal(): any;\nconstructor({\nname,\nport,\nverbosity,\n}?: {\nname?: null | undefined;\nport?: null | undefined;\nverbosity?: number | undefined;\n});\nname: any;\ndestroyed: boolean;\nverbosity: number;\n_readyCapability: import(\"../shared/util.js\").PromiseCapability;\n_port: any;\n_webWorker: Worker | null;\n_messageHandler: MessageHandler | null;\n/**\n* Promise for worker initialization completion.\n* @type {Promise<void>}\n*/\nget promise(): Promise<void>;\n/**\n* The current `workerPort`, when it exists.\n* @type {Worker}\n*/\nget port(): Worker;\n/**\n* The current MessageHandler-instance.\n* @type {MessageHandler}\n*/\nget messageHandler(): MessageHandler;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":71480,"to":71523}}}}],["1094",{"pageContent":"_initializeFromPort(port: any): void;\n_initialize(): void;\n_setupFakeWorker(): void;\n/**\n* Destroys the worker instance.\n*/\ndestroy(): void;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":72896,"to":72903}}}}],["1095",{"pageContent":"namespace PDFWorkerUtil {\nconst isWorkerDisabled: boolean;\nconst fallbackWorkerSrc: null;\nconst fakeWorkerId: number;\n}\n/**\n* Allows controlling of the rendering tasks.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":74306,"to":74313}}}}],["1096",{"pageContent":"class RenderTask {\nconstructor(internalRenderTask: any);\n/**\n* Callback for incremental rendering -- a function that will be called\n* each time the rendering is paused.  To continue rendering call the\n* function that is the first argument to the callback.\n* @type {function}\n*/\nonContinue: Function;\n/**\n* Promise for rendering task completion.\n* @type {Promise<void>}\n*/\nget promise(): Promise<void>;\n/**\n* Cancels the rendering task. If the task is currently rendering it will\n* not be cancelled until graphics pauses with a timeout. The promise that\n* this object extends will be rejected when cancelled.\n*\n* @param {number} [extraDelay]\n*/\ncancel(extraDelay?: number | undefined): void;\n/**\n* Whether form fields are rendered separately from the main operatorList.\n* @type {boolean}\n*/\nget separateAnnots(): boolean;\n#private;\n}\n/** @type {string} */\nexport const version: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":75716,"to":75747}}}}],["1097",{"pageContent":"// Utility for marking only some keys of an interface as optional\n// Compare to Partial<T> which marks all keys as optional\nexport type Optional<T, K extends keyof T> = Omit<T, K> & Partial<Pick<T, K>>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/type-utils.ts","loc":{"lines":{"from":1,"to":3}}}}],["1098",{"pageContent":"import pRetry from \"p-retry\";\nimport PQueueMod from \"p-queue\";\n\nconst STATUS_NO_RETRY = [\n400, // Bad Request\n401, // Unauthorized\n403, // Forbidden\n404, // Not Found\n405, // Method Not Allowed\n406, // Not Acceptable\n407, // Proxy Authentication Required\n408, // Request Timeout\n409, // Conflict\n];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":1,"to":14}}}}],["1099",{"pageContent":"interface AsyncCallerParams {\n/**\n* The maximum number of concurrent calls that can be made.\n* Defaults to `Infinity`, which means no limit.\n*/\nmaxConcurrency?: number;\n/**\n* The maximum number of retries that can be made for a single call,\n* with an exponential backoff between each attempt. Defaults to 6.\n*/\nmaxRetries?: number;\n}\n\n/**\n* A class that can be used to make async calls with concurrency and retry logic.\n*\n* This is useful for making calls to any kind of \"expensive\" external resource,\n* be it because it's rate-limited, subject to network issues, etc.\n*\n* Concurrent calls are limited by the `maxConcurrency` parameter, which defaults\n* to `Infinity`. This means that by default, all calls will be made in parallel.\n*\n* Retries are limited by the `maxRetries` parameter, which defaults to 6. This\n* means that by default, each call will be retried up to 6 times, with an\n* exponential backoff between each attempt.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":109,"to":134}}}}],["1100",{"pageContent":"class AsyncCaller {\nprotected maxConcurrency: AsyncCallerParams[\"maxConcurrency\"];\n\nprotected maxRetries: AsyncCallerParams[\"maxRetries\"];\n\nprivate queue: typeof import(\"p-queue\")[\"default\"][\"prototype\"];\n\nconstructor(params: AsyncCallerParams) {\nthis.maxConcurrency = params.maxConcurrency ?? Infinity;\nthis.maxRetries = params.maxRetries ?? 6;\n\nconst PQueue = \"default\" in PQueueMod ? PQueueMod.default : PQueueMod;\nthis.queue = new PQueue({ concurrency: this.maxConcurrency });\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":207,"to":220}}}}],["1101",{"pageContent":"// eslint-disable-next-line @typescript-eslint/no-explicit-any\ncall<A extends any[], T extends (...args: A) => Promise<any>>(\ncallable: T,\n...args: Parameters<T>\n): Promise<Awaited<ReturnType<T>>> {\nreturn this.queue.add(\n() =>\npRetry(\n() =>\ncallable(...args).catch((error) => {\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (error instanceof Error) {\nthrow error;\n} else {\nthrow new Error(error);\n}\n}),\n{\nonFailedAttempt(error) {\nif (\nerror.message.startsWith(\"Cancel\") ||\nerror.message.startsWith(\"TimeoutError\") ||\nerror.message.startsWith(\"AbortError\")\n) {\nthrow error;\n}\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nif ((error as any)?.code === \"ECONNABORTED\") {\nthrow error;\n}\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconst status = (error as any)?.response?.status;\nif (status && STATUS_NO_RETRY.includes(+status)) {\nthrow error;\n}\n},\nretries: this.maxRetries,\nrandomize: true,\n// If needed we can change some of the defaults here,\n// but they're quite sensible.\n}\n),","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":307,"to":348}}}}],["1102",{"pageContent":"{ throwOnTimeout: true }\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":420,"to":422}}}}],["1103",{"pageContent":"fetch(...args: Parameters<typeof fetch>): ReturnType<typeof fetch> {\nreturn this.call(() =>\nfetch(...args).then((res) => (res.ok ? res : Promise.reject(res)))\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":529,"to":534}}}}],["1104",{"pageContent":"// eslint-disable-next-line import/no-extraneous-dependencies\nimport { AxiosRequestConfig, AxiosPromise } from \"axios\";\n\nexport default function fetchAdapter(config: AxiosRequestConfig): AxiosPromise;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/axios-fetch-adapter.d.ts","loc":{"lines":{"from":1,"to":4}}}}],["1105",{"pageContent":"import type { AxiosRequestConfig } from \"axios\";\nimport type { EventSourceMessage } from \"./event-source-parse.js\";\n\nexport interface StreamingAxiosRequestConfig extends AxiosRequestConfig {\nresponseType: \"stream\";\n\n/**\n* Called when a message is received. NOTE: Unlike the default browser\n* EventSource.onmessage, this callback is called for _all_ events,\n* even ones with a custom `event` field.\n*/\nonmessage?: (ev: EventSourceMessage) => void;\n}\n\nexport type StreamingAxiosConfiguration =\n| StreamingAxiosRequestConfig\n| AxiosRequestConfig;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/axios-types.ts","loc":{"lines":{"from":1,"to":17}}}}],["1106",{"pageContent":"export const chunkArray = <T>(arr: T[], chunkSize: number) =>\narr.reduce((chunks, elem, index) => {\nconst chunkIndex = Math.floor(index / chunkSize);\nconst chunk = chunks[chunkIndex] || [];\n// eslint-disable-next-line no-param-reassign\nchunks[chunkIndex] = chunk.concat([elem]);\nreturn chunks;\n}, [] as T[][]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/chunk.ts","loc":{"lines":{"from":1,"to":8}}}}],["1107",{"pageContent":"import {\nisBrowser,\nisNode,\nisWebWorker,\nisJsDom,\nisDeno,\n} from \"browser-or-node\";\n\nexport const getEnv = () => {\nlet env: string;\nif (isBrowser) {\nenv = \"browser\";\n} else if (isNode) {\nenv = \"node\";\n} else if (isWebWorker) {\nenv = \"webworker\";\n} else if (isJsDom) {\nenv = \"jsdom\";\n} else if (isDeno) {\nenv = \"deno\";\n} else {\nenv = \"other\";\n}\n\nreturn env;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/env.ts","loc":{"lines":{"from":1,"to":26}}}}],["1108",{"pageContent":"/* eslint-disable prefer-template */\n/* eslint-disable default-case */\n/* eslint-disable no-plusplus */\n// Adapted from https://github.com/gfortaine/fetch-event-source/blob/main/src/parse.ts\n// due to a packaging issue in the original.\n// MIT License\n\nexport const EventStreamContentType = \"text/event-stream\";\n\n/**\n* Represents a message sent in an event stream\n* https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":1,"to":13}}}}],["1109",{"pageContent":"interface EventSourceMessage {\n/** The event ID to set the EventSource object's last event ID value. */\nid: string;\n/** A string identifying the type of event described. */\nevent: string;\n/** The event data */\ndata: string;\n/** The reconnection interval (in milliseconds) to wait before retrying the connection */\nretry?: number;\n}\n\n/**\n* Converts a ReadableStream into a callback pattern.\n* @param stream The input ReadableStream.\n* @param onChunk A function that will be called on each new byte chunk in the stream.\n* @returns {Promise<void>} A promise that will be resolved when the stream closes.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":204,"to":220}}}}],["1110",{"pageContent":"async function getBytes(\nstream: ReadableStream<Uint8Array>,\nonChunk: (arr: Uint8Array) => void\n) {\nconst reader = stream.getReader();\nlet result: ReadableStreamReadResult<Uint8Array>;\n// eslint-disable-next-line no-cond-assign\nwhile (!(result = await reader.read()).done) {\nonChunk(result.value);\n}\n}\n\nconst enum ControlChars {\nNewLine = 10,\nCarriageReturn = 13,\nSpace = 32,\nColon = 58,\n}\n\n/**\n* Parses arbitary byte chunks into EventSource line buffers.\n* Each line should be of the format \"field: value\" and ends with \\r, \\n, or \\r\\n.\n* @param onLine A function that will be called on each new EventSource line.\n* @returns A function that should be called for each incoming byte chunk.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":407,"to":431}}}}],["1111",{"pageContent":"function getLines(\nonLine: (line: Uint8Array, fieldLength: number) => void\n) {\nlet buffer: Uint8Array | undefined;\nlet position: number; // current read position\nlet fieldLength: number; // length of the `field` portion of the line\nlet discardTrailingNewline = false;\n\n// return a function that can process each incoming byte chunk:\nreturn function onChunk(arr: Uint8Array) {\nif (buffer === undefined) {\nbuffer = arr;\nposition = 0;\nfieldLength = -1;\n} else {\n// we're still parsing the old line. Append the new bytes into buffer:\nbuffer = concat(buffer, arr);\n}\n\nconst bufLength = buffer.length;\nlet lineStart = 0; // index where the current line starts\nwhile (position < bufLength) {\nif (discardTrailingNewline) {\nif (buffer[position] === ControlChars.NewLine) {\nlineStart = ++position; // skip to next char\n}\n\ndiscardTrailingNewline = false;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":615,"to":643}}}}],["1112",{"pageContent":"// start looking forward till the end of line:\nlet lineEnd = -1; // index of the \\r or \\n char\nfor (; position < bufLength && lineEnd === -1; ++position) {\nswitch (buffer[position]) {\ncase ControlChars.Colon:\nif (fieldLength === -1) {\n// first colon in line\nfieldLength = position - lineStart;\n}\nbreak;\n// eslint-disable-next-line @typescript-eslint/ban-ts-comment\n// @ts-ignore:7029 \\r case below should fallthrough to \\n:\ncase ControlChars.CarriageReturn:\ndiscardTrailingNewline = true;\n// eslint-disable-next-line no-fallthrough\ncase ControlChars.NewLine:\nlineEnd = position;\nbreak;\n}\n}\n\nif (lineEnd === -1) {\n// We reached the end of the buffer but the line hasn't ended.\n// Wait for the next arr and then continue parsing:\nbreak;\n}\n\n// we've reached the line end, send it out:\nonLine(buffer.subarray(lineStart, lineEnd), fieldLength);\nlineStart = position; // we're now on the next line\nfieldLength = -1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":820,"to":851}}}}],["1113",{"pageContent":"if (lineStart === bufLength) {\nbuffer = undefined; // we've finished reading it\n} else if (lineStart !== 0) {\n// Create a new view into buffer beginning at lineStart so we don't\n// need to copy over the previous lines when we get the new arr:\nbuffer = buffer.subarray(lineStart);\nposition -= lineStart;\n}\n};\n}\n\n/**\n* Parses line buffers into EventSourceMessages.\n* @param onId A function that will be called on each `id` field.\n* @param onRetry A function that will be called on each `retry` field.\n* @param onMessage A function that will be called on each message.\n* @returns A function that should be called for each incoming line buffer.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":1027,"to":1044}}}}],["1114",{"pageContent":"function getMessages(\nonMessage?: (msg: EventSourceMessage) => void,\nonId?: (id: string) => void,\nonRetry?: (retry: number) => void\n) {\nlet message = newMessage();\nconst decoder = new TextDecoder();\n\n// return a function that can process each incoming line buffer:\nreturn function onLine(line: Uint8Array, fieldLength: number) {\nif (line.length === 0) {\n// empty line denotes end of message. Trigger the callback and start a new message:\nonMessage?.(message);\nmessage = newMessage();\n} else if (fieldLength > 0) {\n// exclude comments and lines with no values\n// line is of format \"<field>:<value>\" or \"<field>: <value>\"\n// https://html.spec.whatwg.org/multipage/server-sent-events.html#event-stream-interpretation\nconst field = decoder.decode(line.subarray(0, fieldLength));\nconst valueOffset =\nfieldLength + (line[fieldLength + 1] === ControlChars.Space ? 2 : 1);\nconst value = decoder.decode(line.subarray(valueOffset));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":1230,"to":1251}}}}],["1115",{"pageContent":"switch (field) {\ncase \"data\":\n// if this message already has data, append the new value to the old.\n// otherwise, just set to the new value:\nmessage.data = message.data ? message.data + \"\\n\" + value : value; // otherwise,\nbreak;\ncase \"event\":\nmessage.event = value;\nbreak;\ncase \"id\":\nonId?.((message.id = value));\nbreak;\ncase \"retry\": {\nconst retry = parseInt(value, 10);\nif (!Number.isNaN(retry)) {\n// per spec, ignore non-integers\nonRetry?.((message.retry = retry));\n}\nbreak;\n}\n}\n}\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":1427,"to":1450}}}}],["1116",{"pageContent":"concat(a: Uint8Array, b: Uint8Array) {\nconst res = new Uint8Array(a.length + b.length);\nres.set(a);\nres.set(b, a.length);\nreturn res;\n}\n\nfunction newMessage(): EventSourceMessage {\n// data, event, and id must be initialized to empty strings:\n// https://html.spec.whatwg.org/multipage/server-sent-events.html#event-stream-interpretation\n// retry should be initialized to undefined so we return a consistent shape\n// to the js engine all the time: https://mathiasbynens.be/notes/shapes-ics#takeaways\nreturn {\ndata: \"\",\nevent: \"\",\nid: \"\",\nretry: undefined,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":1639,"to":1657}}}}],["1117",{"pageContent":"export const extname = (path: string) => `.${path.split(\".\").pop()}`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/extname.ts","loc":{"lines":{"from":1,"to":1}}}}],["1118",{"pageContent":"import pRetry from \"p-retry\";\n\nimport { FileLoader, LoadValues } from \"./load.js\";\nimport { extname } from \"./extname.js\";\n\nconst fetchWithTimeout = async (\nurl: string,\ninit: Omit<RequestInit, \"signal\"> & { timeout: number }\n) => {\nconst { timeout, ...rest } = init;\nconst res = await fetch(url, {\n...rest,\nsignal: AbortSignal.timeout(timeout),\n});\nreturn res;\n};\n\nconst HUB_PATH_REGEX = /lc(@[^:]+)?:\\/\\/(.*)/;\n\nconst URL_PATH_SEPARATOR = \"/\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/hub.ts","loc":{"lines":{"from":1,"to":20}}}}],["1119",{"pageContent":"const loadFromHub = async <T>(\nuri: string,\nloader: FileLoader<T>,\nvalidPrefix: string,\nvalidSuffixes: Set<string>,\nvalues: LoadValues = {}\n): Promise<T | undefined> => {\nconst LANGCHAIN_HUB_DEFAULT_REF =\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.LANGCHAIN_HUB_DEFAULT_REF\n: undefined) ?? \"master\";\nconst LANGCHAIN_HUB_URL_BASE =\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.LANGCHAIN_HUB_URL_BASE\n: undefined) ??\n\"https://raw.githubusercontent.com/hwchase17/langchain-hub/\";\n\nconst match = uri.match(HUB_PATH_REGEX);\nif (!match) {\nreturn undefined;\n}\nconst [rawRef, remotePath] = match.slice(1);\nconst ref = rawRef ? rawRef.slice(1) : LANGCHAIN_HUB_DEFAULT_REF;\nconst parts = remotePath.split(URL_PATH_SEPARATOR);\nif (parts[0] !== validPrefix) {\nreturn undefined;\n}\n\nif (!validSuffixes.has(extname(remotePath).slice(1))) {\nthrow new Error(\"Unsupported file type.\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/hub.ts","loc":{"lines":{"from":65,"to":97}}}}],["1120",{"pageContent":"const url = [LANGCHAIN_HUB_URL_BASE, ref, remotePath].join(\"/\");\nconst res = await pRetry(() => fetchWithTimeout(url, { timeout: 5000 }), {\nretries: 6,\n});\nif (res.status !== 200) {\nthrow new Error(`Could not find file at ${url}`);\n}\n\nreturn loader(await res.text(), remotePath, values);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/hub.ts","loc":{"lines":{"from":126,"to":135}}}}],["1121",{"pageContent":"// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type LoadValues = Record<string, any>;\n\nexport type FileLoader<T> = (\ntext: string,\nfilePath: string,\nvalues: LoadValues\n) => Promise<T>;\n\nexport const loadFromFile = async <T>(\nuri: string,\nloader: FileLoader<T>,\nvalues: LoadValues = {}\n): Promise<T> => {\ntry {\nconst fs = await import(\"node:fs/promises\");\nreturn loader(await fs.readFile(uri, { encoding: \"utf-8\" }), uri, values);\n} catch (e) {\nconsole.error(e);\nthrow new Error(`Could not load file at ${uri}`);\n}\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/load.ts","loc":{"lines":{"from":1,"to":22}}}}],["1122",{"pageContent":"import * as yaml from \"yaml\";\nimport { extname } from \"./extname.js\";\n\nexport const loadFileContents = (contents: string, format: string) => {\nswitch (format) {\ncase \".json\":\nreturn JSON.parse(contents);\ncase \".yml\":\ncase \".yaml\":\nreturn yaml.parse(contents);\ndefault:\nthrow new Error(`Unsupported filetype ${format}`);\n}\n};\n\nexport const parseFileConfig = (\ntext: string,\npath: string,\nsupportedTypes?: string[]\n) => {\nconst suffix = extname(path);\n\nif (\n![\".json\", \".yaml\"].includes(suffix) ||\n(supportedTypes && !supportedTypes.includes(suffix))\n) {\nthrow new Error(`Unsupported filetype ${suffix}`);\n}\n\nreturn loadFileContents(text, suffix);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/parse.ts","loc":{"lines":{"from":1,"to":31}}}}],["1123",{"pageContent":"/**\n* Source: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set#implementing_basic_set_operations\n*/\n\n/**\n* returns intersection of two sets\n*/\nexport function intersection<T>(setA: Set<T>, setB: Set<T>) {\nconst _intersection = new Set<T>();\nfor (const elem of setB) {\nif (setA.has(elem)) {\n_intersection.add(elem);\n}\n}\nreturn _intersection;\n}\n\n/**\n* returns union of two sets\n*/\nexport function union<T>(setA: Set<T>, setB: Set<T>) {\nconst _union = new Set(setA);\nfor (const elem of setB) {\n_union.add(elem);\n}\nreturn _union;\n}\n\n/**\n* returns difference of two sets\n*/\nexport function difference<T>(setA: Set<T>, setB: Set<T>) {\nconst _difference = new Set(setA);\nfor (const elem of setB) {\n_difference.delete(elem);\n}\nreturn _difference;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/set.ts","loc":{"lines":{"from":1,"to":38}}}}],["1124",{"pageContent":"import type { DataSource, DataSourceOptions } from \"typeorm\";\nimport {\nDEFAULT_SQL_DATABASE_PROMPT,\nSQL_MYSQL_PROMPT,\nSQL_POSTGRES_PROMPT,\nSQL_SQLITE_PROMPT,\n} from \"../chains/sql_db/sql_db_prompt.js\";\nimport { PromptTemplate } from \"../prompts/index.js\";\n\ninterface RawResultTableAndColumn {\ntable_name: string;\ncolumn_name: string;\ndata_type: string | undefined;\nis_nullable: string;\n}\n\nexport interface SqlDatabaseParams {\nincludesTables?: Array<string>;\nignoreTables?: Array<string>;\nsampleRowsInTableInfo?: number;\n}\n\nexport interface SqlDatabaseOptionsParams extends SqlDatabaseParams {\nappDataSourceOptions: DataSourceOptions;\n}\n\nexport interface SqlDatabaseDataSourceParams extends SqlDatabaseParams {\nappDataSource: DataSource;\n}\n\nexport type SerializedSqlDatabase = SqlDatabaseOptionsParams & {\n_type: string;\n};\n\nexport interface SqlTable {\ntableName: string;\ncolumns: SqlColumn[];\n}\n\nexport interface SqlColumn {\ncolumnName: string;\ndataType?: string;\nisNullable?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":1,"to":44}}}}],["1125",{"pageContent":"const verifyListTablesExistInDatabase = (\ntablesFromDatabase: Array<SqlTable>,\nlistTables: Array<string>,\nerrorPrefixMsg: string\n): void => {\nconst onlyTableNames: Array<string> = tablesFromDatabase.map(\n(table: SqlTable) => table.tableName\n);\nif (listTables.length > 0) {\nfor (const tableName of listTables) {\nif (!onlyTableNames.includes(tableName)) {\nthrow new Error(\n`${errorPrefixMsg} the table ${tableName} was not found in the database`\n);\n}\n}\n}\n};\n\nexport const verifyIncludeTablesExistInDatabase = (\ntablesFromDatabase: Array<SqlTable>,\nincludeTables: Array<string>\n): void => {\nverifyListTablesExistInDatabase(\ntablesFromDatabase,\nincludeTables,\n\"Include tables not found in database:\"\n);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":280,"to":308}}}}],["1126",{"pageContent":"const verifyIgnoreTablesExistInDatabase = (\ntablesFromDatabase: Array<SqlTable>,\nignoreTables: Array<string>\n): void => {\nverifyListTablesExistInDatabase(\ntablesFromDatabase,\nignoreTables,\n\"Ignore tables not found in database:\"\n);\n};\n\nconst formatToSqlTable = (\nrawResultsTableAndColumn: Array<RawResultTableAndColumn>\n): Array<SqlTable> => {\nconst sqlTable: Array<SqlTable> = [];\nfor (const oneResult of rawResultsTableAndColumn) {\nconst sqlColumn = {\ncolumnName: oneResult.column_name,\ndataType: oneResult.data_type,\nisNullable: oneResult.is_nullable === \"YES\",\n};\nconst currentTable = sqlTable.find(\n(oneTable) => oneTable.tableName === oneResult.table_name\n);\nif (currentTable) {\ncurrentTable.columns.push(sqlColumn);\n} else {\nconst newTable = {\ntableName: oneResult.table_name,\ncolumns: [sqlColumn],\n};\nsqlTable.push(newTable);\n}\n}\n\nreturn sqlTable;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":559,"to":595}}}}],["1127",{"pageContent":"const getTableAndColumnsName = async (\nappDataSource: DataSource\n): Promise<Array<SqlTable>> => {\nlet sql;\nif (appDataSource.options.type === \"postgres\") {\nconst schema = appDataSource.options?.schema ?? \"public\";\nsql = `SELECT \nt.table_name, \nc.* \nFROM \ninformation_schema.tables t \nJOIN information_schema.columns c \nON t.table_name = c.table_name \nWHERE \nt.table_schema = '${schema}' \nAND c.table_schema = '${schema}' \nORDER BY \nt.table_name,\nc.ordinal_position;`;\nconst rep = await appDataSource.query(sql);\n\nreturn formatToSqlTable(rep);\n}\n\nif (appDataSource.options.type === \"sqlite\") {\nsql =\n\"SELECT \\n\" +\n\"   m.name AS table_name,\\n\" +\n\"   p.name AS column_name,\\n\" +\n\"   p.type AS data_type,\\n\" +\n\"   CASE \\n\" +\n\"      WHEN p.\\\"notnull\\\" = 0 THEN 'YES' \\n\" +\n\"      ELSE 'NO' \\n\" +\n\"   END AS is_nullable \\n\" +\n\"FROM \\n\" +\n\"   sqlite_master m \\n\" +\n\"JOIN \\n\" +\n\"   pragma_table_info(m.name) p \\n\" +\n\"WHERE \\n\" +\n\"   m.type = 'table' AND \\n\" +\n\"   m.name NOT LIKE 'sqlite_%';\\n\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":838,"to":878}}}}],["1128",{"pageContent":"const rep = await appDataSource.query(sql);\n\nreturn formatToSqlTable(rep);\n}\n\nif (appDataSource.options.type === \"mysql\") {\nsql =\n\"SELECT \" +\n\"TABLE_NAME AS table_name, \" +\n\"COLUMN_NAME AS column_name, \" +\n\"DATA_TYPE AS data_type, \" +\n\"IS_NULLABLE AS is_nullable \" +\n\"FROM INFORMATION_SCHEMA.COLUMNS \" +\n`WHERE TABLE_SCHEMA = '${appDataSource.options.database}';`;\n\nconst rep = await appDataSource.query(sql);\n\nreturn formatToSqlTable(rep);\n}\n\nthrow new Error(\"Database type not implemented yet\");\n};\n\nconst formatSqlResponseToSimpleTableString = (rawResult: unknown): string => {\nif (!rawResult || !Array.isArray(rawResult) || rawResult.length === 0) {\nreturn \"\";\n}\n\nlet globalString = \"\";\nfor (const oneRow of rawResult) {\nglobalString += `${Object.values(oneRow).reduce(\n(completeString, columnValue) => `${completeString} ${columnValue}`,\n\"\"\n)}\\n`;\n}\n\nreturn globalString;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":1114,"to":1151}}}}],["1129",{"pageContent":"const generateTableInfoFromTables = async (\ntables: Array<SqlTable> | undefined,\nappDataSource: DataSource,\nnbSampleRow: number\n): Promise<string> => {\nif (!tables) {\nreturn \"\";\n}\n\nlet globalString = \"\";\nfor (const currentTable of tables) {\n// Add the creation of the table in SQL\nconst schema =\nappDataSource.options.type === \"postgres\"\n? appDataSource.options?.schema ?? \"public\"\n: null;\nlet sqlCreateTableQuery = schema\n? `CREATE TABLE \"${schema}\".\"${currentTable.tableName}\" (\\n`\n: `CREATE TABLE ${currentTable.tableName} (\\n`;\nfor (const [key, currentColumn] of currentTable.columns.entries()) {\nif (key > 0) {\nsqlCreateTableQuery += \", \";\n}\nsqlCreateTableQuery += `${currentColumn.columnName} ${\ncurrentColumn.dataType\n} ${currentColumn.isNullable ? \"\" : \"NOT NULL\"}`;\n}\nsqlCreateTableQuery += \") \\n\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":1394,"to":1421}}}}],["1130",{"pageContent":"let sqlSelectInfoQuery;\nif (appDataSource.options.type === \"mysql\") {\n// We use backticks to quote the table names and thus allow for example spaces in table names\nsqlSelectInfoQuery = `SELECT * FROM \\`${currentTable.tableName}\\` LIMIT ${nbSampleRow};\\n`;\n} else if (appDataSource.options.type === \"postgres\") {\nconst schema = appDataSource.options?.schema ?? \"public\";\nsqlSelectInfoQuery = `SELECT * FROM \"${schema}\".\"${currentTable.tableName}\" LIMIT ${nbSampleRow};\\n`;\n} else {\nsqlSelectInfoQuery = `SELECT * FROM \"${currentTable.tableName}\" LIMIT ${nbSampleRow};\\n`;\n}\n\nconst columnNamesConcatString = `${currentTable.columns.reduce(\n(completeString, column) => `${completeString} ${column.columnName}`,\n\"\"\n)}\\n`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":1668,"to":1682}}}}],["1131",{"pageContent":"let sample = \"\";\ntry {\nconst infoObjectResult = nbSampleRow\n? await appDataSource.query(sqlSelectInfoQuery)\n: null;\nsample = formatSqlResponseToSimpleTableString(infoObjectResult);\n} catch (error) {\n// If the request fails we catch it and only display a log message\nconsole.log(error);\n}\n\nglobalString = globalString.concat(\nsqlCreateTableQuery +\nsqlSelectInfoQuery +\ncolumnNamesConcatString +\nsample\n);\n}\n\nreturn globalString;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":1933,"to":1953}}}}],["1132",{"pageContent":"const getPromptTemplateFromDataSource = (\nappDataSource: DataSource\n): PromptTemplate => {\nif (appDataSource.options.type === \"postgres\") {\nreturn SQL_POSTGRES_PROMPT;\n}\n\nif (appDataSource.options.type === \"sqlite\") {\nreturn SQL_SQLITE_PROMPT;\n}\n\nif (appDataSource.options.type === \"mysql\") {\nreturn SQL_MYSQL_PROMPT;\n}\n\nreturn DEFAULT_SQL_DATABASE_PROMPT;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":2214,"to":2230}}}}],["1133",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { AsyncCaller } from \"../async_caller.js\";\nimport { OpenAI } from \"../../llms/openai.js\";\n\ntest(\"AsyncCaller.call passes on arguments and returns return value\", async () => {\nconst caller = new AsyncCaller({});\nconst callable = () => fetch(\"https://httpstat.us/200\");\n\nconst resultDirect = await callable();\nconst resultWrapped = await caller.call(callable);\n\nexpect(resultDirect.status).toEqual(200);\nexpect(resultWrapped.status).toEqual(200);\n});\n\ntest(\"AsyncCaller doesn't retry on axios error 401\", async () => {\nconst llm = new OpenAI({ openAIApiKey: \"invalid\" });\n\nawait expect(() => llm.call(\"test\")).rejects.toThrowError(\n\"Request failed with status code 401\"\n);\n}, 5000);\n\ntest(\"AsyncCaller doesn't retry on timeout\", async () => {\nconst caller = new AsyncCaller({});\nconst callable = () =>\nfetch(\"https://httpstat.us/200?sleep=1000\", {\nsignal: AbortSignal.timeout(10),\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/async_caller.int.test.ts","loc":{"lines":{"from":1,"to":29}}}}],["1134",{"pageContent":"await expect(() => caller.call(callable)).rejects.toThrowError(\n\"TimeoutError: The operation was aborted due to timeout\"\n);\n}, 5000);\n\ntest(\"AsyncCaller doesn't retry on signal abort\", async () => {\nconst controller = new AbortController();\nconst caller = new AsyncCaller({});\nconst callable = () => {\nconst ret = fetch(\"https://httpstat.us/200?sleep=1000\", {\nsignal: controller.signal,\n});\n\ncontroller.abort();\n\nreturn ret;\n};\n\nawait expect(() => caller.call(callable)).rejects.toThrowError(\n\"AbortError: This operation was aborted\"\n);\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/async_caller.int.test.ts","loc":{"lines":{"from":53,"to":74}}}}],["1135",{"pageContent":"import { test, expect, jest } from \"@jest/globals\";\nimport { AsyncCaller } from \"../async_caller.js\";\n\ntest(\"AsyncCaller passes on arguments and returns return value\", async () => {\nconst caller = new AsyncCaller({});\nconst callable = jest.fn((arg1, arg2) => Promise.resolve([arg2, arg1]));\n\nconst resultDirect = await callable(1, 2);\nconst resultWrapped = await caller.call(callable, 1, 2);\n\nexpect(resultDirect).toEqual([2, 1]);\nexpect(resultWrapped).toEqual([2, 1]);\n});\n\ntest(\"AsyncCaller retries on failure\", async () => {\nconst caller = new AsyncCaller({});\n\n// A direct call throws an error.\nlet callable = jest\n.fn<() => Promise<number[]>>()\n.mockRejectedValueOnce(\"error\")\n.mockResolvedValueOnce([2, 1]);\n\nawait expect(() => callable()).rejects.toEqual(\"error\");\n\n// A wrapped call retries and succeeds.\ncallable = jest\n.fn<() => Promise<number[]>>()\n.mockRejectedValueOnce(\"error\")\n.mockResolvedValueOnce([2, 1]);\n\nconst resultWrapped = await caller.call(callable);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/async_caller.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["1136",{"pageContent":"expect(resultWrapped).toEqual([2, 1]);\nexpect(callable.mock.calls).toHaveLength(2);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/async_caller.test.ts","loc":{"lines":{"from":36,"to":38}}}}],["1137",{"pageContent":"import { describe, it, expect, jest } from \"@jest/globals\";\nimport { AxiosResponse } from \"axios\";\nimport fetchAdapter from \"../axios-fetch-adapter.js\";\n\nconst mockFetchForOpenAIStream = async ({\nchunks,\nstatus,\ncontentType,\n}: {\nchunks: Array<string>;\nstatus: number;\ncontentType: string;\n}) => {\n// Mock stream response chunks.\nconst stream = new ReadableStream({\nasync start(controller) {\nchunks.forEach((chunk) => {\ncontroller.enqueue(new TextEncoder().encode(chunk));\n});\ncontroller.close();\n},\n});\n\n// Mock Fetch API call.\njest.spyOn(global, \"fetch\").mockImplementation(\nasync () =>\nnew Response(stream, {\nstatus,\nheaders: {\n\"Content-Type\": contentType,\n},\n})\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/openai-stream.test.ts","loc":{"lines":{"from":1,"to":33}}}}],["1138",{"pageContent":"let error: Error | null = null;\nconst receivedChunks: Array<string> = [];\nconst resp = await fetchAdapter({\nurl: \"https://example.com\",\nmethod: \"POST\",\nresponseType: \"stream\",\nonmessage: (strChunk: { data: string }) => {\nreceivedChunks.push(\nJSON.parse(strChunk.data).choices[0].delta.content ?? \"\"\n);\n},\n} as unknown as never).catch((err) => {\nerror = err;\nreturn null;\n});\n\nreturn { resp, receivedChunks, error } as {\nresp: AxiosResponse | null;\nreceivedChunks: Array<string>;\nerror: Error | null;\n};\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/openai-stream.test.ts","loc":{"lines":{"from":139,"to":160}}}}],["1139",{"pageContent":"describe(\"OpenAI Stream Tests\", () => {\nit(\"should return a 200 response chunk by chunk\", async () => {\n// When stream mode enabled, OpenAI responds with a stream of `data: {...}\\n\\n` chunks.\nconst { resp, receivedChunks, error } = await mockFetchForOpenAIStream({\nstatus: 200,\ncontentType: \"text/event-stream\",\nchunks: [\n'data: {\"choices\":[{\"delta\":{\"role\":\"assistant\"},\"index\":0,\"finish_reason\":null}]}\\n\\n',\n'data: {\"choices\":[{\"delta\":{\"content\":\"Hello\"},\"index\":0,\"finish_reason\":null}]}\\n\\n',\n'data: {\"choices\":[{\"delta\":{\"content\":\" World\"},\"index\":0,\"finish_reason\":null}]}\\n\\n',\n'data: {\"choices\":[{\"delta\":{\"content\":\"!\"},\"index\":0,\"finish_reason\":null}]}\\n\\n',\n'data: {\"choices\":[{\"delta\":{},\"index\":0,\"finish_reason\":\"stop\"}]}\\n\\n',\n],\n});\n\nexpect(error).toEqual(null);\nexpect(resp?.status).toEqual(200);\nexpect(receivedChunks).toEqual([\"\", \"Hello\", \" World\", \"!\", \"\"]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/openai-stream.test.ts","loc":{"lines":{"from":275,"to":293}}}}],["1140",{"pageContent":"it(\"should handle OpenAI 400 json error\", async () => {\n// OpenAI returns errors with application/json content type.\n// Even if stream mode is enabled, the error is returned as a normal JSON body.\n// Error information is in the `error` field.\nconst { resp, receivedChunks, error } = await mockFetchForOpenAIStream({\nstatus: 400,\ncontentType: \"application/json\",\nchunks: [\nJSON.stringify({\nerror: {},\n}),\n],\n});\n\nexpect(error).toEqual(null);\nexpect(resp?.status).toEqual(400);\nexpect(resp?.data).toEqual({ error: {} });\nexpect(receivedChunks).toEqual([]);\n});\n\nit(\"should handle 500 non-json error\", async () => {\nconst { resp, receivedChunks, error } = await mockFetchForOpenAIStream({\nstatus: 500,\ncontentType: \"text/plain\",\nchunks: [\"Some error message...\"],\n});\nexpect(error).toEqual(null);\nexpect(resp?.status).toEqual(500);\nexpect(resp?.data).toEqual(\"Some error message...\");\nexpect(receivedChunks).toEqual([]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/openai-stream.test.ts","loc":{"lines":{"from":389,"to":419}}}}],["1141",{"pageContent":"it(\"should throw on 500 non-json body with json content type\", async () => {\nconst { resp, receivedChunks, error } = await mockFetchForOpenAIStream({\nstatus: 500,\ncontentType: \"application/json\",\nchunks: [\"a non-json error body\"],\n});\nexpect(resp).toEqual(null);\nexpect(error?.message).toContain(\"Unexpected token\");\nexpect(receivedChunks).toEqual([]);\n});\n\nit(\"should throw the generic error if non-stream content is detected\", async () => {\nconst { resp, receivedChunks, error } = await mockFetchForOpenAIStream({\nstatus: 200,\ncontentType: \"text/plain\",\nchunks: [\"a non-stream body\"],\n});\nexpect(resp).toEqual(null);\nexpect(error?.message).toBe(\n\"Expected content-type to be text/event-stream, Actual: text/plain\"\n);\nexpect(receivedChunks).toEqual([]);\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/openai-stream.test.ts","loc":{"lines":{"from":514,"to":537}}}}],["1142",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { difference, intersection, union } from \"../set.js\";\n\ntest(\"difference\", () => {\nconst set1 = new Set([\"a\", \"b\"]);\nconst set2 = new Set([\"b\", \"c\"]);\n\nconst resultSet = difference(set1, set2);\nexpect(resultSet).toMatchInlineSnapshot(`\nSet {\n\"a\",\n}\n`);\n});\n\ntest(\"intersection\", () => {\nconst set1 = new Set([\"a\", \"b\", \"c\", \"d\"]);\nconst set2 = new Set([\"b\", \"c\", \"e\"]);\n\nconst resultSet = intersection(set1, set2);\nexpect(resultSet).toMatchInlineSnapshot(`\nSet {\n\"b\",\n\"c\",\n}\n`);\n});\n\ntest(\"union\", () => {\nconst set1 = new Set([\"a\", \"b\"]);\nconst set2 = new Set([\"c\", \"d\"]);\n\nconst resultSet = union(set1, set2);\nexpect(resultSet).toMatchInlineSnapshot(`\nSet {\n\"a\",\n\"b\",\n\"c\",\n\"d\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/set.test.ts","loc":{"lines":{"from":1,"to":42}}}}],["1143",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { DataSource } from \"typeorm\";\nimport {\ngetPromptTemplateFromDataSource,\nverifyIgnoreTablesExistInDatabase,\nverifyIncludeTablesExistInDatabase,\n} from \"../sql_utils.js\";\nimport { SQL_SQLITE_PROMPT } from \"../../chains/sql_db/sql_db_prompt.js\";\n\ntest(\"Find include tables when there are there\", () => {\nconst includeTables = [\"user\", \"score\"];\nconst allTables = [\n{ tableName: \"plop\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"score\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"user\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"log\", columns: [{ columnName: \"id\" }] },\n];\n\nexpect(() =>\nverifyIncludeTablesExistInDatabase(allTables, includeTables)\n).not.toThrow();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/sql_utils.test.ts","loc":{"lines":{"from":1,"to":22}}}}],["1144",{"pageContent":"test(\"Throw Error when include tables are not there\", () => {\nconst includeTables = [\"user\", \"score\"];\nconst allTables = [\n{ tableName: \"plop\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"score\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"log\", columns: [{ columnName: \"id\" }] },\n];\n\nexpect(() =>\nverifyIncludeTablesExistInDatabase(allTables, includeTables)\n).toThrow();\n});\n\ntest(\"Find include tables when there are there\", () => {\nconst includeTables = [\"user\", \"score\"];\nconst allTables = [\n{ tableName: \"user\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"plop\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"score\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"log\", columns: [{ columnName: \"id\" }] },\n];\n\nexpect(() =>\nverifyIgnoreTablesExistInDatabase(allTables, includeTables)\n).not.toThrow();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/sql_utils.test.ts","loc":{"lines":{"from":74,"to":99}}}}],["1145",{"pageContent":"test(\"Throw Error when include tables are not there\", () => {\nconst includeTables = [\"user\", \"score\"];\nconst allTables = [\n{ tableName: \"plop\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"score\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"log\", columns: [{ columnName: \"id\" }] },\n];\n\nexpect(() =>\nverifyIgnoreTablesExistInDatabase(allTables, includeTables)\n).toThrow();\n});\n\ntest(\"return sqlite template when the DataSource is sqlite\", () => {\nconst datasource = new DataSource({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/sql_utils.test.ts","loc":{"lines":{"from":147,"to":161}}}}],["1146",{"pageContent":": \"sqlite\",\ndatabase: \"Chinook.db\",\n});\n\nconst promptTemplate = getPromptTemplateFromDataSource(datasource);\nexpect(promptTemplate).toEqual(SQL_SQLITE_PROMPT);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/sql_utils.test.ts","loc":{"lines":{"from":219,"to":225}}}}],["1147",{"pageContent":"import { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\nimport { BaseRetriever } from \"../schema/index.js\";\n\nexport interface VectorStoreRetrieverInput<V extends VectorStore> {\nvectorStore: V;\nk?: number;\nfilter?: V[\"FilterType\"];\n}\n\nexport class VectorStoreRetriever<\nV extends VectorStore = VectorStore\n> extends BaseRetriever {\nvectorStore: V;\n\nk = 4;\n\nfilter?: V[\"FilterType\"];\n\nconstructor(fields: VectorStoreRetrieverInput<V>) {\nsuper();\nthis.vectorStore = fields.vectorStore;\nthis.k = fields.k ?? this.k;\nthis.filter = fields.filter;\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nconst results = await this.vectorStore.similaritySearch(\nquery,\nthis.k,\nthis.filter\n);\nreturn results;\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nawait this.vectorStore.addDocuments(documents);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/base.ts","loc":{"lines":{"from":1,"to":39}}}}],["1148",{"pageContent":"abstract class VectorStore {\ndeclare FilterType: object;\n\nembeddings: Embeddings;\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconstructor(embeddings: Embeddings, _dbConfig: Record<string, any>) {\nthis.embeddings = embeddings;\n}\n\nabstract addVectors(\nvectors: number[][],\ndocuments: Document[]\n): Promise<void>;\n\nabstract addDocuments(documents: Document[]): Promise<void>;\n\nabstract similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: this[\"FilterType\"]\n): Promise<[Document, number][]>;\n\nasync similaritySearch(\nquery: string,\nk = 4,\nfilter: this[\"FilterType\"] | undefined = undefined\n): Promise<Document[]> {\nconst results = await this.similaritySearchVectorWithScore(\nawait this.embeddings.embedQuery(query),\nk,\nfilter\n);\n\nreturn results.map((result) => result[0]);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/base.ts","loc":{"lines":{"from":133,"to":168}}}}],["1149",{"pageContent":"async similaritySearchWithScore(\nquery: string,\nk = 4,\nfilter: this[\"FilterType\"] | undefined = undefined\n): Promise<[Document, number][]> {\nreturn this.similaritySearchVectorWithScore(\nawait this.embeddings.embedQuery(query),\nk,\nfilter\n);\n}\n\nstatic fromTexts(\n_texts: string[],\n_metadatas: object[] | object,\n_embeddings: Embeddings,\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_dbConfig: Record<string, any>\n): Promise<VectorStore> {\nthrow new Error(\n\"the Langchain vectorstore implementation you are using forgot to override this, please report a bug\"\n);\n}\n\nstatic fromDocuments(\n_docs: Document[],\n_embeddings: Embeddings,\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_dbConfig: Record<string, any>\n): Promise<VectorStore> {\nthrow new Error(\n\"the Langchain vectorstore implementation you are using forgot to override this, please report a bug\"\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/base.ts","loc":{"lines":{"from":265,"to":298}}}}],["1150",{"pageContent":"asRetriever(\nk?: number,\nfilter?: this[\"FilterType\"]\n): VectorStoreRetriever<this> {\nreturn new VectorStoreRetriever({ vectorStore: this, k, filter });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/base.ts","loc":{"lines":{"from":391,"to":397}}}}],["1151",{"pageContent":"abstract class SaveableVectorStore extends VectorStore {\nabstract save(directory: string): Promise<void>;\n\nstatic load(\n_directory: string,\n_embeddings: Embeddings\n): Promise<SaveableVectorStore> {\nthrow new Error(\"Not implemented\");\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/base.ts","loc":{"lines":{"from":522,"to":531}}}}],["1152",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\nimport type { ChromaClient as ChromaClientT, Collection } from \"chromadb\";\n\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { VectorStore } from \"./base.js\";\nimport { Document } from \"../document.js\";\n\nexport type ChromaLibArgs =\n| {\nurl?: string;\nnumDimensions?: number;\ncollectionName?: string;\n}\n| {\nindex?: ChromaClientT;\nnumDimensions?: number;\ncollectionName?: string;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":1,"to":18}}}}],["1153",{"pageContent":"class Chroma extends VectorStore {\nindex?: ChromaClientT;\n\ncollection?: Collection;\n\ncollectionName: string;\n\nnumDimensions?: number;\n\nurl: string;\n\nconstructor(embeddings: Embeddings, args: ChromaLibArgs) {\nsuper(embeddings, args);\nthis.numDimensions = args.numDimensions;\nthis.embeddings = embeddings;\nthis.collectionName = ensureCollectionName(args.collectionName);\nif (\"index\" in args) {\nthis.index = args.index;\n} else if (\"url\" in args) {\nthis.url = args.url || \"http://localhost:8000\";\n}\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nawait this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync ensureCollection(): Promise<Collection> {\nif (!this.collection) {\nif (!this.index) {\nconst { ChromaClient } = await Chroma.imports();\nthis.index = new ChromaClient(this.url);\n}\nthis.collection = await this.index.getOrCreateCollection(\nthis.collectionName\n);\n}\n\nreturn this.collection;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":190,"to":233}}}}],["1154",{"pageContent":"async addVectors(vectors: number[][], documents: Document[]) {\nif (vectors.length === 0) {\nreturn;\n}\nif (this.numDimensions === undefined) {\nthis.numDimensions = vectors[0].length;\n}\nif (vectors.length !== documents.length) {\nthrow new Error(`Vectors and metadatas must have the same length`);\n}\nif (vectors[0].length !== this.numDimensions) {\nthrow new Error(\n`Vectors must have the same length as the number of dimensions (${this.numDimensions})`\n);\n}\n\nconst collection = await this.ensureCollection();\nconst docstoreSize = await collection.count();\nawait collection.add(\nArray.from({ length: vectors.length }, (_, i) =>\n(docstoreSize + i).toString()\n),\nvectors,\ndocuments.map(({ metadata }) => metadata),\ndocuments.map(({ pageContent }) => pageContent)\n);\n}\n\nasync similaritySearchVectorWithScore(query: number[], k: number) {\nconst collection = await this.ensureCollection();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":382,"to":411}}}}],["1155",{"pageContent":"// similaritySearchVectorWithScore supports one query vector at a time\n// chroma supports multiple query vectors at a time\nconst result = await collection.query(query, k);\n\nconst { ids, distances, documents, metadatas } = result;\nif (!ids || !distances || !documents || !metadatas) {\nreturn [];\n}\n// get the result data from the first and only query vector\nconst [firstIds] = ids;\nconst [firstDistances] = distances;\nconst [firstDocuments] = documents;\nconst [firstMetadatas] = metadatas;\n\nconst results: [Document, number][] = [];\nfor (let i = 0; i < firstIds.length; i += 1) {\nresults.push([\nnew Document({\npageContent: firstDocuments[i],\nmetadata: firstMetadatas[i],\n}),\nfirstDistances[i],\n]);\n}\nreturn results;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":564,"to":589}}}}],["1156",{"pageContent":"static async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig: {\ncollectionName?: string;\nurl?: string;\n}\n): Promise<Chroma> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn Chroma.fromDocuments(docs, embeddings, dbConfig);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: {\ncollectionName?: string;\nurl?: string;\n}\n): Promise<Chroma> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nstatic async fromExistingCollection(\nembeddings: Embeddings,\ndbConfig: {\ncollectionName: string;\nurl?: string;\n}\n): Promise<Chroma> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.ensureCollection();\nreturn instance;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":745,"to":789}}}}],["1157",{"pageContent":"static async imports(): Promise<{\nChromaClient: typeof ChromaClientT;\n}> {\ntry {\nconst { ChromaClient } = await import(\"chromadb\");\nreturn { ChromaClient };\n} catch (e) {\nthrow new Error(\n\"Please install chromadb as a dependency with, e.g. `npm install -S chromadb`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":939,"to":951}}}}],["1158",{"pageContent":"ensureCollectionName(collectionName?: string) {\nif (!collectionName) {\nreturn `langchain-${uuidv4()}`;\n}\nreturn collectionName;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":1131,"to":1136}}}}],["1159",{"pageContent":"import type {\nHierarchicalNSW as HierarchicalNSWT,\nSpaceName,\n} from \"hnswlib-node\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { SaveableVectorStore } from \"./base.js\";\nimport { Document } from \"../document.js\";\nimport { InMemoryDocstore } from \"../docstore/index.js\";\n\nexport interface HNSWLibBase {\nspace: SpaceName;\nnumDimensions?: number;\n}\n\nexport interface HNSWLibArgs extends HNSWLibBase {\ndocstore?: InMemoryDocstore;\nindex?: HierarchicalNSWT;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":1,"to":18}}}}],["1160",{"pageContent":"class HNSWLib extends SaveableVectorStore {\ndeclare FilterType: (doc: Document) => boolean;\n\n_index?: HierarchicalNSWT;\n\ndocstore: InMemoryDocstore;\n\nargs: HNSWLibBase;\n\nconstructor(embeddings: Embeddings, args: HNSWLibArgs) {\nsuper(embeddings, args);\nthis._index = args.index;\nthis.args = args;\nthis.embeddings = embeddings;\nthis.docstore = args?.docstore ?? new InMemoryDocstore();\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nprivate static async getHierarchicalNSW(args: HNSWLibBase) {\nconst { HierarchicalNSW } = await HNSWLib.imports();\nif (!args.space) {\nthrow new Error(\"hnswlib-node requires a space argument\");\n}\nif (args.numDimensions === undefined) {\nthrow new Error(\"hnswlib-node requires a numDimensions argument\");\n}\nreturn new HierarchicalNSW(args.space, args.numDimensions);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":246,"to":280}}}}],["1161",{"pageContent":"private async initIndex(vectors: number[][]) {\nif (!this._index) {\nif (this.args.numDimensions === undefined) {\nthis.args.numDimensions = vectors[0].length;\n}\nthis.index = await HNSWLib.getHierarchicalNSW(this.args);\n}\nif (!this.index.getCurrentCount()) {\nthis.index.initIndex(vectors.length);\n}\n}\n\npublic get index(): HierarchicalNSWT {\nif (!this._index) {\nthrow new Error(\n\"Vector store not initialised yet. Try calling `addTexts` first.\"\n);\n}\nreturn this._index;\n}\n\nprivate set index(index: HierarchicalNSWT) {\nthis._index = index;\n}\n\nasync addVectors(vectors: number[][], documents: Document[]) {\nif (vectors.length === 0) {\nreturn;\n}\nawait this.initIndex(vectors);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":488,"to":517}}}}],["1162",{"pageContent":"// TODO here we could optionally normalise the vectors to unit length\n// so that dot product is equivalent to cosine similarity, like this\n// https://github.com/nmslib/hnswlib/issues/384#issuecomment-1155737730\n// While we only support OpenAI embeddings this isn't necessary\nif (vectors.length !== documents.length) {\nthrow new Error(`Vectors and metadatas must have the same length`);\n}\nif (vectors[0].length !== this.args.numDimensions) {\nthrow new Error(\n`Vectors must have the same length as the number of dimensions (${this.args.numDimensions})`\n);\n}\nconst capacity = this.index.getMaxElements();\nconst needed = this.index.getCurrentCount() + vectors.length;\nif (needed > capacity) {\nthis.index.resizeIndex(needed);\n}\nconst docstoreSize = this.docstore.count;\nfor (let i = 0; i < vectors.length; i += 1) {\nthis.index.addPoint(vectors[i], docstoreSize + i);\nthis.docstore.add({ [docstoreSize + i]: documents[i] });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":734,"to":756}}}}],["1163",{"pageContent":"async similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: this[\"FilterType\"]\n) {\nif (this.args.numDimensions && !this._index) {\nawait this.initIndex([[]]);\n}\nif (query.length !== this.args.numDimensions) {\nthrow new Error(\n`Query vector must have the same length as the number of dimensions (${this.args.numDimensions})`\n);\n}\nif (k > this.index.getCurrentCount()) {\nconst total = this.index.getCurrentCount();\nconsole.warn(\n`k (${k}) is greater than the number of elements in the index (${total}), setting k to ${total}`\n);\n// eslint-disable-next-line no-param-reassign\nk = total;\n}\nconst filterFunction = (label: number): boolean => {\nif (!filter) {\nreturn true;\n}\nconst document = this.docstore.search(String(label));\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (typeof document !== \"string\") {\nreturn filter(document);\n}\nreturn false;\n};\nconst result = this.index.searchKnn(\nquery,\nk,\nfilter ? filterFunction : undefined\n);\nreturn result.neighbors.map(\n(docIndex, resultIndex) =>\n[","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":964,"to":1003}}}}],["1164",{"pageContent":"this.docstore.search(String(docIndex)),\nresult.distances[resultIndex],\n] as [Document, number]\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":1210,"to":1214}}}}],["1165",{"pageContent":"async save(directory: string) {\nconst fs = await import(\"node:fs/promises\");\nconst path = await import(\"node:path\");\nawait fs.mkdir(directory, { recursive: true });\nawait Promise.all([\nthis.index.writeIndex(path.join(directory, \"hnswlib.index\")),\nawait fs.writeFile(\npath.join(directory, \"args.json\"),\nJSON.stringify(this.args)\n),\nawait fs.writeFile(\npath.join(directory, \"docstore.json\"),\nJSON.stringify(Array.from(this.docstore._docs.entries()))\n),\n]);\n}\n\nstatic async load(directory: string, embeddings: Embeddings) {\nconst fs = await import(\"node:fs/promises\");\nconst path = await import(\"node:path\");\nconst args = JSON.parse(\nawait fs.readFile(path.join(directory, \"args.json\"), \"utf8\")\n);\nconst index = await HNSWLib.getHierarchicalNSW(args);\nconst [docstoreFiles] = await Promise.all([\nfs\n.readFile(path.join(directory, \"docstore.json\"), \"utf8\")\n.then(JSON.parse),\nindex.readIndex(path.join(directory, \"hnswlib.index\")),\n]);\nargs.docstore = new InMemoryDocstore(new Map(docstoreFiles));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":1454,"to":1484}}}}],["1166",{"pageContent":"args.index = index;\n\nreturn new HNSWLib(embeddings, args);\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig?: {\ndocstore?: InMemoryDocstore;\n}\n): Promise<HNSWLib> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn HNSWLib.fromDocuments(docs, embeddings, dbConfig);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig?: {\ndocstore?: InMemoryDocstore;\n}\n): Promise<HNSWLib> {\nconst args: HNSWLibArgs = {\ndocstore: dbConfig?.docstore,\nspace: \"cosine\",\n};\nconst instance = new this(embeddings, args);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nstatic async imports(): Promise<{\nHierarchicalNSW: typeof HierarchicalNSWT;\n}> {\ntry {\nconst {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":1691,"to":1736}}}}],["1167",{"pageContent":": { HierarchicalNSW },\n} = await import(\"hnswlib-node\");\n\nreturn { HierarchicalNSW };\n} catch (err) {\nthrow new Error(\n\"Please install hnswlib-node as a dependency with, e.g. `npm install -S hnswlib-node`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":1944,"to":1954}}}}],["1168",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/vectorstores' is deprecated. Import from eg. 'langchain/vectorstores/pinecone' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport { HNSWLib } from \"./hnswlib.js\";\nexport { Chroma } from \"./chroma.js\";\nexport { PineconeStore } from \"./pinecone.js\";\nexport { VectorStore, SaveableVectorStore } from \"./base.js\";\nexport { SupabaseVectorStore } from \"./supabase.js\";\nexport { PrismaVectorStore } from \"./prisma.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/index.ts","loc":{"lines":{"from":1,"to":10}}}}],["1169",{"pageContent":"import { similarity as ml_distance_similarity } from \"ml-distance\";\nimport { VectorStore } from \"./base.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\n\ninterface MemoryVector {\ncontent: string;\nembedding: number[];\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nmetadata: Record<string, any>;\n}\n\nexport interface MemoryVectorStoreArgs {\nsimilarity?: typeof ml_distance_similarity.cosine;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/memory.ts","loc":{"lines":{"from":1,"to":15}}}}],["1170",{"pageContent":"class MemoryVectorStore extends VectorStore {\nmemoryVectors: MemoryVector[] = [];\n\nsimilarity: typeof ml_distance_similarity.cosine;\n\nconstructor(\nembeddings: Embeddings,\n{ similarity, ...rest }: MemoryVectorStoreArgs = {}\n) {\nsuper(embeddings, rest);\n\nthis.similarity = similarity ?? ml_distance_similarity.cosine;\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nconst memoryVectors = vectors.map((embedding, idx) => ({\ncontent: documents[idx].pageContent,\nembedding,\nmetadata: documents[idx].metadata,\n}));\n\nthis.memoryVectors = this.memoryVectors.concat(memoryVectors);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/memory.ts","loc":{"lines":{"from":108,"to":138}}}}],["1171",{"pageContent":"async similaritySearchVectorWithScore(\nquery: number[],\nk: number\n): Promise<[Document, number][]> {\nconst searches = this.memoryVectors\n.map((vector, index) => ({\nsimilarity: this.similarity(query, vector.embedding),\nindex,\n}))\n.sort((a, b) => (a.similarity > b.similarity ? -1 : 0))\n.slice(0, k);\n\nconst result: [Document, number][] = searches.map((search) => [\nnew Document({\nmetadata: this.memoryVectors[search.index].metadata,\npageContent: this.memoryVectors[search.index].content,\n}),\nsearch.similarity,\n]);\n\nreturn result;\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig?: MemoryVectorStoreArgs\n): Promise<MemoryVectorStore> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn MemoryVectorStore.fromDocuments(docs, embeddings, dbConfig);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/memory.ts","loc":{"lines":{"from":217,"to":256}}}}],["1172",{"pageContent":"static async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig?: MemoryVectorStoreArgs\n): Promise<MemoryVectorStore> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nstatic async fromExistingIndex(\nembeddings: Embeddings,\ndbConfig?: MemoryVectorStoreArgs\n): Promise<MemoryVectorStore> {\nconst instance = new this(embeddings, dbConfig);\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/memory.ts","loc":{"lines":{"from":329,"to":346}}}}],["1173",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\nimport { MilvusClient } from \"@zilliz/milvus2-sdk-node\";\nimport {\nDataType,\nDataTypeMap,\n} from \"@zilliz/milvus2-sdk-node/dist/milvus/const/Milvus.js\";\nimport {\nErrorCode,\nFieldType,\n} from \"@zilliz/milvus2-sdk-node/dist/milvus/types.js\";\n\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { VectorStore } from \"./base.js\";\nimport { Document } from \"../document.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":1,"to":14}}}}],["1174",{"pageContent":"interface MilvusLibArgs {\ncollectionName?: string;\nprimaryField?: string;\nvectorField?: string;\ntextField?: string;\nurl?: string; // db address\nssl?: boolean;\nusername?: string;\npassword?: string;\n}\n\ntype IndexType =\n| \"IVF_FLAT\"\n| \"IVF_SQ8\"\n| \"IVF_PQ\"\n| \"HNSW\"\n| \"RHNSW_FLAT\"\n| \"RHNSW_SQ\"\n| \"RHNSW_PQ\"\n| \"IVF_HNSW\"\n| \"ANNOY\";\n\ninterface IndexParam {\nparams: { nprobe?: number; ef?: number; search_k?: number };\n}\n\ninterface InsertRow {\n[x: string]: string | number[];\n}\n\nconst MILVUS_PRIMARY_FIELD_NAME = \"langchain_primaryid\";\nconst MILVUS_VECTOR_FIELD_NAME = \"langchain_vector\";\nconst MILVUS_TEXT_FIELD_NAME = \"langchain_text\";\nconst MILVUS_COLLECTION_NAME_PREFIX = \"langchain_col\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":520,"to":553}}}}],["1175",{"pageContent":"class Milvus extends VectorStore {\ncollectionName: string;\n\nnumDimensions?: number;\n\nautoId?: boolean;\n\nprimaryField: string;\n\nvectorField: string;\n\ntextField: string;\n\nfields: string[];\n\nclient: MilvusClient;\n\ncolMgr: MilvusClient[\"collectionManager\"];\n\nidxMgr: MilvusClient[\"indexManager\"];\n\ndataMgr: MilvusClient[\"dataManager\"];\n\nindexParams: Record<IndexType, IndexParam> = {\nIVF_FLAT: { params: { nprobe: 10 } },\nIVF_SQ8: { params: { nprobe: 10 } },\nIVF_PQ: { params: { nprobe: 10 } },\nHNSW: { params: { ef: 10 } },\nRHNSW_FLAT: { params: { ef: 10 } },\nRHNSW_SQ: { params: { ef: 10 } },\nRHNSW_PQ: { params: { ef: 10 } },\nIVF_HNSW: { params: { nprobe: 10, ef: 10 } },\nANNOY: { params: { search_k: 10 } },\n};\n\nindexCreateParams = {\nindex_type: \"HNSW\",\nmetric_type: \"L2\",\nparams: JSON.stringify({ M: 8, efConstruction: 64 }),\n};\n\nindexSearchParams = JSON.stringify({ ef: 64 });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":1044,"to":1085}}}}],["1176",{"pageContent":"constructor(embeddings: Embeddings, args: MilvusLibArgs) {\nsuper(embeddings, args);\nthis.embeddings = embeddings;\nthis.collectionName = args.collectionName ?? genCollectionName();\nthis.textField = args.textField ?? MILVUS_TEXT_FIELD_NAME;\n\nthis.autoId = true;\nthis.primaryField = args.primaryField ?? MILVUS_PRIMARY_FIELD_NAME;\nthis.vectorField = args.vectorField ?? MILVUS_VECTOR_FIELD_NAME;\nthis.fields = [];\n\nconst url =\nargs.url ??\n// eslint-disable-next-line no-process-env\n(typeof process !== \"undefined\" ? process.env?.MILVUS_URL : undefined);\nif (!url) {\nthrow new Error(\"Milvus URL address is not provided.\");\n}\nthis.client = new MilvusClient(url, args.ssl, args.username, args.password);\nthis.colMgr = this.client.collectionManager;\nthis.idxMgr = this.client.indexManager;\nthis.dataMgr = this.client.dataManager;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":1565,"to":1587}}}}],["1177",{"pageContent":"async addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nawait this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nif (vectors.length === 0) {\nreturn;\n}\nawait this.ensureCollection(vectors, documents);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":2068,"to":2080}}}}],["1178",{"pageContent":"const insertDatas: InsertRow[] = [];\n// eslint-disable-next-line no-plusplus\nfor (let index = 0; index < vectors.length; index++) {\nconst vec = vectors[index];\nconst doc = documents[index];\nconst data: InsertRow = {\n[this.textField]: doc.pageContent,\n[this.vectorField]: vec,\n};\nthis.fields.forEach((field) => {\nswitch (field) {\ncase this.primaryField:\nif (!this.autoId) {\nif (doc.metadata[this.primaryField] === undefined) {\nthrow new Error(\n`The Collection's primaryField is configured with autoId=false, thus its value must be provided through metadata.`\n);\n}\ndata[field] = doc.metadata[this.primaryField];\n}\nbreak;\ncase this.textField:\ndata[field] = doc.pageContent;\nbreak;\ncase this.vectorField:\ndata[field] = vec;\nbreak;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":2587,"to":2613}}}}],["1179",{"pageContent":": // metadata fields\nif (doc.metadata[field] === undefined) {\nthrow new Error(\n`The field \"${field}\" is not provided in documents[${index}].metadata.`\n);\n} else if (typeof doc.metadata[field] === \"object\") {\ndata[field] = JSON.stringify(doc.metadata[field]);\n} else {\ndata[field] = doc.metadata[field];\n}\nbreak;\n}\n});\n\ninsertDatas.push(data);\n}\n\nconst insertResp = await this.dataMgr.insert({\ncollection_name: this.collectionName,\nfields_data: insertDatas,\n});\nif (insertResp.status.error_code !== ErrorCode.SUCCESS) {\nthrow new Error(`Error inserting data: ${JSON.stringify(insertResp)}`);\n}\nawait this.dataMgr.flushSync({ collection_names: [this.collectionName] });\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":3101,"to":3126}}}}],["1180",{"pageContent":"async similaritySearchVectorWithScore(\nquery: number[],\nk: number\n): Promise<[Document, number][]> {\nconst hasColResp = await this.colMgr.hasCollection({\ncollection_name: this.collectionName,\n});\nif (hasColResp.status.error_code !== ErrorCode.SUCCESS) {\nthrow new Error(`Error checking collection: ${hasColResp}`);\n}\nif (hasColResp.value === false) {\nthrow new Error(\n`Collection not found: ${this.collectionName}, please create collection before search.`\n);\n}\n\nawait this.grabCollectionFields();\n\nconst loadResp = await this.colMgr.loadCollectionSync({\ncollection_name: this.collectionName,\n});\nif (loadResp.error_code !== ErrorCode.SUCCESS) {\nthrow new Error(`Error loading collection: ${loadResp}`);\n}\n\n// clone this.field and remove vectorField\nconst outputFields = this.fields.filter(\n(field) => field !== this.vectorField\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":3618,"to":3646}}}}],["1181",{"pageContent":"const searchResp = await this.dataMgr.search({\ncollection_name: this.collectionName,\nsearch_params: {\nanns_field: this.vectorField,\ntopk: k.toString(),\nmetric_type: this.indexCreateParams.metric_type,\nparams: this.indexSearchParams,\n},\noutput_fields: outputFields,\nvector_type: DataType.FloatVector,\nvectors: [query],\n});\nif (searchResp.status.error_code !== ErrorCode.SUCCESS) {\nthrow new Error(`Error searching data: ${JSON.stringify(searchResp)}`);\n}\nconst results: [Document, number][] = [];\nsearchResp.results.forEach((result) => {\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconst fields = { pageContent: \"\", metadata: {} as Record<string, any> };\nObject.keys(result).forEach((key) => {\nif (key === this.textField) {\nfields.pageContent = result[key];\n} else if (this.fields.includes(key)) {\nif (typeof result[key] === \"string\") {\nconst { isJson, obj } = checkJsonString(result[key]);\nfields.metadata[key] = isJson ? obj : result[key];\n} else {\nfields.metadata[key] = result[key];\n}\n}\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":4127,"to":4157}}}}],["1182",{"pageContent":"results.push([new Document(fields), result.score]);\n});\n// console.log(\"Search result: \" + JSON.stringify(results, null, 2));\nreturn results;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":4630,"to":4634}}}}],["1183",{"pageContent":"async ensureCollection(vectors?: number[][], documents?: Document[]) {\nconst hasColResp = await this.colMgr.hasCollection({\ncollection_name: this.collectionName,\n});\nif (hasColResp.status.error_code !== ErrorCode.SUCCESS) {\nthrow new Error(\n`Error checking collection: ${JSON.stringify(hasColResp, null, 2)}`\n);\n}\n\nif (hasColResp.value === false) {\nif (vectors === undefined || documents === undefined) {\nthrow new Error(\n`Collection not found: ${this.collectionName}, please provide vectors and documents to create collection.`\n);\n}\nawait this.createCollection(vectors, documents);\n} else {\nawait this.grabCollectionFields();\n}\n}\n\nasync createCollection(\nvectors: number[][],\ndocuments: Document[]\n): Promise<void> {\nconst fieldList: FieldType[] = [];\n\nfieldList.push(...createFieldTypeForMetadata(documents));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":5148,"to":5176}}}}],["1184",{"pageContent":"fieldList.push(\n{\nname: this.primaryField,\ndescription: \"Primary key\",\ndata_type: DataType.Int64,\nis_primary_key: true,\nautoID: this.autoId,\n},\n{\nname: this.textField,\ndescription: \"Text field\",\ndata_type: DataType.VarChar,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":5658,"to":5669}}}}],["1185",{"pageContent":"_params: {\nmax_length: getTextFieldMaxLength(documents).toString(),\n},\n},\n{\nname: this.vectorField,\ndescription: \"Vector field\",\ndata_type: DataType.FloatVector,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":6180,"to":6187}}}}],["1186",{"pageContent":"_params: {\ndim: getVectorFieldDim(vectors).toString(),\n},\n}\n);\n\nfieldList.forEach((field) => {\nif (!field.autoID) {\nthis.fields.push(field.name);\n}\n});\n\nconst createRes = await this.colMgr.createCollection({\ncollection_name: this.collectionName,\nfields: fieldList,\n});\n\nif (createRes.error_code !== ErrorCode.SUCCESS) {\nconsole.log(createRes);\nthrow new Error(`Failed to create collection: ${createRes}`);\n}\n\nawait this.idxMgr.createIndex({\ncollection_name: this.collectionName,\nfield_name: this.vectorField,\nextra_params: this.indexCreateParams,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":6701,"to":6728}}}}],["1187",{"pageContent":"async grabCollectionFields(): Promise<void> {\nif (!this.collectionName) {\nthrow new Error(\"Need collection name to grab collection fields\");\n}\nif (\nthis.primaryField &&\nthis.vectorField &&\nthis.textField &&\nthis.fields.length > 0\n) {\nreturn;\n}\nconst desc = await this.colMgr.describeCollection({\ncollection_name: this.collectionName,\n});\ndesc.schema.fields.forEach((field) => {\nthis.fields.push(field.name);\nif (field.autoID) {\nconst index = this.fields.indexOf(field.name);\nif (index !== -1) {\nthis.fields.splice(index, 1);\n}\n}\nif (field.is_primary_key) {\nthis.primaryField = field.name;\n}\nconst dtype = DataTypeMap[field.data_type.toLowerCase()];\nif (dtype === DataType.FloatVector || dtype === DataType.BinaryVector) {\nthis.vectorField = field.name;\n}\n\nif (dtype === DataType.VarChar && field.name === MILVUS_TEXT_FIELD_NAME) {\nthis.textField = field.name;\n}\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":7228,"to":7263}}}}],["1188",{"pageContent":"static async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig?: {\ncollectionName?: string;\nurl?: string;\n}\n): Promise<Milvus> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn Milvus.fromDocuments(docs, embeddings, dbConfig);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig?: MilvusLibArgs\n): Promise<Milvus> {\nconst args: MilvusLibArgs = {\ncollectionName: dbConfig?.collectionName || genCollectionName(),\nurl: dbConfig?.url,\n};\nconst instance = new this(embeddings, args);\nawait instance.addDocuments(docs);\nreturn instance;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":7744,"to":7777}}}}],["1189",{"pageContent":"static async fromExistingCollection(\nembeddings: Embeddings,\ndbConfig: MilvusLibArgs\n): Promise<Milvus> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.ensureCollection();\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":8259,"to":8267}}}}],["1190",{"pageContent":"createFieldTypeForMetadata(documents: Document[]): FieldType[] {\nconst sampleMetadata = documents[0].metadata;\nlet textFieldMaxLength = 0;\nlet jsonFieldMaxLength = 0;\ndocuments.forEach(({ metadata }) => {\n// check all keys name and count in metadata is same as sampleMetadata\nObject.keys(metadata).forEach((key) => {\nif (\n!(key in metadata) ||","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":8779,"to":8787}}}}],["1191",{"pageContent":"of metadata[key] !== typeof sampleMetadata[key]\n) {\nthrow new Error(\n\"All documents must have same metadata keys and datatype\"\n);\n}\n\n// find max length of string field and json field, cache json string value\nif (typeof metadata[key] === \"string\") {\nif (metadata[key].length > textFieldMaxLength) {\ntextFieldMaxLength = metadata[key].length;\n}\n} else if (typeof metadata[key] === \"object\") {\nconst json = JSON.stringify(metadata[key]);\nif (json.length > jsonFieldMaxLength) {\njsonFieldMaxLength = json.length;\n}\n}\n});\n});\n\nconst fields: FieldType[] = [];\nfor (const [key, value] of Object.entries(sampleMetadata)) {\nconst type = typeof value;\nif (type === \"string\") {\nfields.push({\nname: key,\ndescription: `Metadata String field`,\ndata_type: DataType.VarChar,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":9294,"to":9322}}}}],["1192",{"pageContent":"_params: {\nmax_length: textFieldMaxLength.toString(),\n},\n});\n} else if (type === \"number\") {\nfields.push({\nname: key,\ndescription: `Metadata Number field`,\ndata_type: DataType.Float,\n});\n} else if (type === \"boolean\") {\nfields.push({\nname: key,\ndescription: `Metadata Boolean field`,\ndata_type: DataType.Bool,\n});\n} else if (value === null) {\n// skip\n} else {\n// use json for other types\ntry {\nfields.push({\nname: key,\ndescription: `Metadata JSON field`,\ndata_type: DataType.VarChar,\ntype_params: {\nmax_length: jsonFieldMaxLength.toString(),\n},\n});\n} catch (e) {\nthrow new Error(\"Failed to parse metadata field as JSON\");\n}\n}\n}\nreturn fields;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":9808,"to":9843}}}}],["1193",{"pageContent":"genCollectionName(): string {\nreturn `${MILVUS_COLLECTION_NAME_PREFIX}_${uuidv4().replaceAll(\"-\", \"\")}`;\n}\n\nfunction getTextFieldMaxLength(documents: Document[]) {\nlet textMaxLength = 0;\n// eslint-disable-next-line no-plusplus\nfor (let i = 0; i < documents.length; i++) {\nconst text = documents[i].pageContent;\nif (text.length > textMaxLength) {\ntextMaxLength = text.length;\n}\n}\nreturn textMaxLength;\n}\n\nfunction getVectorFieldDim(vectors: number[][]) {\nif (vectors.length === 0) {\nthrow new Error(\"No vectors found\");\n}\nreturn vectors[0].length;\n}\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nfunction checkJsonString(value: string): { isJson: boolean; obj: any } {\ntry {\nconst result = JSON.parse(value);\nreturn { isJson: true, obj: result };\n} catch (e) {\nreturn { isJson: false, obj: null };\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":10338,"to":10369}}}}],["1194",{"pageContent":"import type {\nMongoClient,\nCollection,\nDocument as MongoDocument,\n} from \"mongodb\";\nimport { VectorStore } from \"./base.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\n\nexport type MongoLibArgs = {\nclient: MongoClient;\ncollection: Collection<MongoDocument>;\nindexName?: string;\n};\n\nexport type MongoVectorStoreQueryExtension = {\npostQueryPipelineSteps?: MongoDocument[];\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/mongo.ts","loc":{"lines":{"from":1,"to":18}}}}],["1195",{"pageContent":"class MongoVectorStore extends VectorStore {\ndeclare FilterType: MongoVectorStoreQueryExtension;\n\ncollection: Collection<MongoDocument>;\n\nclient: MongoClient;\n\nindexName: string;\n\nconstructor(embeddings: Embeddings, args: MongoLibArgs) {\nsuper(embeddings, args);\nthis.collection = args.collection;\nthis.client = args.client;\nthis.indexName = args.indexName || \"default\";\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nconst items = vectors.map((embedding, idx) => ({\ncontent: documents[idx].pageContent,\nembedding,\nmetadata: documents[idx].metadata,\n}));\n\nawait this.collection.insertMany(items);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/mongo.ts","loc":{"lines":{"from":152,"to":184}}}}],["1196",{"pageContent":"async similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: MongoVectorStoreQueryExtension\n): Promise<[Document, number][]> {\n// Search has to be the first pipeline step (https://www.mongodb.com/docs/atlas/atlas-search/query-syntax/#behavior)\n// We hopefully this changes in the future\nconst pipeline: MongoDocument[] = [\n{\n$search: {\nindex: this.indexName,\nknnBeta: {\npath: \"embedding\",\nvector: query,\nk,\n},\n},\n},\n];\n\n// apply any post-query pipeline steps (idk how useful the option to do this is in practice)\nif (filter?.postQueryPipelineSteps) {\npipeline.push(...filter.postQueryPipelineSteps);\n}\n\npipeline.push({\n$project: {\n_id: 0,\ncontent: 1,\nmetadata: 1,\nsimilarity: {\n$arrayElemAt: [\"$knnBeta.similarity\", 0],\n},\n},\n});\n\nconst results = this.collection.aggregate(pipeline);\n\nconst ret: [Document, number][] = [];\n\nfor await (const result of results) {\nret.push([\nnew Document({\npageContent: result.content,\nmetadata: result.metadata,\n}),\nresult.similarity,\n]);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/mongo.ts","loc":{"lines":{"from":302,"to":350}}}}],["1197",{"pageContent":"// Attempt to warn if it appears that the indexing failed\nif (\nret.length === 0 &&\nk > 0 &&\nfilter?.postQueryPipelineSteps === undefined\n) {\n// check for existence of documents (if nothing is there we should expect no results)\nconst count = await this.collection.countDocuments();\n\nif (count !== 0) {\nconsole.warn(\n\"MongoDB search query returned no results where results were expected:\\n\" +\n\"This may be because the index is improperly configured or because the indexing over recently added documents has not yet completed.\"\n);\n}\n}\n\nreturn ret;\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig: MongoLibArgs\n): Promise<MongoVectorStore> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn MongoVectorStore.fromDocuments(docs, embeddings, dbConfig);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/mongo.ts","loc":{"lines":{"from":463,"to":499}}}}],["1198",{"pageContent":"static async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: MongoLibArgs\n): Promise<MongoVectorStore> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/mongo.ts","loc":{"lines":{"from":612,"to":621}}}}],["1199",{"pageContent":"import { v4 as uuid } from \"uuid\";\nimport { ClickHouseClient, createClient } from \"@clickhouse/client\";\n\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { VectorStore } from \"./base.js\";\nimport { Document } from \"../document.js\";\n\nexport interface MyScaleLibArgs {\nhost: string;\nport: string | number;\nprotocol?: string;\nusername: string;\npassword: string;\nindexType?: string;\nindexParam?: Record<string, string>;\ncolumnMap?: ColumnMap;\ndatabase?: string;\ntable?: string;\nmetric?: metric;\n}\n\nexport interface ColumnMap {\nid: string;\ntext: string;\nvector: string;\nmetadata: string;\n}\n\nexport type metric = \"ip\" | \"cosine\" | \"l2\";\n\nexport interface MyScaleFilter {\nwhereStr: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/myscale.ts","loc":{"lines":{"from":1,"to":33}}}}],["1200",{"pageContent":"class MyScaleStore extends VectorStore {\ndeclare FilterType: MyScaleFilter;\n\nprivate client: ClickHouseClient;\n\nprivate indexType: string;\n\nprivate indexParam: Record<string, string>;\n\nprivate columnMap: ColumnMap;\n\nprivate database: string;\n\nprivate table: string;\n\nprivate metric: metric;\n\nprivate isInitialized = false;\n\nconstructor(embeddings: Embeddings, args: MyScaleLibArgs) {\nsuper(embeddings, args);\n\nthis.indexType = args.indexType || \"IVFFLAT\";\nthis.indexParam = args.indexParam || {};\nthis.columnMap = args.columnMap || {\nid: \"id\",\ntext: \"text\",\nvector: \"vector\",\nmetadata: \"metadata\",\n};\nthis.database = args.database || \"default\";\nthis.table = args.table || \"vector_table\";\nthis.metric = args.metric || \"cosine\";\n\nthis.client = createClient({\nhost: `${args.protocol ?? \"https://\"}${args.host}:${args.port}`,\nusername: args.username,\npassword: args.password,\nsession_id: uuid(),\n});\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nif (vectors.length === 0) {\nreturn;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/myscale.ts","loc":{"lines":{"from":232,"to":277}}}}],["1201",{"pageContent":"if (!this.isInitialized) {\nawait this.initialize(vectors[0].length);\n}\n\nconst queryStr = this.buildInsertQuery(vectors, documents);\nawait this.client.exec({ query: queryStr });\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(documents.map((d) => d.pageContent)),\ndocuments\n);\n}\n\nasync similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: this[\"FilterType\"]\n): Promise<[Document, number][]> {\nif (!this.isInitialized) {\nawait this.initialize(query.length);\n}\nconst queryStr = this.buildSearchQuery(query, k, filter);\n\nconst queryResultSet = await this.client.query({ query: queryStr });\nconst queryResult: {\ndata: { text: string; metadata: object; dist: number }[];\n} = await queryResultSet.json();\n\nconst result: [Document, number][] = queryResult.data.map((item) => [\nnew Document({ pageContent: item.text, metadata: item.metadata }),\nitem.dist,\n]);\n\nreturn result;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/myscale.ts","loc":{"lines":{"from":459,"to":495}}}}],["1202",{"pageContent":"static async fromTexts(\ntexts: string[],\nmetadatas: object | object[],\nembeddings: Embeddings,\nargs: MyScaleLibArgs\n): Promise<MyScaleStore> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn MyScaleStore.fromDocuments(docs, embeddings, args);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\nargs: MyScaleLibArgs\n): Promise<MyScaleStore> {\nconst instance = new this(embeddings, args);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nstatic async fromExistingIndex(\nembeddings: Embeddings,\nargs: MyScaleLibArgs\n): Promise<MyScaleStore> {\nconst instance = new this(embeddings, args);\n\nawait instance.initialize();\nreturn instance;\n}\n\nprivate async initialize(dimension?: number): Promise<void> {\nconst dim = dimension ?? (await this.embeddings.embedQuery(\"test\")).length;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/myscale.ts","loc":{"lines":{"from":680,"to":719}}}}],["1203",{"pageContent":"let indexParamStr = \"\";\nfor (const [key, value] of Object.entries(this.indexParam)) {\nindexParamStr += `, '${key}=${value}'`;\n}\n\nconst query = `\nCREATE TABLE IF NOT EXISTS ${this.database}.${this.table}(\n${this.columnMap.id} String,\n${this.columnMap.text} String,\n${this.columnMap.vector} Array(Float32),\n${this.columnMap.metadata} JSON,\nCONSTRAINT cons_vec_len CHECK length(${this.columnMap.vector}) = ${dim},\nVECTOR INDEX vidx ${this.columnMap.vector} TYPE ${this.indexType}('metric_type=${this.metric}'${indexParamStr})\n) ENGINE = MergeTree ORDER BY ${this.columnMap.id}\n`;\n\nawait this.client.exec({ query: \"SET allow_experimental_object_type=1\" });\nawait this.client.exec({\nquery: \"SET output_format_json_named_tuples_as_objects = 1\",\n});\nawait this.client.exec({ query });\nthis.isInitialized = true;\n}\n\nprivate buildInsertQuery(vectors: number[][], documents: Document[]): string {\nconst columnsStr = Object.values(this.columnMap).join(\", \");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/myscale.ts","loc":{"lines":{"from":901,"to":926}}}}],["1204",{"pageContent":"const data: string[] = [];\nfor (let i = 0; i < vectors.length; i += 1) {\nconst vector = vectors[i];\nconst document = documents[i];\nconst item = [\n`'${uuid()}'`,\n`'${this.escapeString(document.pageContent)}'`,\n`[${vector}]`,\n`'${JSON.stringify(document.metadata)}'`,\n].join(\", \");\ndata.push(`(${item})`);\n}\nconst dataStr = data.join(\", \");\n\nreturn `\nINSERT INTO TABLE\n${this.database}.${this.table}(${columnsStr})\nVALUES\n${dataStr}\n`;\n}\n\nprivate escapeString(str: string): string {\nreturn str.replace(/\\\\/g, \"\\\\\\\\\").replace(/'/g, \"\\\\'\");\n}\n\nprivate buildSearchQuery(\nquery: number[],\nk: number,\nfilter?: MyScaleFilter\n): string {\nconst order = this.metric === \"ip\" ? \"DESC\" : \"ASC\";\n\nconst whereStr = filter ? `PREWHERE ${filter.whereStr}` : \"\";\nreturn `\nSELECT ${this.columnMap.text} AS text, ${this.columnMap.metadata} AS metadata, dist\nFROM ${this.database}.${this.table}\n${whereStr}\nORDER BY distance(${this.columnMap.vector}, [${query}]) AS dist ${order}\nLIMIT ${k}\n`;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/myscale.ts","loc":{"lines":{"from":1112,"to":1154}}}}],["1205",{"pageContent":"import { Client, RequestParams, errors } from \"@opensearch-project/opensearch\";\nimport { v4 as uuid } from \"uuid\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\nimport { VectorStore } from \"./base.js\";\n\ntype OpenSearchEngine = \"nmslib\" | \"hnsw\";\ntype OpenSearchSpaceType = \"l2\" | \"cosinesimil\" | \"ip\";\n\ninterface VectorSearchOptions {\nreadonly engine?: OpenSearchEngine;\nreadonly spaceType?: OpenSearchSpaceType;\nreadonly m?: number;\nreadonly efConstruction?: number;\nreadonly efSearch?: number;\n}\n\nexport interface OpenSearchClientArgs {\nreadonly client: Client;\nreadonly indexName?: string;\n\nreadonly vectorSearchOptions?: VectorSearchOptions;\n}\n\ntype OpenSearchFilter = object;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":1,"to":25}}}}],["1206",{"pageContent":"class OpenSearchVectorStore extends VectorStore {\ndeclare FilterType: OpenSearchFilter;\n\nprivate readonly client: Client;\n\nprivate readonly indexName: string;\n\nprivate readonly engine: OpenSearchEngine;\n\nprivate readonly spaceType: OpenSearchSpaceType;\n\nprivate readonly efConstruction: number;\n\nprivate readonly efSearch: number;\n\nprivate readonly m: number;\n\nconstructor(embeddings: Embeddings, args: OpenSearchClientArgs) {\nsuper(embeddings, args);\n\nthis.spaceType = args.vectorSearchOptions?.spaceType ?? \"l2\";\nthis.engine = args.vectorSearchOptions?.engine ?? \"nmslib\";\nthis.m = args.vectorSearchOptions?.m ?? 16;\nthis.efConstruction = args.vectorSearchOptions?.efConstruction ?? 512;\nthis.efSearch = args.vectorSearchOptions?.efSearch ?? 512;\n\nthis.client = args.client;\nthis.indexName = args.indexName ?? \"documents\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":240,"to":268}}}}],["1207",{"pageContent":"async addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nawait this.ensureIndexExists(\nvectors[0].length,\nthis.engine,\nthis.spaceType,\nthis.efSearch,\nthis.efConstruction,\nthis.m\n);\nconst operations = vectors.flatMap((embedding, idx) => [\n{\nindex: {\n_index: this.indexName,\n_id: uuid(),\n},\n},\n{\nembedding,\nmetadata: documents[idx].metadata,\ntext: documents[idx].pageContent,\n},\n]);\nawait this.client.bulk({ body: operations });\nawait this.client.indices.refresh({ index: this.indexName });\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":480,"to":512}}}}],["1208",{"pageContent":"async similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: OpenSearchFilter | undefined\n): Promise<[Document, number][]> {\nconst search: RequestParams.Search = {\nindex: this.indexName,\nbody: {\nquery: {\nbool: {\nfilter: { bool: { must: this.buildMetadataTerms(filter) } },\nmust: [\n{\nknn: {\nembedding: { vector: query, k },\n},\n},\n],\n},\n},\nsize: k,\n},\n};\n\nconst { body } = await this.client.search(search);\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nreturn body.hits.hits.map((hit: any) => [\nnew Document({\npageContent: hit._source.text,\nmetadata: hit._source.metadata,\n}),\nhit._score,\n]);\n}\n\nstatic fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\nargs: OpenSearchClientArgs\n): Promise<OpenSearchVectorStore> {\nconst documents = texts.map((text, idx) => {\nconst metadata = Array.isArray(metadatas) ? metadatas[idx] : metadatas;\nreturn new Document({ pageContent: text, metadata });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":727,"to":772}}}}],["1209",{"pageContent":"return OpenSearchVectorStore.fromDocuments(documents, embeddings, args);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: OpenSearchClientArgs\n): Promise<OpenSearchVectorStore> {\nconst store = new OpenSearchVectorStore(embeddings, dbConfig);\nawait store.addDocuments(docs).then(() => store);\nreturn store;\n}\n\nstatic async fromExistingIndex(\nembeddings: Embeddings,\ndbConfig: OpenSearchClientArgs\n): Promise<OpenSearchVectorStore> {\nconst store = new OpenSearchVectorStore(embeddings, dbConfig);\nawait store.client.cat.indices({ index: store.indexName });\nreturn store;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":978,"to":998}}}}],["1210",{"pageContent":"private async ensureIndexExists(\ndimension: number,\nengine = \"nmslib\",\nspaceType = \"l2\",\nefSearch = 512,\nefConstruction = 512,\nm = 16\n): Promise<void> {\nconst body = {\nsettings: {\nindex: {\nnumber_of_shards: 5,\nnumber_of_replicas: 1,\nknn: true,\n\"knn.algo_param.ef_search\": efSearch,\n},\n},\nmappings: {\ndynamic_templates: [\n{\n// map all metadata properties to be keyword\n\"metadata.*\": {\nmatch_mapping_type: \"*\",\nmapping: { type: \"keyword\" },\n},\n},\n],\nproperties: {\ntext: { type: \"text\" },\nmetadata: { type: \"object\" },\nembedding: {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":1219,"to":1249}}}}],["1211",{"pageContent":": \"knn_vector\",\ndimension,\nmethod: {\nname: \"hnsw\",\nengine,\nspace_type: spaceType,\nparameters: { ef_construction: efConstruction, m },\n},\n},\n},\n},\n};\n\nconst indexExists = await this.doesIndexExist();\nif (indexExists) return;\n\nawait this.client.indices.create({ index: this.indexName, body });\n}\n\nprivate buildMetadataTerms(\nfilter?: OpenSearchFilter\n): { term: Record<string, unknown> }[] {\nif (filter == null) return [];\nconst result = [];\nfor (const [key, value] of Object.entries(filter)) {\nresult.push({ term: { [`metadata.${key}`]: value } });\n}\nreturn result;\n}\n\nasync doesIndexExist(): Promise<boolean> {\ntry {\nawait this.client.cat.indices({ index: this.indexName });\nreturn true;\n} catch (err: unknown) {\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (err instanceof errors.ResponseError && err.statusCode === 404) {\nreturn false;\n}\nthrow err;\n}\n}\n\nasync deleteIfExists(): Promise<void> {\nconst indexExists = await this.doesIndexExist();\nif (!indexExists) return;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":1474,"to":1519}}}}],["1212",{"pageContent":"await this.client.indices.delete({ index: this.indexName });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":1723,"to":1725}}}}],["1213",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\nimport flatten from \"flat\";\n\nimport { VectorStore } from \"./base.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\n\n// eslint-disable-next-line @typescript-eslint/ban-types, @typescript-eslint/no-explicit-any\ntype PineconeMetadata = Record<string, any>;\n\ntype VectorOperationsApi = ReturnType<\nimport(\"@pinecone-database/pinecone\").PineconeClient[\"Index\"]\n>;\n\nexport interface PineconeLibArgs {\npineconeIndex: VectorOperationsApi;\ntextKey?: string;\nnamespace?: string;\nfilter?: PineconeMetadata;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":1,"to":20}}}}],["1214",{"pageContent":"class PineconeStore extends VectorStore {\ndeclare FilterType: PineconeMetadata;\n\ntextKey: string;\n\nnamespace?: string;\n\npineconeIndex: VectorOperationsApi;\n\nfilter?: PineconeMetadata;\n\nconstructor(embeddings: Embeddings, args: PineconeLibArgs) {\nsuper(embeddings, args);\n\nthis.embeddings = embeddings;\nthis.namespace = args.namespace;\nthis.pineconeIndex = args.pineconeIndex;\nthis.textKey = args.textKey ?? \"text\";\nthis.filter = args.filter;\n}\n\nasync addDocuments(documents: Document[], ids?: string[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments,\nids\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":188,"to":216}}}}],["1215",{"pageContent":"async addVectors(\nvectors: number[][],\ndocuments: Document[],\nids?: string[]\n): Promise<void> {\nconst documentIds = ids == null ? documents.map(() => uuidv4()) : ids;\nconst pineconeVectors = vectors.map((values, idx) => {\n// Pinecone doesn't support nested objects, so we flatten them\nconst metadata: {\n[key: string]: string | number | boolean | null;\n} = flatten({\n...documents[idx].metadata,\n[this.textKey]: documents[idx].pageContent,\n});\n// Pinecone doesn't support null values, so we remove them\nfor (const key of Object.keys(metadata)) {\nif (metadata[key] == null) {\ndelete metadata[key];\n} else if (","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":380,"to":398}}}}],["1216",{"pageContent":"of metadata[key] === \"object\" &&\nObject.keys(metadata[key] as unknown as object).length === 0\n) {\ndelete metadata[key];\n}\n}\nreturn {\nid: documentIds[idx],\nmetadata,\nvalues,\n};\n});\n\n// Pinecone recommends a limit of 100 vectors per upsert request\nconst chunkSize = 50;\nfor (let i = 0; i < pineconeVectors.length; i += chunkSize) {\nconst chunk = pineconeVectors.slice(i, i + chunkSize);\nawait this.pineconeIndex.upsert({\nupsertRequest: {\nvectors: chunk,\nnamespace: this.namespace,\n},\n});\n}\n}\n\nasync similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: PineconeMetadata\n): Promise<[Document, number][]> {\nif (filter && this.filter) {\nthrow new Error(\"cannot provide both `filter` and `this.filter`\");\n}\nconst _filter = filter ?? this.filter;\nconst results = await this.pineconeIndex.query({\nqueryRequest: {\nincludeMetadata: true,\nnamespace: this.namespace,\ntopK: k,\nvector: query,\nfilter: _filter,\n},\n});\n\nconst result: [Document, number][] = [];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":563,"to":608}}}}],["1217",{"pageContent":"if (results.matches) {\nfor (const res of results.matches) {\nconst { [this.textKey]: pageContent, ...metadata } = (res.metadata ??\n{}) as PineconeMetadata;\nif (res.score) {\nresult.push([new Document({ metadata, pageContent }), res.score]);\n}\n}\n}\n\nreturn result;\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig:\n| {\n/**\n* @deprecated Use pineconeIndex instead\n*/\npineconeClient: VectorOperationsApi;\ntextKey?: string;\nnamespace?: string | undefined;\n}\n| PineconeLibArgs\n): Promise<PineconeStore> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":758,"to":794}}}}],["1218",{"pageContent":"const args: PineconeLibArgs = {\npineconeIndex:\n\"pineconeIndex\" in dbConfig\n? dbConfig.pineconeIndex\n: dbConfig.pineconeClient,\ntextKey: dbConfig.textKey,\nnamespace: dbConfig.namespace,\n};\nreturn PineconeStore.fromDocuments(docs, embeddings, args);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: PineconeLibArgs\n): Promise<PineconeStore> {\nconst args = dbConfig;\nargs.textKey = dbConfig.textKey ?? \"text\";\n\nconst instance = new this(embeddings, args);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nstatic async fromExistingIndex(\nembeddings: Embeddings,\ndbConfig: PineconeLibArgs\n): Promise<PineconeStore> {\nconst instance = new this(embeddings, dbConfig);\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":950,"to":981}}}}],["1219",{"pageContent":"import { VectorStore } from \"./base.js\";\nimport { Document } from \"../document.js\";\nimport { type Embeddings } from \"../embeddings/base.js\";\n\nconst IdColumnSymbol = Symbol(\"id\");\nconst ContentColumnSymbol = Symbol(\"content\");\n\ntype ColumnSymbol = typeof IdColumnSymbol | typeof ContentColumnSymbol;\n\ndeclare type Value = unknown;\ndeclare type RawValue = Value | Sql;\n\ndeclare class Sql {\nstrings: string[];\n\nconstructor(\nrawStrings: ReadonlyArray<string>,\nrawValues: ReadonlyArray<RawValue>\n);\n}\n\ntype PrismaNamespace = {\nModelName: Record<string, string>;\nSql: typeof Sql;\nraw: (sql: string) => Sql;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":1,"to":26}}}}],["1220",{"pageContent":"PrismaClient = {\n$queryRaw<T = unknown>(\nquery: TemplateStringsArray | Sql,\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n...values: any[]\n): Promise<T>;\n$executeRaw(\nquery: TemplateStringsArray | Sql,\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n...values: any[]\n): // eslint-disable-next-line @typescript-eslint/no-explicit-any\nPromise<any>;\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n$transaction<P extends Promise<any>[]>(arg: [...P]): Promise<any>;\n};\n\ntype ObjectIntersect<A, B> = {\n[P in keyof A & keyof B]: A[P] | B[P];\n};\n\ntype ModelColumns<TModel extends Record<string, unknown>> = {\n[K in keyof TModel]?: true | ColumnSymbol;\n};\n\ntype SimilarityModel<\nTModel extends Record<string, unknown> = Record<string, unknown>,\nTColumns extends ModelColumns<TModel> = ModelColumns<TModel>\n> = Pick<TModel, keyof ObjectIntersect<TModel, TColumns>> & {\n_distance: number | null;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":320,"to":350}}}}],["1221",{"pageContent":"DefaultPrismaVectorStore = PrismaVectorStore<\nRecord<string, unknown>,\nstring,\nModelColumns<Record<string, unknown>>\n>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":633,"to":637}}}}],["1222",{"pageContent":"class PrismaVectorStore<\nTModel extends Record<string, unknown>,\nTModelName extends string,\nTSelectModel extends ModelColumns<TModel>\n> extends VectorStore {\ntableSql: Sql;\n\nvectorColumnSql: Sql;\n\nselectSql: Sql;\n\nidColumn: keyof TModel & string;\n\ncontentColumn: keyof TModel & string;\n\nstatic IdColumn: typeof IdColumnSymbol = IdColumnSymbol;\n\nstatic ContentColumn: typeof ContentColumnSymbol = ContentColumnSymbol;\n\nprotected db: PrismaClient;\n\nprotected Prisma: PrismaNamespace;\n\nconstructor(\nembeddings: Embeddings,\nconfig: {\ndb: PrismaClient;\nprisma: PrismaNamespace;\ntableName: TModelName;\nvectorColumnName: string;\ncolumns: TSelectModel;\n}\n) {\nsuper(embeddings, {});\n\nthis.Prisma = config.prisma;\nthis.db = config.db;\n\nconst entries = Object.entries(config.columns);\nconst idColumn = entries.find((i) => i[1] === IdColumnSymbol)?.[0];\nconst contentColumn = entries.find(\n(i) => i[1] === ContentColumnSymbol\n)?.[0];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":953,"to":995}}}}],["1223",{"pageContent":"if (idColumn == null) throw new Error(\"Missing ID column\");\nif (contentColumn == null) throw new Error(\"Missing content column\");\n\nthis.idColumn = idColumn;\nthis.contentColumn = contentColumn;\n\nthis.tableSql = this.Prisma.raw(`\"${config.tableName}\"`);\nthis.vectorColumnSql = this.Prisma.raw(`\"${config.vectorColumnName}\"`);\n\nthis.selectSql = this.Prisma.raw(\nentries\n.map(([key, alias]) => (alias && key) || null)\n.filter((x): x is string => !!x)\n.map((key) => `\"${key}\"`)\n.join(\", \")\n);\n}\n\nstatic withModel<TModel extends Record<string, unknown>>(db: PrismaClient) {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":1278,"to":1296}}}}],["1224",{"pageContent":"create<\nTPrisma extends PrismaNamespace,\nTColumns extends ModelColumns<TModel>\n>(\nembeddings: Embeddings,\nconfig: {\nprisma: TPrisma;\ntableName: keyof TPrisma[\"ModelName\"] & string;\nvectorColumnName: string;\ncolumns: TColumns;\n}\n) {\ntype ModelName = keyof TPrisma[\"ModelName\"] & string;\nreturn new PrismaVectorStore<TModel, ModelName, TColumns>(embeddings, {\n...config,\ndb,\n});\n}\n\nasync function fromTexts<\nTPrisma extends PrismaNamespace,\nTColumns extends ModelColumns<TModel>\n>(\ntexts: string[],\nmetadatas: TModel[],\nembeddings: Embeddings,\ndbConfig: {\nprisma: TPrisma;\ntableName: keyof TPrisma[\"ModelName\"] & string;\nvectorColumnName: string;\ncolumns: TColumns;\n}\n) {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\n\nreturn PrismaVectorStore.fromDocuments(docs, embeddings, {\n...dbConfig,\ndb,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":1592,"to":1639}}}}],["1225",{"pageContent":"fromDocuments<\nTPrisma extends PrismaNamespace,\nTColumns extends ModelColumns<TModel>\n>(\ndocs: Document<TModel>[],\nembeddings: Embeddings,\ndbConfig: {\nprisma: TPrisma;\ntableName: keyof TPrisma[\"ModelName\"] & string;\nvectorColumnName: string;\ncolumns: TColumns;\n}\n) {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":1920,"to":1932}}}}],["1226",{"pageContent":"ModelName = keyof TPrisma[\"ModelName\"] & string;\nconst instance = new PrismaVectorStore<TModel, ModelName, TColumns>(\nembeddings,\n{ ...dbConfig, db }\n);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nreturn { create, fromTexts, fromDocuments };\n}\n\nasync addModels(models: TModel[]) {\nreturn this.addDocuments(\nmodels.map((metadata) => {\nconst pageContent = metadata[this.contentColumn];\nif (typeof pageContent !== \"string\")\nthrow new Error(\"Content column must be a string\");\nreturn new Document({ pageContent, metadata });\n})\n);\n}\n\nasync addDocuments(documents: Document<TModel>[]) {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document<TModel>[]) {\nconst idSql = this.Prisma.raw(`\"${this.idColumn}\"`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":2243,"to":2275}}}}],["1227",{"pageContent":"await this.db.$transaction(\nvectors.map(\n(vector, idx) => this.db.$executeRaw`\nUPDATE ${this.tableSql}\nSET ${this.vectorColumnSql} = ${`[${vector.join(\",\")}]`}::vector\nWHERE ${idSql} = ${documents[idx].metadata[this.idColumn]}\n`\n)\n);\n}\n\nasync similaritySearch(\nquery: string,\nk = 4\n): Promise<Document<SimilarityModel<TModel, TSelectModel>>[]> {\nconst results = await this.similaritySearchVectorWithScore(\nawait this.embeddings.embedQuery(query),\nk\n);\n\nreturn results.map((result) => result[0]);\n}\n\nasync similaritySearchVectorWithScore(\nquery: number[],\nk: number\n): Promise<[Document<SimilarityModel<TModel, TSelectModel>>, number][]> {\nconst vectorQuery = `[${query.join(\",\")}]`;\nconst articles = await this.db.$queryRaw<\nArray<SimilarityModel<TModel, TSelectModel>>\n>`\nSELECT ${this.selectSql}, ${this.vectorColumnSql} <=> ${vectorQuery}::vector as \"_distance\" \nFROM ${this.tableSql}\nORDER BY \"_distance\" ASC\nLIMIT ${k};\n`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":2559,"to":2594}}}}],["1228",{"pageContent":"const results: [Document<SimilarityModel<TModel, TSelectModel>>, number][] =\n[];\nfor (const article of articles) {\nif (article._distance != null && article[this.contentColumn] != null) {\nresults.push([\nnew Document({\npageContent: article[this.contentColumn] as string,\nmetadata: article,\n}),\narticle._distance,\n]);\n}\n}\n\nreturn results;\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[],\nembeddings: Embeddings,\ndbConfig: {\ndb: PrismaClient;\nprisma: PrismaNamespace;\ntableName: string;\nvectorColumnName: string;\ncolumns: ModelColumns<Record<string, unknown>>;\n}\n): Promise<DefaultPrismaVectorStore> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\n\nreturn PrismaVectorStore.fromDocuments(docs, embeddings, dbConfig);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":2877,"to":2917}}}}],["1229",{"pageContent":"static async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: {\ndb: PrismaClient;\nprisma: PrismaNamespace;\ntableName: string;\nvectorColumnName: string;\ncolumns: ModelColumns<Record<string, unknown>>;\n}\n): Promise<DefaultPrismaVectorStore> {\nconst instance = new PrismaVectorStore(embeddings, dbConfig);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":3200,"to":3215}}}}],["1230",{"pageContent":"import type { SupabaseClient } from \"@supabase/supabase-js\";\nimport { VectorStore } from \"./base.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\n\ninterface SearchEmbeddingsParams {\nquery_embedding: number[];\nmatch_count: number; // int\nfilter?: SupabaseMetadata;\n}\n\n// eslint-disable-next-line @typescript-eslint/ban-types, @typescript-eslint/no-explicit-any\ntype SupabaseMetadata = Record<string, any>;\n\ninterface SearchEmbeddingsResponse {\nid: number;\ncontent: string;\nmetadata: object;\nsimilarity: number;\n}\n\nexport interface SupabaseLibArgs {\nclient: SupabaseClient;\ntableName?: string;\nqueryName?: string;\nfilter?: SupabaseMetadata;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/supabase.ts","loc":{"lines":{"from":1,"to":27}}}}],["1231",{"pageContent":"class SupabaseVectorStore extends VectorStore {\ndeclare FilterType: SupabaseMetadata;\n\nclient: SupabaseClient;\n\ntableName: string;\n\nqueryName: string;\n\nfilter?: SupabaseMetadata;\n\nconstructor(embeddings: Embeddings, args: SupabaseLibArgs) {\nsuper(embeddings, args);\n\nthis.client = args.client;\nthis.tableName = args.tableName || \"documents\";\nthis.queryName = args.queryName || \"match_documents\";\nthis.filter = args.filter;\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nconst rows = vectors.map((embedding, idx) => ({\ncontent: documents[idx].pageContent,\nembedding,\nmetadata: documents[idx].metadata,\n}));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/supabase.ts","loc":{"lines":{"from":154,"to":187}}}}],["1232",{"pageContent":"// upsert returns 500/502/504 (yes really any of them) if given too many rows/characters\n// ~2000 trips it, but my data is probably smaller than average pageContent and metadata\nconst chunkSize = 500;\nfor (let i = 0; i < rows.length; i += chunkSize) {\nconst chunk = rows.slice(i, i + chunkSize);\n\nconst res = await this.client.from(this.tableName).insert(chunk);\nif (res.error) {\nthrow new Error(\n`Error inserting: ${res.error.message} ${res.status} ${res.statusText}`\n);\n}\n}\n}\n\nasync similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: this[\"FilterType\"]\n): Promise<[Document, number][]> {\nif (filter && this.filter) {\nthrow new Error(\"cannot provide both `filter` and `this.filter`\");\n}\nconst _filter = filter ?? this.filter;\nconst matchDocumentsParams: SearchEmbeddingsParams = {\nfilter: _filter,\nquery_embedding: query,\nmatch_count: k,\n};\n\nconst { data: searches, error } = await this.client.rpc(\nthis.queryName,\nmatchDocumentsParams\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/supabase.ts","loc":{"lines":{"from":306,"to":339}}}}],["1233",{"pageContent":"if (error) {\nthrow new Error(\n`Error searching for documents: ${error.code} ${error.message} ${error.details}`\n);\n}\n\nconst result: [Document, number][] = (\nsearches as SearchEmbeddingsResponse[]\n).map((resp) => [\nnew Document({\nmetadata: resp.metadata,\npageContent: resp.content,\n}),\nresp.similarity,\n]);\n\nreturn result;\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig: SupabaseLibArgs\n): Promise<SupabaseVectorStore> {\nconst docs = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn SupabaseVectorStore.fromDocuments(docs, embeddings, dbConfig);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: SupabaseLibArgs\n): Promise<SupabaseVectorStore> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.addDocuments(docs);\nreturn instance;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/supabase.ts","loc":{"lines":{"from":452,"to":497}}}}],["1234",{"pageContent":"static async fromExistingIndex(\nembeddings: Embeddings,\ndbConfig: SupabaseLibArgs\n): Promise<SupabaseVectorStore> {\nconst instance = new this(embeddings, dbConfig);\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/supabase.ts","loc":{"lines":{"from":609,"to":616}}}}],["1235",{"pageContent":"import { test, expect } from \"@jest/globals\";\n\nimport { Chroma } from \"../chroma.js\";\n\n// We'd want a much more thorough test here,\n// but sadly Chroma isn't very easy to test locally at the moment.\ntest(\"Chroma imports correctly\", async () => {\nconst { ChromaClient } = await Chroma.imports();\n\nexpect(ChromaClient).toBeDefined();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/chroma.test.ts","loc":{"lines":{"from":1,"to":11}}}}],["1236",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as fs from \"node:fs/promises\";\nimport * as path from \"node:path\";\nimport * as os from \"node:os\";\n\nimport { HNSWLib } from \"../hnswlib.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test HNSWLib.fromTexts\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }],\nnew OpenAIEmbeddings()\n);\nexpect(vectorStore.index?.getCurrentCount()).toBe(3);\n\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconst resultOneMetadatas = resultOne.map(({ metadata }) => metadata);\nexpect(resultOneMetadatas).toEqual([{ id: 2 }]);\n\nconst resultTwo = await vectorStore.similaritySearch(\"hello world\", 3);\nconst resultTwoMetadatas = resultTwo.map(({ metadata }) => metadata);\nexpect(resultTwoMetadatas).toEqual([{ id: 2 }, { id: 3 }, { id: 1 }]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.int.test.ts","loc":{"lines":{"from":1,"to":25}}}}],["1237",{"pageContent":"test(\"Test HNSWLib.fromTexts + addDocuments\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }],\nnew OpenAIEmbeddings()\n);\nexpect(vectorStore.index?.getMaxElements()).toBe(3);\nexpect(vectorStore.index?.getCurrentCount()).toBe(3);\n\nawait vectorStore.addDocuments([\nnew Document({\npageContent: \"hello worldklmslksmn\",\nmetadata: { id: 4 },\n}),\n]);\nexpect(vectorStore.index?.getMaxElements()).toBe(4);\n\nconst resultTwo = await vectorStore.similaritySearch(\"hello world\", 3);\nconst resultTwoMetadatas = resultTwo.map(({ metadata }) => metadata);\nexpect(resultTwoMetadatas).toEqual([{ id: 2 }, { id: 3 }, { id: 4 }]);\n});\n\ntest(\"Test HNSWLib.load and HNSWLib.save\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }],\nnew OpenAIEmbeddings()\n);\nexpect(vectorStore.index?.getCurrentCount()).toBe(3);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.int.test.ts","loc":{"lines":{"from":89,"to":117}}}}],["1238",{"pageContent":"const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconst resultOneMetadatas = resultOne.map(({ metadata }) => metadata);\nexpect(resultOneMetadatas).toEqual([{ id: 2 }]);\n\nconst resultTwo = await vectorStore.similaritySearch(\"hello world\", 3);\nconst resultTwoMetadatas = resultTwo.map(({ metadata }) => metadata);\nexpect(resultTwoMetadatas).toEqual([{ id: 2 }, { id: 3 }, { id: 1 }]);\n\nconst tempDirectory = await fs.mkdtemp(path.join(os.tmpdir(), \"lcjs-\"));\n\nconsole.log(tempDirectory);\n\nawait vectorStore.save(tempDirectory);\n\nconst loadedVectorStore = await HNSWLib.load(\ntempDirectory,\nnew OpenAIEmbeddings()\n);\n\nconst resultThree = await loadedVectorStore.similaritySearch(\n\"hello world\",\n1\n);\n\nconst resultThreeMetadatas = resultThree.map(({ metadata }) => metadata);\nexpect(resultThreeMetadatas).toEqual([{ id: 2 }]);\n\nconst resultFour = await loadedVectorStore.similaritySearch(\"hello world\", 3);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.int.test.ts","loc":{"lines":{"from":181,"to":208}}}}],["1239",{"pageContent":"const resultFourMetadatas = resultFour.map(({ metadata }) => metadata);\nexpect(resultFourMetadatas).toEqual([{ id: 2 }, { id: 3 }, { id: 1 }]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.int.test.ts","loc":{"lines":{"from":272,"to":274}}}}],["1240",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { HNSWLib } from \"../hnswlib.js\";\nimport { Document } from \"../../document.js\";\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";\n\ntest(\"Test HNSWLib.fromTexts + addVectors\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\"],\n[{ id: 2 }],\nnew FakeEmbeddings()\n);\nexpect(vectorStore.index?.getMaxElements()).toBe(1);\nexpect(vectorStore.index?.getCurrentCount()).toBe(1);\n\nawait vectorStore.addVectors(\n[\n[0, 1, 0, 0],\n[1, 0, 0, 0],\n[0.5, 0.5, 0.5, 0.5],\n],\n[\nnew Document({\npageContent: \"hello bye\",\nmetadata: { id: 5 },\n}),\nnew Document({\npageContent: \"hello worlddwkldnsk\",\nmetadata: { id: 4 },\n}),\nnew Document({\npageContent: \"hello you\",\nmetadata: { id: 6 },\n}),\n]\n);\nexpect(vectorStore.index?.getMaxElements()).toBe(4);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.test.ts","loc":{"lines":{"from":1,"to":36}}}}],["1241",{"pageContent":"const resultTwo = await vectorStore.similaritySearchVectorWithScore(\n[1, 0, 0, 0],\n3\n);\nconst resultTwoMetadatas = resultTwo.map(([{ metadata }]) => metadata);\nexpect(resultTwoMetadatas).toEqual([{ id: 4 }, { id: 6 }, { id: 2 }]);\n});\n\ntest(\"Test HNSWLib metadata filtering\", async () => {\nconst pageContent = \"Hello world\";\n\nconst vectorStore = await HNSWLib.fromTexts(\n[pageContent, pageContent, pageContent],\n[{ id: 2 }, { id: 3 }, { id: 4 }],\nnew FakeEmbeddings()\n);\n\n// If the filter wasn't working, we'd get all 3 documents back\nconst results = await vectorStore.similaritySearch(\npageContent,\n3,\n(document) => document.metadata.id === 3\n);\n\nexpect(results).toEqual([new Document({ metadata: { id: 3 }, pageContent })]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.test.ts","loc":{"lines":{"from":69,"to":94}}}}],["1242",{"pageContent":"import { test, expect } from \"@jest/globals\";\n\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { Document } from \"../../document.js\";\nimport { MemoryVectorStore } from \"../memory.js\";\n\ntest(\"MemoryVectorStore with external ids\", async () => {\nconst embeddings = new OpenAIEmbeddings();\n\nconst store = new MemoryVectorStore(embeddings);\n\nexpect(store).toBeDefined();\n\nawait store.addDocuments([\n{ pageContent: \"hello\", metadata: { a: 1 } },\n{ pageContent: \"hi\", metadata: { a: 1 } },\n{ pageContent: \"bye\", metadata: { a: 1 } },\n{ pageContent: \"what's this\", metadata: { a: 1 } },\n]);\n\nconst results = await store.similaritySearch(\"hello\", 1);\n\nexpect(results).toHaveLength(1);\n\nexpect(results).toEqual([\nnew Document({ metadata: { a: 1 }, pageContent: \"hello\" }),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/memory.int.test.ts","loc":{"lines":{"from":1,"to":28}}}}],["1243",{"pageContent":"import { test, expect, afterAll, beforeAll } from \"@jest/globals\";\nimport { ErrorCode } from \"@zilliz/milvus2-sdk-node/dist/milvus/types.js\";\nimport { MilvusClient } from \"@zilliz/milvus2-sdk-node/dist/milvus/index.js\";\n\nimport { Milvus } from \"../milvus.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/index.js\";\n\nlet collectionName: string;\nlet embeddings: OpenAIEmbeddings;\n\nbeforeAll(async () => {\nembeddings = new OpenAIEmbeddings();\ncollectionName = `test_collection_${Math.random().toString(36).substring(7)}`;\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/milvus.int.test.ts","loc":{"lines":{"from":1,"to":14}}}}],["1244",{"pageContent":"test.skip(\"Test Milvus.fromtext\", async () => {\nconst texts = [\n`Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little \nHarmonic Labyrinth of the dreaded Majotaur?`,\n\"Achilles: Yiikes! What is that?\",\n`Tortoise: They say-although I person never believed it myself-that an I \nMajotaur has created a tiny labyrinth sits in a pit in the middle of \nit, waiting innocent victims to get lost in its fears complexity. \nThen, when they wander and dazed into the center, he laughs and \nlaughs at them-so hard, that he laughs them to death!`,\n\"Achilles: Oh, no!\",\n\"Tortoise: But it's only a myth. Courage, Achilles.\",\n];\nconst objA = { A: { B: \"some string\" } };\nconst objB = { A: { B: \"some other string\" } };\nconst metadatas: object[] = [\n{ id: 2, other: objA },\n{ id: 1, other: objB },\n{ id: 3, other: objA },\n{ id: 4, other: objB },\n{ id: 5, other: objA },\n];\nconst milvus = await Milvus.fromTexts(texts, metadatas, embeddings, {\ncollectionName,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/milvus.int.test.ts","loc":{"lines":{"from":86,"to":110}}}}],["1245",{"pageContent":"const query = \"who is achilles?\";\nconst result = await milvus.similaritySearch(query, 1);\nconst resultMetadatas = result.map(({ metadata }) => metadata);\nexpect(resultMetadatas).toEqual([{ id: 1, other: objB }]);\n\nconst resultTwo = await milvus.similaritySearch(query, 3);\nconst resultTwoMetadatas = resultTwo.map(({ metadata }) => metadata);\nexpect(resultTwoMetadatas).toEqual([\n{ id: 1, other: objB },\n{ id: 4, other: objB },\n{ id: 5, other: objA },\n]);\n});\n\ntest.skip(\"Test Milvus.fromExistingCollection\", async () => {\nconst milvus = await Milvus.fromExistingCollection(embeddings, {\ncollectionName,\n});\n\nconst query = \"who is achilles?\";\nconst result = await milvus.similaritySearch(query, 1);\nconst resultMetadatas = result.map(({ metadata }) => metadata);\nexpect(resultMetadatas.length).toBe(1);\nexpect(resultMetadatas[0].id).toEqual(1);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/milvus.int.test.ts","loc":{"lines":{"from":172,"to":195}}}}],["1246",{"pageContent":"const resultTwo = await milvus.similaritySearch(query, 3);\nconst resultTwoMetadatas = resultTwo.map(({ metadata }) => metadata);\nexpect(resultTwoMetadatas.length).toBe(3);\nexpect(resultTwoMetadatas[0].id).toEqual(1);\nexpect(resultTwoMetadatas[1].id).toEqual(4);\nexpect(resultTwoMetadatas[2].id).toEqual(5);\n});\n\nafterAll(async () => {\n// eslint-disable-next-line no-process-env\nif (!process.env.MILVUS_URL) return;\n// eslint-disable-next-line no-process-env\nconst client = new MilvusClient(process.env.MILVUS_URL as string);\nconst dropRes = await client.collectionManager.dropCollection({\ncollection_name: collectionName,\n});\n// console.log(\"Drop collection response: \", dropRes)\nexpect(dropRes.error_code).toBe(ErrorCode.SUCCESS);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/milvus.int.test.ts","loc":{"lines":{"from":258,"to":276}}}}],["1247",{"pageContent":"/* eslint-disable no-process-env */\n/* eslint-disable no-promise-executor-return */\n\nimport { test, expect } from \"@jest/globals\";\nimport { MongoClient } from \"mongodb\";\nimport { CohereEmbeddings } from \"../../embeddings/cohere.js\";\nimport { MongoVectorStore, MongoVectorStoreQueryExtension } from \"../mongo.js\";\n\nimport { Document } from \"../../document.js\";\n\n/**\n* The following json can be used to create an index in atlas for cohere embeddings:\n\n{\n\"mappings\": {\n\"fields\": {\n\"embedding\": [\n{\n\"dimensions\": 1024,\n\"similarity\": \"euclidean\",\n\"type\": \"knnVector\"\n}\n]\n}\n}\n}\n\n*/\n\ntest.skip(\"MongoVectorStore with external ids\", async () => {\nexpect(process.env.MONGO_URI).toBeDefined();\n\n// eslint-disable-next-line @typescript-eslint/no-non-null-assertion\nconst client = new MongoClient(process.env.MONGO_URI!);\n\ntry {\nconst collection = client.db(\"langchain\").collection(\"test\");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/mongo.int.test.ts","loc":{"lines":{"from":1,"to":37}}}}],["1248",{"pageContent":"const vectorStore = new MongoVectorStore(new CohereEmbeddings(), {\nclient,\ncollection,\n// indexName: \"default\", // make sure that this matches the index name in atlas if not using \"default\"\n});\n\nexpect(vectorStore).toBeDefined();\n\n// check if the database is empty\nconst count = await collection.countDocuments();\n\nconst justInserted = count === 0;\nif (justInserted) {\nawait vectorStore.addDocuments([\n{ pageContent: \"Dogs are tough.\", metadata: { a: 1 } },\n{ pageContent: \"Cats have fluff.\", metadata: { b: 1 } },\n{ pageContent: \"What is a sandwich?\", metadata: { c: 1 } },\n{ pageContent: \"That fence is purple.\", metadata: { d: 1, e: 2 } },\n]);\n}\n\n// This test is awkward because the index in atlas takes time to index new documents\n// This means from a fresh insert the query will return nothing\nlet triesLeft = 4;\n\nlet results: Document[] = [];\nwhile (triesLeft > 0) {\nresults = await vectorStore.similaritySearch(\"Sandwich\", 1);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/mongo.int.test.ts","loc":{"lines":{"from":106,"to":133}}}}],["1249",{"pageContent":"if (justInserted && results.length === 0 && triesLeft > 0) {\n// wait and try again in hopes that the indexing has finished\nawait new Promise((resolve) => setTimeout(resolve, 3000));\n}\n\ntriesLeft -= 1;\n}\n\nexpect(results).toEqual([\n{ pageContent: \"What is a sandwich?\", metadata: { c: 1 } },\n]);\n\n// we can filter the search with custom pipeline stages\nconst filter: MongoVectorStoreQueryExtension = {\npostQueryPipelineSteps: [\n{\n$match: {\n\"metadata.e\": { $exists: true },\n},\n},\n],\n};\n\nconst filteredResults = await vectorStore.similaritySearch(\n\"Sandwich\",\n4,\nfilter\n);\n\nexpect(filteredResults).toEqual([\n{ pageContent: \"That fence is purple.\", metadata: { d: 1, e: 2 } },\n]);\n} finally {\nawait client.close();\n}\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/mongo.int.test.ts","loc":{"lines":{"from":199,"to":234}}}}],["1250",{"pageContent":"/* eslint-disable no-process-env */\nimport { test, expect } from \"@jest/globals\";\n\nimport { MyScaleStore } from \"../myscale.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { Document } from \"../../document.js\";\n\ntest.skip(\"MyScaleStore.fromText\", async () => {\nconst vectorStore = await MyScaleStore.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\"],\n[\n{ id: 2, name: \"2\" },\n{ id: 1, name: \"1\" },\n{ id: 3, name: \"3\" },\n],\nnew OpenAIEmbeddings(),\n{\nhost: process.env.MYSCALE_HOST || \"localhost\",\nport: process.env.MYSCALE_PORT || \"8443\",\nusername: process.env.MYSCALE_USERNAME || \"username\",\npassword: process.env.MYSCALE_PASSWORD || \"password\",\n}\n);\n\nconst results = await vectorStore.similaritySearch(\"hello world\", 1);\nexpect(results).toEqual([\nnew Document({\npageContent: \"Hello world\",\nmetadata: { id: 2, name: \"2\" },\n}),\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/myscale.int.test.ts","loc":{"lines":{"from":1,"to":31}}}}],["1251",{"pageContent":"const filteredResults = await vectorStore.similaritySearch(\"hello world\", 1, {\nwhereStr: \"metadata.name = '1'\",\n});\nexpect(filteredResults).toEqual([\nnew Document({\npageContent: \"Bye bye\",\nmetadata: { id: 1, name: \"1\" },\n}),\n]);\n});\n\ntest.skip(\"MyScaleStore.fromExistingIndex\", async () => {\nawait MyScaleStore.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\"],\n[\n{ id: 2, name: \"2\" },\n{ id: 1, name: \"1\" },\n{ id: 3, name: \"3\" },\n],\nnew OpenAIEmbeddings(),\n{\nhost: process.env.MYSCALE_HOST || \"localhost\",\nport: process.env.MYSCALE_PORT || \"8443\",\nusername: process.env.MYSCALE_USERNAME || \"username\",\npassword: process.env.MYSCALE_PASSWORD || \"password\",\ntable: \"test_table\",\n}\n);\n\nconst vectorStore = await MyScaleStore.fromExistingIndex(\nnew OpenAIEmbeddings(),\n{\nhost: process.env.MYSCALE_HOST || \"localhost\",\nport: process.env.MYSCALE_PORT || \"8443\",\nusername: process.env.MYSCALE_USERNAME || \"username\",\npassword: process.env.MYSCALE_PASSWORD || \"password\",\ntable: \"test_table\",\n}\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/myscale.int.test.ts","loc":{"lines":{"from":94,"to":132}}}}],["1252",{"pageContent":"const results = await vectorStore.similaritySearch(\"hello world\", 1);\nexpect(results).toEqual([\nnew Document({\npageContent: \"Hello world\",\nmetadata: { id: 2, name: \"2\" },\n}),\n]);\n\nconst filteredResults = await vectorStore.similaritySearch(\"hello world\", 1, {\nwhereStr: \"metadata.name = '1'\",\n});\nexpect(filteredResults).toEqual([\nnew Document({\npageContent: \"Bye bye\",\nmetadata: { id: 1, name: \"1\" },\n}),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/myscale.int.test.ts","loc":{"lines":{"from":189,"to":206}}}}],["1253",{"pageContent":"/* eslint-disable no-process-env */\nimport { test, expect } from \"@jest/globals\";\nimport { Client } from \"@opensearch-project/opensearch\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { OpenSearchVectorStore } from \"../opensearch.js\";\nimport { Document } from \"../../document.js\";\n\ntest.skip(\"OpenSearchVectorStore integration\", async () => {\nif (!process.env.OPENSEARCH_URL) {\nthrow new Error(\"OPENSEARCH_URL not set\");\n}\n\nconst client = new Client({\nnodes: [process.env.OPENSEARCH_URL],\n});\n\nconst indexName = \"test_index\";\n\nconst embeddings = new OpenAIEmbeddings(undefined, {\nbaseOptions: { temperature: 0 },\n});\nconst store = new OpenSearchVectorStore(embeddings, { client, indexName });\nawait store.deleteIfExists();\n\nexpect(store).toBeDefined();\n\nawait store.addDocuments([\n{ pageContent: \"hello\", metadata: { a: 2 } },\n{ pageContent: \"car\", metadata: { a: 1 } },\n{ pageContent: \"adjective\", metadata: { a: 1 } },\n{ pageContent: \"hi\", metadata: { a: 1 } },\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/opensearch.int.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["1254",{"pageContent":"const results1 = await store.similaritySearch(\"hello!\", 1);\n\nexpect(results1).toHaveLength(1);\nexpect(results1).toEqual([\nnew Document({ metadata: { a: 2 }, pageContent: \"hello\" }),\n]);\n\nconst results2 = await store.similaritySearchWithScore(\"hello!\", 1, {\na: 1,\n});\n\nexpect(results2).toHaveLength(1);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/opensearch.int.test.ts","loc":{"lines":{"from":48,"to":60}}}}],["1255",{"pageContent":"/* eslint-disable no-process-env */\n/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport { beforeEach, describe, expect, test } from \"@jest/globals\";\nimport { faker } from \"@faker-js/faker\";\nimport { PineconeClient } from \"@pinecone-database/pinecone\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { Document } from \"../../document.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { PineconeStore } from \"../pinecone.js\";\n\ndescribe(\"PineconeStore\", () => {\nlet pineconeStore: PineconeStore;\n\nbeforeEach(async () => {\nconst client = new PineconeClient();\n\nawait client.init({\nenvironment: process.env.PINECONE_ENVIRONMENT!,\napiKey: process.env.PINECONE_API_KEY!,\n});\n\nconst embeddings = new OpenAIEmbeddings();\nconst pineconeIndex = client.Index(process.env.PINECONE_INDEX!);\npineconeStore = new PineconeStore(embeddings, { pineconeIndex });\n});\n\ntest(\"user-provided ids\", async () => {\nconst documentId = uuidv4();\nconst pageContent = faker.lorem.sentence(5);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/pinecone.int.test.ts","loc":{"lines":{"from":1,"to":29}}}}],["1256",{"pageContent":"await pineconeStore.addDocuments(\n[{ pageContent, metadata: {} }],\n[documentId]\n);\n\nconst results = await pineconeStore.similaritySearch(pageContent, 1);\n\nexpect(results).toEqual([new Document({ metadata: {}, pageContent })]);\n});\n\ntest(\"auto-generated ids\", async () => {\nconst pageContent = faker.lorem.sentence(5);\n\nawait pineconeStore.addDocuments([\n{ pageContent, metadata: { foo: \"bar\" } },\n]);\n\nconst results = await pineconeStore.similaritySearch(pageContent, 1);\n\nexpect(results).toEqual([\nnew Document({ metadata: { foo: \"bar\" }, pageContent }),\n]);\n});\n\ntest(\"metadata filtering\", async () => {\nconst pageContent = faker.lorem.sentence(5);\nconst uuid = uuidv4();\n\nawait pineconeStore.addDocuments([\n{ pageContent, metadata: { foo: \"bar\" } },\n{ pageContent, metadata: { foo: uuid } },\n{ pageContent, metadata: { foo: \"qux\" } },\n]);\n\n// If the filter wasn't working, we'd get all 3 documents back\nconst results = await pineconeStore.similaritySearch(pageContent, 3, {\nfoo: uuid,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/pinecone.int.test.ts","loc":{"lines":{"from":75,"to":112}}}}],["1257",{"pageContent":"expect(results).toEqual([\nnew Document({ metadata: { foo: uuid }, pageContent }),\n]);\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/pinecone.int.test.ts","loc":{"lines":{"from":158,"to":162}}}}],["1258",{"pageContent":"/* eslint-disable @typescript-eslint/no-explicit-any */\nimport { jest, test, expect } from \"@jest/globals\";\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";\n\nimport { PineconeStore } from \"../pinecone.js\";\n\ntest(\"PineconeStore with external ids\", async () => {\nconst client = {\nupsert: jest.fn(),\nquery: jest.fn<any>().mockResolvedValue({\nmatches: [],\n}),\n};\nconst embeddings = new FakeEmbeddings();\n\nconst store = new PineconeStore(embeddings, { pineconeIndex: client as any });\n\nexpect(store).toBeDefined();\n\nawait store.addDocuments(\n[\n{\npageContent: \"hello\",\nmetadata: {\na: 1,\nb: { nested: [1, { a: 4 }] },\n},\n},\n],\n[\"id1\"]\n);\n\nexpect(client.upsert).toHaveBeenCalledTimes(1);\n\nexpect(client.upsert).toHaveBeenCalledWith({\nupsertRequest: {\nnamespace: undefined,\nvectors: [\n{\nid: \"id1\",\nmetadata: { a: 1, \"b.nested.0\": 1, \"b.nested.1.a\": 4, text: \"hello\" },\nvalues: [0.1, 0.2, 0.3, 0.4],\n},\n],\n},\n});\n\nconst results = await store.similaritySearch(\"hello\", 1);\n\nexpect(results).toHaveLength(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/pinecone.test.ts","loc":{"lines":{"from":1,"to":51}}}}],["1259",{"pageContent":"test(\"PineconeStore with generated ids\", async () => {\nconst client = {\nupsert: jest.fn(),\nquery: jest.fn<any>().mockResolvedValue({\nmatches: [],\n}),\n};\nconst embeddings = new FakeEmbeddings();\n\nconst store = new PineconeStore(embeddings, { pineconeIndex: client as any });\n\nexpect(store).toBeDefined();\n\nawait store.addDocuments([{ pageContent: \"hello\", metadata: { a: 1 } }]);\n\nexpect(client.upsert).toHaveBeenCalledTimes(1);\n\nconst results = await store.similaritySearch(\"hello\", 1);\n\nexpect(results).toHaveLength(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/pinecone.test.ts","loc":{"lines":{"from":83,"to":103}}}}],["1260",{"pageContent":"/* eslint-disable no-process-env */\n/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport { test, expect } from \"@jest/globals\";\nimport { createClient } from \"@supabase/supabase-js\";\n\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { Document } from \"../../document.js\";\nimport { SupabaseVectorStore } from \"../supabase.js\";\n\ntest.skip(\"SupabaseVectorStore with external ids\", async () => {\nconst client = createClient(\nprocess.env.SUPABASE_URL!,\nprocess.env.SUPABASE_PRIVATE_KEY!\n);\n\nconst embeddings = new OpenAIEmbeddings();\n\nconst store = new SupabaseVectorStore(embeddings, { client });\n\nexpect(store).toBeDefined();\n\nawait store.addDocuments([\n{ pageContent: \"hello\", metadata: { a: 1 } },\n{ pageContent: \"hi\", metadata: { a: 1 } },\n{ pageContent: \"bye\", metadata: { a: 1 } },\n{ pageContent: \"what's this\", metadata: { a: 1 } },\n]);\n\nconst results = await store.similaritySearch(\"hello\", 1);\n\nexpect(results).toHaveLength(1);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/supabase.int.test.ts","loc":{"lines":{"from":1,"to":31}}}}],["1261",{"pageContent":"expect(results).toEqual([\nnew Document({ metadata: { a: 1 }, pageContent: \"hello\" }),\n]);\n});\n\ntest.skip(\"Search a SupabaseVectorStore using a metadata filter\", async () => {\nconst client = createClient(\nprocess.env.SUPABASE_URL!,\nprocess.env.SUPABASE_PRIVATE_KEY!\n);\n\nconst embeddings = new OpenAIEmbeddings();\n\nconst store = new SupabaseVectorStore(embeddings, {\nclient,\ntableName: \"documents\",\n});\n\nexpect(store).toBeDefined();\n\nconst createdAt = new Date().getTime();\n\nawait store.addDocuments([\n{ pageContent: \"hello 0\", metadata: { created_at: createdAt } },\n{ pageContent: \"hello 1\", metadata: { created_at: createdAt + 1 } },\n{ pageContent: \"hello 2\", metadata: { created_at: createdAt + 2 } },\n{ pageContent: \"hello 3\", metadata: { created_at: createdAt + 3 } },\n]);\n\nconst results = await store.similaritySearch(\"hello\", 1, {\ncreated_at: createdAt + 2,\n});\n\nexpect(results).toHaveLength(1);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/supabase.int.test.ts","loc":{"lines":{"from":76,"to":109}}}}],["1262",{"pageContent":"expect(results).toEqual([\nnew Document({\nmetadata: { created_at: createdAt + 2 },\npageContent: \"hello 2\",\n}),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/supabase.int.test.ts","loc":{"lines":{"from":157,"to":163}}}}],["1263",{"pageContent":"import { test, expect, jest } from \"@jest/globals\";\nimport { SupabaseClient } from \"@supabase/supabase-js\";\n\nimport { SupabaseVectorStore } from \"../supabase.js\";\n\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";\n\ntest(\"similaritySearchVectorWithScore should call RPC with the vectorstore filters\", async () => {\nconst supabaseClientMock = {\nrpc: jest.fn().mockReturnValue(Promise.resolve({ data: [] })),\n} as Partial<SupabaseClient>;\n\nconst embeddings = new FakeEmbeddings();\nconst vectorStore = new SupabaseVectorStore(embeddings, {\nclient: supabaseClientMock as SupabaseClient,\ntableName: \"documents\",\nqueryName: \"match_documents\",\nfilter: { a: 2 },\n});\nawait vectorStore.similaritySearchVectorWithScore([1, 2, 3], 5);\nexpect(supabaseClientMock.rpc).toHaveBeenCalledWith(\"match_documents\", {\nfilter: { a: 2 },\nquery_embedding: [1, 2, 3],\nmatch_count: 5,\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/supabase.test.ts","loc":{"lines":{"from":1,"to":26}}}}],["1264",{"pageContent":"test(\"similaritySearchVectorWithScore should call RPC with the passed filters\", async () => {\nconst supabaseClientMock = {\nrpc: jest.fn().mockReturnValue(Promise.resolve({ data: [] })),\n} as Partial<SupabaseClient>;\n\nconst embeddings = new FakeEmbeddings();\nconst vectorStore = new SupabaseVectorStore(embeddings, {\nclient: supabaseClientMock as SupabaseClient,\ntableName: \"documents\",\nqueryName: \"match_documents\",\n});\n\nawait vectorStore.similaritySearchVectorWithScore([1, 2, 3], 5, { b: 3 });\nexpect(supabaseClientMock.rpc).toHaveBeenCalledWith(\"match_documents\", {\nfilter: { b: 3 },\nquery_embedding: [1, 2, 3],\nmatch_count: 5,\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/supabase.test.ts","loc":{"lines":{"from":49,"to":67}}}}],["1265",{"pageContent":"/* eslint-disable no-process-env */\nimport { test, expect } from \"@jest/globals\";\nimport weaviate from \"weaviate-ts-client\";\nimport { WeaviateStore } from \"../weaviate.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { Document } from \"../../document.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/weaviate.int.test.ts","loc":{"lines":{"from":1,"to":6}}}}],["1266",{"pageContent":"test.skip(\"WeaviateStore\", async () => {\n// Something wrong with the weaviate-ts-client types, so we need to disable\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconst client = (weaviate as any).client({\nscheme: process.env.WEAVIATE_SCHEME || \"https\",\nhost: process.env.WEAVIATE_HOST || \"localhost\",\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\napiKey: new (weaviate as any).ApiKey(\nprocess.env.WEAVIATE_API_KEY || \"default\"\n),\n});\nconst store = await WeaviateStore.fromTexts(\n[\"hello world\", \"hi there\", \"how are you\", \"bye now\"],\n[{ foo: \"bar\" }, { foo: \"baz\" }, { foo: \"qux\" }, { foo: \"bar\" }],\nnew OpenAIEmbeddings(),\n{\nclient,\nindexName: \"Test\",\ntextKey: \"text\",\nmetadataKeys: [\"foo\"],\n}\n);\n\nconst results = await store.similaritySearch(\"hello world\", 1);\nexpect(results).toEqual([\nnew Document({ pageContent: \"hello world\", metadata: { foo: \"bar\" } }),\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/weaviate.int.test.ts","loc":{"lines":{"from":89,"to":115}}}}],["1267",{"pageContent":"const results2 = await store.similaritySearch(\"hello world\", 1, {\nwhere: {\noperator: \"Equal\",\npath: [\"foo\"],\nvalueText: \"baz\",\n},\n});\nexpect(results2).toEqual([\nnew Document({ pageContent: \"hi there\", metadata: { foo: \"baz\" } }),\n]);\n\nconst testDocumentWithObjectMetadata = new Document({\npageContent: \"this is the deep document world!\",\nmetadata: {\ndeep: {\nstring: \"deep string\",\ndeepdeep: {\nstring: \"even a deeper string\",\n},\n},\n},\n});\nconst documentStore = await WeaviateStore.fromDocuments(\n[testDocumentWithObjectMetadata],\nnew OpenAIEmbeddings(),\n{\nclient,\nindexName: \"DocumentTest\",\ntextKey: \"text\",\nmetadataKeys: [\"deep.string\", \"deep.deepdeep.string\"],\n}\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/weaviate.int.test.ts","loc":{"lines":{"from":183,"to":214}}}}],["1268",{"pageContent":"const result3 = await documentStore.similaritySearch(\n\"this is the deep document world!\",\n1,\n{\nwhere: {\noperator: \"Equal\",\npath: [\"deep.string\"],\nvalueText: \"deep string\",\n},\n}\n);\nexpect(result3).toEqual([\nnew Document({\npageContent: \"this is the deep document world!\",\nmetadata: {\n\"deep.string\": \"deep string\",\n\"deep.deepdeep.string\": \"even a deeper string\",\n},\n}),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/weaviate.int.test.ts","loc":{"lines":{"from":289,"to":309}}}}],["1269",{"pageContent":"import { test, expect } from \"@jest/globals\";\n\nimport { flattenObjectForWeaviate } from \"../weaviate.js\";\n\ntest(\"flattenObjectForWeaviate\", () => {\nexpect(\nflattenObjectForWeaviate({\narray2: [{}, \"a\"],\ndeep: {\nstring: \"deep string\",\narray: [\"1\", 2],\narray3: [1, 3],\ndeepdeep: {\nstring: \"even a deeper string\",\n},\n},\n})\n).toMatchInlineSnapshot(`\n{\n\"deep_array3\": [\n1,\n3,\n],\n\"deep_deepdeep_string\": \"even a deeper string\",\n\"deep_string\": \"deep string\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/weaviate.test.ts","loc":{"lines":{"from":1,"to":28}}}}],["1270",{"pageContent":"import { v4 } from \"uuid\";\nimport type {\nWeaviateObject,\nWeaviateClient,\nWhereFilter,\n} from \"weaviate-ts-client\";\nimport { VectorStore } from \"./base.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\n\n// Note this function is not generic, it is designed specifically for Weaviate\n// https://weaviate.io/developers/weaviate/config-refs/datatypes#introduction","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":1,"to":12}}}}],["1271",{"pageContent":"const flattenObjectForWeaviate = (\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nobj: Record<string, any>\n) => {\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconst flattenedObject: Record<string, any> = {};\n\nfor (const key in obj) {\nif (!Object.hasOwn(obj, key)) {\ncontinue;\n}\nconst value = obj[key];\nif (typeof obj[key] === \"object\" && !Array.isArray(value)) {\nconst recursiveResult = flattenObjectForWeaviate(value);\n\nfor (const deepKey in recursiveResult) {\nif (Object.hasOwn(obj, key)) {\nflattenedObject[`${key}_${deepKey}`] = recursiveResult[deepKey];\n}\n}\n} else if (Array.isArray(value)) {\nif (\nvalue.length > 0 &&","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":206,"to":228}}}}],["1272",{"pageContent":"of value[0] !== \"object\" &&\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nvalue.every((el: any) => typeof el === typeof value[0])\n) {\n// Weaviate only supports arrays of primitive types,\n// where all elements are of the same type\nflattenedObject[key] = value;\n}\n} else {\nflattenedObject[key] = value;\n}\n}\n\nreturn flattenedObject;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":416,"to":430}}}}],["1273",{"pageContent":"interface WeaviateLibArgs {\nclient: WeaviateClient;\n/**\n* The name of the class in Weaviate. Must start with a capital letter.\n*/\nindexName: string;\ntextKey?: string;\nmetadataKeys?: string[];\n}\n\ninterface ResultRow {\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n[key: string]: any;\n}\n\nexport interface WeaviateFilter {\ndistance?: number;\nwhere: WhereFilter;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":624,"to":642}}}}],["1274",{"pageContent":"class WeaviateStore extends VectorStore {\ndeclare FilterType: WeaviateFilter;\n\nprivate client: WeaviateClient;\n\nprivate indexName: string;\n\nprivate textKey: string;\n\nprivate queryAttrs: string[];\n\nconstructor(public embeddings: Embeddings, args: WeaviateLibArgs) {\nsuper(embeddings, args);\n\nthis.client = args.client;\nthis.indexName = args.indexName;\nthis.textKey = args.textKey || \"text\";\nthis.queryAttrs = [this.textKey];\n\nif (args.metadataKeys) {\nthis.queryAttrs = this.queryAttrs.concat(args.metadataKeys);\n}\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nconst batch: WeaviateObject[] = documents.map((document, index) => {\nif (Object.hasOwn(document.metadata, \"id\"))\nthrow new Error(\n\"Document inserted to Weaviate vectorstore should not have `id` in their metadata.\"\n);\n\nconst flattenedMetadata = flattenObjectForWeaviate(document.metadata);\nreturn {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":836,"to":868}}}}],["1275",{"pageContent":": this.indexName,\nid: v4(),\nvector: vectors[index],\nproperties: {\n[this.textKey]: document.pageContent,\n...flattenedMetadata,\n},\n};\n});\n\ntry {\nawait this.client.batch\n.objectsBatcher()\n.withObjects(...batch)\n.do();\n} catch (e) {\nthrow Error(`'Error in addDocuments' ${e}`);\n}\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(documents.map((d) => d.pageContent)),\ndocuments\n);\n}\n\nasync similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: WeaviateFilter\n): Promise<[Document, number][]> {\ntry {\nlet builder = await this.client.graphql\n.get()\n.withClassName(this.indexName)\n.withFields(`${this.queryAttrs.join(\" \")} _additional { distance }`)\n.withNearVector({\nvector: query,\ndistance: filter?.distance,\n})\n.withLimit(k);\n\nif (filter?.where) {\nbuilder = builder.withWhere(filter.where);\n}\n\nconst result = await builder.do();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":1048,"to":1095}}}}],["1276",{"pageContent":"const documents: [Document, number][] = [];\nfor (const data of result.data.Get[this.indexName]) {\nconst { [this.textKey]: text, _additional, ...rest }: ResultRow = data;\n\ndocuments.push([\nnew Document({\npageContent: text,\nmetadata: rest,\n}),\n_additional.distance,\n]);\n}\nreturn documents;\n} catch (e) {\nthrow Error(`'Error in similaritySearch' ${e}`);\n}\n}\n\nstatic fromTexts(\ntexts: string[],\nmetadatas: object | object[],\nembeddings: Embeddings,\nargs: WeaviateLibArgs\n): Promise<WeaviateStore> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn WeaviateStore.fromDocuments(docs, embeddings, args);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":1275,"to":1309}}}}],["1277",{"pageContent":"static async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\nargs: WeaviateLibArgs\n): Promise<WeaviateStore> {\nconst instance = new this(embeddings, args);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nstatic async fromExistingIndex(\nembeddings: Embeddings,\nargs: WeaviateLibArgs\n): Promise<WeaviateStore> {\nreturn new this(embeddings, args);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":1490,"to":1506}}}}],["1278",{"pageContent":"# Databerry\n\nThis page covers how to use the [Databerry](https://databerry.ai) within LangChain.\n\n## What is Databerry?\n\nDataberry is an [open source](https://github.com/gmpetrov/databerry) document retrievial platform that helps to connect your personal data with Large Language Models.\n\n![Databerry](/img/DataberryDashboard.png)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/ecosystem/databerry.md","loc":{"lines":{"from":1,"to":9}}}}],["1279",{"pageContent":"Quick start\n\nRetrieving documents stored in Databerry from LangChain is very easy!\n\n```typescript\nimport { DataberryRetriever } from \"langchain/retrievers/databerry\";\n\nconst retriever = new DataberryRetriever({\n  datastoreUrl: \"https://api.databerry.ai/query/clg1xg2h80000l708dymr0fxc\",\n  apiKey: \"DATABERRY_API_KEY\", // optional: needed for private datastores\n  topK: 8, // optional: default value is 3\n});\n\n// Create a chain that uses the OpenAI LLM and Databerry retriever.\nconst chain = RetrievalQAChain.fromLLM(model, retriever);\n\n// Call the chain with a query.\nconst res = await chain.call({\n  query: \"What's Databerry?\",\n});\n\nconsole.log({ res });\n/*\n{\n  res: {\n    text: 'Databerry provides a user-friendly solution to quickly setup a semantic search system over your personal data without any technical knowledge.'\n  }\n}\n*/\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/ecosystem/databerry.md","loc":{"lines":{"from":11,"to":40}}}}],["1280",{"pageContent":"# Helicone\n\nThis page covers how to use the [Helicone](https://helicone.ai) within LangChain.\n\n## What is Helicone?\n\nHelicone is an [open source](https://github.com/Helicone/helicone) observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.\n\n![Helicone](/img/HeliconeDashboard.png)\n\n## Quick start\n\nWith your LangChain environment you can just add the following parameter.\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\nNow head over to [helicone.ai](https://helicone.ai/onboarding?step=2) to create your account, and add your OpenAI API key within our dashboard to view your logs.\n\n![Helicone](/img/HeliconeKeys.png)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/ecosystem/helicone.md","loc":{"lines":{"from":1,"to":27}}}}],["1281",{"pageContent":"How to enable Helicone caching\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n    baseOptions: {\n      headers: {\n        \"Helicone-Cache-Enabled\": \"true\",\n      },\n    },\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\n[Helicone caching docs](https://docs.helicone.ai/advanced-usage/caching)\n\n## How to use Helicone custom properties\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n    baseOptions: {\n      headers: {\n        \"Helicone-Property-Session\": \"24\",\n        \"Helicone-Property-Conversation\": \"support_issue_2\",\n        \"Helicone-Property-App\": \"mobile\",\n      },\n    },\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\n[Helicone property docs](https://docs.helicone.ai/advanced-usage/custom-properties)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/ecosystem/helicone.md","loc":{"lines":{"from":29,"to":67}}}}],["1282",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\n\n# Unstructured\n\nThis page covers how to use [Unstructured](https://unstructured.io) within LangChain.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/ecosystem/unstructured.mdx","loc":{"lines":{"from":1,"to":5}}}}],["1283",{"pageContent":"What is Unstructured?\n\nUnstructured is an [open source](https://github.com/Unstructured-IO/unstructured) Python package for extracting text from raw documents for use in machine learning applications. Currently, Unstructured supports partitioning Word documents (in `.doc` or `.docx` format), PowerPoints (in `.ppt` or `.pptx` format), PDFs, HTML files, images, emails (in `.eml` or `.msg` format), epubs, markdown, and plain text files.\n\n`unstructured` is a Python package and cannot be used directly with TS/JS, however Unstructured also maintains a [REST API](https://github.com/Unstructured-IO/unstructured-api) to support pre-processing pipelines written in other programming languages. The endpoint for the hosted Unstructured API is `https://api.unstructured.io/general/v0/general`, or you can run the service locally using the instructions found [here](https://github.com/Unstructured-IO/unstructured-api#dizzy-instructions-for-using-the-docker-image).","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/ecosystem/unstructured.mdx","loc":{"lines":{"from":7,"to":11}}}}],["1284",{"pageContent":"Currently (as of April 26th, 2023), the Unstructured API does not require an API key. The API will begin to require an API key in the near future. The [Unstructured documentation page](https://unstructured-io.github.io/unstructured/) will include instructions on how to obtain an API key once they are available.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/ecosystem/unstructured.mdx","loc":{"lines":{"from":13,"to":13}}}}],["1285",{"pageContent":"Quick start\n\nYou can use Unstructured in `langchain` with the following code.\nReplace the filename with the file you would like to process.\nIf you are running the container locally, switch the url to `http://127.0.0.1:8000/general/v0/general`.\nCheck out the [API documentation page](https://api.unstructured.io/general/docs) for additional details.\n\nimport SingleExample from \"@examples/document_loaders/unstructured.ts\";\n\n<CodeBlock language=\"typescript\">{SingleExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/ecosystem/unstructured.mdx","loc":{"lines":{"from":15,"to":24}}}}],["1286",{"pageContent":"Directories\n\nYou can also load all of the files in the directory using `UnstructuredDirectoryLoader`, which inherits from [`DirectoryLoader`](../modules/indexes/document_loaders/examples/file_loaders/directory.md):\n\nimport DirectoryExample from \"@examples/document_loaders/unstructured_directory.ts\";\n\n<CodeBlock language=\"typescript\">{DirectoryExample}</CodeBlock>\n\nCurrently, the `UnstructuredLoader` supports the following document types:\n\n- Plain text files (`.txt`/`.text`)\n- PDFs (`.pdf`)\n- Word Documents (`.doc`/`.docx`)\n- PowerPoints (`.ppt`/`.pptx`)\n- Images (`.jpg`/`.jpeg`)\n- Emails (`.eml`/`.msg`)\n- HTML (`.html`)\n- Markdown Files (`.md`)\n\nThe output from the `UnstructuredLoader` will be an array of `Document` objects that looks like the following:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/ecosystem/unstructured.mdx","loc":{"lines":{"from":26,"to":45}}}}],["1287",{"pageContent":"```typescript\n[\n  Document {\n    pageContent: `Decoder: The decoder is also composed of a stack of N = 6\n  identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a\n  third sub-layer, wh\n  ich performs multi-head attention over the output of the encoder stack. Similar to the encoder, we\n  employ residual connections around each of the sub-layers, followed by layer normalization. We also\n  modify the self\n  -attention sub-layer in the decoder stack to prevent positions from attending to subsequent\n  positions. This masking, combined with fact that the output embeddings are offset by one position,\n  ensures that the predic\n  tions for position i can depend only on the known outputs at positions less than i.`,\n    metadata: {\n      page_number: 3,\n      filename: '1706.03762.pdf',\n      category: 'NarrativeText'\n    }\n  },\n  Document {\n    pageContent: '3.2 Attention',\n    metadata: { page_number: 3, filename: '1706.03762.pdf', category: 'Title'\n  }\n]\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/ecosystem/unstructured.mdx","loc":{"lines":{"from":47,"to":71}}}}],["1288",{"pageContent":"---\nsidebar_position: 3\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/chat/chat_streaming_stdout.ts\";\n\n# Quickstart, using Chat Models\n\nChat models are a variation on language models.\nWhile chat models use language models under the hood, the interface they expose is a bit different.\nRather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\n\nChat model APIs are fairly new, so we are still figuring out the correct abstractions.\n\n## Installation and Setup\n\nTo get started, follow the [installation instructions](./install) to install LangChain.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":1,"to":18}}}}],["1289",{"pageContent":"Getting Started\n\nThis section covers how to get started with chat models. The interface is based around messages rather than raw text.\n\n```typescript\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanChatMessage, SystemChatMessage } from \"langchain/schema\";\n\nconst chat = new ChatOpenAI({ temperature: 0 });\n```\n\nHere we create a chat model using the API key stored in the environment variable `OPENAI_API_KEY` or `AZURE_OPENAI_API_KEY` in case you are using Azure OpenAI. We'll be calling this chat model throughout this section.\n\n> **&#9432;** Note, if you are using Azure OpenAI make sure to also set the environment variables `AZURE_OPENAI_API_INSTANCE_NAME`, `AZURE_OPENAI_API_DEPLOYMENT_NAME` and `AZURE_OPENAI_API_VERSION`.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":20,"to":33}}}}],["1290",{"pageContent":"Chat Models: Message in, Message out\n\nYou can get chat completions by passing one or more messages to the chat model. The response will also be a message. The types of messages currently supported in LangChain are `AIChatMessage`, `HumanChatMessage`, `SystemChatMessage`, and a generic `ChatMessage` -- ChatMessage takes in an arbitrary role parameter, which we won't be using here. Most of the time, you'll just be dealing with `HumanChatMessage`, `AIChatMessage`, and `SystemChatMessage`.\n\n```typescript\nconst response = await chat.call([\n  new HumanChatMessage(\n    \"Translate this sentence from English to French. I love programming.\"\n  ),\n]);\n\nconsole.log(response);\n```\n\n```\nAIChatMessage { text: \"J'aime programmer.\" }\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":35,"to":51}}}}],["1291",{"pageContent":"Multiple Messages\n\nOpenAI's chat-based models (currently `gpt-3.5-turbo` and `gpt-4` and in case of azure OpenAI `gpt-4-32k`) support multiple messages as input. See [here](https://platform.openai.com/docs/guides/chat/chat-vs-completions) for more information. Here is an example of sending a system and user message to the chat model:\n\n> **&#9432;** Note, if you are using Azure OpenAI make sure to change the deployment name to the deployment for the model you choose.\n\n```typescript\nconst responseB = await chat.call([\n  new SystemChatMessage(\n    \"You are a helpful assistant that translates English to French.\"\n  ),\n  new HumanChatMessage(\"Translate: I love programming.\"),\n]);\n\nconsole.log(responseB);\n```\n\n```\nAIChatMessage { text: \"J'aime programmer.\" }\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":53,"to":72}}}}],["1292",{"pageContent":"Multiple Completions\n\nYou can go one step further and generate completions for multiple sets of messages using generate. This returns an LLMResult with an additional message parameter.\n\n```typescript\nconst responseC = await chat.generate([\n  [\n    new SystemChatMessage(\n      \"You are a helpful assistant that translates English to French.\"\n    ),\n    new HumanChatMessage(\n      \"Translate this sentence from English to French. I love programming.\"\n    ),\n  ],\n  [\n    new SystemChatMessage(\n      \"You are a helpful assistant that translates English to French.\"\n    ),\n    new HumanChatMessage(\n      \"Translate this sentence from English to French. I love artificial intelligence.\"\n    ),\n  ],\n]);\n\nconsole.log(responseC);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":74,"to":98}}}}],["1293",{"pageContent":"```\n{\n  generations: [\n    [\n      {\n        text: \"J'aime programmer.\",\n        message: AIChatMessage { text: \"J'aime programmer.\" },\n      }\n    ],\n    [\n      {\n        text: \"J'aime l'intelligence artificielle.\",\n        message: AIChatMessage { text: \"J'aime l'intelligence artificielle.\" }\n      }\n    ]\n  ]\n}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":101,"to":118}}}}],["1294",{"pageContent":"Chat Prompt Templates: Manage Prompts for Chat Models\n\nYou can make use of templating by using a `MessagePromptTemplate`. You can build a `ChatPromptTemplate` from one or more `MessagePromptTemplates`. You can use `ChatPromptTemplate`'s `formatPromptValue` -- this returns a `PromptValue`, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.\n\nContinuing with the previous example:\n\n```typescript\nimport {\n  SystemMessagePromptTemplate,\n  HumanMessagePromptTemplate,\n  ChatPromptTemplate,\n} from \"langchain/prompts\";\n```\n\nFirst we create a reusable template:\n\n```typescript\nconst translationPrompt = ChatPromptTemplate.fromPromptMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n  ),\n  HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":120,"to":142}}}}],["1295",{"pageContent":"Then we can use the template to generate a response:\n\n```typescript\nconst responseA = await chat.generatePrompt([\n  await translationPrompt.formatPromptValue({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  }),\n]);\n\nconsole.log(responseA);\n```\n\n```\n{\n  generations: [\n    [\n      {\n        text: \"J'aime programmer.\",\n        message: AIChatMessage { text: \"J'aime programmer.\" }\n      }\n    ]\n  ]\n}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":145,"to":170}}}}],["1296",{"pageContent":"Model + Prompt = LLMChain\n\nThis pattern of asking for the completion of a formatted prompt is quite common, so we introduce the next piece of the puzzle: LLMChain\n\n```typescript\nconst chain = new LLMChain({\n  prompt: translationPrompt,\n  llm: chat,\n});\n```\n\nThen you can call the chain:\n\n```typescript\nconst responseB = await chain.call({\n  input_language: \"English\",\n  output_language: \"French\",\n  text: \"I love programming.\",\n});\n\nconsole.log(responseB);\n```\n\n```\n{ text: \"J'aime programmer.\" }\n```\n\nThe chain will internally accumulate the messages sent to the model, and the ones received as output. Then it will inject the messages into the prompt on the next call. So you can call the chain a few times, and it remembers previous messages:\n\n```typescript\nconst responseD = await chain.call({\n  input: \"hi from London, how are you doing today\",\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":172,"to":204}}}}],["1297",{"pageContent":"```\n{\n  response: \"Hello! As an AI language model, I don't have feelings, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\"\n}\n```\n\n```typescript\nconst responseE = await chain.call({\n  input: \"Do you know where I am?\",\n});\n\nconsole.log(responseE);\n```\n\n```\n{\n  response: \"Yes, you mentioned that you are from London. However, as an AI language model, I don't have access to your current location unless you provide me with that information.\"\n}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":207,"to":225}}}}],["1298",{"pageContent":"Agents: Dynamically Run Chains Based on User Input\n\nFinally, we introduce Tools and Agents, which extend the model with other abilities, such as search, or a calculator.\n\nA tool is a function that takes a string (such as a search query) and returns a string (such as a search result). They also have a name and description, which are used by the chat model to identify which tool it should call.\n\n```typescript\nclass Tool {\n  name: string;\n  description: string;\n  call(arg: string): Promise<string>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":227,"to":238}}}}],["1299",{"pageContent":"An agent is a stateless wrapper around an agent prompt chain (such as MRKL) which takes care of formatting tools into the prompt, as well as parsing the responses obtained from the chat model.\n\n```typescript\ninterface AgentStep {\n  action: AgentAction;\n  observation: string;\n}\n\ninterface AgentAction {\n  tool: string; // Tool.name\n  toolInput: string; // Tool.call argument\n}\n\ninterface AgentFinish {\n  returnValues: object;\n}\n\nclass Agent {\n  plan(steps: AgentStep[], inputs: object): Promise<AgentAction | AgentFinish>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":241,"to":260}}}}],["1300",{"pageContent":"To make agents more powerful we need to make them iterative, ie. call the model multiple times until they arrive at the final answer. That's the job of the AgentExecutor.\n\n```typescript\nclass AgentExecutor {\n  // a simplified implementation\n  run(inputs: object) {\n    const steps = [];\n    while (true) {\n      const step = await this.agent.plan(steps, inputs);\n      if (step instanceof AgentFinish) {\n        return step.returnValues;\n      }\n      steps.push(step);\n    }\n  }\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":263,"to":278}}}}],["1301",{"pageContent":"And finally, we can use the AgentExecutor to run an agent:\n\n```typescript\n// Define the list of tools the agent can use\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n];\n// Create the agent from the chat model and the tools\nconst agent = ChatAgent.fromLLMAndTools(new ChatOpenAI(), tools);\n// Create an executor, which calls to the agent until an answer is found\nconst executor = AgentExecutor.fromAgentAndTools({ agent, tools });\n\nconst responseG = await executor.run(\n  \"How many people live in canada as of 2023?\"\n);\n\nconsole.log(responseG);\n```\n\n```\n38,626,704.\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":281,"to":306}}}}],["1302",{"pageContent":"Memory: Add State to Chains and Agents\n\nYou can also use the chain to store state. This is useful for eg. chatbots, where you want to keep track of the conversation history. MessagesPlaceholder is a special prompt template that will be replaced with the messages passed in each call.\n\n```typescript\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n  ),\n  new MessagesPlaceholder(\"history\"),\n  HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n]);\n\nconst chain = new ConversationChain({\n  memory: new BufferMemory({ returnMessages: true, memoryKey: \"history\" }),\n  prompt: chatPrompt,\n  llm: chat,\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":308,"to":326}}}}],["1303",{"pageContent":"Streaming\n\nYou can also use the streaming API to get words streamed back to you as they are generated. This is useful for eg. chatbots, where you want to show the user what is being generated as it is being generated. Note: OpenAI as of this writing does not support `tokenUsage` reporting while streaming is enabled.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":328,"to":332}}}}],["1304",{"pageContent":"---\nsidebar_position: 2\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/llm/llm_streaming_stdout.ts\";\n\n# Quickstart, using LLMs\n\nThis tutorial gives you a quick walkthrough about building an end-to-end language model application with LangChain.\n\n## Installation and Setup\n\nTo get started, follow the [installation instructions](./install) to install LangChain.\n\n## Picking up a LLM\n\nUsing LangChain will usually require integrations with one or more model providers, data stores, apis, etc.\n\nFor this example, we will be using OpenAI's APIs, so no additional setup is required.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":1,"to":20}}}}],["1305",{"pageContent":"Building a Language Model Application\n\nNow that we have installed LangChain, we can start building our language model application.\n\nLangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":22,"to":26}}}}],["1306",{"pageContent":"LLMs: Get Predictions from a Language Model\n\nThe most basic building block of LangChain is calling an LLM on some input. Let's walk through a simple example of how to do this. For this purpose, let's pretend we are building a service that generates a company name based on what the company makes.\n\nIn order to do this, we first need to import the LLM wrapper.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\nWe will then need to set the environment variable for the OpenAI key. Three options here:\n\n1. We can do this by setting the value in a `.env` file and use the [dotenv](https://github.com/motdotla/dotenv) package to read it.\n\n   1.1. For OpenAI Api\n\n   ```bash\n   OPENAI_API_KEY=\"...\"","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":28,"to":45}}}}],["1307",{"pageContent":"1.2. For Azure OpenAI:\n\n   ```bash\n   AZURE_OPENAI_API_KEY=\"...\"\n   AZURE_OPENAI_API_INSTANCE_NAME=\"...\"\n   AZURE_OPENAI_API_DEPLOYMENT_NAME=\"...\"\n   AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME=\"...\"\n   AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=\"...\"\n   AZURE_OPENAI_API_VERSION=\"...\"\n   ```\n\n2. Or we can export the environment variable with the following command in your shell:\n\n   2.1. For OpenAI Api\n\n   ```bash\n   export OPENAI_API_KEY=sk-....\n   ```\n\n   2.2. For Azure OpenAI:\n\n   ```bash\n   export AZURE_OPENAI_API_KEY=\"...\"\n   export AZURE_OPENAI_API_INSTANCE_NAME=\"...\"\n   export AZURE_OPENAI_API_DEPLOYMENT_NAME=\"...\"\n   export AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME=\"...\"\n   export AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=\"...\"\n   export AZURE_OPENAI_API_VERSION=\"...\"","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":48,"to":75}}}}],["1308",{"pageContent":"3. Or we can do it when initializing the wrapper along with other arguments. In this example, we probably want the outputs to be MORE random, so we'll initialize it with a HIGH temperature.\n\n   3.1. For OpenAI Api\n\n   ```typescript\n   const model = new OpenAI({ openAIApiKey: \"sk-...\", temperature: 0.9 });\n   ```\n\n   3.2. For Azure OpenAI:\n\n   ```bash\n   const model = new OpenAI({\n     azureOpenAIApiKey: \"...\",\n     azureOpenAIApiInstanceName: \"....\",\n     azureOpenAIApiDeploymentName: \"....\",\n     azureOpenAIApiVersion: \"....\",\n     temperature: 0.9\n   });\n   ```\n\nOnce we have initialized the wrapper, we can now call it on some input!\n\n```typescript\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log(res);\n```\n\n```shell\n{ res: '\\n\\nFantasy Sockery' }\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":78,"to":109}}}}],["1309",{"pageContent":"Prompt Templates: Manage Prompts for LLMs\n\nCalling an LLM is a great first step, but it's just the beginning. Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you are probably taking user input and constructing a prompt, and then sending that to the LLM.\n\nFor example, in the previous example, the text we passed in was hardcoded to ask for a name for a company that made colorful socks. In this imaginary service, what we would want to do is take only the user input describing what the company does, and then format the prompt with that information.\n\nThis is easy to do with LangChain!\n\nFirst lets define the prompt template:\n\n```typescript\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst template = \"What is a good name for a company that makes {product}?\";\nconst prompt = new PromptTemplate({\n  template: template,\n  inputVariables: [\"product\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":111,"to":128}}}}],["1310",{"pageContent":"Let's now see how this works! We can call the `.format` method to format it.\n\n```typescript\nconst res = await prompt.format({ product: \"colorful socks\" });\nconsole.log(res);\n```\n\n```shell\n{ res: 'What is a good name for a company that makes colorful socks?' }\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":131,"to":140}}}}],["1311",{"pageContent":"Chains: Combine LLMs and Prompts in Multi-Step Workflows\n\nUp until now, we've worked with the PromptTemplate and LLM primitives by themselves. But of course, a real application is not just one primitive, but rather a combination of them.\n\nA chain in LangChain is made up of links, which can be either primitives like LLMs or other chains.\n\nThe most core type of chain is an LLMChain, which consists of a PromptTemplate and an LLM.\n\nExtending the previous example, we can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst model = new OpenAI({ temperature: 0.9 });\nconst template = \"What is a good name for a company that makes {product}?\";\nconst prompt = new PromptTemplate({\n  template: template,\n  inputVariables: [\"product\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":142,"to":161}}}}],["1312",{"pageContent":"We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM:\n\n```typescript\nimport { LLMChain } from \"langchain/chains\";\n\nconst chain = new LLMChain({ llm: model, prompt: prompt });\n```\n\nNow we can run that chain only specifying the product!\n\n```typescript\nconst res = await chain.call({ product: \"colorful socks\" });\nconsole.log(res);\n```\n\n```shell\n{ res: { text: '\\n\\nColorfulCo Sockery.' } }\n```\n\nThere we go! There's the first chain - an LLM Chain. This is one of the simpler types of chains, but understanding how it works will set you up well for working with more complex chains.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":164,"to":183}}}}],["1313",{"pageContent":"Agents: Dynamically Run Chains Based on User Input\n\nSo far the chains we've looked at run in a predetermined order.\n\nAgents no longer do: they use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.\n\nWhen used correctly agents can be extremely powerful. In this tutorial, we show you how to easily use agents through the simplest, highest level API.\n\nIn order to load agents, you should understand the following concepts:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":185,"to":193}}}}],["1314",{"pageContent":"- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, code REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.\n- LLM: The language model powering the agent.\n- Agent: The agent to use. This should be a string that references a support agent class. Because this tutorial focuses on the simplest, highest level API, this only covers using the standard supported agents.\n\nFor this example, you'll need to set the SerpAPI environment variables in the `.env` file.\n\n```bash\nSERPAPI_API_KEY=\"...\"","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":195,"to":202}}}}],["1315",{"pageContent":"Install `serpapi` package (Google Search API):\n\n```bash npm2yarn\nnpm install -S serpapi\n```\n\nNow we can get started!\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input =\n  \"Who is Olivia Wilde's boyfriend?\" +\n  \" What is his current age raised to the 0.23 power?\";\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":205,"to":241}}}}],["1316",{"pageContent":"```shell\nlangchain-examples:start: Executing with input \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"...\nlangchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the 0.23 power is 2.4242784855673896.\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":244,"to":247}}}}],["1317",{"pageContent":"Memory: Add State to Chains and Agents\n\nSo far, all the chains and agents we've gone through have been stateless. But often, you may want a chain or agent to have some concept of \"memory\" so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of \"short-term memory\". On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of \"long-term memory\".\n\nLangChain provides several specially created chains just for this purpose. This section walks through using one of those chains (the `ConversationChain`).\n\nBy default, the `ConversationChain` has a simple type of memory that remembers all previous inputs/outputs and adds them to the context that is passed. Let's take a look at using this chain.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":249,"to":255}}}}],["1318",{"pageContent":"```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst memory = new BufferMemory();\nconst chain = new ConversationChain({ llm: model, memory: memory });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log(res1);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":257,"to":266}}}}],["1319",{"pageContent":"```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log(res2);\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":269,"to":280}}}}],["1320",{"pageContent":"Streaming\n\nYou can also use the streaming API to get words streamed back to you as they are generated. This is useful for eg. chatbots, where you want to show the user what is being generated as it is being generated. Note: OpenAI as of this writing does not support `tokenUsage` reporting while streaming is enabled.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":282,"to":286}}}}],["1321",{"pageContent":"---\nsidebar_position: 1\n---\n\n# Setup and Installation\n\n:::info\nUpdating from <0.0.52? See [this section](#updating-from-0052) for instructions.\n:::\n\n## Supported Environments\n\nLangChain is written in TypeScript and can be used in:\n\n- Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x\n- Cloudflare Workers\n- Vercel / Next.js (Browser, Serverless and Edge functions)\n- Supabase Edge Functions\n- Browser\n- Deno\n\n## Quickstart\n\nIf you want to get started quickly on using LangChain in Node.js, [clone this repository](https://github.com/domeccleston/langchain-ts-starter) and follow the README instructions for a boilerplate project with those dependencies set up.\n\nIf you prefer to set things up yourself, or you want to run LangChain in other environments, read on for instructions.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/install.md","loc":{"lines":{"from":1,"to":26}}}}],["1322",{"pageContent":"Installation\n\nTo get started, install LangChain with the following command:\n\n```bash npm2yarn\nnpm install -S langchain\n```\n\n### TypeScript\n\nLangChain is written in TypeScript and provides type definitions for all of its public APIs.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/install.md","loc":{"lines":{"from":28,"to":38}}}}],["1323",{"pageContent":"Loading the library\n\n### ESM\n\nLangChain provides an ESM build targeting Node.js environments. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\nIf you are using TypeScript in an ESM project we suggest updating your `tsconfig.json` to include the following:\n\n```json title=\"tsconfig.json\"\n{\n  \"compilerOptions\": {\n    ...\n    \"target\": \"ES2020\", // or higher\n    \"module\": \"nodenext\",\n  }\n}\n```\n\n### CommonJS\n\nLangChain provides a CommonJS build targeting Node.js environments. You can import it using the following syntax:\n\n```typescript\nconst { OpenAI } = require(\"langchain/llms/openai\");\n```\n\n### Cloudflare Workers\n\nLangChain can be used in Cloudflare Workers. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/install.md","loc":{"lines":{"from":40,"to":76}}}}],["1324",{"pageContent":"Vercel / Next.js\n\nLangChain can be used in Vercel / Next.js. We support using LangChain in frontend components, in Serverless functions and in Edge functions. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\nIf you want to use LangChain in frontend `pages`, you need to add the following to your `next.config.js` to enable support for WebAssembly modules (which is required by the tokenizer library `@dqbd/tiktoken`):\n\n```js title=\"next.config.js\"\nconst nextConfig = {\n  webpack(config) {\n    config.experiments = {\n      asyncWebAssembly: true,\n      layers: true,\n    };\n\n    return config;\n  },\n};\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/install.md","loc":{"lines":{"from":78,"to":99}}}}],["1325",{"pageContent":"Deno / Supabase Edge Functions\n\nLangChain can be used in Deno / Supabase Edge Functions. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"https://esm.sh/langchain/llms/openai\";\n```\n\nWe recommend looking at our [Supabase Template](https://github.com/langchain-ai/langchain-template-supabase) for an example of how to use LangChain in Supabase Edge Functions.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/install.md","loc":{"lines":{"from":101,"to":109}}}}],["1326",{"pageContent":"Browser\n\nLangChain can be used in the browser. In our CI we test bundling LangChain with Webpack and Vite, but other bundlers should work too. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\n#### Create React App\n\nIf you're using `create-react-app` by default it doesn't support WebAssembly modules, so the tokenizer library `@dqbd/tiktoken` will not work in the browser. You can follow the instructions [here](https://github.com/dqbd/tiktoken/tree/main/js#create-react-app) to enable support for WebAssembly modules.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/install.md","loc":{"lines":{"from":111,"to":121}}}}],["1327",{"pageContent":"Vite\n\nIf you're using Vite, you need to add the following to your `vite.config.js` to enable support for WebAssembly modules (which is required by the tokenizer library `@dqbd/tiktoken`):\n\n```bash npm2yarn\nnpm install -D vite-plugin-wasm vite-plugin-top-level-await\n```\n\n```js title=\"vite.config.js\"\nimport wasm from \"vite-plugin-wasm\";\nimport topLevelAwait from \"vite-plugin-top-level-await\";\nimport { defineConfig } from \"vite\";\n\nexport default defineConfig({\n  plugins: [wasm(), topLevelAwait()],\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/install.md","loc":{"lines":{"from":123,"to":139}}}}],["1328",{"pageContent":"Updating from <0.0.52\n\nIf you are updating from a version of LangChain prior to 0.0.52, you will need to update your imports to use the new path structure.\n\nFor example, if you were previously doing\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\n```\n\nyou will now need to do\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/install.md","loc":{"lines":{"from":141,"to":154}}}}],["1329",{"pageContent":"This applies to all imports from the following 6 modules, which have been split into submodules for each integration. The combined modules are deprecated, do not work outside of Node.js, and will be removed in a future version.\n\n- If you were using `langchain/llms`, see [LLMs](../modules/models/llms/integrations) for updated import paths.\n- If you were using `langchain/chat_models`, see [Chat Models](../modules/models/chat/integrations) for updated import paths.\n- If you were using `langchain/embeddings`, see [Embeddings](../modules/models/embeddings/integrations) for updated import paths.\n- If you were using `langchain/vectorstores`, see [Vector Stores](../modules/indexes/vector_stores/integrations/) for updated import paths.\n- If you were using `langchain/document_loaders`, see [Document Loaders](../modules/indexes/document_loaders/examples/) for updated import paths.\n- If you were using `langchain/retrievers`, see [Retrievers](../modules/indexes/retrievers/) for updated import paths.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/install.md","loc":{"lines":{"from":157,"to":164}}}}],["1330",{"pageContent":"Other modules are not affected by this change, and you can continue to import them from the same path.\n\nAdditionally, there are some breaking changes that were needed to support new environments:\n\n- `import { Calculator } from \"langchain/tools\";` now moved to\n  - `import { Calculator } from \"langchain/tools/calculator\";`\n- `import { loadLLM } from \"langchain/llms\";` now moved to\n  - `import { loadLLM } from \"langchain/llms/load\";`\n- `import { loadAgent } from \"langchain/agents\";` now moved to\n  - `import { loadAgent } from \"langchain/agents/load\";`\n- `import { loadPrompt } from \"langchain/prompts\";` now moved to\n  - `import { loadPrompt } from \"langchain/prompts/load\";`\n- `import { loadChain } from \"langchain/chains\";` now moved to\n  - `import { loadChain } from \"langchain/chains/load\";`","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/install.md","loc":{"lines":{"from":166,"to":179}}}}],["1331",{"pageContent":"Unsupported: Node.js 16\n\nWe do not support Node.js 16, but if you still want to run LangChain on Node.js 16, you will need to follow the instructions in this section. We do not guarantee that these instructions will continue to work in the future.\n\nYou will have to make `fetch` available globally, either:\n\n- run your application with `NODE_OPTIONS='--experimental-fetch' node ...`, or\n- install `node-fetch` and follow the instructions [here](https://github.com/node-fetch/node-fetch#providing-global-access)\n\nAdditionally you'll have to polyfill `unstructuredClone`, eg. by installing `core-js` and following the instructions [here](https://github.com/zloirock/core-js).\n\nIf you are running this on Node.js 18+, you do not need to do anything.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/getting-started/install.md","loc":{"lines":{"from":181,"to":192}}}}],["1332",{"pageContent":"# Welcome to LangChain\n\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:\n\n- _Be data-aware_: connect a language model to other sources of data\n- _Be agentic_: allow a language model to interact with its environment\n\nThe LangChain framework is designed with the above principles in mind.\n\n## Getting Started\n\nCheckout the guide below for a walkthrough of how to get started using LangChain to create a Language Model application.\n\n- [Quickstart, using LLMs](./getting-started/guide-llm.mdx)\n- [Quickstart, using Chat Models](./getting-started/guide-chat.mdx)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/index.md","loc":{"lines":{"from":1,"to":15}}}}],["1333",{"pageContent":"Components\n\nThere are several main modules that LangChain provides support for. For each module we provide some examples to get started and get familiar with some of the concepts. Each example links to API documentation for the modules used.\n\nThese modules are, in increasing order of complexity:\n\n- [Schema](./modules/schema/): This includes interfaces and base classes used throughout the library.\n\n- [Models](./modules/models/): This includes integrations with a variety of LLMs, Chat Models and Embeddings models.\n\n- [Prompts](./modules/prompts/): This includes prompt Templates and functionality to work with prompts like Output Parsers and Example Selectors\n\n- [Indexes](./modules/indexes/): This includes patterns and functionality for working with your own data, and making it ready to interact with language models (including document loaders, vectorstores, text splitters and retrievers).","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/index.md","loc":{"lines":{"from":17,"to":29}}}}],["1334",{"pageContent":"- [Memory](./modules/memory/): Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n\n- [Chains](./modules/chains/): Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n\n- [Agents](./modules/agents/): Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/index.md","loc":{"lines":{"from":31,"to":35}}}}],["1335",{"pageContent":"API Reference\n\n[Here](./api/) you can find the API reference for all of the modules in LangChain, as well as full documentation for all exported classes and functions.\n\n## Production\n\nAs you move from prototyping into production, we're developing resources to help you do so.\nThese including:\n\n- [Deployment](./production/deployment): resources on how to deploy your application to production.\n- [Events/Callbacks](./production/callbacks): resources on the events exposed by LangChain modules.\n- [Tracing](./production/tracing): resouces on how to use tracing to log and debug your application.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/index.md","loc":{"lines":{"from":37,"to":48}}}}],["1336",{"pageContent":"Additional Resources\n\nAdditional collection of resources we think may be useful as you develop your application!\n\n- [LangChainHub](https://github.com/hwchase17/langchain-hub): The LangChainHub is a place to share and explore other prompts, chains, and agents.\n\n- [Discord](https://discord.gg/6adMQxSpJS): Join us on our Discord to discuss all things LangChain!\n\n- [Production Support](https://forms.gle/57d8AmXBYp8PP8tZA): As you move your LangChains into production, we'd love to offer more comprehensive support. Please fill out this form and we'll set up a dedicated support Slack channel.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/index.md","loc":{"lines":{"from":50,"to":58}}}}],["1337",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/custom_llm_agent.ts\";\n\n# Custom LLM Agent\n\nThis example covers how to create a custom Agent powered by an LLM.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/agents/custom_llm.mdx","loc":{"lines":{"from":1,"to":13}}}}],["1338",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/custom_llm_agent_chat.ts\";\n\n# Custom LLM Agent (with Chat Model)\n\nThis example covers how to create a custom Agent powered by a Chat Model.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/agents/custom_llm_chat.mdx","loc":{"lines":{"from":1,"to":13}}}}],["1339",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 2\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/chat_mrkl.ts\";\n\n# MRKL Agent for Chat Models\n\nThis example covers how to use an agent that uses the ReAct Framework (based on the descriptions of tools) to decide what action to take. This agent is optimized to be used with Chat Models. If you want to use it with an LLM, you can use the [LLM MRKL Agent](./llm_mrkl) instead.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/agents/examples/chat_mrkl.mdx","loc":{"lines":{"from":1,"to":13}}}}],["1340",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 3\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/chat_convo_with_tracing.ts\";\n\n# Conversational Agent\n\nThis example covers how to create a conversational agent for a chat model. It will utilize chat specific prompts.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/agents/examples/conversational_agent.mdx","loc":{"lines":{"from":1,"to":13}}}}],["1341",{"pageContent":"````\nLoaded agent.\nEntering new agent_executor chain...\n{\n    \"action\": \"Final Answer\",\n    \"action_input\": \"Hello Bob! How can I assist you today?\"\n}\nFinished chain.\nGot output Hello Bob! How can I assist you today?\nEntering new agent_executor chain...\n{\n    \"action\": \"Final Answer\",\n    \"action_input\": \"Your name is Bob.\"\n}\nFinished chain.\nGot output Your name is Bob.\nEntering new agent_executor chain...\n```json\n{\n    \"action\": \"search\",\n    \"action_input\": \"weather in pomfret\"\n}\n```\nA steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.\n```json\n{\n    \"action\": \"Final Answer\",\n    \"action_input\": \"The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.\"\n}\n```\nFinished chain.\nGot output The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.\n````","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/agents/examples/conversational_agent.mdx","loc":{"lines":{"from":15,"to":47}}}}],["1342",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chat/agent.ts\";\n\n# Agent with Custom Prompt, using Chat Models\n\nThis example covers how to create a custom agent for a chat model. It will utilize chat specific prompts.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/agents/examples/custom_agent_chat.mdx","loc":{"lines":{"from":1,"to":12}}}}],["1343",{"pageContent":"---\nsidebar_label: Examples\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Examples: Agents\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/agents/examples/index.mdx","loc":{"lines":{"from":1,"to":10}}}}],["1344",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/mrkl.ts\";\n\n# MRKL Agent for LLMs\n\nThis example covers how to use an agent that uses the ReAct Framework (based on the descriptions of tools) to decide what action to take. This agent is optimized to be used with LLMs. If you want to use it with a chat model, try the [Chat MRKL Agent](./chat_mrkl).\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/agents/examples/llm_mrkl.mdx","loc":{"lines":{"from":1,"to":13}}}}],["1345",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Agents\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/agent)\n:::\n\nAn agent is a stateless wrapper around an agent prompt chain (such as MRKL) which takes care of formatting tools into the prompt, as well as parsing the responses obtained from the chat model. It takes in user input and returns a response corresponding to an action to take and a corresponding action input.\n\n```typescript\ninterface AgentStep {\n  action: AgentAction;\n  observation: string;\n}\n\ninterface AgentAction {\n  tool: string; // Tool.name\n  toolInput: string; // Tool.call argument\n}\n\ninterface AgentFinish {\n  returnValues: object;\n}\n\nclass Agent {\n  plan(steps: AgentStep[], inputs: object): Promise<AgentAction | AgentFinish>;\n}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/agents/index.mdx","loc":{"lines":{"from":1,"to":33}}}}],["1346",{"pageContent":"Which agent to choose?\n\nThe agent you choose depends on the type of task you want to perform. Here's a quick guide to help you pick the right agent for your use case:\n\n- If you're using a text LLM, first try `zero-shot-react-description`, aka. the [MRKL agent for LLMs](./examples/llm_mrkl).\n- If you're using a Chat Model, try `chat-zero-shot-react-description`, aka. the [MRKL agent for Chat Models](./examples/chat_mrkl).\n- If you're using a Chat Model and want to use memory, try `chat-conversational-react-description`, the [Conversational agent](./examples/conversational_agent).\n\n## All Agents\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/agents/index.mdx","loc":{"lines":{"from":35,"to":45}}}}],["1347",{"pageContent":"---\nsidebar_label: Getting Started\nhide_table_of_contents: true\n---\n\n# Getting Started: Agent Executors\n\nAgents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.\n\nWhen used correctly agents can be extremely powerful. In this tutorial, we show you how to easily use agents through the simplest, highest level API.\n\nIn order to load agents, you should understand the following concepts:\n\n- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, code REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.\n- LLM: The language model powering the agent.\n- Agent: The agent to use. This should be a string that references a support agent class. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/executor/getting-started.md","loc":{"lines":{"from":1,"to":16}}}}],["1348",{"pageContent":"For this example, you'll need to set the SerpAPI environment variables in the `.env` file.\n\n```bash\nSERPAPI_API_KEY=\"...\"","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/executor/getting-started.md","loc":{"lines":{"from":18,"to":21}}}}],["1349",{"pageContent":"Now we can get started!\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input =\n  \"Who is Olivia Wilde's boyfriend?\" +\n  \" What is his current age raised to the 0.23 power?\";\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/executor/getting-started.md","loc":{"lines":{"from":24,"to":54}}}}],["1350",{"pageContent":"```shell\nlangchain-examples:start: Executing with input \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"...\nlangchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the 0.23 power is 2.4242784855673896.\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/executor/getting-started.md","loc":{"lines":{"from":57,"to":60}}}}],["1351",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Agent Executors\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/agent-executor)\n:::\n\nTo make agents more powerful we need to make them iterative, ie. call the model multiple times until they arrive at the final answer. That's the job of the AgentExecutor.\n\n```typescript\nclass AgentExecutor {\n  // a simplified implementation\n  run(inputs: object) {\n    const steps = [];\n    while (true) {\n      const step = await this.agent.plan(steps, inputs);\n      if (step instanceof AgentFinish) {\n        return step.returnValues;\n      }\n      steps.push(step);\n    }\n  }\n}\n```\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/executor/index.mdx","loc":{"lines":{"from":1,"to":31}}}}],["1352",{"pageContent":"---\nsidebar_position: 7\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Agents\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents)\n:::\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/index.mdx","loc":{"lines":{"from":1,"to":14}}}}],["1353",{"pageContent":"---\nsidebar_label: Toolkits\nsidebar_position: 2\nhide_table_of_contents: true\n---\n\n# Getting Started: Toolkits\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/toolkit)\n:::\n\nGroups of [tools](../tools/) that can be used/are necessary to solve a particular problem.\n\n```typescript\ninterface Toolkit {\n  tools: Tool[];\n}\n```\n\n## All Toolkits\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/toolkits/index.mdx","loc":{"lines":{"from":1,"to":25}}}}],["1354",{"pageContent":"# JSON Agent Toolkit\n\nThis example shows how to load and use an agent with a JSON toolkit.\n\n```typescript\nimport * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { JsonToolkit, createJsonAgent } from \"langchain/agents\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const toolkit = new JsonToolkit(new JsonSpec(data));\n  const model = new OpenAI({ temperature: 0 });\n  const executor = createJsonAgent(model, toolkit);\n\n  const input = `What are the required parameters in the request body to the /completions endpoint?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/toolkits/json.md","loc":{"lines":{"from":1,"to":33}}}}],["1355",{"pageContent":"console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/toolkits/json.md","loc":{"lines":{"from":35,"to":45}}}}],["1356",{"pageContent":"# OpenAPI Agent Toolkit\n\nThis example shows how to load and use an agent with a OpenAPI toolkit.\n\n```typescript\nimport * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { createOpenApiAgent, OpenApiToolkit } from \"langchain/agents\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const headers = {\n    \"Content-Type\": \"application/json\",\n    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,\n  };\n  const model = new OpenAI({ temperature: 0 });\n  const toolkit = new OpenApiToolkit(new JsonSpec(data), model, headers);\n  const executor = createOpenApiAgent(model, toolkit);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/toolkits/openapi.md","loc":{"lines":{"from":1,"to":31}}}}],["1357",{"pageContent":"const input = `Make a POST request to openai /completions. The prompt should be 'tell me a joke.'`;\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/toolkits/openapi.md","loc":{"lines":{"from":33,"to":47}}}}],["1358",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# SQL Agent Toolkit\n\nThis example shows how to load and use an agent with a SQL toolkit.\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/sql.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/toolkits/sql.mdx","loc":{"lines":{"from":1,"to":12}}}}],["1359",{"pageContent":"# VectorStore Agent Toolkit\n\nThis example shows how to load and use an agent with a vectorstore toolkit.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport {\n  VectorStoreToolkit,\n  createVectorStoreAgent,\n  VectorStoreInfo,\n} from \"langchain/agents\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  /* Load in the file we want to do question answering over */\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  /* Split the text into chunks */\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  /* Create the vectorstore */\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/toolkits/vectorstore.md","loc":{"lines":{"from":1,"to":25}}}}],["1360",{"pageContent":"/* Create the agent */\n  const vectorStoreInfo: VectorStoreInfo = {\n    name: \"state_of_union_address\",\n    description: \"the most recent state of the Union address\",\n    vectorStore,\n  };\n\n  const toolkit = new VectorStoreToolkit(vectorStoreInfo, model);\n  const agent = createVectorStoreAgent(model, toolkit);\n\n  const input =\n    \"What did biden say about Ketanji Brown Jackson is the state of the union address?\";\n  console.log(`Executing: ${input}`);\n  const result = await agent.call({ input });\n  console.log(`Got output ${result.output}`);\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/toolkits/vectorstore.md","loc":{"lines":{"from":27,"to":50}}}}],["1361",{"pageContent":"# Agents with Vector Stores\n\nThis notebook covers how to combine agents and vector stores. The use case for this is that youve ingested your data into a vector store and want to interact with it in an agentic manner.\n\nThe recommended method for doing so is to create a VectorDBQAChain and then use that as a tool in the overall agent. Lets take a look at doing this below. You can do this with multiple different vector databases, and use the agent as a way to choose between them. There are two different ways of doing this - you can either let the agent use the vector stores as normal tools, or you can set `returnDirect: true` to just use the agent as a router.\n\nFirst, you'll want to import the relevant modules:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":1,"to":7}}}}],["1362",{"pageContent":"```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI, ChainTool } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":9,"to":18}}}}],["1363",{"pageContent":"Next, you'll want to create the vector store with your data, and then the QA chain to interact with that vector store.\n\n```typescript\nconst model = new OpenAI({ temperature: 0 });\n/* Load in the file we want to do question answering over */\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n/* Split the text into chunks */\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n/* Create the vectorstore */\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n/* Create the chain */\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":21,"to":33}}}}],["1364",{"pageContent":"Now that you have that chain, you can create a tool to use that chain. Note that you should update the name and description to be specific to your QA chain.\n\n```typescript\nconst qaTool = new ChainTool({\n  name: \"state-of-union-qa\",\n  description:\n    \"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address.\",\n  chain: chain,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":36,"to":44}}}}],["1365",{"pageContent":"Now you can construct and using the tool just as you would any other!\n\n```typescript\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n  qaTool,\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `What did biden say about ketanji brown jackson is the state of the union address?`;\n\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":47,"to":71}}}}],["1366",{"pageContent":"You can also set `returnDirect: true` if you intend to use the agent as a router and just want to directly return the result of the VectorDBQAChain.\n\n```typescript\nconst qaTool = new ChainTool({\n  name: \"state-of-union-qa\",\n  description:\n    \"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address.\",\n  chain: chain,\n  returnDirect: true,\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":74,"to":84}}}}],["1367",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/aiplugin-tool.ts\";\n\n# ChatGPT Plugins\n\nThis example shows how to use ChatGPT Plugins within LangChain abstractions.\n\nNote 1: This currently only works for plugins with no auth.\n\nNote 2: There are almost certainly other ways to do this, this is just a first pass. If you have better ideas, please open a PR!\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n````\nEntering new agent_executor chain...\nThought: Klarna is a payment provider, not a store. I need to check if there is a Klarna Shopping API that I can use to search for t-shirts.\nAction:\n```\n\n{\n\"action\": \"KlarnaProducts\",\n\"action_input\": \"\"\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/aiplugin-tool.mdx","loc":{"lines":{"from":1,"to":27}}}}],["1368",{"pageContent":"Usage Guide: Use the Klarna plugin to get relevant product suggestions for any shopping or researching purpose. The query to be sent should not include stopwords like articles, prepositions and determinants. The api works best when searching for words that are related to products, like their name, brand, model or category. Links will always be returned and should be shown to the user.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/aiplugin-tool.mdx","loc":{"lines":{"from":31,"to":31}}}}],["1369",{"pageContent":"OpenAPI Spec: {\"openapi\":\"3.0.1\",\"info\":{\"version\":\"v0\",\"title\":\"Open AI Klarna product Api\"},\"servers\":[{\"url\":\"https://www.klarna.com/us/shopping\"}],\"tags\":[{\"name\":\"open-ai-product-endpoint\",\"description\":\"Open AI Product Endpoint. Query for products.\"}],\"paths\":{\"/public/openai/v0/products\":{\"get\":{\"tags\":[\"open-ai-product-endpoint\"],\"summary\":\"API for fetching Klarna product information\",\"operationId\":\"productsUsingGET\",\"parameters\":[{\"name\":\"q\",\"in\":\"query\",\"description\":\"query, must be between 2 and 100 characters\",\"required\":true,\"schema\":{\"type\":\"string\"}},{\"name\":\"size\",\"in\":\"query\",\"description\":\"number of products returned\",\"required\":false,\"schema\":{\"type\":\"integer\"}},{\"name\":\"budget\",\"in\":\"query\",\"description\":\"maximum price of the matching product in local currency, filters results\",\"required\":false,\"schema\":{\"type\":\"integer\"}}],\"responses\":{\"200\":{\"description\":\"Products found\",\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/ProductResponse\"}}}},\"503\":{\"description\":\"one or more services","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/aiplugin-tool.mdx","loc":{"lines":{"from":33,"to":33}}}}],["1370",{"pageContent":"are unavailable\"}},\"deprecated\":false}}},\"components\":{\"schemas\":{\"Product\":{\"type\":\"object\",\"properties\":{\"attributes\":{\"type\":\"array\",\"items\":{\"type\":\"string\"}},\"name\":{\"type\":\"string\"},\"price\":{\"type\":\"string\"},\"url\":{\"type\":\"string\"}},\"title\":\"Product\"},\"ProductResponse\":{\"type\":\"object\",\"properties\":{\"products\":{\"type\":\"array\",\"items\":{\"$ref\":\"#/components/schemas/Product\"}}},\"title\":\"ProductResponse\"}}}}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/aiplugin-tool.mdx","loc":{"lines":{"from":33,"to":33}}}}],["1371",{"pageContent":"Now that I know there is a Klarna Shopping API, I can use it to search for t-shirts. I will make a GET request to the API with the query parameter \"t-shirt\".\nAction:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/aiplugin-tool.mdx","loc":{"lines":{"from":34,"to":35}}}}],["1372",{"pageContent":"{\n\"action\": \"requests_get\",\n\"action_input\": \"https://www.klarna.com/us/shopping/public/openai/v0/products?q=t-shirt\"\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/aiplugin-tool.mdx","loc":{"lines":{"from":38,"to":41}}}}],["1373",{"pageContent":"{\"products\":[{\"name\":\"Psycho Bunny Mens Copa Gradient Logo Graphic Tee\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203663222/Clothing/Psycho-Bunny-Mens-Copa-Gradient-Logo-Graphic-Tee/?source=openai\",\"price\":\"$35.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:White,Blue,Black,Orange\"]},{\"name\":\"T-shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203506327/Clothing/T-shirt/?source=openai\",\"price\":\"$20.45\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray,White,Blue,Black,Orange\"]},{\"name\":\"Palm Angels Bear T-shirt - Black\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201090513/Clothing/Palm-Angels-Bear-T-shirt-Black/?source=openai\",\"price\":\"$168.36\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Black\"]},{\"name\":\"Tommy Hilfiger Essential Flag Logo","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/aiplugin-tool.mdx","loc":{"lines":{"from":46,"to":46}}}}],["1374",{"pageContent":"T-shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201840629/Clothing/Tommy-Hilfiger-Essential-Flag-Logo-T-shirt/?source=openai\",\"price\":\"$22.52\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Red,Gray,White,Blue,Black\",\"Pattern:Solid Color\",\"Environmental Attributes :Organic\"]},{\"name\":\"Coach Outlet Signature T Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203005573/Clothing/Coach-Outlet-Signature-T-Shirt/?source=openai\",\"price\":\"$75.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray\"]}]}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/aiplugin-tool.mdx","loc":{"lines":{"from":46,"to":46}}}}],["1375",{"pageContent":"Finished chain.\n{\n  result: {\n    output: 'The available t-shirts in Klarna are Psycho Bunny Mens Copa Gradient Logo Graphic Tee, T-shirt, Palm Angels Bear T-shirt - Black, Tommy Hilfiger Essential Flag Logo T-shirt, and Coach Outlet Signature T Shirt.',\n    intermediateSteps: [ [Object], [Object] ]\n  }\n}\n````","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/aiplugin-tool.mdx","loc":{"lines":{"from":47,"to":54}}}}],["1376",{"pageContent":"---\nsidebar_position: 1\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Tools\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/tool)\n:::\n\nA tool is an abstraction around a function that makes it easy for a language model to interact with it. Specifically, the interface of a tool has a single text input and a single text output. It includes a name and description that communicate to the [Model](../../models/) what the tool does and when to use it.\n\n```typescript\ninterface Tool {\n  call(arg: string): Promise<string>;\n\n  name: string;\n\n  description: string;\n}\n```\n\n## All Tools\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/index.mdx","loc":{"lines":{"from":1,"to":28}}}}],["1377",{"pageContent":"Advanced\n\nTo implement a custom tool you can subclass the `Tool` class and implement the `_call` method. The `_call` method is called with the input text and should return the output text. The Tool superclass implements the `call` method, which takes care of calling the right CallbackManager methods before and after calling your `_call` method. When an error occurs, the `_call` method should when possible return a string representing an error, rather than throwing an error. This allows the error to be passed to the LLM and the LLM can decide how to handle it. If an error is thrown then execution of the agent will stop.\n\n```typescript\nabstract class Tool {\n  abstract _call(arg: string): Promise<string>;\n\n  abstract name: string;\n\n  abstract description: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/index.mdx","loc":{"lines":{"from":30,"to":41}}}}],["1378",{"pageContent":"Another option is to create a tool on the fly using a `DynamicTool`. This is useful if you don't need the overhead of subclassing `Tool`.\nThe `DynamicTool` class takes as input a name, a description, and a function. Importantly, the name and the description will be used by the language model to determine when to call this function and with what parameters! So make sure to set these to some values the language model can reason about. The function provided is what will actually be called. When an error occurs, the function should, when possible, return a string representing an error, rather than throwing an error. This allows the error to be passed to the LLM and the LLM can decide how to handle it. If an error is thrown then execution of the agent will stop.\n\nSee below for an example of defining and using `DynamicTool`s.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/index.mdx","loc":{"lines":{"from":44,"to":47}}}}],["1379",{"pageContent":"```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { DynamicTool } from \"langchain/tools\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new DynamicTool({\n      name: \"FOO\",\n      description:\n        \"call this to get the value of foo. input should be an empty string.\",\n      func: () => \"baz\",\n    }),\n    new DynamicTool({\n      name: \"BAR\",\n      description:\n        \"call this to get the value of bar. input should be an empty string.\",\n      func: () => \"baz1\",\n    }),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n  });\n\n  console.log(\"Loaded agent.\");\n\n  const input = `What is the value of foo?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/index.mdx","loc":{"lines":{"from":49,"to":85}}}}],["1380",{"pageContent":"---\nsidebar_label: Integrations\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Integrations: Tools\n\nLangChain provides the following tools you can use out of the box:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/integrations/index.mdx","loc":{"lines":{"from":1,"to":11}}}}],["1381",{"pageContent":"- [`AWSLambda`][AWSLambda] - A wrapper around the AWS Lambda API, invoked via the Amazon Web Services Node.js SDK. Useful for invoking serverless functions with any behavior which you need to provide to an Agent.\n- [`BingSerpAPI`][BingSerpAPI] - A wrapper around the Bing Search API. Useful for when you need to answer questions about current events. Input should be a search query.\n- [`Calculator`][Calculator] - Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.\n- [`IFTTTWebHook`][IFTTTWebHook] - A wrapper around the IFTTT Webhook API. Useful for triggering IFTTT actions.\n- [`JsonListKeysTool`][JsonListKeysTool] and [`JsonGetValueTool`][JsonGetValueTool] - Useful for extracting data from JSON objects. These tools can be used collectively in a [`JsonToolkit`][JsonToolkit].","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/integrations/index.mdx","loc":{"lines":{"from":13,"to":17}}}}],["1382",{"pageContent":"- [`RequestsGetTool`][RequestsGetTool] and [`RequestsPostTool`][RequestsPostTool] - Useful for making HTTP requests.\n- [`SerpAPI`][SerpAPI] - A wrapper around the SerpAPI API. Useful for when you need to answer questions about current events. Input should be a search query.\n- [`QuerySqlTool`][QuerySqlTool], [`InfoSqlTool`][InfoSqlTool], [`ListTablesSqlTool`][ListTablesSqlTool], and [`QueryCheckerTool`][QueryCheckerTool] - Useful for interacting with SQL databases. Can be used together in a [`SqlToolkit`][SqlToolkit].\n- [`VectorStoreQATool`][VectorStoreQATool] - Useful for retrieving relevant text data from a vector store.\n- [`ZapierNLARunAction`][ZapierNLARunAction] - A wrapper around the Zapier NLP API. Useful for triggering Zapier actions with a natural language input. Best when used in a [`ZapierToolkit`][ZapierToolkit].","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/integrations/index.mdx","loc":{"lines":{"from":18,"to":22}}}}],["1383",{"pageContent":"[AWSLambda]: /docs/api/tools_aws_lambda/classes/AWSLambda\n[BingSerpAPI]: /docs/api/tools/classes/BingSerpAPI\n[Calculator]: /docs/api/tools_calculator/classes/Calculator\n[IFTTTWebHook]: /docs/api/tools/classes/IFTTTWebHook\n[JsonListKeysTool]: /docs/api/tools/classes/JsonListKeysTool\n[JsonGetValueTool]: /docs/api/tools/classes/JsonGetValueTool\n[JsonToolkit]: ../../toolkits/json\n[RequestsGetTool]: /docs/api/tools/classes/RequestsGetTool\n[RequestsPostTool]: /docs/api/tools/classes/RequestsPostTool\n[SerpAPI]: /docs/api/tools/classes/SerpAPI\n[QuerySqlTool]: /docs/api/tools/classes/QuerySqlTool\n[InfoSqlTool]: /docs/api/tools/classes/InfoSqlTool\n[ListTablesSqlTool]: /docs/api/tools/classes/ListTablesSqlTool\n[QueryCheckerTool]: /docs/api/tools/classes/QueryCheckerTool\n[SqlToolkit]: ../../toolkits/sql\n[VectorStoreQATool]: /docs/api/tools/classes/VectorStoreQATool\n[ZapierNLARunAction]: /docs/api/tools/classes/ZapierNLARunAction\n[ZapierToolkit]: ../zapier_agent","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/integrations/index.mdx","loc":{"lines":{"from":24,"to":41}}}}],["1384",{"pageContent":"---\nsidebar_label: Agent with AWS Lambda\nhide_table_of_contents: true\n---\n\n# Agent with AWS Lambda Integration\n\nFull docs here: https://docs.aws.amazon.com/lambda/index.html\n\n**AWS Lambda** is a serverless computing service provided by Amazon Web Services (AWS), designed to allow developers to build and run applications and services without the need for provisioning or managing servers. This serverless architecture enables you to focus on writing and deploying code, while AWS automatically takes care of scaling, patching, and managing the infrastructure required to run your applications.\n\nBy including a AWSLambda in the list of tools provided to an Agent, you can grant your Agent the ability to invoke code running in your AWS Cloud for whatever purposes you need.\n\nWhen an Agent uses the AWSLambda tool, it will provide an argument of type `string` which will in turn be passed into the Lambda function via the `event` parameter.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/lambda_agent.md","loc":{"lines":{"from":1,"to":14}}}}],["1385",{"pageContent":"This quick start will demonstrate how an Agent could use a Lambda function to send an email via [Amazon Simple Email Service](https://aws.amazon.com/ses/). The lambda code which sends the email is not provided, but if you'd like to learn how this could be done, see [here](https://repost.aws/knowledge-center/lambda-send-email-ses). Keep in mind this is an intentionally simple example; Lambda can used to execute code for a near infinite number of other purposes (including executing more Langchains)!","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/lambda_agent.md","loc":{"lines":{"from":16,"to":16}}}}],["1386",{"pageContent":"Note about credentials:\n\n- If you have not run [`aws configure`](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) via the AWS CLI, the `region`, `accessKeyId`, and `secretAccessKey` must be provided to the AWSLambda constructor.\n- The IAM role corresponding to those credentials must have permission to invoke the lambda function.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { AWSLambda } from \"langchain/tools/aws_lambda\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/lambda_agent.md","loc":{"lines":{"from":18,"to":27}}}}],["1387",{"pageContent":"const model = new OpenAI({ temperature: 0 });\nconst emailSenderTool = new AWSLambda({\n  name: \"email-sender\",\n  // tell the Agent precisely what the tool does\n  description:\n    \"Sends an email with the specified content to testing123@gmail.com\",\n  region: \"us-east-1\", // optional: AWS region in which the function is deployed\n  accessKeyId: \"abc123\", // optional: access key id for a IAM user with invoke permissions\n  secretAccessKey: \"xyz456\", // optional: secret access key for that IAM user\n  functionName: \"SendEmailViaSES\", // the function name as seen in AWS Console\n});\nconst tools = [emailSenderTool, new SerpAPI(\"api_key_goes_here\")];\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\n\nconst input = `Find out the capital of Croatia. Once you have it, email the answer to testing123@gmail.com.`;\nconst result = await executor.call({ input });\nconsole.log(result);\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/lambda_agent.md","loc":{"lines":{"from":29,"to":48}}}}],["1388",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Web Browser Tool\n\nThe Webbrowser Tool gives your agent the ability to visit a website and extract information. It is described to the agent as\n\n```\nuseful for when you need to find something on or summarize a webpage. input should be a comma seperated list of \"valid URL including protocol\",\"what you want to find on the page or empty string for a summary\".\n```\n\nIt exposes two modes of operation:\n\n- when called by the Agent with only a URL it produces a summary of the website contents\n- when called by the Agent with a URL and a description of what to find it will instead use an in-memory Vector Store to find the most relevant snippets and summarise those\n\n## Setup\n\nTo use the Webbrowser Tool you need to install the dependencies:\n\n```bash npm2yarn\nnpm install cheerio axios\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/webbrowser.mdx","loc":{"lines":{"from":1,"to":26}}}}],["1389",{"pageContent":"Usage, standalone\n\nimport ToolExample from \"@examples/tools/webbrowser.ts\";\n\n<CodeBlock language=\"typescript\">{ToolExample}</CodeBlock>\n\n## Usage, in an Agent\n\nimport AgentExample from \"@examples/agents/mrkl_browser.ts\";\n\n<CodeBlock language=\"typescript\">{AgentExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/webbrowser.mdx","loc":{"lines":{"from":28,"to":38}}}}],["1390",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\n\n# Agent with Zapier NLA Integration\n\nFull docs here: https://nla.zapier.com/api/v1/dynamic/docs\n\n**Zapier Natural Language Actions** gives you access to the 5k+ apps and 20k+ actions on Zapier's platform through a natural language API interface.\n\nNLA supports apps like Gmail, Salesforce, Trello, Slack, Asana, HubSpot, Google Sheets, Microsoft Teams, and thousands more apps: https://zapier.com/apps\n\nZapier NLA handles ALL the underlying API auth and translation from natural language --> underlying API call --> return simplified output for LLMs. The key idea is you, or your users, expose a set of actions via an oauth-like setup window, which you can then query and execute via a REST API.\n\nNLA offers both API Key and OAuth for signing NLA API requests.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/zapier_agent.mdx","loc":{"lines":{"from":1,"to":13}}}}],["1391",{"pageContent":"Server-side (API Key): for quickly getting started, testing, and production scenarios where LangChain will only use actions exposed in the developer's Zapier account (and will use the developer's connected accounts on Zapier.com)\n\nUser-facing (Oauth): for production scenarios where you are deploying an end-user facing application and LangChain needs access to end-user's exposed actions and connected accounts on Zapier.com\n\nThis quick start will focus on the server-side use case for brevity. Review full docs or reach out to nla@zapier.com for user-facing oauth developer support.\n\nThe example below demonstrates how to use the Zapier integration as an Agent:\n\nimport Example from \"@examples/agents/zapier_mrkl.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/agents/tools/zapier_agent.mdx","loc":{"lines":{"from":15,"to":25}}}}],["1392",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Chains\nsidebar_position: 6\n---\n\nimport DocCardList from \"@theme/DocCardList\";\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Getting Started: Chains\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains)\n:::info\n\nUsing a language model in isolation is fine for some applications, but it is often useful to combine language models with other sources of information, third-party APIs, or even other language models. This is where the concept of a chain comes in.\n\nLangChain provides a standard interface for chains, as well as a number of built-in chains that can be used out of the box. You can also create your own chains.\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index.mdx","loc":{"lines":{"from":1,"to":20}}}}],["1393",{"pageContent":"Advanced\n\nTo implement your own custom chain you can subclass `BaseChain` and implement the following methods:\n\nimport SubclassInterface from \"@examples/chains/advanced_subclass.ts\";\n\n<CodeBlock language=\"typescript\">{SubclassInterface}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index.mdx","loc":{"lines":{"from":22,"to":28}}}}],["1394",{"pageContent":"Subclassing `BaseChain`\n\nThe `_call` method is the main method custom chains must implement. It takes a record of inputs and returns a record of outputs. The inputs received should conform the `inputKeys` array, and the outputs returned should conform to the `outputKeys` array.\n\nWhen implementing this method in a custom chain it's worth paying special attention to the `runManager` argument, which is what allows your custom chains to participate in the same [callbacks system](../../production/callbacks/) as the built-in chains.\n\nIf you call into another chain/model/agent inside your custom chain then you should pass it the result of calling `runManager?.getChild()` which will produce a new callback manager scoped to that inner run. An example:\n\nimport SubclassCall from \"@examples/chains/advanced_subclass_call.ts\";\n\n<CodeBlock language=\"typescript\">{SubclassCall}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index.mdx","loc":{"lines":{"from":30,"to":40}}}}],["1395",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 3\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport ConvoRetrievalQAExample from \"@examples/chains/conversational_qa.ts\";\n\n# Conversational Retrieval QA\n\nThe `ConversationalRetrievalQA` chain builds on `RetrievalQAChain` to provide a chat history component.\n\nIt requires two inputs: a question and the chat history. It first combines the chat history and the question into a standalone question, then looks up relevant documents from the retriever, and then passes those documents and the question to a question answering chain to return a response.\n\nTo create one, you will need a retriever. In the below example, we will create one from a vectorstore, which can be created from embeddings.\nimport Example from \"@examples/chains/conversational_qa.ts\";\n\n<CodeBlock language=\"typescript\">{ConvoRetrievalQAExample}</CodeBlock>\n\nIn this code snippet, the fromLLM method of the `ConversationalRetrievalQAChain` class has the following signature:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index_related_chains/conversational_retrieval.mdx","loc":{"lines":{"from":1,"to":20}}}}],["1396",{"pageContent":"```typescript\nstatic fromLLM(\n  llm: BaseLanguageModel,\n  retriever: BaseRetriever,\n  options?: {\n    questionGeneratorTemplate?: string;\n    qaTemplate?: string;\n    returnSourceDocuments?: boolean;\n  }\n): ChatVectorDBQAChain","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index_related_chains/conversational_retrieval.mdx","loc":{"lines":{"from":22,"to":31}}}}],["1397",{"pageContent":"Here's an explanation of each of the attributes of the options object:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index_related_chains/conversational_retrieval.mdx","loc":{"lines":{"from":34,"to":34}}}}],["1398",{"pageContent":"- `questionGeneratorTemplate`: A string that specifies a question generation template. If provided, the `ConversationalRetrievalQAChain` will use this template to generate a question from the conversation context, instead of using the question provided in the question parameter. This can be useful if the original question does not contain enough information to retrieve a suitable answer.\n- `qaTemplate`: A string that specifies a response template. If provided, the `ConversationalRetrievalQAChain` will use this template to format a response before returning the result. This can be useful if you want to customize the way the response is presented to the end user.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index_related_chains/conversational_retrieval.mdx","loc":{"lines":{"from":36,"to":37}}}}],["1399",{"pageContent":"- `returnSourceDocuments`: A boolean value that indicates whether the `ConversationalRetrievalQAChain` should return the source documents that were used to retrieve the answer. If set to true, the documents will be included in the result returned by the call() method. This can be useful if you want to allow the user to see the sources used to generate the answer. If not set, the default value will be false.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index_related_chains/conversational_retrieval.mdx","loc":{"lines":{"from":38,"to":38}}}}],["1400",{"pageContent":"In summary, the `questionGeneratorTemplate`, `qaTemplate`, and `returnSourceDocuments` options allow the user to customize the behavior of the `ConversationalRetrievalQAChain`","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index_related_chains/conversational_retrieval.mdx","loc":{"lines":{"from":40,"to":40}}}}],["1401",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\nimport QAExample from \"@examples/chains/question_answering.ts\";\nimport RefineExample from \"@examples/chains/qa_refine.ts\";\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Document QA\n\nLangChain provides chains used for processing unstructured text data: `StuffDocumentsChain`, `MapReduceDocumentsChain` and `RefineDocumentsChain`.\nThese chains are the building blocks more complex chains for processing unstructured text data and receive both documents and a question as input. They then utilize the language model to provide an answer to the question based on the given documents.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index_related_chains/document_qa.mdx","loc":{"lines":{"from":1,"to":13}}}}],["1402",{"pageContent":"- `StuffDocumentsChain`: This chain is the simplest of the 3 chains and simply injects all documents passes in into the prompt. It then returns the answer to the question, using all documents as context. It is suitable for QA tasks over a small number of documents.\n- `MapReduceDocumentsChain`: This chain adds a preprocessing step to select relevant portions of each document until the total number of tokens is less than the maximum number of tokens allowed by the model. It then uses the transformed docs as context to answer the question. It is suitable for QA tasks over larger documents, and it runs the preprocessing step in parallel, which can reduce the running time.\n- `RefineDocumentsChain`: This chain iterates over the documents one by one to update a running answer, at each turn using the previous version of the answer and the next doc as context. It is suitable for QA tasks over a large number of documents.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index_related_chains/document_qa.mdx","loc":{"lines":{"from":15,"to":17}}}}],["1403",{"pageContent":"Usage, `StuffDocumentsChain` and `MapReduceDocumentsChain`\n\n<CodeBlock language=\"typescript\">{QAExample}</CodeBlock>\n\n## Usage, `RefineDocumentsChain`\n\n<CodeBlock language=\"typescript\">{RefineExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index_related_chains/document_qa.mdx","loc":{"lines":{"from":19,"to":25}}}}],["1404",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Index Related Chains\nsidebar_position: 2\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Index Related Chains\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains/index_related_chains)\n:::\n\nChains related to working with unstructured data stored in indexes.\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index_related_chains/index.mdx","loc":{"lines":{"from":1,"to":17}}}}],["1405",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 2\n---\n\nimport RetrievalQAExample from \"@examples/chains/retrieval_qa.ts\";\nimport RetrievalQAExampleCustom from \"@examples/chains/retrieval_qa_custom.ts\";\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Retrieval QA\n\nThe `RetrievalQAChain` is a chain that combines a `Retriever` and a QA chain (described above). It is used to retrieve documents from a `Retriever` and then use a `QA` chain to answer a question based on the retrieved documents.\n\n## Usage\n\nIn the below example, we are using a `VectorStore` as the `Retriever`. By default, the `StuffDocumentsChain` is used as the `QA` chain.\n\n<CodeBlock language=\"typescript\">{RetrievalQAExample}</CodeBlock>\n\n## Usage, with a custom `QA` chain\n\nIn the below example, we are using a `VectorStore` as the `Retriever` and a `RefineDocumentsChain` as the `QA` chain.\n\n<CodeBlock language=\"typescript\">{RetrievalQAExampleCustom}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/index_related_chains/retrieval_qa.mdx","loc":{"lines":{"from":1,"to":24}}}}],["1406",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: LLM Chain\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chains/llm_chain.ts\";\n\n# Getting Started: LLMChain\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains/llm-chain)\n:::\n\nAn `LLMChain` is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.\n\nAn `LLMChain` consists of a `PromptTemplate` and a language model (either an [LLM](../models/llms/) or [chat model](../models/chat/)).\n\nWe can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/llmchain.mdx","loc":{"lines":{"from":1,"to":22}}}}],["1407",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport AnalyzeDocumentExample from \"@examples/chains/analyze_document_chain_summarize.ts\";\n\n# `AnalyzeDocumentChain`\n\nYou can use the `AnalyzeDocumentChain`, which accepts a single piece of text as input and operates over it.\nThis chain takes care of splitting up the text and then passing it to the `MapReduceDocumentsChain` to generate a summary.\n\n<CodeBlock language=\"typescript\">{AnalyzeDocumentExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/other_chains/analyze_document.mdx","loc":{"lines":{"from":1,"to":9}}}}],["1408",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport ConstitutionalChainExample from \"@examples/chains/constitutional_chain.ts\";\n\n# `ConstitutionalChain`\n\nThe `ConstitutionalChain` is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. By incorporating specific rules and guidelines, the `ConstitutionalChain` filters and modifies the generated content to align with these principles, thus providing more controlled, ethical, and contextually appropriate responses. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context.\n\n<CodeBlock language=\"typescript\">{ConstitutionalChainExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/other_chains/constitutional_chain.mdx","loc":{"lines":{"from":1,"to":8}}}}],["1409",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Other Chains\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Other Chains\n\nThis section highlights other examples of chains that exist.\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/other_chains/index.mdx","loc":{"lines":{"from":1,"to":12}}}}],["1410",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport OpenAIModerationExample from \"@examples/chains/openai_moderation.ts\";\n\n# `OpenAIModerationChain`\n\nYou can use the `OpenAIModerationChain` which takes care of evaluating the input and identifying whether it violates OpenAI's Terms of Service (TOS). If the input contains any content that breaks the TOS and throwError is set to true, an error will be thrown and caught. If throwError is set to false the chain will return \"Text was found that violates OpenAI's content policy.\"\n\n<CodeBlock language=\"typescript\">{OpenAIModerationExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/other_chains/moderation_chain.mdx","loc":{"lines":{"from":1,"to":8}}}}],["1411",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport SqlDBExample from \"@examples/chains/sql_db.ts\";\n\n# `SqlDatabaseChain`\n\nThe `SqlDatabaseChain` allows you to answer questions over a SQL database.\nThis example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/other_chains/sql.mdx","loc":{"lines":{"from":1,"to":7}}}}],["1412",{"pageContent":"Set up\n\nFirst install `typeorm`:\n\n```bash npm2yarn\nnpm install typeorm\n```\n\nThen install the dependencies needed for your database. For example, for SQLite:\n\n```bash npm2yarn\nnpm install sqlite3\n```\n\nFor other databases see https://typeorm.io/#installation\n\nFinally follow the instructions on https://database.guide/2-sample-databases-sqlite/ to get the sample database for this example.\n\n<CodeBlock language=\"typescript\">{SqlDBExample}</CodeBlock>\n\nYou can include or exclude tables when creating the `SqlDatabase` object to help the chain focus on the tables you want.\nIt can also reduce the number of tokens used in the chain.\n\n```typescript\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n  includeTables: [\"Track\"],\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/other_chains/sql.mdx","loc":{"lines":{"from":9,"to":37}}}}],["1413",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport SummarizeExample from \"@examples/chains/summarization_map_reduce.ts\";\nimport SummarizeExampleIntermediateSteps from \"@examples/chains/summarization_map_reduce_intermediate_steps.ts\";\n\n# Summarization\n\nA summarization chain can be used to summarize multiple documents. One way is to input multiple smaller documents, after they have been divided into chunks, and operate over them with a `MapReduceDocumentsChain`. You can also choose instead for the chain that does summarization to be a StuffDocumentsChain, or a RefineDocumentsChain. See more about the differences between them [here](../index_related_chains/document_qa)\n\n<CodeBlock language=\"typescript\">{SummarizeExample}</CodeBlock>\n\n## Intermediate Steps\n\nWe can also return the intermediate steps for `map_reduce` chains, should we want to inspect them. This is done with the `returnIntermediateSteps` parameter.\n\n<CodeBlock language=\"typescript\">{SummarizeExampleIntermediateSteps}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/other_chains/summarization.mdx","loc":{"lines":{"from":1,"to":15}}}}],["1414",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Prompt Selectors\n---\n\n# Prompt Selectors\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains/prompt-selector)\n:::\n\nOftentimes, you will want to programmatically select a prompt based on the type of model you are using in a chain. This is especially relevant when swapping chat models and LLMs.\n\nThe interface for prompt selectors is quite simple:\n\n```typescript\nabstract class BasePromptSelector {\n  abstract getPrompt(llm: BaseLanguageModel): BasePromptTemplate;\n}\n```\n\nThe `getPrompt` method takes in a language model and returns an appropriate prompt template.\n\nWe currently offer a `ConditionalPromptSelector` that allows you to specify a set of conditions and prompt templates. The first condition that evaluates to true will be used to select the prompt template.\n\n```typescript\nconst QA_PROMPT_SELECTOR = new ConditionalPromptSelector(DEFAULT_QA_PROMPT, [\n  [isChatModel, CHAT_PROMPT],\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/prompt_selectors/index.mdx","loc":{"lines":{"from":1,"to":29}}}}],["1415",{"pageContent":"This will return `DEFAULT_QA_PROMPT` if the model is not a chat model, and `CHAT_PROMPT` if it is.\n\nThe example below shows how to use a prompt selector when loading a chain:\n\n```typescript\nconst loadQAStuffChain = (\n  llm: BaseLanguageModel,\n  params: StuffQAChainParams = {}\n) => {\n  const { prompt = QA_PROMPT_SELECTOR.getPrompt(llm) } = params;\n  const llmChain = new LLMChain({ prompt, llm });\n  const chain = new StuffDocumentsChain({ llmChain });\n  return chain;\n};\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/prompt_selectors/index.mdx","loc":{"lines":{"from":32,"to":46}}}}],["1416",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 3\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport SimpleSequentialChainExample from \"@examples/chains/simple_sequential_chain.ts\";\nimport SequentialChainExample from \"@examples/chains/sequential_chain.ts\";\n\n# Sequential Chain\n\nSequential chains allow you to connect multiple chains and compose them into pipelines that execute some specific scenario.\n\n## `SimpleSequentialChain`\n\nLet's start with the simplest possible case which is `SimpleSequentialChain`.\n\nAn `SimpleSequentialChain` is a chain that allows you to join multiple single-input/single-output chains into one chain.\n\nThe example below shows a sample usecase. In the first step, given a title, a synopsis of a play is generated. In the second step, based on the generated synopsis, a review of the play is generated.\n\n<CodeBlock language=\"typescript\">{SimpleSequentialChainExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/sequential_chain.mdx","loc":{"lines":{"from":1,"to":22}}}}],["1417",{"pageContent":"`SequentialChain`\n\nMore advanced scenario useful when you have multiple chains that have more than one input or ouput keys.\n\n<CodeBlock language=\"typescript\">{SequentialChainExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/chains/sequential_chain.mdx","loc":{"lines":{"from":24,"to":28}}}}],["1418",{"pageContent":"# CSV files\n\nThis example goes over how to load data from CSV files. The second argument is the `column` name to extract from the CSV file. One document will be created for each row in the CSV file. When `column` is not specified, each row is converted into a key/value pair with each key/value pair outputted to a new line in the document's `pageContent`. When `column` is specified, one document is created for each row, and the value of the specified column is used as the document's pageContent.\n\n## Setup\n\n```bash npm2yarn\nnpm install d3-dsv@2\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/csv.md","loc":{"lines":{"from":1,"to":9}}}}],["1419",{"pageContent":"Usage, extracting all columns\n\nExample CSV file:\n\n```csv\nid,text\n1,This is a sentence.\n2,This is another sentence.\n```\n\nExample code:\n\n```typescript\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\nconst loader = new CSVLoader(\"src/document_loaders/example_data/example.csv\");\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"line\": 1,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"id: 1\ntext: This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"line\": 2,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"id: 2\ntext: This is another sentence.\",\n  },\n]\n*/\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/csv.md","loc":{"lines":{"from":11,"to":49}}}}],["1420",{"pageContent":"Usage, extracting a single column\n\nExample CSV file:\n\n```csv\nid,text\n1,This is a sentence.\n2,This is another sentence.\n```\n\nExample code:\n\n```typescript\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\nconst loader = new CSVLoader(\n  \"src/document_loaders/example_data/example.csv\",\n  \"text\"\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"line\": 1,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"line\": 2,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/csv.md","loc":{"lines":{"from":51,"to":90}}}}],["1421",{"pageContent":"---\nsidebar_position: 1\nhide_table_of_contents: true\n---\n\n# Folders with multiple files\n\nThis example goes over how to load data from folders with multiple files. The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.\n\nExample folder:\n\n```text\nsrc/document_loaders/example_data/example/\n example.json\n example.jsonl\n example.txt\n example.csv","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/directory.md","loc":{"lines":{"from":1,"to":17}}}}],["1422",{"pageContent":"Example code:\n\n```typescript\nimport { DirectoryLoader } from \"langchain/document_loaders/fs/directory\";\nimport {\n  JSONLoader,\n  JSONLinesLoader,\n} from \"langchain/document_loaders/fs/json\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\nconst loader = new DirectoryLoader(\n  \"src/document_loaders/example_data/example\",\n  {\n    \".json\": (path) => new JSONLoader(path, \"/texts\"),\n    \".jsonl\": (path) => new JSONLinesLoader(path, \"/html\"),\n    \".txt\": (path) => new TextLoader(path),\n    \".csv\": (path) => new CSVLoader(path, \"text\"),\n  }\n);\nconst docs = await loader.load();\nconsole.log({ docs });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/directory.md","loc":{"lines":{"from":20,"to":42}}}}],["1423",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Docx files\n\nThis example goes over how to load data from docx files.\n\n# Setup\n\n```bash npm2yarn\nnpm install mammoth\n```\n\n# Usage\n\n```typescript\nimport { DocxLoader } from \"langchain/document_loaders/fs/docx\";\n\nconst loader = new DocxLoader(\n  \"src/document_loaders/tests/example_data/attention.docx\"\n);\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/docx.md","loc":{"lines":{"from":1,"to":25}}}}],["1424",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# EPUB files\n\nThis example goes over how to load data from EPUB files. By default, one document will be created for each chapter in the EPUB file, you can change this behavior by setting the `splitChapters` option to `false`.\n\n# Setup\n\n```bash npm2yarn\nnpm install epub2 html-to-text\n```\n\n# Usage, one document per chapter\n\n```typescript\nimport { EPubLoader } from \"langchain/document_loaders/fs/epub\";\n\nconst loader = new EPubLoader(\"src/document_loaders/example_data/example.epub\");\n\nconst docs = await loader.load();\n```\n\n# Usage, one document per file\n\n```typescript\nimport { EPubLoader } from \"langchain/document_loaders/fs/epub\";\n\nconst loader = new EPubLoader(\n  \"src/document_loaders/example_data/example.epub\",\n  {\n    splitChapters: false,\n  }\n);\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/epub.md","loc":{"lines":{"from":1,"to":38}}}}],["1425",{"pageContent":"---\nlabel: \"File Loaders\"\nhide_table_of_contents: true\nsidebar_class_name: node-only-category\n---\n\n# File Loaders\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nThese loaders are used to load files given a filesystem path or a Blob object.\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/index.mdx","loc":{"lines":{"from":1,"to":17}}}}],["1426",{"pageContent":"# JSON files\n\nThe JSON loader use [JSON pointer](https://github.com/janl/node-jsonpointer) to target keys in your JSON files you want to target.\n\n### No JSON pointer example\n\nThe most simple way of using it, is to specify no JSON pointer.\nThe loader will load all strings it finds in the JSON object.\n\nExample JSON file:\n\n```json\n{\n  \"texts\": [\"This is a sentence.\", \"This is another sentence.\"]\n}\n```\n\nExample code:\n\n```typescript\nimport { JSONLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLoader(\"src/document_loaders/example_data/example.json\");\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/json.md","loc":{"lines":{"from":1,"to":46}}}}],["1427",{"pageContent":"Using JSON pointer example\n\nYou can do a more advanced scenario by choosing which keys in your JSON object you want to extract string from.\n\nIn this example, we want to only extract information from \"from\" and \"surname\" entries.\n\n```json\n{\n  \"1\": {\n    \"body\": \"BD 2023 SUMMER\",\n    \"from\": \"LinkedIn Job\",\n    \"labels\": [\"IMPORTANT\", \"CATEGORY_UPDATES\", \"INBOX\"]\n  },\n  \"2\": {\n    \"body\": \"Intern, Treasury and other roles are available\",\n    \"from\": \"LinkedIn Job2\",\n    \"labels\": [\"IMPORTANT\"],\n    \"other\": {\n      \"name\": \"plop\",\n      \"surname\": \"bob\"\n    }\n  }\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/json.md","loc":{"lines":{"from":48,"to":70}}}}],["1428",{"pageContent":"Example code:\n\n```typescript\nimport { JSONLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLoader(\n  \"src/document_loaders/example_data/example.json\",\n  [\"/from\", \"/surname\"]\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"BD 2023 SUMMER\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"LinkedIn Job\",\n  },\n  ...\n]\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/json.md","loc":{"lines":{"from":73,"to":104}}}}],["1429",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# JSONLines files\n\nThis example goes over how to load data from JSONLines or JSONL files. The second argument is a JSONPointer to the property to extract from each JSON object in the file. One document will be created for each JSON object in the file.\n\nExample JSONLines file:\n\n```json\n{\"html\": \"This is a sentence.\"}\n{\"html\": \"This is another sentence.\"}\n```\n\nExample code:\n\n```typescript\nimport { JSONLinesLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLinesLoader(\n  \"src/document_loaders/example_data/example.jsonl\",\n  \"/html\"\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/jsonl+json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/jsonl+json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/jsonlines.md","loc":{"lines":{"from":1,"to":47}}}}],["1430",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Notion markdown export\n\nThis example goes over how to load data from your Notion pages exported from the notion dashboard.\n\nFirst, export your notion pages as **Markdown & CSV** as per the offical explanation [here](https://www.notion.so/help/export-your-content). Make sure to select `include subpages` and `Create folders for subpages.`\n\nThen, unzip the downloaded file and move the unzipped folder into your repository. It should contain the markdown files of your pages.\n\nOnce the folder is in your repository, simply run the example below:\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/document_loaders/notion_markdown.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/notion_markdown.mdx","loc":{"lines":{"from":1,"to":18}}}}],["1431",{"pageContent":"# PDF files\n\nThis example goes over how to load data from PDF files. By default, one document will be created for each page in the PDF file, you can change this behavior by setting the `splitPages` option to `false`.\n\n## Setup\n\n```bash npm2yarn\nnpm install pdf-parse\n```\n\n## Usage, one document per page\n\n```typescript\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\");\n\nconst docs = await loader.load();\n```\n\n## Usage, one document per file\n\n```typescript\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  splitPages: false,\n});\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/pdf.md","loc":{"lines":{"from":1,"to":31}}}}],["1432",{"pageContent":"Usage, custom `pdfjs` build\n\nBy default we use the `pdfjs` build bundled with `pdf-parse`, which is compatible with most environments, including Node.js and modern browsers. If you want to use a more recent version of `pdfjs-dist` or if you want to use a custom build of `pdfjs-dist`, you can do so by providing a custom `pdfjs` function that returns a promise that resolves to the `PDFJS` object.\n\nIn the following example we use the \"legacy\" (see [pdfjs docs](https://github.com/mozilla/pdf.js/wiki/Frequently-Asked-Questions#which-browsersenvironments-are-supported)) build of `pdfjs-dist`, which includes several polyfills not included in the default build.\n\n```bash npm2yarn\nnpm install pdfjs-dist","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/pdf.md","loc":{"lines":{"from":33,"to":40}}}}],["1433",{"pageContent":"```typescript\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  // you may need to add `.then(m => m.default)` to the end of the import\n  pdfjs: () => import(\"pdfjs-dist/legacy/build/pdf.js\"),\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/pdf.md","loc":{"lines":{"from":43,"to":50}}}}],["1434",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Subtitles\n\nThis example goes over how to load data from subtitle files. One document will be created for each subtitles file.\n\n## Setup\n\n```bash npm2yarn\nnpm install srt-parser-2\n```\n\n## Usage\n\n```typescript\nimport { SRTLoader } from \"langchain/document_loaders/fs/srt\";\n\nconst loader = new SRTLoader(\n  \"src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt\"\n);\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/subtitles.md","loc":{"lines":{"from":1,"to":25}}}}],["1435",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Text files\n\nThis example goes over how to load data from text files.\n\n```typescript\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/text.md","loc":{"lines":{"from":1,"to":15}}}}],["1436",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Unstructured\n\nThis example covers how to use [Unstructured](../../../../../ecosystem/unstructured) to load files of many types. Unstructured currently supports loading of text files, powerpoints, html, pdfs, images, and more.\n\n## Setup\n\nYou can run Unstructured locally in your computer using Docker. To do so, you need to have Docker installed. You can find the instructions to install Docker [here](https://docs.docker.com/get-docker/).\n\n```bash\ndocker run -p 8000:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0\n```\n\n## Usage\n\nOnce Unstructured is running, you can use it to load files from your computer. You can use the following code to load a file from your computer.\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/document_loaders/unstructured.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/unstructured.mdx","loc":{"lines":{"from":1,"to":24}}}}],["1437",{"pageContent":"Directories\n\nYou can also load all of the files in the directory using `UnstructuredDirectoryLoader`, which inherits from [`DirectoryLoader`](./directory.md):\n\nimport DirectoryExample from \"@examples/document_loaders/unstructured_directory.ts\";\n\n<CodeBlock language=\"typescript\">{DirectoryExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/file_loaders/unstructured.mdx","loc":{"lines":{"from":26,"to":32}}}}],["1438",{"pageContent":"---\nsidebar_label: Examples\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Examples: Document Loaders\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/index.mdx","loc":{"lines":{"from":1,"to":10}}}}],["1439",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# College Confidential\n\nThis example goes over how to load data from the college confidential website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { CollegeConfidentialLoader } from \"langchain/document_loaders/web/college_confidential\";\n\nconst loader = new CollegeConfidentialLoader(\n  \"https://www.collegeconfidential.com/colleges/brown-university/\"\n);\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/college_confidential.md","loc":{"lines":{"from":1,"to":25}}}}],["1440",{"pageContent":"---\nsidebar_class_name: node-only\n---\n\n# Confluence\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nThis covers how to load document objects from pages in a Confluence space.\n\n## Credentials\n\n- You'll need to set up an access token and provide it along with your confluence username in order to authenticate the request\n- You'll also need the `space key` for the space containing the pages to load as documents. This can be found in the url when navigating to your space e.g. `https://example.atlassian.net/wiki/spaces/{SPACE_KEY}`\n- And you'll need to install `html-to-text` to parse the pages into plain text\n\n```bash npm2yarn\nnpm install html-to-text\n```\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/document_loaders/confluence.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/confluence.mdx","loc":{"lines":{"from":1,"to":28}}}}],["1441",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# GitBook\n\nThis example goes over how to load data from any GitBook, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Load from single GitBook page\n\n```typescript\nimport { GitbookLoader } from \"langchain/document_loaders/web/gitbook\";\n\nconst loader = new GitbookLoader(\n  \"https://docs.gitbook.com/product-tour/navigation\"\n);\n\nconst docs = await loader.load();\n```\n\n## Load from all paths in a given GitBook\n\nFor this to work, the GitbookLoader needs to be initialized with the root path (https://docs.gitbook.com in this example) and have `shouldLoadAllPaths` set to `true`.\n\n```typescript\nimport { GitbookLoader } from \"langchain/document_loaders/web/gitbook\";\n\nconst loader = new GitbookLoader(\"https://docs.gitbook.com\", {\n  shouldLoadAllPaths: true,\n});\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/gitbook.md","loc":{"lines":{"from":1,"to":39}}}}],["1442",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# GitHub\n\nThis example goes over how to load data from a GitHub repository.\nYou can set the `GITHUB_ACCESS_TOKEN` environment variable to a GitHub access token to increase the rate limit and access private repositories.\n\n```typescript\nimport { GithubRepoLoader } from \"langchain/document_loaders/web/github\";\n\nconst loader = new GithubRepoLoader(\n  \"https://github.com/hwchase17/langchainjs\",\n  { branch: \"main\", recursive: false, unknown: \"warn\" }\n);\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/github.md","loc":{"lines":{"from":1,"to":18}}}}],["1443",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Hacker News\n\nThis example goes over how to load data from the hacker news website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { HNLoader } from \"langchain/document_loaders/web/hn\";\n\nconst loader = new HNLoader(\"https://news.ycombinator.com/item?id=34817881\");\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/hn.md","loc":{"lines":{"from":1,"to":23}}}}],["1444",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# IMSDB\n\nThis example goes over how to load data from the internet movie script database website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { IMSDBLoader } from \"langchain/document_loaders/web/imsdb\";\n\nconst loader = new IMSDBLoader(\"https://imsdb.com/scripts/BlacKkKlansman.html\");\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/imsdb.md","loc":{"lines":{"from":1,"to":23}}}}],["1445",{"pageContent":"---\nlabel: \"Web Loaders\"\nhide_table_of_contents: true\n---\n\n# Web Loaders\n\nThese loaders are used to load web resources.\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/index.mdx","loc":{"lines":{"from":1,"to":12}}}}],["1446",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_class_name: node-only\n---\n\n# S3 File\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nThis covers how to load document objects from an s3 file object.\n\n## Setup\n\nTo run this index you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.\n\nSee the docs [here](https://js.langchain.com/docs/modules/indexes/document_loaders/examples/file_loaders/unstructured) for information on how to do that.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/s3.mdx","loc":{"lines":{"from":1,"to":18}}}}],["1447",{"pageContent":"Usage\n\nOnce Unstructured is configured, you can use the S3 loader to load files and then convert them into a Document.\n\nYou can optionally provide a s3Config parameter to specify your bucket region, access key, and secret access key. If these are not provided, you will need to have them in your environment (e.g., by running `aws configure`).\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/document_loaders/s3.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/s3.mdx","loc":{"lines":{"from":20,"to":29}}}}],["1448",{"pageContent":"---\nsidebar_position: 1\nsidebar_label: Cheerio\nhide_table_of_contents: true\n---\n\n# Webpages, with Cheerio\n\nThis example goes over how to load data from webpages using Cheerio. One document will be created for each webpage.\n\nCheerio is a fast and lightweight library that allows you to parse and traverse HTML documents using a jQuery-like syntax. You can use Cheerio to extract data from web pages, without having to render them in a browser.\n\nHowever, Cheerio does not simulate a web browser, so it cannot execute JavaScript code on the page. This means that it cannot extract data from dynamic web pages that require JavaScript to render. To do that, you can use the [PlaywrightWebBaseLoader](./web_playwright.md) or [PuppeteerWebBaseLoader](./web_puppeteer.md) instead.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_cheerio.md","loc":{"lines":{"from":1,"to":19}}}}],["1449",{"pageContent":"Usage\n\n```typescript\nimport { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://news.ycombinator.com/item?id=34817881\"\n);\n\nconst docs = await loader.load();\n```\n\n## Usage, with a custom selector\n\n```typescript\nimport { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://news.ycombinator.com/item?id=34817881\",\n  {\n    selector: \"p.athing\",\n  }\n);\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_cheerio.md","loc":{"lines":{"from":21,"to":46}}}}],["1450",{"pageContent":"---\nsidebar_position: 3\nhide_table_of_contents: true\nsidebar_class_name: node-only\nsidebar_label: Playwright\n---\n\n# Webpages, with Playwright\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nThis example goes over how to load data from webpages using Playwright. One document will be created for each webpage.\n\nPlaywright is a Node.js library that provides a high-level API for controlling multiple browser engines, including Chromium, Firefox, and WebKit. You can use Playwright to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.\n\nIf you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the [CheerioWebBaseLoader](./web_cheerio.md) instead.\n\n## Setup\n\n```bash npm2yarn\nnpm install playwright\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright.md","loc":{"lines":{"from":1,"to":24}}}}],["1451",{"pageContent":"Usage\n\n```typescript\nimport { PlaywrightWebBaseLoader } from \"langchain/document_loaders/web/playwright\";\n\n/**\n * Loader uses `page.content()`\n * as default evaluate function\n **/\nconst loader = new PlaywrightWebBaseLoader(\"https://www.tabnews.com.br/\");\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright.md","loc":{"lines":{"from":26,"to":38}}}}],["1452",{"pageContent":"Options\n\nHere's an explanation of the parameters you can pass to the PlaywrightWebBaseLoader constructor using the PlaywrightWebBaseLoaderOptions interface:\n\n```typescript\ntype PlaywrightWebBaseLoaderOptions = {\n  launchOptions?: LaunchOptions;\n  gotoOptions?: PlaywrightGotoOptions;\n  evaluate?: PlaywrightEvaluate;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright.md","loc":{"lines":{"from":40,"to":49}}}}],["1453",{"pageContent":"1. `launchOptions`: an optional object that specifies additional options to pass to the playwright.chromium.launch() method. This can include options such as the headless flag to launch the browser in headless mode.\n\n2. `gotoOptions`: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.\n\n3. `evaluate`: an optional function that can be used to evaluate JavaScript code on the page using a custom evaluation function. This can be useful for extracting data from the page or interacting with page elements. The function should return a Promise that resolves to a string containing the result of the evaluation.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright.md","loc":{"lines":{"from":52,"to":56}}}}],["1454",{"pageContent":"By passing these options to the `PlaywrightWebBaseLoader` constructor, you can customize the behavior of the loader and use Playwright's powerful features to scrape and interact with web pages.\n\nHere is a basic example to do it:\n\n```typescript\nimport { PlaywrightWebBaseLoader } from \"langchain/document_loaders/web/playwright\";\n\nconst loader = new PlaywrightWebBaseLoader(\"https://www.tabnews.com.br/\", {\n  launchOptions: {\n    headless: true,\n  },\n  gotoOptions: {\n    waitUntil: \"domcontentloaded\",\n  },\n  /** Pass custom evaluate, in this case you get page and browser instances */\n  async evaluate(page: Page, browser: Browser) {\n    await page.waitForResponse(\"https://www.tabnews.com.br/va/view\");\n\n    const result = await page.evaluate(() => document.body.innerHTML);\n    return result;\n  },\n});\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright.md","loc":{"lines":{"from":58,"to":82}}}}],["1455",{"pageContent":"---\nsidebar_position: 2\nsidebar_label: Puppeteer\nhide_table_of_contents: true\nsidebar_class_name: node-only\n---\n\n# Webpages, with Puppeteer\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nThis example goes over how to load data from webpages using Puppeteer. One document will be created for each webpage.\n\nPuppeteer is a Node.js library that provides a high-level API for controlling headless Chrome or Chromium. You can use Puppeteer to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.\n\nIf you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the [CheerioWebBaseLoader](./web_cheerio.md) instead.\n\n## Setup\n\n```bash npm2yarn\nnpm install puppeteer\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer.md","loc":{"lines":{"from":1,"to":24}}}}],["1456",{"pageContent":"Usage\n\n```typescript\nimport { PuppeteerWebBaseLoader } from \"langchain/document_loaders/web/puppeteer\";\n\n/**\n * Loader uses `page.evaluate(() => document.body.innerHTML)`\n * as default evaluate function\n **/\nconst loader = new PuppeteerWebBaseLoader(\"https://www.tabnews.com.br/\");\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer.md","loc":{"lines":{"from":26,"to":38}}}}],["1457",{"pageContent":"Options\n\nHere's an explanation of the parameters you can pass to the PuppeteerWebBaseLoader constructor using the PuppeteerWebBaseLoaderOptions interface:\n\n```typescript\ntype PuppeteerWebBaseLoaderOptions = {\n  launchOptions?: PuppeteerLaunchOptions;\n  gotoOptions?: PuppeteerGotoOptions;\n  evaluate?: (page: Page, browser: Browser) => Promise<string>;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer.md","loc":{"lines":{"from":40,"to":49}}}}],["1458",{"pageContent":"1. `launchOptions`: an optional object that specifies additional options to pass to the puppeteer.launch() method. This can include options such as the headless flag to launch the browser in headless mode, or the slowMo option to slow down Puppeteer's actions to make them easier to follow.\n\n2. `gotoOptions`: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.\n\n3. `evaluate`: an optional function that can be used to evaluate JavaScript code on the page using the page.evaluate() method. This can be useful for extracting data from the page or interacting with page elements. The function should return a Promise that resolves to a string containing the result of the evaluation.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer.md","loc":{"lines":{"from":52,"to":56}}}}],["1459",{"pageContent":"By passing these options to the `PuppeteerWebBaseLoader` constructor, you can customize the behavior of the loader and use Puppeteer's powerful features to scrape and interact with web pages.\n\nHere is a basic example to do it:\n\n```typescript\nimport { PuppeteerWebBaseLoader } from \"langchain/document_loaders/web/puppeteer\";\n\nconst loader = new PuppeteerWebBaseLoader(\"https://www.tabnews.com.br/\", {\n  launchOptions: {\n    headless: true,\n  },\n  gotoOptions: {\n    waitUntil: \"domcontentloaded\",\n  },\n  /** Pass custom evaluate, in this case you get page and browser instances */\n  async evaluate(page: Page, browser: Browser) {\n    await page.waitForResponse(\"https://www.tabnews.com.br/va/view\");\n\n    const result = await page.evaluate(() => document.body.innerHTML);\n    return result;\n  },\n});\n\nconst docs = await loader.load();\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer.md","loc":{"lines":{"from":58,"to":82}}}}],["1460",{"pageContent":"---\nsidebar_label: Document Loaders\nsidebar_position: 1\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Document Loaders\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/document-loaders)\n:::\n\nDocument loaders make it easy to create [Documents](../../schema/document.md) from a variety of sources. These documents can then be loaded onto [Vector Stores](../vector_stores/) to load documents from a source.\n\n```typescript\ninterface DocumentLoader {\n  load(): Promise<Document[]>;\n\n  loadAndSplit(textSplitter?: TextSplitter): Promise<Document[]>;\n}\n```\n\nDocument Loaders expose two methods, `load` and `loadAndSplit`. `load` will load the documents from the source and return them as an array of [Documents](../../schema/document.md). `loadAndSplit` will load the documents from the source, split them using the provided [TextSplitter](../text_splitters/index.mdx), and return them as an array of [Documents](../../schema/document.md).","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/index.mdx","loc":{"lines":{"from":1,"to":24}}}}],["1461",{"pageContent":"All Document Loaders\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/index.mdx","loc":{"lines":{"from":26,"to":28}}}}],["1462",{"pageContent":"Advanced\n\nIf you want to implement your own Document Loader, you have a few options.\n\n### Subclassing `BaseDocumentLoader`\n\nYou can extend the `BaseDocumentLoader` class directly. The `BaseDocumentLoader` class provides a few convenience methods for loading documents from a variety of sources.\n\n```typescript\nabstract class BaseDocumentLoader implements DocumentLoader {\n  abstract load(): Promise<Document[]>;\n}\n```\n\n### Subclassing `TextLoader`\n\nIf you want to load documents from a text file, you can extend the `TextLoader` class. The `TextLoader` class takes care of reading the file, so all you have to do is implement a parse method.\n\n```typescript\nabstract class TextLoader extends BaseDocumentLoader {\n  abstract parse(raw: string): Promise<string[]>;\n}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/index.mdx","loc":{"lines":{"from":30,"to":52}}}}],["1463",{"pageContent":"Subclassing `BufferLoader`\n\nIf you want to load documents from a binary file, you can extend the `BufferLoader` class. The `BufferLoader` class takes care of reading the file, so all you have to do is implement a parse method.\n\n```typescript\nabstract class BufferLoader extends BaseDocumentLoader {\n  abstract parse(\n    raw: Buffer,\n    metadata: Document[\"metadata\"]\n  ): Promise<Document[]>;\n}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/document_loaders/index.mdx","loc":{"lines":{"from":54,"to":65}}}}],["1464",{"pageContent":"---\nsidebar_position: 4\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Indexes\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing)\n:::\n\nThis section deals with everything related to bringing your own data into LangChain, indexing it, and making it available for LLMs/Chat Models.\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/index.mdx","loc":{"lines":{"from":1,"to":16}}}}],["1465",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# ChatGPT Plugin Retriever\n\nThis example shows how to use the ChatGPT Retriever Plugin within LangChain.\n\nTo set up the ChatGPT Retriever Plugin, please follow instructions [here](https://github.com/openai/chatgpt-retrieval-plugin).\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/chatgpt-plugin.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/chatgpt-retriever-plugin.mdx","loc":{"lines":{"from":1,"to":16}}}}],["1466",{"pageContent":"# Contextual Compression Retriever\n\nA Contextual Compression Retriever is designed to improve the answers returned from vector store document similarity searches by better taking into account the context from the query.\n\nIt wraps another retriever, and uses a Document Compressor as an intermediate step after the initial similarity search that removes information irrelevant to the initial query from the retrieved documents.\nThis reduces the amount of distraction a subsequent chain has to deal with when parsing the retrieved documents and making its final judgements.\n\n## Usage\n\nThis example shows how to intialize a `ContextualCompressionRetriever` with a vector store and a document compressor:\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/contextual_compression.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/contextual-compression-retriever.mdx","loc":{"lines":{"from":1,"to":15}}}}],["1467",{"pageContent":"# Databerry Retriever\n\nThis example shows how to use the Databerry Retriever in a `RetrievalQAChain` to retrieve documents from a Databerry.ai datastore.\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/databerry.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/databerry-retriever.mdx","loc":{"lines":{"from":1,"to":10}}}}],["1468",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# HyDE Retriever\n\nThis example shows how to use the HyDE Retriever, which implements Hypothetical Document Embeddings (HyDE) as described in [this paper](https://arxiv.org/abs/2212.10496).\n\nAt a high level, HyDE is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example.\n\nIn order to use HyDE, we therefore need to provide a base embedding model, as well as an LLM that can be used to generate those documents. By default, the HyDE class comes with some default prompts to use (see the paper for more details on them), but we can also create our own, which should have a single input variable `{question}`.\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/hyde.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/hyde.mdx","loc":{"lines":{"from":1,"to":18}}}}],["1469",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 4\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Retrievers\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/retriever)\n:::\n\nA way of storing data such that it can be queried by a language model. The only interface this object must expose is a `getRelevantDocuments` method which takes in a string query and returns a list of Documents.\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/index.mdx","loc":{"lines":{"from":1,"to":16}}}}],["1470",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Metal Retriever\n\nThis example shows how to use the Metal Retriever in a `RetrievalQAChain` to retrieve documents from a Metal index.\n\n## Setup\n\n```bash npm2yarn\nnpm i @getmetal/metal-sdk\n```\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/metal.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/metal-retriever.mdx","loc":{"lines":{"from":1,"to":20}}}}],["1471",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Remote Retriever\n\nThis example shows how to use a Remote Retriever in a `RetrievalQAChain` to retrieve documents from a remote server.\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chains/retrieval_qa_with_remote.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/remote-retriever.mdx","loc":{"lines":{"from":1,"to":14}}}}],["1472",{"pageContent":"# Supabase Hybrid Search\n\nLangchain supports hybrid search with a Supabase Postgres database. The hybrid search combines the postgres `pgvector` extension (similarity search) and Full-Text Search (keyword search) to retrieve documents. You can add documents via SupabaseVectorStore `addDocuments` function. SupabaseHybridKeyWordSearch accepts embedding, supabase client, number of results for similarity search, and number of results for keyword search as parameters. The `getRelevantDocuments` function produces a list of documents that has duplicates removed and is sorted by relevance score.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/supabase-hybrid.mdx","loc":{"lines":{"from":1,"to":3}}}}],["1473",{"pageContent":"Setup\n\n### Install the library with\n\n```bash npm2yarn\nnpm install -S @supabase/supabase-js\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/supabase-hybrid.mdx","loc":{"lines":{"from":5,"to":11}}}}],["1474",{"pageContent":"Create a table and search functions in your database\n\nRun this in your database:\n\n```sql\n-- Enable the pgvector extension to work with embedding vectors\ncreate extension vector;\n\n-- Create a table to store your documents\ncreate table documents (\n  id bigserial primary key,\n  content text, -- corresponds to Document.pageContent\n  metadata jsonb, -- corresponds to Document.metadata\n  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/supabase-hybrid.mdx","loc":{"lines":{"from":13,"to":27}}}}],["1475",{"pageContent":"-- Create a function to similarity search for documents\ncreate function match_documents (\n  query_embedding vector(1536),\n  match_count int,\n  filter jsonb DEFAULT '{}'\n) returns table (\n  id bigint,\n  content text,\n  metadata jsonb,\n  similarity float\n)\nlanguage plpgsql\nas $$\n#variable_conflict use_column\nbegin\n  return query\n  select\n    id,\n    content,\n    metadata,\n    1 - (documents.embedding <=> query_embedding) as similarity\n  from documents\n  where metadata @> filter\n  order by documents.embedding <=> query_embedding\n  limit match_count;\nend;\n$$;\n\n-- Create a function to keyword search for documents\ncreate function kw_match_documents(query_text text, match_count int)\nreturns table (id bigint, content text, metadata jsonb, similarity real)\nas $$","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/supabase-hybrid.mdx","loc":{"lines":{"from":29,"to":60}}}}],["1476",{"pageContent":"begin\nreturn query execute\nformat('select id, content, metadata, ts_rank(to_tsvector(content), plainto_tsquery($1)) as similarity\nfrom documents\nwhere to_tsvector(content) @@ plainto_tsquery($1)\norder by similarity desc\nlimit $2')\nusing query_text, match_count;\nend;\n$$ language plpgsql;\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/supabase-hybrid.mdx","loc":{"lines":{"from":62,"to":72}}}}],["1477",{"pageContent":"Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/supabase_hybrid.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/supabase-hybrid.mdx","loc":{"lines":{"from":74,"to":79}}}}],["1478",{"pageContent":"# Time-Weighted Retriever\n\nA Time-Weighted Retriever is a retriever that takes into account recency in addition to similarity. The scoring algorithm is:\n\n```typescript\nlet score = (1.0 - this.decayRate) ** hoursPassed + vectorRelevance;\n```\n\nNotably, `hoursPassed` above refers to the time since the object in the retriever was last accessed, not since it was created. This means that frequently accessed objects remain \"fresh\" and score higher.\n\n`this.decayRate` is a configurable decimal number between 0 and 1. A lower number means that documents will be \"remembered\" for longer, while a higher number strongly weights more recently accessed documents.\n\nNote that setting a decay rate of exactly 0 or 1 makes `hoursPassed` irrelevant and makes this retriever equivalent to a standard vector lookup.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/time-weighted-retriever.mdx","loc":{"lines":{"from":1,"to":13}}}}],["1479",{"pageContent":"Usage\n\nThis example shows how to intialize a `TimeWeightedVectorStoreRetriever` with a vector store.\nIt is important to note that due to required metadata, all documents must be added to the backing vector store using the `addDocuments` method on the **retriever**, not the vector store itself.\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/time-weighted-retriever.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/time-weighted-retriever.mdx","loc":{"lines":{"from":15,"to":23}}}}],["1480",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Vector Store\n\nOnce you've created a [Vector Store](../vector_stores/), the way to use it as a Retriever is very simple:\n\n```typescript\nvectorStore = ...\nretriever = vectorStore.asRetriever()\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/retrievers/vectorstore.md","loc":{"lines":{"from":1,"to":12}}}}],["1481",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# CharacterTextSplitter\n\nBesides the `RecursiveCharacterTextSplitter`, there is also the more standard `CharacterTextSplitter`. This splits only on one type of character (defaults to `\"\\n\\n\"`). You can use it in the exact same way.\n\n```typescript\nimport { Document } from \"langchain/document\";\nimport { CharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = \"foo bar baz 123\";\nconst splitter = new CharacterTextSplitter({\n  separator: \" \",\n  chunkSize: 7,\n  chunkOverlap: 3,\n});\nconst output = await splitter.createDocuments([text]);\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/text_splitters/examples/character.mdx","loc":{"lines":{"from":1,"to":20}}}}],["1482",{"pageContent":"---\nsidebar_label: Examples\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Text Splitters: Examples\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/text_splitters/examples/index.mdx","loc":{"lines":{"from":1,"to":9}}}}],["1483",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# `MarkdownTextSplitter`\n\nIf your content is in Markdown format then `MarkdownTextSplitter`. This class will split your content into documents based on the Markdown headers. For example, if you have the following Markdown content:\n\n```markdown\n# Header 1\n\nThis is some content.\n\n## Header 2\n\nThis is some more content.\n\n# Header 3\n\nThis is even more content.\n```\n\nThen the `MarkdownTextSplitter` will split the content into three documents:\n\n```typescript\nimport { MarkdownTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `# Header 1\n\nThis is some content.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/text_splitters/examples/markdown.mdx","loc":{"lines":{"from":1,"to":30}}}}],["1484",{"pageContent":"Header 2\n\nThis is some more content.\n\n# Header 3\n\nThis is even more content.`;\n\nconst splitter = new MarkdownTextSplitter();\n\nconst output = await splitter.createDocuments([text], {\n  metadata: \"something\",\n});\n/*\n[\n  {\n    \"pageContent\": \"# Header 1\\n\\nThis is some content.\",\n    \"metadata\": \"something\"\n  },\n  {\n    \"pageContent\": \"## Header 2\\n\\nThis is some more content.\",\n    \"metadata\": \"something\"\n  },\n  {\n    \"pageContent\": \"# Header 3\\n\\nThis is even more content.\",\n    \"metadata\": \"something\"\n  }\n]\n*/\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/text_splitters/examples/markdown.mdx","loc":{"lines":{"from":32,"to":61}}}}],["1485",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# `RecursiveCharacterTextSplitter`\n\nThe recommended TextSplitter is the `RecursiveCharacterTextSplitter`. This will split documents recursively by different characters - starting with `\"\\n\\n\"`, then `\"\\n\"`, then `\" \"`. This is nice because it will try to keep all the semantically relevant content in the same place for as long as possible.\n\nImportant parameters to know here are `chunkSize` and `chunkOverlap`. `chunkSize` controls the max size (in terms of number of characters) of the final documents. `chunkOverlap` specifies how much overlap there should be between chunks. This is often helpful to make sure that the text isn't split weirdly. In the example below we set these values to be small (for illustration purposes), but in practice they default to `4000` and `200` respectively.\n\n```typescript\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/text_splitters/examples/recursive_character.mdx","loc":{"lines":{"from":1,"to":12}}}}],["1486",{"pageContent":"const text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\nThis is a weird text to write, but gotta test the splittingggg some how.\\n\\n\nBye!\\n\\n-H.`;\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10,\n  chunkOverlap: 1,\n});\n\nconst output = await splitter.createDocuments([text]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/text_splitters/examples/recursive_character.mdx","loc":{"lines":{"from":14,"to":22}}}}],["1487",{"pageContent":"You'll note that in the above example we are splitting a raw text string and getting back a list of documents. We can also split documents directly.\n\n```typescript\nimport { Document } from \"langchain/document\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\nThis is a weird text to write, but gotta test the splittingggg some how.\\n\\n\nBye!\\n\\n-H.`;\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10,\n  chunkOverlap: 1,\n});\n\nconst docOutput = await splitter.splitDocuments([\n  new Document({ pageContent: text }),\n]);\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/text_splitters/examples/recursive_character.mdx","loc":{"lines":{"from":25,"to":42}}}}],["1488",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# TokenTextSplitter\n\nFinally, `TokenTextSplitter` splits a raw text string by first converting the text into BPE tokens, then split these tokens into chunks and convert the tokens within a single chunk back into text.\n\nTo utilize the `TokenTextSplitter`, first install the accompanying required library\n\n```bash npm2yarn\nnpm install -S @dqbd/tiktoken\n```\n\nThen, you can use it like so:\n\n```typescript\nimport { Document } from \"langchain/document\";\nimport { TokenTextSplitter } from \"langchain/text_splitter\";\n\nconst text = \"foo bar baz 123\";\n\nconst splitter = new TokenTextSplitter({\n  encodingName: \"gpt2\",\n  chunkSize: 10,\n  chunkOverlap: 0,\n});\n\nconst output = await splitter.createDocuments([text]);\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/text_splitters/examples/token.mdx","loc":{"lines":{"from":1,"to":30}}}}],["1489",{"pageContent":"---\nsidebar_label: Text Splitters\nhide_table_of_contents: true\nsidebar_position: 2\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Text Splitters\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/text-splitters)\n:::\n\nLanguage Models are often limited by the amount of text that you can pass to them. Therefore, it is neccessary to split them up into smaller chunks. LangChain provides several utilities for doing so.\n\nUsing a Text Splitter can also help improve the results from vector store searches, as eg. smaller chunks may sometimes be more likely to match a query. Testing different chunk sizes (and chunk overlap) is a worthwhile exercise to tailor the results to your use case.\n\n```typescript\ninterface TextSplitter {\n  chunkSize: number;\n\n  chunkOverlap: number;\n\n  createDocuments(\n    texts: string[],\n    metadatas?: Record<string, any>[]\n  ): Promise<Document[]>;\n\n  splitDocuments(documents: Document[]): Promise<Document[]>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/text_splitters/index.mdx","loc":{"lines":{"from":1,"to":31}}}}],["1490",{"pageContent":"Text Splitters expose two methods, `createDocuments` and `splitDocuments`. The former takes a list of raw text strings and returns a list of documents. The latter takes a list of documents and returns a list of documents. The difference is that `createDocuments` will split the raw text strings into chunks, while `splitDocuments` will split the documents into chunks.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/text_splitters/index.mdx","loc":{"lines":{"from":34,"to":34}}}}],["1491",{"pageContent":"All Text Splitters\n\n<DocCardList />\n\n## Advanced\n\nIf you want to implement your own custom Text Splitter, you only need to subclass TextSplitter and implement a single method `splitText`. The method takes a string and returns a list of strings. The returned strings will be used as the chunks.\n\n```typescript\nabstract class TextSplitter {\n  abstract splitText(text: string): Promise<string[]>;\n}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/text_splitters/index.mdx","loc":{"lines":{"from":36,"to":48}}}}],["1492",{"pageContent":"---\nsidebar_label: \"Vector Stores\"\nsidebar_position: 3\n---\n\n# Getting Started: Vector Stores\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/vectorstore)\n:::\n\nA vector store is a particular type of database optimized for storing documents and their [embeddings](../../models/embeddings/), and then fetching of the most relevant documents for a particular query, ie. those whose embeddings are most similar to the embedding of the query.\n\n```typescript\ninterface VectorStore {\n  /**\n   * Add more documents to an existing VectorStore\n   */\n  addDocuments(documents: Document[]): Promise<void>;\n\n  /**\n   * Search for the most similar documents to a query\n   */\n  similaritySearch(\n    query: string,\n    k?: number,\n    filter?: object | undefined\n  ): Promise<Document[]>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/index.mdx","loc":{"lines":{"from":1,"to":28}}}}],["1493",{"pageContent":"/**\n   * Search for the most similar documents to a query,\n   * and return their similarity score\n   */\n  similaritySearchWithScore(\n    query: string,\n    k = 4,\n    filter: object | undefined = undefined\n  ): Promise<[object, number][]>;\n\n  /**\n   * Turn a VectorStore into a Retriever\n   */\n  asRetriever(k?: number): BaseRetriever;\n\n  /**\n   * Advanced: Add more documents to an existing VectorStore,\n   * when you already have their embeddings\n   */\n  addVectors(vectors: number[][], documents: Document[]): Promise<void>;\n\n  /**\n   * Advanced: Search for the most similar documents to a query,\n   * when you already have the embedding of the query\n   */\n  similaritySearchVectorWithScore(\n    query: number[],\n    k: number,\n    filter?: object\n  ): Promise<[Document, number][]>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/index.mdx","loc":{"lines":{"from":30,"to":60}}}}],["1494",{"pageContent":"You can create a vector store from a list of [Documents](../../schema/document), or from a list of texts and their corresponding metadata. You can also create a vector store from an existing index, the signature of this method depends on the vector store you're using, check the documentation of the vector store you're interested in.\n\n```typescript\nabstract class BaseVectorStore implements VectorStore {\n  static fromTexts(\n    texts: string[],\n    metadatas: object[] | object,\n    embeddings: Embeddings,\n    dbConfig: Record<string, any>\n  ): Promise<VectorStore>;\n\n  static fromDocuments(\n    docs: Document[],\n    embeddings: Embeddings,\n    dbConfig: Record<string, any>\n  ): Promise<VectorStore>;\n}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/index.mdx","loc":{"lines":{"from":63,"to":80}}}}],["1495",{"pageContent":"Which one to pick?\n\nHere's a quick guide to help you pick the right vector store for your use case:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/index.mdx","loc":{"lines":{"from":82,"to":84}}}}],["1496",{"pageContent":"- If you're after something that can just run inside your Node.js application, in-memory, without any other servers to stand up, then go for [HNSWLib](./integrations/hnswlib)\n- If you're looking for something that can run in-memory in browser-like environments, then go for [MemoryVectorStore](./integrations/memory)\n- If you come from Python and you were looking for something similar to FAISS, pick [HNSWLib](./integrations/hnswlib)\n- If you're looking for an open-source full-featured vector database that you can run locally in a docker container, then go for [Chroma](./integrations/chroma)\n- If you're using Supabase already then look at the [Supabase](./integrations/supabase) vector store to use the same Postgres database for your embeddings too\n- If you're looking for a production-ready vector store you don't have to worry about hosting yourself, then go for [Pinecone](./integrations/pinecone)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/index.mdx","loc":{"lines":{"from":86,"to":91}}}}],["1497",{"pageContent":"All Vector Stores\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/index.mdx","loc":{"lines":{"from":93,"to":97}}}}],["1498",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\n\n# Chroma\n\nChroma is an open-source Apache 2.0 embedding database.\n\n## Setup\n\n1. Run chroma with Docker on your computer [docs](https://docs.trychroma.com/api-reference)\n2. Install the Chroma JS SDK.\n\n```bash npm2yarn\nnpm install -S chromadb\n```\n\n## Usage, Index and query Documents\n\nimport FromDocs from \"@examples/indexes/vector_stores/chroma/fromDocs.ts\";\n\n<CodeBlock language=\"typescript\">{FromDocs}</CodeBlock>\n\n## Usage, Index and query texts\n\nimport FromTexts from \"@examples/indexes/vector_stores/chroma/fromTexts.ts\";\n\n<CodeBlock language=\"typescript\">{FromTexts}</CodeBlock>\n\n## Usage, Query docs from existing collection\n\nimport Search from \"@examples/indexes/vector_stores/chroma/search.ts\";\n\n<CodeBlock language=\"typescript\">{Search}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/chroma.mdx","loc":{"lines":{"from":1,"to":32}}}}],["1499",{"pageContent":"---\nsidebar_class_name: node-only\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# HNSWLib\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nHNSWLib is an in-memory vectorstore that can be saved to a file. It uses [HNSWLib](https://github.com/nmslib/hnswlib).\n\n## Setup\n\n:::caution\n\n**On Windows**, you might need to install [Visual Studio](https://visualstudio.microsoft.com/downloads/) first in order to properly build the `hnswlib-node` package.\n\n:::\n\nYou can install it with\n\n```bash npm2yarn\nnpm install hnswlib-node\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/hnswlib.mdx","loc":{"lines":{"from":1,"to":27}}}}],["1500",{"pageContent":"Usage\n\n### Create a new index from texts\n\nimport ExampleTexts from \"@examples/indexes/vector_stores/hnswlib.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleTexts}</CodeBlock>\n\n### Create a new index from a loader\n\nimport ExampleLoader from \"@examples/indexes/vector_stores/hnswlib_fromdocs.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleLoader}</CodeBlock>\n\n### Save an index to a file and load it again\n\nimport ExampleSave from \"@examples/indexes/vector_stores/hnswlib_saveload.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleSave}</CodeBlock>\n\n### Filter documents\n\nimport ExampleFilter from \"@examples/indexes/vector_stores/hnswlib_filter.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleFilter}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/hnswlib.mdx","loc":{"lines":{"from":29,"to":53}}}}],["1501",{"pageContent":"---\nsidebar_label: Integrations\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Vector Stores: Integrations\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/index.mdx","loc":{"lines":{"from":1,"to":9}}}}],["1502",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Memory\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# `MemoryVectorStore`\n\nMemoryVectorStore is an in-memory, ephemeral vectorstore that stores embeddings in-memory and does an exact, linear search for the most similar embeddings. The default similarity metric is cosine similarity, but can be changed to any of the similarity metrics supported by [ml-distance](https://mljs.github.io/distance/modules/similarity.html).","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/memory.mdx","loc":{"lines":{"from":1,"to":11}}}}],["1503",{"pageContent":"Usage\n\n### Create a new index from texts\n\nimport ExampleTexts from \"@examples/indexes/vector_stores/memory.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleTexts}</CodeBlock>\n\n### Create a new index from a loader\n\nimport ExampleLoader from \"@examples/indexes/vector_stores/memory_fromdocs.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleLoader}</CodeBlock>\n\n### Use a custom similarity metric\n\nimport ExampleCustom from \"@examples/indexes/vector_stores/memory_custom_similarity.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleCustom}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/memory.mdx","loc":{"lines":{"from":13,"to":31}}}}],["1504",{"pageContent":"---\nsidebar_class_name: node-only\n---\n\n# Milvus\n\n[Milvus](https://milvus.io/) is a vector database built for embeddings similarity search and AI applications.\n\n:::tip Compatibility\nOnly available on Node.js.\n:::","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":1,"to":11}}}}],["1505",{"pageContent":"Setup\n\n1. Run Milvus instance with Docker on your computer [docs](https://milvus.io/docs/v2.1.x/install_standalone-docker.md)\n2. Install the Milvus Node.js SDK.\n\n   ```bash npm2yarn\n   npm install -S @zilliz/milvus2-sdk-node\n   ```\n\n3. Setup Env variables for Milvus before running the code\n\n   3.1 OpenAI\n\n   ```bash\n   export OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE\n   export MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":13,"to":28}}}}],["1506",{"pageContent":"3.2 Azure OpenAI\n\n   ```bash\n   export AZURE_OPENAI_API_KEY=YOUR_AZURE_OPENAI_API_KEY_HERE\n   export AZURE_OPENAI_API_INSTANCE_NAME=YOUR_AZURE_OPENAI_INSTANCE_NAME_HERE\n   export AZURE_OPENAI_API_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_DEPLOYMENT_NAME_HERE\n   export AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_COMPLETIONS_DEPLOYMENT_NAME_HERE\n   export AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME_HERE\n   export AZURE_OPENAI_API_VERSION=YOUR_AZURE_OPENAI_API_VERSION_HERE\n   export MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530\n   ```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":31,"to":41}}}}],["1507",{"pageContent":"Index and query docs\n\n```typescript\nimport { Milvus } from \"langchain/vectorstores/milvus\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":43,"to":47}}}}],["1508",{"pageContent":"// text sample from Godel, Escher, Bach\nconst vectorStore = await Milvus.fromTexts(\n  [\n    \"Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\\\n            Harmonic Labyrinth of the dreaded Majotaur?\",\n    \"Achilles: Yiikes! What is that?\",\n    \"Tortoise: They say-although I person never believed it myself-that an I\\\n            Majotaur has created a tiny labyrinth sits in a pit in the middle of\\\n            it, waiting innocent victims to get lost in its fears complexity.\\\n            Then, when they wander and dazed into the center, he laughs and\\\n            laughs at them-so hard, that he laughs them to death!\",\n    \"Achilles: Oh, no!\",\n    \"Tortoise: But it's only a myth. Courage, Achilles.\",\n  ],\n  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel_escher_bach\",\n  }\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":49,"to":68}}}}],["1509",{"pageContent":"// or alternatively from docs\nconst vectorStore = await Milvus.fromDocuments(docs, new OpenAIEmbeddings(), {\n  collectionName: \"goldel_escher_bach\",\n});\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":70,"to":76}}}}],["1510",{"pageContent":"Query docs from existing collection\n\n```typescript\nimport { Milvus } from \"langchain/vectorstores/milvus\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await Milvus.fromExistingCollection(\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel_escher_bach\",\n  }\n);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":78,"to":92}}}}],["1511",{"pageContent":"---\nsidebar_class_name: node-only\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# MyScale\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\n[MyScale](https://myscale.com/) is an emerging AI database that harmonizes the power of vector search and SQL analytics, providing a managed, efficient, and responsive experience.\n\n## Setup\n\n1. Launch a cluster through [MyScale's Web Console](https://console.myscale.com/). See [MyScale's official documentation](https://docs.myscale.com/en/quickstart/) for more information.\n2. After launching a cluster, view your `Connection Details` from your cluster's `Actions` menu. You will need the host, port, username, and password.\n3. Install the required Node.js peer dependency in your workspace.\n\n```bash npm2yarn\nnpm install -S @clickhouse/client\n```\n\n## Index and Query Docs\n\nimport InsertExample from \"@examples/indexes/vector_stores/myscale_fromTexts.ts\";\n\n<CodeBlock language=\"typescript\">{InsertExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/myscale.mdx","loc":{"lines":{"from":1,"to":29}}}}],["1512",{"pageContent":"Query Docs From an Existing Collection\n\nimport SearchExample from \"@examples/indexes/vector_stores/myscale_search.ts\";\n\n<CodeBlock language=\"typescript\">{SearchExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/myscale.mdx","loc":{"lines":{"from":31,"to":35}}}}],["1513",{"pageContent":"---\nsidebar_class_name: node-only\n---\n\n# OpenSearch\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\n[OpenSearch](https://opensearch.org/) is a fork of [Elasticsearch](https://www.elastic.co/elasticsearch/) that is fully compatible with the Elasticsearch API. Read more about their support for Approximate Nearest Neighbors [here](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/).\n\nLangchain.js accepts [@opensearch-project/opensearch](https://opensearch.org/docs/latest/clients/javascript/index/) as the client for OpenSearch vectorstore.\n\n## Setup\n\n```bash npm2yarn\nnpm install -S @opensearch-project/opensearch\n```\n\nYou'll also need to have an OpenSearch instance running. You can use the [official Docker image](https://opensearch.org/docs/latest/opensearch/install/docker/) to get started. You can also find an example docker-compose file [here](https://github.com/hwchase17/langchainjs/blob/main/examples/src/indexes/vector_stores/opensearch/docker-compose.yml).","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/opensearch.md","loc":{"lines":{"from":1,"to":21}}}}],["1514",{"pageContent":"Index docs\n\n```typescript\nimport { Client } from \"@opensearch-project/opensearch\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { OpenSearchVectorStore } from \"langchain/vectorstores/opensearch\";\n\nconst client = new Client({\n  nodes: [process.env.OPENSEARCH_URL ?? \"http://127.0.0.1:9200\"],\n});\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"opensearch is also a vector db\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent:\n      \"OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications\",\n  }),\n];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/opensearch.md","loc":{"lines":{"from":23,"to":53}}}}],["1515",{"pageContent":"await OpenSearchVectorStore.fromDocuments(docs, new OpenAIEmbeddings(), {\n  client,\n  indexName: process.env.OPENSEARCH_INDEX, // Will default to `documents`\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/opensearch.md","loc":{"lines":{"from":55,"to":59}}}}],["1516",{"pageContent":"Query docs\n\n```typescript\nimport { Client } from \"@opensearch-project/opensearch\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { OpenSearchVectorStore } from \"langchain/vectorstores/opensearch\";\n\nconst client = new Client({\n  nodes: [process.env.OPENSEARCH_URL ?? \"http://127.0.0.1:9200\"],\n});\n\nconst vectorStore = new OpenSearchVectorStore(new OpenAIEmbeddings(), {\n  client,\n});\n\n/* Search the vector DB independently with meta filters */\nconst results = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(JSON.stringify(results, null, 2));\n/* [\n    {\n      \"pageContent\": \"Hello world\",\n      \"metadata\": {\n        \"id\": 2\n      }\n    }\n  ] */","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/opensearch.md","loc":{"lines":{"from":61,"to":88}}}}],["1517",{"pageContent":"/* Use as part of a chain (currently no metadata filters) */\nconst model = new OpenAI();\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore, {\n  k: 1,\n  returnSourceDocuments: true,\n});\nconst response = await chain.call({ query: \"What is opensearch?\" });\n\nconsole.log(JSON.stringify(response, null, 2));\n/* \n  {\n    \"text\": \" Opensearch is a collection of technologies that allow search engines to publish search results in a standard format, making it easier for users to search across multiple sites.\",\n    \"sourceDocuments\": [\n      {\n        \"pageContent\": \"What's this?\",\n        \"metadata\": {\n          \"id\": 3\n        }\n      }\n    ]\n  } \n  */\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/opensearch.md","loc":{"lines":{"from":90,"to":112}}}}],["1518",{"pageContent":"---\nsidebar_class_name: node-only\n---\n\n# Pinecone\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nLangchain.js accepts [@pinecone-database/pinecone](https://docs.pinecone.io/docs/node-client) as the client for Pinecone vectorstore. Install the library with\n\n```bash npm2yarn\nnpm install -S dotenv langchain @pinecone-database/pinecone\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/pinecone.md","loc":{"lines":{"from":1,"to":15}}}}],["1519",{"pageContent":"Index docs\n\n```typescript\nimport { PineconeClient } from \"@pinecone-database/pinecone\";\nimport * as dotenv from \"dotenv\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PineconeStore } from \"langchain/vectorstores/pinecone\";\n\ndotenv.config();\n\nconst client = new PineconeClient();\nawait client.init({\n  apiKey: process.env.PINECONE_API_KEY,\n  environment: process.env.PINECONE_ENVIRONMENT,\n});\nconst pineconeIndex = client.Index(process.env.PINECONE_INDEX);\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"pinecone is a vector db\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"pinecones are the woody fruiting body and of a pine tree\",\n  }),\n];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/pinecone.md","loc":{"lines":{"from":17,"to":52}}}}],["1520",{"pageContent":"await PineconeStore.fromDocuments(docs, new OpenAIEmbeddings(), {\n  pineconeIndex,\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/pinecone.md","loc":{"lines":{"from":54,"to":57}}}}],["1521",{"pageContent":"Query docs\n\n```typescript\nimport { PineconeClient } from \"@pinecone-database/pinecone\";\nimport * as dotenv from \"dotenv\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PineconeStore } from \"langchain/vectorstores/pinecone\";\n\ndotenv.config();\n\nconst client = new PineconeClient();\nawait client.init({\n  apiKey: process.env.PINECONE_API_KEY,\n  environment: process.env.PINECONE_ENVIRONMENT,\n});\nconst pineconeIndex = client.Index(process.env.PINECONE_INDEX);\n\nconst vectorStore = await PineconeStore.fromExistingIndex(\n  new OpenAIEmbeddings(),\n  { pineconeIndex }\n);\n\n/* Search the vector DB independently with meta filters */\nconst results = await vectorStore.similaritySearch(\"pinecone\", 1, {\n  foo: \"bar\",\n});\nconsole.log(results);\n/*\n[\n  Document {\n    pageContent: 'pinecone is a vector db',\n    metadata: { foo: 'bar' }\n  }\n]\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/pinecone.md","loc":{"lines":{"from":59,"to":95}}}}],["1522",{"pageContent":"/* Use as part of a chain (currently no metadata filters) */\nconst model = new OpenAI();\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore, {\n  k: 1,\n  returnSourceDocuments: true,\n});\nconst response = await chain.call({ query: \"What is pinecone?\" });\nconsole.log(response);\n/*\n{\n  text: ' A pinecone is the woody fruiting body of a pine tree.',\n  sourceDocuments: [\n    Document {\n      pageContent: 'pinecones are the woody fruiting body and of a pine tree',\n      metadata: [Object]\n    }\n  ]\n}\n*/\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/pinecone.md","loc":{"lines":{"from":97,"to":116}}}}],["1523",{"pageContent":"# Prisma\n\nFor augmenting existing models in PostgreSQL database with vector search, Langchain supports using [Prisma](https://www.prisma.io/) together with PostgreSQL and [`pgvector`](https://github.com/pgvector/pgvector) Postgres extension.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/prisma.mdx","loc":{"lines":{"from":1,"to":3}}}}],["1524",{"pageContent":"Setup\n\n### Setup database instance with Supabase\n\nRefer to the [Prisma and Supabase integration guide](https://supabase.com/docs/guides/integrations/prisma) to setup a new database instance with Supabase and Prisma.\n\n### Install Prisma\n\n```bash npm2yarn\nnpm install prisma\n```\n\n### Setup `pgvector` self hosted instance with `docker-compose`\n\n`pgvector` provides a prebuilt Docker image that can be used to quickly setup a self-hosted Postgres instance.\n\n```yaml\nservices:\n  db:\n    image: ankane/pgvector\n    ports:\n      - 5432:5432\n    volumes:\n      - db:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_PASSWORD=\n      - POSTGRES_USER=\n      - POSTGRES_DB=\n\nvolumes:\n  db:\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/prisma.mdx","loc":{"lines":{"from":5,"to":36}}}}],["1525",{"pageContent":"Create a new schema\n\nAssuming you haven't created a schema yet, create a new model with a `vector` field of type `Unsupported(\"vector\")`:\n\n```prisma\nmodel Document {\n  id      String                 @id @default(cuid())\n  content String\n  vector  Unsupported(\"vector\")?\n}\n```\n\nAfterwards, create a new migration with `--create-only` to avoid running the migration directly.\n\n```bash npm2yarn\nnpx prisma migrate dev --create-only\n```\n\nAdd the following line to the newly created migration to enable `pgvector` extension if it hasn't been enabled yet:\n\n```sql\nCREATE EXTENSION IF NOT EXISTS vector;\n```\n\nRun the migration afterwards:\n\n```bash npm2yarn\nnpx prisma migrate dev\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/prisma.mdx","loc":{"lines":{"from":38,"to":66}}}}],["1526",{"pageContent":"Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/indexes/vector_stores/prisma_vectorstore/prisma.ts\";\nimport Schema from \"@examples/indexes/vector_stores/prisma_vectorstore/prisma/schema.prisma\";\n\nUse the `withModel` method to get proper type hints for `metadata` field:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\nThe sample above uses the following schema:\n\n<CodeBlock language=\"prisma\">{Schema}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/prisma.mdx","loc":{"lines":{"from":68,"to":80}}}}],["1527",{"pageContent":"# Supabase\n\nLangchain supports using Supabase Postgres database as a vector store, using the `pgvector` postgres extension. Refer to the [Supabase blog post](https://supabase.com/blog/openai-embeddings-postgres-vector) for more information.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/supabase.mdx","loc":{"lines":{"from":1,"to":3}}}}],["1528",{"pageContent":"Setup\n\n### Install the library with\n\n```bash npm2yarn\nnpm install -S @supabase/supabase-js\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/supabase.mdx","loc":{"lines":{"from":5,"to":11}}}}],["1529",{"pageContent":"Create a table and search function in your database\n\nRun this in your database:\n\n```sql\n-- Enable the pgvector extension to work with embedding vectors\ncreate extension vector;\n\n-- Create a table to store your documents\ncreate table documents (\n  id bigserial primary key,\n  content text, -- corresponds to Document.pageContent\n  metadata jsonb, -- corresponds to Document.metadata\n  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/supabase.mdx","loc":{"lines":{"from":13,"to":27}}}}],["1530",{"pageContent":"-- Create a function to search for documents\ncreate function match_documents (\n  query_embedding vector(1536),\n  match_count int,\n  filter jsonb DEFAULT '{}'\n) returns table (\n  id bigint,\n  content text,\n  metadata jsonb,\n  similarity float\n)\nlanguage plpgsql\nas $$\n#variable_conflict use_column\nbegin\n  return query\n  select\n    id,\n    content,\n    metadata,\n    1 - (documents.embedding <=> query_embedding) as similarity\n  from documents\n  where metadata @> filter\n  order by documents.embedding <=> query_embedding\n  limit match_count;\nend;\n$$;\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/supabase.mdx","loc":{"lines":{"from":29,"to":56}}}}],["1531",{"pageContent":"Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/indexes/vector_stores/supabase.ts\";\nimport MetadataFilterExample from \"@examples/indexes/vector_stores/supabase_with_metadata_filter.ts\";\n\n### Standard Usage\n\nThe below example shows how to perform a basic similarity search with Supabase:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n### Metadata Filtering\n\nGiven the above `match_documents` Postgres function, you can also pass a filter parameter to only documents with a specific metadata field value.\n\n**Note:** If you've previously been using `SupabaseVectorStore`, you may need to drop and recreate the `match_documents` function per the updated SQL above to use this functionality.\n\n<CodeBlock language=\"typescript\">{MetadataFilterExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/supabase.mdx","loc":{"lines":{"from":58,"to":76}}}}],["1532",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Weaviate\n\nWeaviate is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering. LangChain connects to Weaviate via the `weaviate-ts-client` package, the official Typescript client for Weaviate.\n\nLangChain inserts vectors directly to Weaviate, and queries Weaviate for the nearest neighbors of a given vector, so that you can use all the LangChain Embeddings integrations with Weaviate.\n\n## Setup\n\n```bash npm2yarn\nnpm install weaviate-ts-client graphql\n```\n\nYou'll need to run Weaviate either locally or on a server, see [the Weaviate documentation](https://weaviate.io/developers/weaviate/installation) for more information.\n\n## Usage, insert documents\n\nimport InsertExample from \"@examples/indexes/vector_stores/weaviate_fromTexts.ts\";\n\n<CodeBlock language=\"typescript\">{InsertExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/weaviate.mdx","loc":{"lines":{"from":1,"to":25}}}}],["1533",{"pageContent":"Usage, query documents\n\nimport QueryExample from \"@examples/indexes/vector_stores/weaviate_search.ts\";\n\n<CodeBlock language=\"typescript\">{QueryExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/indexes/vector_stores/integrations/weaviate.mdx","loc":{"lines":{"from":27,"to":31}}}}],["1534",{"pageContent":"# Buffer Memory\n\nBufferMemory is the simplest type of memory - it just remembers previous conversational back and forths directly.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst memory = new BufferMemory();\nconst chain = new ConversationChain({ llm: model, memory: memory });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n```\n\n```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/examples/buffer_memory.md","loc":{"lines":{"from":1,"to":27}}}}],["1535",{"pageContent":"You can also load messages into a `BufferMemory` instance by creating and passing in a `ChatHistory` object.\nThis lets you easily pick up state from past conversations:\n\n```typescript\nimport { ChatMessageHistory } from \"langchain/memory\";\nimport { HumanChatMessage, AIChatMessage } from \"langchain/schema\";\n\nconst pastMessages = [\n  new HumanChatMessage(\"My name's Jonas\"),\n  new AIChatMessage(\"Nice to meet you, Jonas!\"),\n];\n\nconst memory = new BufferMemory({\n  chatHistory: new ChatMessageHistory(pastMessages),\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/examples/buffer_memory.md","loc":{"lines":{"from":30,"to":45}}}}],["1536",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chat/memory.ts\";\n\n# Using Buffer Memory with Chat Models\n\nThis example covers how to use chat-specific memory classes with chat models.\nThe key thing to notice is that setting `returnMessages: true` makes the memory return a list of chat messages instead of a string.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/examples/buffer_memory_chat.mdx","loc":{"lines":{"from":1,"to":13}}}}],["1537",{"pageContent":"# Buffer Window Memory\n\nBufferWindowMemory keeps track of the back-and-forths in conversation, and then uses a window of size `k` to surface the last `k` back-and-forths to use as memory.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { BufferWindowMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst memory = new BufferWindowMemory({ k: 1 });\nconst chain = new ConversationChain({ llm: model, memory: memory });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n```\n\n```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/examples/buffer_window_memory.md","loc":{"lines":{"from":1,"to":28}}}}],["1538",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Conversation Summary\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Conversation Summary Memory\n\nThe Conversation Summary Memory summarizes the conversation as it happens and stores the current summary in memory. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens.\n\n## Usage, with an LLM\n\nimport TextExample from \"@examples/memory/summary_llm.ts\";\n\n<CodeBlock language=\"typescript\">{TextExample}</CodeBlock>\n\n## Usage, with a Chat Model\n\nimport ChatExample from \"@examples/memory/summary_chat.ts\";\n\n<CodeBlock language=\"typescript\">{ChatExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/examples/conversation_summary.mdx","loc":{"lines":{"from":1,"to":22}}}}],["1539",{"pageContent":"---\nsidebar_label: Examples\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Examples: Memory\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/examples/index.mdx","loc":{"lines":{"from":1,"to":9}}}}],["1540",{"pageContent":"# Motrhead Memory\n\n[Motrhead](https://github.com/getmetal/motorhead) is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.\n\n## Setup\n\nSee instructions at [Motrhead](https://github.com/getmetal/motorhead) for running the server locally.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/examples/motorhead_memory.md","loc":{"lines":{"from":1,"to":7}}}}],["1541",{"pageContent":"Usage\n\n```typescript\nimport { ConversationChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models\";\nimport { MotorheadMemory } from \"langchain/memory\";\n\nconst model = new ChatOpenAI({});\nconst memory = new MotorheadMemory({\n  sessionId: \"user-id\",\n  motorheadUrl: \"localhost:8080\",\n});\n\nawait memory.init(); // loads previous state from Motrhead \nconst context = memory.context\n  ? `\nHere's previous context: ${memory.context}`\n  : \"\";\n\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.${context}`\n  ),\n  new MessagesPlaceholder(\"history\"),\n  HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n]);\n\nconst chain = new ConversationChain({\n  memory,\n  prompt: chatPrompt,\n  llm: chat,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/examples/motorhead_memory.md","loc":{"lines":{"from":9,"to":40}}}}],["1542",{"pageContent":"const res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/examples/motorhead_memory.md","loc":{"lines":{"from":42,"to":43}}}}],["1543",{"pageContent":"```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/examples/motorhead_memory.md","loc":{"lines":{"from":46,"to":57}}}}],["1544",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/memory/vector_store.ts\";\n\n# VectorStore-backed Memory\n\n`VectorStoreRetrieverMemory` stores memories in a VectorDB and queries the top-K most \"salient\" docs every time it is called.\n\nThis differs from most of the other Memory classes in that it doesn't explicitly track the order of interactions.\n\nIn this case, the \"docs\" are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/examples/vector_store_memory.mdx","loc":{"lines":{"from":1,"to":16}}}}],["1545",{"pageContent":"---\nsidebar_label: Memory\nsidebar_position: 5\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Memory\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/memory)\n:::\n\nMemory is the concept of storing and retrieving data in the process of a conversation. There are two main methods, `loadMemoryVariables` and `saveContext`. The first method is used to retrieve data from memory (optionally using the current input values), and the second method is used to store data in memory.\n\n```typescript\nexport type InputValues = Record<string, any>;\n\nexport type OutputValues = Record<string, any>;\n\ninterface BaseMemory {\n  loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n\n  saveContext(\n    inputValues: InputValues,\n    outputValues: OutputValues\n  ): Promise<void>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/index.mdx","loc":{"lines":{"from":1,"to":28}}}}],["1546",{"pageContent":":::note\nDo not share the same memory instance between two different chains, a memory instance represents the history of a single conversation\n:::\n\n:::note\nIf you deploy your LangChain app on a serverless environment do not store memory instances in a variable, as your hosting provider may have reset it by the next time the function is called.\n:::","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/index.mdx","loc":{"lines":{"from":31,"to":37}}}}],["1547",{"pageContent":"All Memory classes\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/index.mdx","loc":{"lines":{"from":39,"to":41}}}}],["1548",{"pageContent":"Advanced\n\nTo implement your own memory class you have two options:\n\n### Subclassing `BaseChatMemory`\n\nThis is the easiest way to implement your own memory class. You can subclass `BaseChatMemory`, which takes care of `saveContext` by saving inputs and outputs as [Chat Messages](../schema/chat-messages.md), and implement only the `loadMemoryVariables` method. This method is responsible for returning the memory variables that are relevant for the current input values.\n\n```typescript\nabstract class BaseChatMemory extends BaseMemory {\n  chatHistory: ChatMessageHistory;\n\n  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/index.mdx","loc":{"lines":{"from":43,"to":57}}}}],["1549",{"pageContent":"Subclassing `BaseMemory`\n\nIf you want to implement a more custom memory class, you can subclass `BaseMemory` and implement both `loadMemoryVariables` and `saveContext` methods. The `saveContext` method is responsible for storing the input and output values in memory. The `loadMemoryVariables` method is responsible for returning the memory variables that are relevant for the current input values.\n\n```typescript\nabstract class BaseMemory {\n  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n\n  abstract saveContext(\n    inputValues: InputValues,\n    outputValues: OutputValues\n  ): Promise<void>;\n}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/memory/index.mdx","loc":{"lines":{"from":59,"to":72}}}}],["1550",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/chat/chat.ts\";\nimport StreamingExample from \"@examples/models/chat/chat_streaming.ts\";\nimport TimeoutExample from \"@examples/models/chat/chat_timeout.ts\";\n\n# Additional Functionality: Chat Models\n\nWe offer a number of additional features for chat models. In the examples below, we'll be using the `ChatOpenAI` model.\n\n## Additional Methods\n\nLangChain provides a number of additional methods for interacting with chat models:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Streaming\n\nSimilar to LLMs, you can stream responses from a chat model. This is useful for chatbots that need to respond to user input in real-time.\n\n<CodeBlock language=\"typescript\">{StreamingExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/chat/additional_functionality.mdx","loc":{"lines":{"from":1,"to":24}}}}],["1551",{"pageContent":"Adding a timeout\n\nBy default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a `timeout` option, in milliseconds, when you instantiate the model. For example, for OpenAI:\n\n<CodeBlock language=\"typescript\">{TimeoutExample}</CodeBlock>\n\nCurrently, the timeout option is only supported for OpenAI models.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/chat/additional_functionality.mdx","loc":{"lines":{"from":26,"to":32}}}}],["1552",{"pageContent":"Dealing with Rate Limits\n\nSome providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a `maxConcurrency` option when instantiating a Chat Model. This option allows you to specify the maximum number of concurrent requests you want to make to the provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.\n\nFor example, if you set `maxConcurrency: 5`, then LangChain will only send 5 requests to the provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.\n\nTo use this feature, simply pass `maxConcurrency: <number>` when you instantiate the LLM. For example:\n\n```typescript\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({ maxConcurrency: 5 });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/chat/additional_functionality.mdx","loc":{"lines":{"from":34,"to":46}}}}],["1553",{"pageContent":"Dealing with API Errors\n\nIf the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a `maxRetries` option when you instantiate the model. For example:\n\n```typescript\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({ maxRetries: 10 });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/chat/additional_functionality.mdx","loc":{"lines":{"from":48,"to":56}}}}],["1554",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Chat Models\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/chat/chat_quick_start.ts\";\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Chat Models\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/chat-model)\n:::\n\nLangChain provides a standard interface for using chat models. Chat models are a variation on language models.\nWhile chat models use language models under the hood, the interface they expose is a bit different.\nRather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/chat/index.mdx","loc":{"lines":{"from":1,"to":18}}}}],["1555",{"pageContent":"Chat Messages\n\nA `ChatMessage` is what we refer to as the modular unit of information for a chat model.\nAt the moment, this consists of a `\"text\"` field, which refers to the content of the chat message.\n\nThere are currently four different classes of `ChatMessage` supported by LangChain:\n\n- `HumanChatMessage`: A chat message that is sent as if from a Human's point of view.\n- `AIChatMessage`: A chat message that is sent from the point of view of the AI system to which the Human is corresponding.\n- `SystemChatMessage`: A chat message that gives the AI system some information about the conversation. This is usually sent at the beginning of a conversation.\n- `ChatMessage`: A generic chat message, with not only a `\"text\"` field but also an arbitrary `\"role\"` field.\n\n> **_Note:_** Currently, the only chat-based model we support is `ChatOpenAI` (with gpt-4 and gpt-3.5-turbo), but anticipate adding more in the future.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/chat/index.mdx","loc":{"lines":{"from":20,"to":32}}}}],["1556",{"pageContent":"To get started, simply use the `call` method of an `LLM` implementation, passing in a `string` input. In this example, we are using the `ChatOpenAI` implementation:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/chat/index.mdx","loc":{"lines":{"from":34,"to":36}}}}],["1557",{"pageContent":"Dig deeper\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/chat/index.mdx","loc":{"lines":{"from":38,"to":40}}}}],["1558",{"pageContent":"---\nsidebar_position: 3\nsidebar_label: Integrations\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Integrations: Chat Models\n\nLangChain offers a number of Chat Models implementations that integrate with various model providers. These are:\n\n## `ChatOpenAI`\n\nimport OpenAI from \"@examples/models/chat/integration_openai.ts\";\n\n<CodeBlock language=\"typescript\">{OpenAI}</CodeBlock>\n\n## Azure `ChatOpenAI`\n\nimport AzureOpenAI from \"@examples/models/chat/integration_azure_openai.ts\";\n\n<CodeBlock language=\"typescript\">{AzureOpenAI}</CodeBlock>\n\n## `ChatAnthropic`\n\nimport Anthropic from \"@examples/models/chat/integration_anthropic.ts\";\n\n<CodeBlock language=\"typescript\">{Anthropic}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/chat/integrations.mdx","loc":{"lines":{"from":1,"to":28}}}}],["1559",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport TimeoutExample from \"@examples/models/embeddings/openai_timeout.ts\";\n\n# Additional Functionality: Embeddings\n\nWe offer a number of additional features for chat models. In the examples below, we'll be using the `ChatOpenAI` model.\n\n## Adding a timeout\n\nBy default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a `timeout` option, in milliseconds, when you instantiate the model. For example, for OpenAI:\n\n<CodeBlock language=\"typescript\">{TimeoutExample}</CodeBlock>\n\nCurrently, the timeout option is only supported for OpenAI models.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/additional_functionality.mdx","loc":{"lines":{"from":1,"to":18}}}}],["1560",{"pageContent":"Dealing with Rate Limits\n\nSome providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a `maxConcurrency` option when instantiating an Embeddings model. This option allows you to specify the maximum number of concurrent requests you want to make to the provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.\n\nFor example, if you set `maxConcurrency: 5`, then LangChain will only send 5 requests to the provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.\n\nTo use this feature, simply pass `maxConcurrency: <number>` when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst model = new OpenAIEmbeddings({ maxConcurrency: 5 });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/additional_functionality.mdx","loc":{"lines":{"from":20,"to":32}}}}],["1561",{"pageContent":"Dealing with API Errors\n\nIf the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a `maxRetries` option when you instantiate the model. For example:\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst model = new OpenAIEmbeddings({ maxRetries: 10 });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/additional_functionality.mdx","loc":{"lines":{"from":34,"to":42}}}}],["1562",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Embeddings\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Embeddings\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/text-embedding-model)\n:::\n\nEmbeddings can be used to create a numerical representation of textual data. This numerical representation is useful because it can be used to find similar documents.\n\nBelow is an example of how to use the OpenAI embeddings. Embeddings occasionally have different embedding methods for queries versus documents, so the embedding class exposes a `embedQuery` and `embedDocuments` method.\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n/* Create instance */\nconst embeddings = new OpenAIEmbeddings();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/index.mdx","loc":{"lines":{"from":1,"to":22}}}}],["1563",{"pageContent":"/* Embed queries */\nconst res = await embeddings.embedQuery(\"Hello world\");\n/*\n[\n   -0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,\n    0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,\n    0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,\n   0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,\n   -0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,\n  -0.0019203906,   0.012161949,  -0.019194454,   0.030373365, -0.031028723,\n   0.0036170771,  -0.007813894, -0.0060778237,  -0.017820721, 0.0048647798,\n   -0.015640393,   0.001373733,  -0.015552171,   0.019534737, -0.016169721,\n    0.007316074,   0.008273906,   0.011418369,   -0.01390117, -0.033347685,\n    0.011248227,  0.0042503807,  -0.012792102, -0.0014595914,  0.028356876,\n    0.025407761, 0.00076445413,  -0.016308354,   0.017455231, -0.016396577,\n    0.008557475,   -0.03312083,   0.031104341,   0.032389853,  -0.02132437,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/index.mdx","loc":{"lines":{"from":24,"to":39}}}}],["1564",{"pageContent":"0.003324056,  0.0055610985, -0.0078012915,   0.006090427, 0.0062038545,\n      0.0169133,  0.0036391325,  0.0076815626,  -0.018841568,  0.026037913,\n    0.024550753,  0.0055264398, -0.0015824712, -0.0047765584,  0.018425668,\n   0.0030656934, -0.0113742575, -0.0020322427,   0.005069579, 0.0022701253,\n    0.036095154,  -0.027449455,  -0.008475555,   0.015388331,  0.018917186,\n   0.0018999106,  -0.003349262,   0.020895867,  -0.014480911, -0.025042271,\n    0.012546342,   0.013850759,  0.0069253794,   0.008588983, -0.015199285,\n  -0.0029585673,  -0.008759124,   0.016749462,   0.004111747,  -0.04804285,\n  ... 1436 more items\n]\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/index.mdx","loc":{"lines":{"from":40,"to":50}}}}],["1565",{"pageContent":"/* Embed documents */\nconst documentRes = await embeddings.embedDocuments([\"Hello world\", \"Bye bye\"]);\n/*\n[\n  [\n    -0.0047852774,  0.0048640342,   -0.01645707,  -0.024395779, -0.017263541,\n      0.012512918,  -0.019191515,   0.009053908,  -0.010213212, -0.026890801,\n      0.022883644,   0.010251015,  -0.023589306,  -0.006584088,  0.007989113,\n      0.002720268,   0.025088841,  -0.012153786,   0.012928754,  0.013054766,\n      -0.010395928, -0.0035566676,  0.0040008575,   0.008600268, -0.020678446,\n    -0.0019106456,   0.012178987,  -0.019241918,   0.030444318,  -0.03102397,\n      0.0035692686,  -0.007749692,   -0.00604854,   -0.01781799,  0.004860884,\n      -0.015612794,  0.0014097509,  -0.015637996,   0.019443536,  -0.01612944,\n      0.0072960514,   0.008316742,   0.011548932,  -0.013987249,  -0.03336778,\n      0.011341013,    0.00425603, -0.0126578305, -0.0013861238,  0.028302127,\n      0.025466874,  0.0007029065,  -0.016318457,   0.017427357, -0.016394064,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/index.mdx","loc":{"lines":{"from":52,"to":67}}}}],["1566",{"pageContent":"0.008499459,  -0.033241767,   0.031200387,    0.03238489,   -0.0212833,\n      0.0032416396,   0.005443686,  -0.007749692,  0.0060201874,  0.006281661,\n      0.016923312,   0.003528315,  0.0076740854,   -0.01881348,  0.026109532,\n      0.024660403,   0.005472039, -0.0016712243, -0.0048136297,  0.018397642,\n      0.003011669,  -0.011385117, -0.0020193304,   0.005138109, 0.0022335495,\n        0.03603922,  -0.027495656,  -0.008575066,   0.015436378,  0.018851284,\n      0.0018019609, -0.0034338066,    0.02094307,  -0.014503895, -0.024950229,\n      0.012632628,   0.013735226,  0.0069936244,   0.008575066, -0.015196957,\n    -0.0030541976,  -0.008745181,   0.016746895,  0.0040481114, -0.048010286,\n    ... 1436 more items\n  ],\n  [\n      -0.009446913,  -0.013253193,   0.013174579,  0.0057552797,  -0.038993083,\n      0.0077763423,    -0.0260478, -0.0114384955, -0.0022683728,  -0.016509168,\n      0.041797023,    0.01787183,    0.00552271, -0.0049789557,   0.018146982,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/index.mdx","loc":{"lines":{"from":68,"to":82}}}}],["1567",{"pageContent":"-0.01542166,   0.033752076,   0.006112323,   0.023872782,  -0.016535373,\n      -0.006623321,   0.016116094, -0.0061090477, -0.0044155475,  -0.016627092,\n      -0.022077737, -0.0009286407,   -0.02156674,   0.011890532,  -0.026283644,\n        0.02630985,   0.011942943,  -0.026126415,  -0.018264906,  -0.014045896,\n      -0.024187243,  -0.019037955,  -0.005037917,   0.020780588, -0.0049527506,\n      0.002399398,   0.020767486,  0.0080908025,  -0.019666875,  -0.027934562,\n      0.017688395,   0.015225122,  0.0046186363, -0.0045007137,   0.024265857,\n        0.03244183,  0.0038848957,   -0.03244183,  -0.018893827, -0.0018065092,\n      0.023440398,  -0.021763276,   0.015120302,   -0.01568371,  -0.010861984,\n      0.011739853,  -0.024501702,  -0.005214801,   0.022955606,   0.001315165,\n      -0.00492327,  0.0020358032,  -0.003468891,  -0.031079166,  0.0055259857,\n      0.0028547104,   0.012087069,   0.007992534, -0.0076256637,   0.008110457,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/index.mdx","loc":{"lines":{"from":83,"to":94}}}}],["1568",{"pageContent":"0.002998838,  -0.024265857,   0.006977089,  -0.015185814, -0.0069115767,\n      0.006466091,  -0.029428247,  -0.036241557,   0.036713246,   0.032284595,\n    -0.0021144184,  -0.014255536,   0.011228855,  -0.027227025,  -0.021619149,\n    0.00038242966,    0.02245771, -0.0014748519,    0.01573612,  0.0041010873,\n      0.006256451,  -0.007992534,   0.038547598,   0.024658933,  -0.012958387,\n    ... 1436 more items\n  ]\n]\n*/\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/index.mdx","loc":{"lines":{"from":95,"to":104}}}}],["1569",{"pageContent":"Dig deeper\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/index.mdx","loc":{"lines":{"from":106,"to":108}}}}],["1570",{"pageContent":"---\nsidebar_position: 3\nsidebar_label: Integrations\n---\n\n# Integrations: Embeddings\n\nLangChain offers a number of Embeddings implementations that integrate with various model providers. These are:\n\n## `OpenAIEmbeddings`\n\nThe `OpenAIEmbeddings` class uses the OpenAI API to generate embeddings for a given text. By default it strips new line characters from the text, as recommended by OpenAI, but you can disable this by passing `stripNewLines: false` to the constructor.\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  openAIApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/integrations.mdx","loc":{"lines":{"from":1,"to":20}}}}],["1571",{"pageContent":"Azure `OpenAIEmbeddings`\n\nThe `OpenAIEmbeddings` class uses the OpenAI API on Azure to generate embeddings for a given text. By default it strips new line characters from the text, as recommended by OpenAI, but you can disable this by passing `stripNewLines: false` to the constructor.\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  azureOpenAIApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiInstanceName: \"YOUR-INSTANCE-NAME\", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME\n  azureOpenAIApiDeploymentName: \"YOUR-DEPLOYMENT-NAME\", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n  azureOpenAIApiVersion: \"YOUR-API-VERSION\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/integrations.mdx","loc":{"lines":{"from":22,"to":35}}}}],["1572",{"pageContent":"`CohereEmbeddings`\n\nThe `CohereEmbeddings` class uses the Cohere API to generate embeddings for a given text.\n\n```bash npm2yarn\nnpm install cohere-ai\n```\n\n```typescript\nimport { CohereEmbeddings } from \"langchain/embeddings/cohere\";\n\nconst embeddings = new CohereEmbeddings({\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.COHERE_API_KEY\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/integrations.mdx","loc":{"lines":{"from":37,"to":51}}}}],["1573",{"pageContent":"`TensorFlowEmbeddings`\n\nThis Embeddings integration runs the embeddings entirely in your browser or Node.js environment, using [TensorFlow.js](https://www.tensorflow.org/js). This means that your data isn't sent to any third party, and you don't need to sign up for any API keys. However, it does require more memory and processing power than the other integrations.\n\n```bash npm2yarn\nnpm install @tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu\n```\n\n```typescript\nimport \"@tensorflow/tfjs-backend-cpu\";\nimport { TensorFlowEmbeddings } from \"langchain/embeddings/tensorflow\";\n\nconst embeddings = new TensorFlowEmbeddings();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/integrations.mdx","loc":{"lines":{"from":53,"to":65}}}}],["1574",{"pageContent":"This example uses the CPU backend, which works in any JS environment. However, you can use any of the backends supported by TensorFlow.js, including GPU and WebAssembly, which will be a lot faster. For Node.js you can use the `@tensorflow/tfjs-node` package, and for the browser you can use the `@tensorflow/tfjs-backend-webgl` package. See the [TensorFlow.js documentation](https://www.tensorflow.org/js/guide/platform_environment) for more information.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/integrations.mdx","loc":{"lines":{"from":68,"to":68}}}}],["1575",{"pageContent":"`HuggingFaceInferenceEmbeddings`\n\nThis Embeddings integration uses the HuggingFace Inference API to generate embeddings for a given text using by default the `sentence-transformers/distilbert-base-nli-mean-tokens` model. You can pass a different model name to the constructor to use a different model.\n\n```bash npm2yarn\nnpm install @huggingface/inference@1\n```\n\n```typescript\nimport { HuggingFaceInferenceEmbeddings } from \"langchain/embeddings/hf\";\n\nconst embeddings = new HuggingFaceInferenceEmbeddings({\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY\n});\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/embeddings/integrations.mdx","loc":{"lines":{"from":70,"to":84}}}}],["1576",{"pageContent":"---\nsidebar_position: 2\nhide_table_of_contents: true\nsidebar_label: Models\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Models\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/)\n:::\n\nModels are a core component of LangChain. LangChain is not a provider of models, but rather provides a standard interface through which you can interact with a variety of language models.\nLangChain provides support for both text-based Large Language Models (LLMs), Chat Models, and Text Embedding models.\n\nLLMs use a text-based input and output, while Chat Models use a message-based input and output.\n\n> **_Note:_** Chat model APIs are fairly new, so we are still figuring out the correct abstractions. If you have any feedback, please let us know!\n\n## All Models\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/index.mdx","loc":{"lines":{"from":1,"to":24}}}}],["1577",{"pageContent":"Advanced\n\n_This section is for users who want a deeper technical understanding of how LangChain works. If you are just getting started, you can skip this section._\n\nBoth LLMs and Chat Models are built on top of the `BaseLanguageModel` class. This class provides a common interface for all models, and allows us to easily swap out models in chains without changing the rest of the code.\n\nThe `BaseLanguageModel` class has two abstract methods: `generatePrompt` and `getNumTokens`, which are implemented by `BaseChatModel` and `BaseLLM` respectively.\n\n`BaseLLM` is a subclass of `BaseLanguageModel` that provides a common interface for LLMs while `BaseChatModel` is a subclass of `BaseLanguageModel` that provides a common interface for chat models.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/index.mdx","loc":{"lines":{"from":26,"to":34}}}}],["1578",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/llm/llm.ts\";\nimport DebuggingExample from \"@examples/models/llm/llm_debugging.ts\";\nimport StreamingExample from \"@examples/models/llm/llm_streaming.ts\";\nimport TimeoutExample from \"@examples/models/llm/llm_timeout.ts\";\nimport CancellationExample from \"@examples/models/llm/openai_cancellation.ts\";\n\n# Additional Functionality: LLMs\n\nWe offer a number of additional features for LLMs. In most of the examples below, we'll be using the `OpenAI` LLM. However, all of these features are available for all LLMs.\n\n## Additional Methods\n\nLangChain provides a number of additional methods for interacting with LLMs:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/additional_functionality.mdx","loc":{"lines":{"from":1,"to":20}}}}],["1579",{"pageContent":"Streaming Responses\n\nSome LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated.\nLangChain currently provides streaming for the `OpenAI` LLM:\n\n<CodeBlock language=\"typescript\">{StreamingExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/additional_functionality.mdx","loc":{"lines":{"from":22,"to":27}}}}],["1580",{"pageContent":"Caching\n\nLangChain provides an optional caching layer for LLMs. This is useful for two reasons:\n\n1. It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.\n2. It can speed up your application by reducing the number of API calls you make to the LLM provider.\n\n### Caching in-memory\n\nThe default cache is stored in-memory. This means that if you restart your application, the cache will be cleared.\n\nTo enable it you can pass `cache: true` when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({ cache: true });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/additional_functionality.mdx","loc":{"lines":{"from":29,"to":46}}}}],["1581",{"pageContent":"Caching with Redis\n\nLangChain also provides a Redis-based cache. This is useful if you want to share the cache across multiple processes or servers. To use it, you'll need to install the `redis` package:\n\n```bash npm2yarn\nnpm install redis\n```\n\nThen, you can pass a `cache` option when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { RedisCache } from \"langchain/cache/redis\";\nimport { createClient } from \"redis\";\n\n// See https://github.com/redis/node-redis for connection options\nconst client = createClient();\nconst cache = new RedisCache(client);\n\nconst model = new OpenAI({ cache });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/additional_functionality.mdx","loc":{"lines":{"from":48,"to":68}}}}],["1582",{"pageContent":"Adding a timeout\n\nBy default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a `timeout` option, in milliseconds, when you instantiate the model. For example, for OpenAI:\n\n<CodeBlock language=\"typescript\">{TimeoutExample}</CodeBlock>\n\nCurrently, the timeout option is only supported for OpenAI models.\n\n## Cancelling requests\n\nYou can cancel a request by passing a `signal` option when you call the model. For example, for OpenAI:\n\n<CodeBlock language=\"typescript\">{CancellationExample}</CodeBlock>\n\nCurrently, the signal option is only supported for OpenAI models.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/additional_functionality.mdx","loc":{"lines":{"from":70,"to":84}}}}],["1583",{"pageContent":"Dealing with Rate Limits\n\nSome LLM providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a `maxConcurrency` option when instantiating an LLM. This option allows you to specify the maximum number of concurrent requests you want to make to the LLM provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.\n\nFor example, if you set `maxConcurrency: 5`, then LangChain will only send 5 requests to the LLM provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.\n\nTo use this feature, simply pass `maxConcurrency: <number>` when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({ maxConcurrency: 5 });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/additional_functionality.mdx","loc":{"lines":{"from":86,"to":98}}}}],["1584",{"pageContent":"Dealing with API Errors\n\nIf the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a `maxRetries` option when you instantiate the model. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({ maxRetries: 10 });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/additional_functionality.mdx","loc":{"lines":{"from":100,"to":108}}}}],["1585",{"pageContent":"Logging for Debugging\n\nEspecially when using an agent, there can be a lot of back-and-forth going on behind the scenes as a LLM processes a chain. For agents, the response object contains an intermediateSteps object that you can print to see an overview of the steps it took to get there. If that's not enough and you want to see every exchange with the LLM, you can use the LLMCallbackManager to write yourself custom logging (or anything else you want to do) as the model goes through the steps:\n\n<CodeBlock language=\"typescript\">{DebuggingExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/additional_functionality.mdx","loc":{"lines":{"from":110,"to":114}}}}],["1586",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: LLMs\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/llm/llm_quick_start.ts\";\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: LLMs\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/language-model)\n:::\n\nLangChain provides a standard interface for using a variety of LLMs.\n\nTo get started, simply use the `call` method of an `LLM` implementation, passing in a `string` input. In this example, we are using the `OpenAI` implementation:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Dig deeper\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/index.mdx","loc":{"lines":{"from":1,"to":24}}}}],["1587",{"pageContent":"---\nsidebar_position: 3\nsidebar_label: Integrations\n---\n\n# Integrations: LLMs\n\nLangChain offers a number of LLM implementations that integrate with various model providers. These are:\n\n## `OpenAI`\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({\n  temperature: 0.9,\n  openAIApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n});\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });\n```\n\n## Azure `OpenAI`\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({\n  temperature: 0.9,\n  azureOpenAIApiKey: \"YOUR-API-KEY\",\n  azureOpenAIApiInstanceName: \"YOUR-INSTANCE-NAME\",\n  azureOpenAIApiDeploymentName: \"YOUR-DEPLOYMENT-NAME\",\n  azureOpenAIApiVersion: \"YOUR-API-VERSION\",\n});\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/integrations.mdx","loc":{"lines":{"from":1,"to":41}}}}],["1588",{"pageContent":"`HuggingFaceInference`\n\n```bash npm2yarn\nnpm install @huggingface/inference@1\n```\n\n```typescript\nimport { HuggingFaceInference } from \"langchain/llms/hf\";\n\nconst model = new HuggingFaceInference({\n  model: \"gpt2\",\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY\n});\nconst res = await model.call(\"1 + 1 =\");\nconsole.log({ res });\n```\n\n## `Cohere`\n\n```bash npm2yarn\nnpm install cohere-ai\n```\n\n```typescript\nimport { Cohere } from \"langchain/llms/cohere\";\n\nconst model = new Cohere({\n  maxTokens: 20,\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.COHERE_API_KEY\n});\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/integrations.mdx","loc":{"lines":{"from":43,"to":77}}}}],["1589",{"pageContent":"`Replicate`\n\n```bash npm2yarn\nnpm install replicate\n```\n\n```typescript\nimport { Replicate } from \"langchain/llms/replicate\";\n\nconst model = new Replicate({\n  model:\n    \"daanelson/flan-t5:04e422a9b85baed86a4f24981d7f9953e20c5fd82f6103b74ebc431588e1cec8\",\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.REPLICATE_API_KEY\n});\nconst res = await modelA.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/integrations.mdx","loc":{"lines":{"from":79,"to":97}}}}],["1590",{"pageContent":"Additional LLM Implementations\n\n### `PromptLayerOpenAI`\n\nLangChain integrates with PromptLayer for logging and debugging prompts and responses. To add support for PromptLayer:\n\n1. Create a PromptLayer account here: [https://promptlayer.com](https://promptlayer.com).\n2. Create an API token and pass it either as `promptLayerApiKey` argument in the `PromptLayerOpenAI` constructor or in the `PROMPTLAYER_API_KEY` environment variable.\n\n```typescript\nimport { PromptLayerOpenAI } from \"langchain/llms/openai\";\n\nconst model = new PromptLayerOpenAI({\n  temperature: 0.9,\n  openAIApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n  promptLayerApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.PROMPTLAYER_API_KEY\n});\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/integrations.mdx","loc":{"lines":{"from":99,"to":119}}}}],["1591",{"pageContent":"Azure `PromptLayerOpenAI`\n\nLangChain integrates with PromptLayer for logging and debugging prompts and responses. To add support for PromptLayer:\n\n1. Create a PromptLayer account here: [https://promptlayer.com](https://promptlayer.com).\n2. Create an API token and pass it either as `promptLayerApiKey` argument in the `PromptLayerOpenAI` constructor or in the `PROMPTLAYER_API_KEY` environment variable.\n\n```typescript\nimport { PromptLayerOpenAI } from \"langchain/llms/openai\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/integrations.mdx","loc":{"lines":{"from":121,"to":129}}}}],["1592",{"pageContent":"const model = new PromptLayerOpenAI({\n  temperature: 0.9,\n  azureOpenAIApiKey: \"YOUR-AOAI-API-KEY\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiInstanceName: \"YOUR-AOAI-INSTANCE-NAME\", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME\n  azureOpenAIApiDeploymentName: \"YOUR-AOAI-DEPLOYMENT-NAME\", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n  azureOpenAIApiCompletionsDeploymentName:\n    \"YOUR-AOAI-COMPLETIONS-DEPLOYMENT-NAME\", // In Node.js defaults to process.env.AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME\n  azureOpenAIApiEmbeddingsDeploymentName:\n    \"YOUR-AOAI-EMBEDDINGS-DEPLOYMENT-NAME\", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME\n  azureOpenAIApiVersion: \"YOUR-AOAI-API-VERSION\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  promptLayerApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.PROMPTLAYER_API_KEY\n});\nconst res = await model.call(","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/integrations.mdx","loc":{"lines":{"from":131,"to":143}}}}],["1593",{"pageContent":"\"What would be a good company name a company that makes colorful socks?\"\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/integrations.mdx","loc":{"lines":{"from":143,"to":144}}}}],["1594",{"pageContent":"The request and the response will be logged in the [PromptLayer dashboard](https://promptlayer.com/home).\n\n> **_Note:_** In streaming mode PromptLayer will not log the response.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/models/llms/integrations.mdx","loc":{"lines":{"from":271,"to":273}}}}],["1595",{"pageContent":"---\nsidebar_label: Example Selectors\nsidebar_position: 3\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Example Selectors\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/prompts/example-selectors)\n:::\n\nIf you have a large number of examples, you may need to programmatically select which ones to include in the prompt. The ExampleSelector is the class responsible for doing so. The base interface is defined as below.\n\n```typescript\nclass BaseExampleSelector {\n  addExample(example: Example): Promise<void | string>;\n\n  selectExamples(input_variables: Example): Promise<Example[]>;\n}\n```\n\nIt needs to expose a `selectExamples` - this takes in the input variables and then returns a list of examples method - and an `addExample` method, which saves an example for later selection. It is up to each specific implementation as to how those examples are saved and selected. Lets take a look at some below.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/prompts/example_selectors/index.mdx","loc":{"lines":{"from":1,"to":24}}}}],["1596",{"pageContent":"Select by Length\n\nThis `ExampleSelector` selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.\n\nimport ExampleLength from \"@examples/prompts/length_based_example_selector.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleLength}</CodeBlock>\n\n## Select by Similarity\n\nThe `SemanticSimilarityExampleSelector` selects examples based on which examples are most similar to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.\n\nimport ExampleSimilarity from \"@examples/prompts/semantic_similarity_example_selector.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleSimilarity}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/prompts/example_selectors/index.mdx","loc":{"lines":{"from":26,"to":40}}}}],["1597",{"pageContent":"---\nsidebar_position: 3\nhide_table_of_contents: true\nsidebar_label: Prompts\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Prompts\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/prompts)\n:::\n\nLangChain provides several utilities to help manage prompts for language models, including chat models.\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/prompts/index.mdx","loc":{"lines":{"from":1,"to":17}}}}],["1598",{"pageContent":"---\nsidebar_label: Output Parsers\nsidebar_position: 2\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Output Parsers\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/prompts/output-parser)\n:::\n\nLanguage models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\n\n- `getFormatInstructions(): str` A method which returns a string containing instructions for how the output of a language model should be formatted.\n- `parse(raw: string): any` A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n\nAnd then one optional one:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/prompts/output_parsers/index.mdx","loc":{"lines":{"from":1,"to":21}}}}],["1599",{"pageContent":"- `parseWithPrompt(text: string, prompt: BasePromptValue): any`: A method which takes in a string (assumed to be the response from a language model) and a formatted prompt (assumed to the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\n\nBelow we go over some examples of output parsers.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/prompts/output_parsers/index.mdx","loc":{"lines":{"from":23,"to":25}}}}],["1600",{"pageContent":"Structured Output Parser\n\nThis output parser can be used when you want to return multiple fields.\n\nimport Structured from \"@examples/prompts/structured_parser.ts\";\n\n<CodeBlock language=\"typescript\">{Structured}</CodeBlock>\n\n## Structured Output Parser with Zod Schema\n\nThis output parser can be also be used when you want to define the output schema using Zod, a TypeScript validation library. The Zod schema passed in needs be parseable from a JSON string, so eg. `z.date()` is not allowed, but `z.coerce.date()` is.\n\nimport StructuredZod from \"@examples/prompts/structured_parser_zod.ts\";\n\n<CodeBlock language=\"typescript\">{StructuredZod}</CodeBlock>\n\n## Output Fixing Parser\n\nThis output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.\n\nimport Fix from \"@examples/prompts/fix_parser.ts\";\n\n<CodeBlock language=\"typescript\">{Fix}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/prompts/output_parsers/index.mdx","loc":{"lines":{"from":27,"to":49}}}}],["1601",{"pageContent":"Comma-separated List Parser\n\nThis output parser can be used when you want to return a list of items.\n\nimport Comma from \"@examples/prompts/comma_list_parser.ts\";\n\n<CodeBlock language=\"typescript\">{Comma}</CodeBlock>\n\n## Combining Output Parsers\n\nOutput parsers can be combined using `CombiningOutputParser`. This output parser takes in a list of output parsers, and will ask for (and parse) a combined output that contains all the fields of all the parsers.\n\nimport Combining from \"@examples/prompts/combining_parser.ts\";\n\n<CodeBlock language=\"typescript\">{Combining}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/prompts/output_parsers/index.mdx","loc":{"lines":{"from":51,"to":65}}}}],["1602",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport PromptValue from \"@examples/prompts/prompt_value.ts\";\nimport PartialValue from \"@examples/prompts/partial.ts\";\nimport FewShot from \"@examples/prompts/few_shot.ts\";\n\n# Additional Functionality: Prompt Templates\n\nWe offer a number of extra features for prompt templates, as shown below:\n\n## Prompt Values\n\nA `PromptValue` is an object returned by the `formatPromptValue` of a `PromptTemplate`. It can be converted to a string or list of `ChatMessage` objects.\n\n<CodeBlock language=\"typescript\">{PromptValue}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/prompts/prompt_templates/additional_functionality.mdx","loc":{"lines":{"from":1,"to":18}}}}],["1603",{"pageContent":"Partial Values\n\nLike other methods, it can make sense to \"partial\" a prompt template - eg pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.\n\nLangChain supports this in two ways:\n\n1. Partial formatting with string values.\n2. Partial formatting with functions that return string values.\n\nThese two different ways support different use cases. In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.\n\n<CodeBlock language=\"typescript\">{PartialValue}</CodeBlock>\n\n## Few-Shot Prompt Templates\n\nA few-shot prompt template is a prompt template you can build with examples.\n\n<CodeBlock language=\"typescript\">{FewShot}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/prompts/prompt_templates/additional_functionality.mdx","loc":{"lines":{"from":20,"to":37}}}}],["1604",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Prompt Templates\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/prompts/prompts.ts\";\nimport DocCardList from \"@theme/DocCardList\";\n\n# Prompt Templates\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/prompts/prompt-template)\n:::\n\nA `PromptTemplate` allows you to make use of templating to generate a prompt. This is useful for when you want to use the same prompt outline in multiple places, but with certain values changed.\nPrompt templates are supported for both LLMs and chat models, as shown below:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Dig deeper\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/prompts/prompt_templates/index.mdx","loc":{"lines":{"from":1,"to":24}}}}],["1605",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\n# Chat Messages\n\nThe primary interface through which end users interact with LLMs is a chat interface. For this reason, some model providers have started providing access to the underlying API in a way that expects chat messages. These messages have a content field (which is usually text) and are associated with a user (or role). Right now the supported users are System, Human, and AI.\n\n## SystemChatMessage\n\nA chat message representing information that should be instructions to the AI system.\n\n```typescript\nimport { SystemChatMessage } from \"langchain/schema\";\n\nnew SystemChatMessage(\"You are a nice assistant\");\n```\n\n## HumanChatMessage\n\nA chat message representing information coming from a human interacting with the AI system.\n\n```typescript\nimport { HumanChatMessage } from \"langchain/schema\";\n\nnew HumanChatMessage(\"Hello, how are you?\");\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/schema/chat-messages.md","loc":{"lines":{"from":1,"to":28}}}}],["1606",{"pageContent":"AIChatMessage\n\nA chat message representing information coming from the AI system.\n\n```typescript\nimport { AIChatMessage } from \"langchain/schema\";\n\nnew AIChatMessage(\"I am doing well, thank you!\");\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/schema/chat-messages.md","loc":{"lines":{"from":30,"to":38}}}}],["1607",{"pageContent":"# Document\n\nLanguage models only know information about what they were trained on. In order to get them answer questions or summarize other information you have to pass it to the language model. Therefore, it is very important to have a concept of a document.\n\nA document at its core is fairly simple. It consists of a piece of text and optional metadata. The piece of text is what we interact with the language model, while the optional metadata is useful for keeping track of metadata about the document (such as the source).\n\n```typescript\ninterface Document {\n  pageContent: string;\n  metadata: Record<string, any>;\n}\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/schema/document.md","loc":{"lines":{"from":1,"to":12}}}}],["1608",{"pageContent":"Creating a Document\n\nYou can create a document object rather easily in LangChain with:\n\n```typescript\nimport { Document } from \"langchain/document\";\n\nconst doc = new Document({ pageContent: \"foo\" });\n```\n\nYou can create one with metadata with:\n\n```typescript\nimport { Document } from \"langchain/document\";\n\nconst doc = new Document({ pageContent: \"foo\", metadata: { source: \"1\" } });\n```\n\nAlso check out [Document Loaders](../indexes/document_loaders/) for a way to load documents from a variety of sources.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/schema/document.md","loc":{"lines":{"from":14,"to":32}}}}],["1609",{"pageContent":"---\n---\n\n# Examples\n\nExamples are input/output pairs that represent inputs to a function and then expected output. They can be used in both training and evaluation of models.\n\n```typescript\ntype Example = Record<string, string>;\n```\n\n## Creating an Example\n\nYou can create an Example like this:\n\n```typescript\nconst example = {\n  input: \"foo\",\n  output: \"bar\",\n};\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/schema/example.md","loc":{"lines":{"from":1,"to":21}}}}],["1610",{"pageContent":"---\nsidebar_position: 1\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Schema\n\nThis section speaks about interfaces that are used throughout the rest of the library.\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/modules/schema/index.mdx","loc":{"lines":{"from":1,"to":11}}}}],["1611",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\n\n# Creating callback handlers\n\n## Creating a custom handler\n\nYou can also create your own handler by implementing the `BaseCallbackHandler` interface. This is useful if you want to do something more complex than just logging to the console, eg. send the events to a logging service. As an example here is a simple implementation of a handler that logs to the console:\n\nimport CustomHandlerExample from \"@examples/callbacks/custom_handler.ts\";\n\n<CodeBlock language=\"typescript\">{CustomHandlerExample}</CodeBlock>\n\nYou could then use it as described in the [section](#built-in-handlers) above.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/callbacks/create-handlers.mdx","loc":{"lines":{"from":1,"to":13}}}}],["1612",{"pageContent":"---\nsidebar_label: Callbacks in custom Chains\n---\n\n# Callbacks in custom Chains/Agents\n\nLangChain is designed to be extensible. You can add your own custom Chains and Agents to the library. This page will show you how to add callbacks to your custom Chains and Agents.\n\n## Adding callbacks to custom Chains\n\nWhen you create a custom chain you can easily set it up to use the same callback system as all the built-in chains. See this guide for more information on how to [create custom chains and use callbacks inside them](../../modules/chains#subclassing-basechain).","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/callbacks/creating-subclasses.mdx","loc":{"lines":{"from":1,"to":11}}}}],["1613",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\n\n# Events / Callbacks\n\nLangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, [monitoring](../tracing), [streaming](../../modules/models/llms/additional_functionality#streaming-responses), and other tasks.\n\nYou can subscribe to these events by using the `callbacks` argument available throughout the API. This method accepts a list of handler objects, which are expected to implement one or more of the methods described in the [API docs](../../api/callbacks/interfaces/CallbackHandlerMethods).\n\n## Dive deeper\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/callbacks/index.mdx","loc":{"lines":{"from":1,"to":13}}}}],["1614",{"pageContent":"How to use callbacks\n\nThe `callbacks` argument is available on most objects throughout the API ([Chains](../../modules/chains/), [Models](../../modules/models/), [Tools](../../modules/agents/tools/), [Agents](../../modules/agents/agents/), etc.) in two different places:\n\n### Constructor callbacks\n\nDefined in the constructor, eg. `new LLMChain({ callbacks: [handler] })`, which will be used for all calls made on that object, and will be scoped to that object only, eg. if you pass a handler to the `LLMChain` constructor, it will not be used by the Model attached to that chain.\n\nimport ConstructorExample from \"@examples/callbacks/docs_constructor_callbacks.ts\";\n\n<CodeBlock language=\"typescript\">{ConstructorExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/callbacks/index.mdx","loc":{"lines":{"from":15,"to":25}}}}],["1615",{"pageContent":"Request callbacks\n\nDefined in the `call()`/`run()`/`apply()` methods used for issuing a request, eg. `chain.call({ input: '...' }, [handler])`, which will be used for that specific request only, and all sub-requests that it contains (eg. a call to an LLMChain triggers a call to a Model, which uses the same handler passed in the `call()` method).\n\nimport RequestExample from \"@examples/callbacks/docs_request_callbacks.ts\";\n\n<CodeBlock language=\"typescript\">{RequestExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/callbacks/index.mdx","loc":{"lines":{"from":27,"to":33}}}}],["1616",{"pageContent":"Verbose mode\n\nThe `verbose` argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, eg. `new LLMChain({ verbose: true })`, and it is equivalent to passing a `ConsoleCallbackHandler` to the `callbacks` argument of that object and all child objects. This is useful for debugging, as it will log all events to the console.\n\nimport VerboseExample from \"@examples/callbacks/docs_verbose.ts\";\n\n<CodeBlock language=\"typescript\">{VerboseExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/callbacks/index.mdx","loc":{"lines":{"from":35,"to":41}}}}],["1617",{"pageContent":"When do you want to use each of these?\n\n- Constructor callbacks are most useful for use cases such as logging, monitoring, etc., which are _not specific to a single request_, but rather to the entire chain. For example, if you want to log all the requests made to an LLMChain, you would pass a handler to the constructor.\n- Request callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific websocket connection, or other similar use cases. For example, if you want to stream the output of a single request to a websocket, you would pass a handler to the `call()` method","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/callbacks/index.mdx","loc":{"lines":{"from":43,"to":46}}}}],["1618",{"pageContent":"Usage examples\n\n### Built-in handlers\n\nLangChain provides a few built-in handlers that you can use to get started. These are available in the `langchain/callbacks` module. The most basic handler is the `ConsoleCallbackHandler`, which simply logs all events to the console. In the future we will add more default handlers to the library. Note that when the `verbose` flag on the object is set to `true`, the `ConsoleCallbackHandler` will be invoked even without being explicitly passed in.\n\nimport ConsoleExample from \"@examples/callbacks/console_handler.ts\";\n\n<CodeBlock language=\"typescript\">{ConsoleExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/callbacks/index.mdx","loc":{"lines":{"from":48,"to":56}}}}],["1619",{"pageContent":"One-off handlers\n\nYou can create a one-off handler inline by passing a plain object to the `callbacks` argument. This object should implement the [`CallbackHandlerMethods`](../../api/callbacks/interfaces/CallbackHandlerMethods) interface. This is useful if eg. you need to create a handler that you will use only for a single request, eg to stream the output of an LLM/Agent/etc to a websocket.\n\nimport StreamingExample from \"@examples/models/llm/llm_streaming.ts\";\n\n<CodeBlock language=\"typescript\">{StreamingExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/callbacks/index.mdx","loc":{"lines":{"from":58,"to":64}}}}],["1620",{"pageContent":"Multiple handlers\n\nWe offer a method on the `CallbackManager` class that allows you to create a one-off handler. This is useful if eg. you need to create a handler that you will use only for a single request, eg to stream the output of an LLM/Agent/etc to a websocket.\n\nThis is a more complete example that passes a `CallbackManager` to a ChatModel, and LLMChain, a Tool, and an Agent.\n\nimport AgentExample from \"@examples/agents/streaming.ts\";\n\n<CodeBlock language=\"typescript\">{AgentExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/callbacks/index.mdx","loc":{"lines":{"from":66,"to":74}}}}],["1621",{"pageContent":"# Deployment\n\nYou've built your LangChain app and now you're looking to deploy it to production? You've come to the right place. This guide will walk you through the options you have for deploying your app, and the considerations you should make when doing so.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/deployment.md","loc":{"lines":{"from":1,"to":3}}}}],["1622",{"pageContent":"Overview\n\nLangChain is a library for building applications that use language models. It is not a web framework, and does not provide any built-in functionality for serving your app over the web. Instead, it provides a set of tools that you can integrate in your API or backend server.\n\nThere are a couple of high-level options for deploying your app:\n\n- Deploying to a VM or container\n  - Persistent filesystem means you can save and load files from disk\n  - Always-running process means you can cache some things in memory\n  - You can support long-running requests, such as WebSockets\n- Deploying to a serverless environment\n  - No persistent filesystem means you can load files from disk, but not save them for later\n  - Cold start means you can't cache things in memory and expect them to be cached between requests\n  - Function timeouts mean you can't support long-running requests, such as WebSockets\n\nSome other considerations include:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/deployment.md","loc":{"lines":{"from":5,"to":20}}}}],["1623",{"pageContent":"- Do you deploy your backend and frontend together, or separately?\n- Do you deploy your backend co-located with your database, or separately?\n\nAs you move your LangChains into production, we'd love to offer more comprehensive support. Please fill out [this form](https://forms.gle/57d8AmXBYp8PP8tZA) and we'll set up a dedicated support Slack channel.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/deployment.md","loc":{"lines":{"from":22,"to":25}}}}],["1624",{"pageContent":"Deployment Options\n\nSee below for a list of deployment options for your LangChain app. If you don't see your preferred option, please get in touch and we can add it to this list.\n\n### Deploying to Fly.io\n\n[Fly.io](https://fly.io) is a platform for deploying apps to the cloud. It's a great option for deploying your app to a container environment.\n\nSee [our Fly.io template](https://github.com/hwchase17/langchain-template-node-fly) for an example of how to deploy your app to Fly.io.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/deployment.md","loc":{"lines":{"from":27,"to":35}}}}],["1625",{"pageContent":"# Tracing\n\nSimilar to the Python `langchain` package, JS `langchain` also supports tracing.\n\nYou can view an overview of tracing [here.](https://langchain.readthedocs.io/en/latest/tracing.html)\nTo spin up the tracing backend, run `docker compose up` (or `docker-compose up` if on using an older version of `docker`) in the `langchain` directory.\nYou can also use the `langchain-server` command if you have the python `langchain` package installed.\n\nHere's an example of how to use tracing in `langchain.js`. All that needs to be done is setting the `LANGCHAIN_TRACING` environment variable to `true`.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport process from \"process\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/tracing.md","loc":{"lines":{"from":1,"to":16}}}}],["1626",{"pageContent":"export const run = async () => {\n  process.env.LANGCHAIN_TRACING = \"true\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/tracing.md","loc":{"lines":{"from":18,"to":44}}}}],["1627",{"pageContent":"Concurrency\n\nTracing works with concurrency out of the box.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport process from \"process\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_TRACING = \"true\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/tracing.md","loc":{"lines":{"from":46,"to":78}}}}],["1628",{"pageContent":"// This will result in a lot of errors, because the shared Tracer is not concurrency-safe.\n  const [resultA, resultB, resultC] = await Promise.all([\n    executor.call({ input }),\n    executor.call({ input }),\n    executor.call({ input }),\n  ]);\n\n  console.log(`Got output ${resultA.output} ${resultA.__run.runId}`);\n  console.log(`Got output ${resultB.output} ${resultB.__run.runId}`);\n  console.log(`Got output ${resultC.output} ${resultC.__run.runId}`);\n\n  /*\n    Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. b8fb98aa-07a5-45bd-b593-e8d7376b05ca\n    Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. c8d916d5-ca1d-4702-8dd7-cab5e438578b\n    Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. bf5fe04f-ef29-4e55-8ce1-e4aa974f9484\n    */\n};\n```","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/production/tracing.md","loc":{"lines":{"from":80,"to":97}}}}],["1629",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 4\n---\n\n# Interacting with APIs\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/apis)\n:::\n\nLots of data and information is stored behind APIs.\nThis page covers all resources available in LangChain for working with APIs.\n\n## Chains\n\nIf you are just getting started, and you have relatively apis, you should get started with chains.\nChains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you\nunderstand what is happening better.\n\nTODO: add an API chain and then add an example here.\n\n## Agents\n\nAgents are more complex, and involve multiple queries to the LLM to understand what to do.\nThe downside of agents are that you have less control. The upside is that they are more powerful,\nwhich allows you to use them on larger and more complex schemas.\n\n- [OpenAPI Agent](../modules/agents/toolkits/openapi.md)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/api.mdx","loc":{"lines":{"from":1,"to":29}}}}],["1630",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\n\n# AutoGPT\n\n:::info\nOriginal Repo: https://github.com/Significant-Gravitas/Auto-GPT\n:::\n\nAutoGPT is a custom agent that uses long-term memory along with a prompt designed for independent work (ie. without asking user input) to perform tasks.\n\n## Isomorphic Example\n\nimport IsomorphicExample from \"@examples/experimental/autogpt/weather_browser.ts\";\n\nIn this example we use AutoGPT to predict the weather for a given location. This example is designed to run in all JS environments, including the browser.\n\n<CodeBlock language=\"typescript\">{IsomorphicExample}</CodeBlock>\n\n## Node.js Example\n\nimport NodeExample from \"@examples/experimental/autogpt/weather.ts\";\n\nIn this example we use AutoGPT to predict the weather for a given location. This example is designed to run in Node.js, so it uses the local filesystem, and a Node-only vector store.\n\n<CodeBlock language=\"typescript\">{NodeExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/autonomous_agents/auto_gpt.mdx","loc":{"lines":{"from":1,"to":25}}}}],["1631",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\n\n# BabyAGI\n\n:::info\nOriginal Repo: https://github.com/yoheinakajima/babyagi\n:::\n\nBabyAGI is made up of 3 components:\n\n- A chain responsible for creating tasks\n- A chain responsible for prioritising tasks\n- A chain responsible for executing tasks\n\nThese chains are executed in sequence until the task list is empty or the maximum number of iterations is reached.\n\n## Simple Example\n\nimport SimpleExample from \"@examples/experimental/babyagi/weather.ts\";\n\nIn this example we use BabyAGI directly without any tools. You'll see this results in successfully creating a list of tasks but when it comes to executing the tasks we do not get concrete results. This is because we have not provided any tools to the BabyAGI. We'll see how to do that in the next example.\n\n<CodeBlock language=\"typescript\">{SimpleExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/autonomous_agents/baby_agi.mdx","loc":{"lines":{"from":1,"to":23}}}}],["1632",{"pageContent":"Example with Tools\n\nimport ToolsExample from \"@examples/experimental/babyagi/weather_with_tools.ts\";\n\nIn this next example we replace the execution chain with a custom agent with a Search tool. This gives BabyAGI the ability to use real-world data when executing tasks, which makes it much more powerful. You can add additional tools to give it more capabilities.\n\n<CodeBlock language=\"typescript\">{ToolsExample}</CodeBlock>","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/autonomous_agents/baby_agi.mdx","loc":{"lines":{"from":25,"to":31}}}}],["1633",{"pageContent":"import DocCardList from \"@theme/DocCardList\";\n\n# Autonomous Agents\n\nAutonomous Agents are agents that designed to be more long running. You give them one or multiple long term goals, and they independently execute towards those goals. The applications combine tool usage and long term memory.\n\nAt the moment, Autonomous Agents are fairly experimental and based off of other open-source projects. By implementing these open source projects in LangChain primitives we can get the benefits of LangChain - easy switching and experimenting with multiple LLMs, usage of different vectorstores as memory, usage of LangChain's collection of tools.\n\n<DocCardList />","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/autonomous_agents/index.mdx","loc":{"lines":{"from":1,"to":9}}}}],["1634",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\n# Personal Assistants\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/personal-assistants)\n:::\n\nWe use \"personal assistant\" here in a very broad sense.\nPersonal assistants have a few characteristics:\n\n- They can interact with the outside world\n- They have knowledge of your data\n- They remember your interactions\n\nReally all of the functionality in LangChain is relevant for building a personal assistant.\nHighlighting specific parts:\n\n- [Agent Documentation](../modules/agents/index.mdx) (for interacting with the outside world)\n- [Index Documentation](../modules/indexes/index.mdx) (for giving them knowledge of your data)\n- [Memory](../modules/memory/index.mdx) (for helping them remember interactions)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/personal_assistants.mdx","loc":{"lines":{"from":1,"to":24}}}}],["1635",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 2\n---\n\n# Question Answering\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/qa-docs)\n:::\n\nQuestion answering in this context refers to question answering over your document data.\nThere are a few different types of question answering:\n\n- [Retrieval Question Answering](../modules/chains/index_related_chains/retrieval_qa): Use this to ingest documents, index them into a vectorstore, and then be able to ask questions about it.\n- [Chat Retrieval](../modules/chains/index_related_chains/conversational_retrieval): Similar to above in that you ingest and index documents, but this lets you have more a conversation (ask follow up questions, etc) rather than just asking one-off questions.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/question_answering.mdx","loc":{"lines":{"from":1,"to":16}}}}],["1636",{"pageContent":"Indexing\n\nFor question answering over many documents, you almost always want to create an index over the data.\nThis can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money).\n\nTherefore, it is really important to understand how to create indexes, and so you should familiarize yourself with all the documentation related to that.\n\n- [Indexes](../modules/indexes/index.mdx)\n\n## Chains\n\nAfter you create an index, you can then use it in a chain.\nYou can just do normal question answering over it, or you can use it a conversational way.\nFor an overview of these chains (and more) see the below documentation.\n\n- [Index related chains](../modules/chains/index_related_chains/index.mdx)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/question_answering.mdx","loc":{"lines":{"from":18,"to":33}}}}],["1637",{"pageContent":"Agents\n\nIf you want to be able to answer more complex, multi-hop questions you should look into combining your indexes with an agent.\nFor an example of how to do that, please see the below.\n\n- [Vectorstore Agent](../modules/agents/toolkits/vectorstore.md)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/question_answering.mdx","loc":{"lines":{"from":35,"to":40}}}}],["1638",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 6\n---\n\n# Summarization\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/summarization)\n:::\n\nA common use case is wanting to summarize long documents.\nThis naturally runs into the context window limitations.\nUnlike in question-answering, you can't just do some semantic search hacks to only select the chunks of text most relevant to the question (because, in this case, there is no particular question - you want to summarize everything).\nSo what do you do then?\n\nTo get started, we would recommend checking out the summarization chain which attacks this problem in a recursive manner.\n\n- [Summarization Chain](../modules/chains/other_chains/summarization)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/summarization.mdx","loc":{"lines":{"from":1,"to":19}}}}],["1639",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 3\n---\n\n# Tabular Question Answering\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/qa-tabular)\n:::\n\nLots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables.\nThis page covers all resources available in LangChain for working with data in this format.\n\n## Chains\n\nIf you are just getting started, and you have relatively small/simple tabular data, you should get started with chains.\nChains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you\nunderstand what is happening better.\n\n- [SQL Database Chain](../modules/chains/other_chains/sql)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/tabular.mdx","loc":{"lines":{"from":1,"to":21}}}}],["1640",{"pageContent":"Agents\n\nAgents are more complex, and involve multiple queries to the LLM to understand what to do.\nThe downside of agents are that you have less control. The upside is that they are more powerful,\nwhich allows you to use them on larger databases and more complex schemas.\n\n- [SQL Agent](../modules/agents/toolkits/sql.mdx)","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/docs/docs/use_cases/tabular.mdx","loc":{"lines":{"from":23,"to":29}}}}]]
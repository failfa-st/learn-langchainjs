[["0",{"pageContent":"import { BaseLanguageModel } from \"../base_language/index.js\";\nimport { CallbackManager, Callbacks } from \"../callbacks/manager.js\";\nimport { LLMChain } from \"../chains/llm_chain.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport {\nAgentAction,\nAgentFinish,\nAgentStep,\nBaseChatMessage,\nChainValues,\n} from \"../schema/index.js\";\nimport { Tool } from \"../tools/base.js\";\nimport {\nAgentActionOutputParser,\nAgentInput,\nSerializedAgent,\nStoppingMethod,\n} from \"./types.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type OutputParserArgs = Record<string, any>;\n\nclass ParseError extends Error {\noutput: string;\n\nconstructor(msg: string, output: string) {\nsuper(msg);\nthis.output = output;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":1,"to":30}}}}],["1",{"pageContent":"abstract class BaseAgent {\nabstract get inputKeys(): string[];\n\nget returnValues(): string[] {\nreturn [\"output\"];\n}\n\nget allowedTools(): string[] | undefined {\nreturn undefined;\n}\n\n/**\n* Return the string type key uniquely identifying this class of agent.\n*/\n_agentType(): string {\nthrow new Error(\"Not implemented\");\n}\n\n/**\n* Return the string type key uniquely identifying multi or single action agents.\n*/\nabstract _agentActionType(): string;\n\n/**\n* Return response when agent has been stopped due to max iterations\n*/\nreturnStoppedResponse(\nearlyStoppingMethod: StoppingMethod,\n_steps: AgentStep[],\n_inputs: ChainValues,\n_callbackManager?: CallbackManager\n): Promise<AgentFinish> {\nif (earlyStoppingMethod === \"force\") {\nreturn Promise.resolve({\nreturnValues: { output: \"Agent stopped due to max iterations.\" },\nlog: \"\",\n});\n}\n\nthrow new Error(`Invalid stopping method: ${earlyStoppingMethod}`);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":401,"to":441}}}}],["2",{"pageContent":"/**\n* Prepare the agent for output, if needed\n*/\nasync prepareForOutput(\n_returnValues: AgentFinish[\"returnValues\"],\n_steps: AgentStep[]\n): Promise<AgentFinish[\"returnValues\"]> {\nreturn {};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":802,"to":811}}}}],["3",{"pageContent":"abstract class BaseSingleActionAgent extends BaseAgent {\n_agentActionType(): string {\nreturn \"single\" as const;\n}\n\n/**\n* Decide what to do, given some input.\n*\n* @param steps - Steps the LLM has taken so far, along with observations from each.\n* @param inputs - User inputs.\n* @param callbackManager - Callback manager.\n*\n* @returns Action specifying what tool to use.\n*/\nabstract plan(\nsteps: AgentStep[],\ninputs: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<AgentAction | AgentFinish>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":1205,"to":1224}}}}],["4",{"pageContent":"abstract class BaseMultiActionAgent extends BaseAgent {\n_agentActionType(): string {\nreturn \"multi\" as const;\n}\n\n/**\n* Decide what to do, given some input.\n*\n* @param steps - Steps the LLM has taken so far, along with observations from each.\n* @param inputs - User inputs.\n* @param callbackManager - Callback manager.\n*\n* @returns Actions specifying what tools to use.\n*/\nabstract plan(\nsteps: AgentStep[],\ninputs: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<AgentAction[] | AgentFinish>;\n}\n\nexport interface LLMSingleActionAgentInput {\nllmChain: LLMChain;\noutputParser: AgentActionOutputParser;\nstop?: string[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":1604,"to":1629}}}}],["5",{"pageContent":"class LLMSingleActionAgent extends BaseSingleActionAgent {\nllmChain: LLMChain;\n\noutputParser: AgentActionOutputParser;\n\nstop?: string[];\n\nconstructor(input: LLMSingleActionAgentInput) {\nsuper();\nthis.stop = input.stop;\nthis.llmChain = input.llmChain;\nthis.outputParser = input.outputParser;\n}\n\nget inputKeys(): string[] {\nreturn this.llmChain.inputKeys;\n}\n\n/**\n* Decide what to do given some input.\n*\n* @param steps - Steps the LLM has taken so far, along with observations from each.\n* @param inputs - User inputs.\n* @param callbackManager - Callback manager.\n*\n* @returns Action specifying what tool to use.\n*/\nasync plan(\nsteps: AgentStep[],\ninputs: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<AgentAction | AgentFinish> {\nconst output = await this.llmChain.call(\n{\nintermediate_steps: steps,\nstop: this.stop,\n...inputs,\n},\ncallbackManager\n);\nreturn this.outputParser.parse(\noutput[this.llmChain.outputKey],\ncallbackManager\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":2004,"to":2049}}}}],["6",{"pageContent":"interface AgentArgs {\noutputParser?: AgentActionOutputParser;\n\ncallbacks?: Callbacks;\n\n/**\n* @deprecated Use `callbacks` instead.\n*/\ncallbackManager?: CallbackManager;\n}\n\n/**\n* Class responsible for calling a language model and deciding an action.\n*\n* @remarks This is driven by an LLMChain. The prompt in the LLMChain *must*\n* include a variable called \"agent_scratchpad\" where the agent can put its\n* intermediary work.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":2408,"to":2425}}}}],["7",{"pageContent":"abstract class Agent extends BaseSingleActionAgent {\nllmChain: LLMChain;\n\noutputParser: AgentActionOutputParser;\n\nprivate _allowedTools?: string[] = undefined;\n\nget allowedTools(): string[] | undefined {\nreturn this._allowedTools;\n}\n\nget inputKeys(): string[] {\nreturn this.llmChain.inputKeys.filter((k) => k !== \"agent_scratchpad\");\n}\n\nconstructor(input: AgentInput) {\nsuper();\nthis.llmChain = input.llmChain;\nthis._allowedTools = input.allowedTools;\nthis.outputParser = input.outputParser;\n}\n\n/**\n* Prefix to append the observation with.\n*/\nabstract observationPrefix(): string;\n\n/**\n* Prefix to append the LLM call with.\n*/\nabstract llmPrefix(): string;\n\n/**\n* Return the string type key uniquely identifying this class of agent.\n*/\nabstract _agentType(): string;\n\n/**\n* Get the default output parser for this agent.\n*/\nstatic getDefaultOutputParser(\n_fields?: OutputParserArgs\n): AgentActionOutputParser {\nthrow new Error(\"Not implemented\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":2809,"to":2853}}}}],["8",{"pageContent":"/**\n* Create a prompt for this class\n*\n* @param _tools - List of tools the agent will have access to, used to format the prompt.\n* @param _fields - Additional fields used to format the prompt.\n*\n* @returns A PromptTemplate assembled from the given tools and fields.\n* */\nstatic createPrompt(\n_tools: Tool[],\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_fields?: Record<string, any>\n): BasePromptTemplate {\nthrow new Error(\"Not implemented\");\n}\n\n/** Construct an agent from an LLM and a list of tools */\nstatic fromLLMAndTools(\n_llm: BaseLanguageModel,\n_tools: Tool[],\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_args?: AgentArgs\n): Agent {\nthrow new Error(\"Not implemented\");\n}\n\n/**\n* Validate that appropriate tools are passed in\n*/\nstatic validateTools(_tools: Tool[]): void {}\n\n_stop(): string[] {\nreturn [`\\n${this.observationPrefix()}`];\n}\n\n/**\n* Name of tool to use to terminate the chain.\n*/\nfinishToolName(): string {\nreturn \"Final Answer\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":3212,"to":3252}}}}],["9",{"pageContent":"/**\n* Construct a scratchpad to let the agent continue its thought process\n*/\nasync constructScratchPad(\nsteps: AgentStep[]\n): Promise<string | BaseChatMessage[]> {\nreturn steps.reduce(\n(thoughts, { action, observation }) =>\nthoughts +\n[\naction.log,\n`${this.observationPrefix()}${observation}`,\nthis.llmPrefix(),\n].join(\"\\n\"),\n\"\"\n);\n}\n\nprivate async _plan(\nsteps: AgentStep[],\ninputs: ChainValues,\nsuffix?: string,\ncallbackManager?: CallbackManager\n): Promise<AgentAction | AgentFinish> {\nconst thoughts = await this.constructScratchPad(steps);\nconst newInputs: ChainValues = {\n...inputs,\nagent_scratchpad: suffix ? `${thoughts}${suffix}` : thoughts,\n};\n\nif (this._stop().length !== 0) {\nnewInputs.stop = this._stop();\n}\n\nconst output = await this.llmChain.predict(newInputs, callbackManager);\nreturn this.outputParser.parse(output, callbackManager);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":3606,"to":3642}}}}],["10",{"pageContent":"/**\n* Decide what to do given some input.\n*\n* @param steps - Steps the LLM has taken so far, along with observations from each.\n* @param inputs - User inputs.\n* @param callbackManager - Callback manager to use for this call.\n*\n* @returns Action specifying what tool to use.\n*/\nplan(\nsteps: AgentStep[],\ninputs: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<AgentAction | AgentFinish> {\nreturn this._plan(steps, inputs, undefined, callbackManager);\n}\n\n/**\n* Return response when agent has been stopped due to max iterations\n*/\nasync returnStoppedResponse(\nearlyStoppingMethod: StoppingMethod,\nsteps: AgentStep[],\ninputs: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<AgentFinish> {\nif (earlyStoppingMethod === \"force\") {\nreturn {\nreturnValues: { output: \"Agent stopped due to max iterations.\" },\nlog: \"\",\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":4005,"to":4036}}}}],["11",{"pageContent":"if (earlyStoppingMethod === \"generate\") {\ntry {\nconst action = await this._plan(\nsteps,\ninputs,\n\"\\n\\nI now need to return a final answer based on the previous steps:\",\ncallbackManager\n);\nif (\"returnValues\" in action) {\nreturn action;\n}\n\nreturn { returnValues: { output: action.log }, log: action.log };\n} catch (err) {\n// fine to use instanceof because we're in the same module\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (!(err instanceof ParseError)) {\nthrow err;\n}\nreturn { returnValues: { output: err.output }, log: err.output };\n}\n}\n\nthrow new Error(`Invalid stopping method: ${earlyStoppingMethod}`);\n}\n\n/**\n* Load an agent from a json-like object describing it.\n*/\nstatic async deserialize(\ndata: SerializedAgent & { llm?: BaseLanguageModel; tools?: Tool[] }\n): Promise<Agent> {\nswitch (data._type) {\ncase \"zero-shot-react-description\": {\nconst { ZeroShotAgent } = await import(\"./mrkl/index.js\");\nreturn ZeroShotAgent.deserialize(data);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":4401,"to":4437}}}}],["12",{"pageContent":":\nthrow new Error(\"Unknown agent type\");\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent.ts","loc":{"lines":{"from":4795,"to":4799}}}}],["13",{"pageContent":"import { Tool } from \"../../tools/base.js\";\n\nexport abstract class Toolkit {\nabstract tools: Tool[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/base.ts","loc":{"lines":{"from":1,"to":5}}}}],["14",{"pageContent":"export { JsonToolkit, createJsonAgent } from \"./json/json.js\";\nexport { SqlToolkit, createSqlAgent, SqlCreatePromptArgs } from \"./sql/sql.js\";\nexport {\nRequestsToolkit,\nOpenApiToolkit,\ncreateOpenApiAgent,\n} from \"./openapi/openapi.js\";\nexport {\nVectorStoreInfo,\nVectorStoreToolkit,\nVectorStoreRouterToolkit,\ncreateVectorStoreAgent,\ncreateVectorStoreRouterAgent,\n} from \"./vectorstore/vectorstore.js\";\nexport { ZapierToolKit } from \"./zapier/zapier.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/index.ts","loc":{"lines":{"from":1,"to":15}}}}],["15",{"pageContent":"import { BaseLanguageModel } from \"../../../base_language/index.js\";\nimport { Tool } from \"../../../tools/base.js\";\nimport {\nJsonGetValueTool,\nJsonListKeysTool,\nJsonSpec,\n} from \"../../../tools/json.js\";\nimport { JSON_PREFIX, JSON_SUFFIX } from \"./prompt.js\";\nimport { LLMChain } from \"../../../chains/llm_chain.js\";\nimport { ZeroShotCreatePromptArgs, ZeroShotAgent } from \"../../mrkl/index.js\";\nimport { Toolkit } from \"../base.js\";\nimport { AgentExecutor } from \"../../executor.js\";\n\nexport class JsonToolkit extends Toolkit {\ntools: Tool[];\n\nconstructor(public jsonSpec: JsonSpec) {\nsuper();\nthis.tools = [\nnew JsonListKeysTool(jsonSpec),\nnew JsonGetValueTool(jsonSpec),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/json.ts","loc":{"lines":{"from":1,"to":24}}}}],["16",{"pageContent":"function createJsonAgent(\nllm: BaseLanguageModel,\ntoolkit: JsonToolkit,\nargs?: ZeroShotCreatePromptArgs\n) {\nconst {\nprefix = JSON_PREFIX,\nsuffix = JSON_SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\n} = args ?? {};\nconst { tools } = toolkit;\nconst prompt = ZeroShotAgent.createPrompt(tools, {\nprefix,\nsuffix,\ninputVariables,\n});\nconst chain = new LLMChain({ prompt, llm });\nconst agent = new ZeroShotAgent({\nllmChain: chain,\nallowedTools: tools.map((t) => t.name),\n});\nreturn AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/json.ts","loc":{"lines":{"from":55,"to":81}}}}],["17",{"pageContent":"export const JSON_PREFIX = `You are an agent designed to interact with JSON.\nYour goal is to return a final answer by interacting with the JSON.\nYou have access to the following tools which help you learn more about the JSON you are interacting with.\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\nDo not make up any information that is not contained in the JSON.\nYour input to the tools should be in the form of in json pointer syntax (e.g. /key1/0/key2).\nYou must escape a slash in a key with a ~1, and escape a tilde with a ~0.\nFor example, to access the key /foo, you would use /~1foo\nYou should only use keys that you know for a fact exist. You must validate that a key exists by seeing it previously when calling 'json_list_keys'.\nIf you have not seen a key in one of those responses, you cannot use it.\nYou should only add one key at a time to the path. You cannot add multiple keys at once.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/prompt.ts","loc":{"lines":{"from":1,"to":11}}}}],["18",{"pageContent":"If you encounter a null or undefined value, go back to the previous key, look at the available keys, and try again.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/prompt.ts","loc":{"lines":{"from":12,"to":12}}}}],["19",{"pageContent":"If the question does not seem to be related to the JSON, just return \"I don't know\" as the answer.\nAlways begin your interaction with the 'json_list_keys' with an empty string as the input to see what keys exist in the JSON.\n\nNote that sometimes the value at a given path is large. In this case, you will get an error \"Value is a large dictionary, should explore its keys directly\".\nIn this case, you should ALWAYS follow up by using the 'json_list_keys' tool to see what keys exist at that path.\nDo not simply refer the user to the JSON or a section of the JSON, as this is not a valid answer. Keep digging until you find the answer and explicitly return it.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/prompt.ts","loc":{"lines":{"from":14,"to":19}}}}],["20",{"pageContent":"const JSON_SUFFIX = `Begin!\"\n\nQuestion: {input}\nThought: I should look at the keys that exist to see what I can query. I should use the 'json_list_keys' tool with an empty string as the input.\n{agent_scratchpad}`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/json/prompt.ts","loc":{"lines":{"from":21,"to":25}}}}],["21",{"pageContent":"import { BaseLanguageModel } from \"../../../base_language/index.js\";\nimport { Tool } from \"../../../tools/base.js\";\nimport { DynamicTool } from \"../../../tools/dynamic.js\";\nimport { JsonSpec } from \"../../../tools/json.js\";\nimport { AgentExecutor } from \"../../executor.js\";\nimport {\nOPENAPI_PREFIX,\nOPENAPI_SUFFIX,\nJSON_EXPLORER_DESCRIPTION,\n} from \"./prompt.js\";\nimport { LLMChain } from \"../../../chains/llm_chain.js\";\nimport { ZeroShotCreatePromptArgs, ZeroShotAgent } from \"../../mrkl/index.js\";\nimport { Toolkit } from \"../base.js\";\nimport {\nHeaders,\nRequestsGetTool,\nRequestsPostTool,\n} from \"../../../tools/requests.js\";\nimport { createJsonAgent, JsonToolkit } from \"../json/json.js\";\n\nexport class RequestsToolkit extends Toolkit {\ntools: Tool[];\n\nconstructor(headers?: Headers) {\nsuper();\nthis.tools = [new RequestsGetTool(headers), new RequestsPostTool(headers)];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/openapi.ts","loc":{"lines":{"from":1,"to":28}}}}],["22",{"pageContent":"class OpenApiToolkit extends RequestsToolkit {\nconstructor(jsonSpec: JsonSpec, llm: BaseLanguageModel, headers?: Headers) {\nsuper(headers);\nconst jsonAgent = createJsonAgent(llm, new JsonToolkit(jsonSpec));\nthis.tools = [\n...this.tools,\nnew DynamicTool({\nname: \"json_explorer\",\nfunc: async (input: string) => {\nconst result = await jsonAgent.call({ input });\nreturn result.output as string;\n},\ndescription: JSON_EXPLORER_DESCRIPTION,\n}),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/openapi.ts","loc":{"lines":{"from":78,"to":94}}}}],["23",{"pageContent":"function createOpenApiAgent(\nllm: BaseLanguageModel,\nopenApiToolkit: OpenApiToolkit,\nargs?: ZeroShotCreatePromptArgs\n) {\nconst {\nprefix = OPENAPI_PREFIX,\nsuffix = OPENAPI_SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\n} = args ?? {};\n\nconst { tools } = openApiToolkit;\nconst prompt = ZeroShotAgent.createPrompt(tools, {\nprefix,\nsuffix,\ninputVariables,\n});\nconst chain = new LLMChain({\nprompt,\nllm,\n});\nconst toolNames = tools.map((tool) => tool.name);\nconst agent = new ZeroShotAgent({ llmChain: chain, allowedTools: toolNames });\nreturn AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/openapi.ts","loc":{"lines":{"from":158,"to":186}}}}],["24",{"pageContent":"export const OPENAPI_PREFIX = `You are an agent designed to answer questions by making web requests to an API given the OpenAPI spec.\n\nIf the question does not seem related to the API, return I don't know. Do not make up an answer.\nOnly use information provided by the tools to construct your response.\n\nTo find information in the OpenAPI spec, use the 'json_explorer' tool. The input to this tool is a question about the API.\n\nTake the following steps:\nFirst, find the base URL needed to make the request.\n\nSecond, find the relevant paths needed to answer the question. Take note that, sometimes, you might need to make more than one request to more than one path to answer the question.\n\nThird, find the required parameters needed to make the request. For GET requests, these are usually URL parameters and for POST requests, these are request body parameters.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/prompt.ts","loc":{"lines":{"from":1,"to":13}}}}],["25",{"pageContent":"Fourth, make the requests needed to answer the question. Ensure that you are sending the correct parameters to the request by checking which parameters are required. For parameters with a fixed set of values, please use the spec to look at which values are allowed.\n\nUse the exact parameter names as listed in the spec, do not make up any names or abbreviate the names of parameters.\nIf you get a not found error, ensure that you are using a path that actually exists in the spec.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/prompt.ts","loc":{"lines":{"from":15,"to":18}}}}],["26",{"pageContent":"const OPENAPI_SUFFIX = `Begin!\"\n\nQuestion: {input}\nThought: I should explore the spec to find the base url for the API.\n{agent_scratchpad}`;\nexport const JSON_EXPLORER_DESCRIPTION = `\nCan be used to answer questions about the openapi spec for the API. Always use this tool before trying to make a request. \nExample inputs to this tool: \n'What are the required query parameters for a GET request to the /bar endpoint?'\n'What are the required parameters in the request body for a POST request to the /foo endpoint?'\nAlways give this tool a specific question.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/openapi/prompt.ts","loc":{"lines":{"from":30,"to":40}}}}],["27",{"pageContent":"export const SQL_PREFIX = `You are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results using the LIMIT clause.\nYou can order the results by a relevant column to return the most interesting examples in the database.\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\nYou have access to tools for interacting with the database.\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\nYou MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/sql/prompt.ts","loc":{"lines":{"from":1,"to":10}}}}],["28",{"pageContent":"If the question does not seem related to the database, just return \"I don't know\" as the answer.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/sql/prompt.ts","loc":{"lines":{"from":12,"to":12}}}}],["29",{"pageContent":"const SQL_SUFFIX = `Begin!\n\nQuestion: {input}\nThought: I should look at the tables in the database to see what I can query.\n{agent_scratchpad}`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/sql/prompt.ts","loc":{"lines":{"from":14,"to":18}}}}],["30",{"pageContent":"import { Tool } from \"../../../tools/base.js\";\nimport {\nInfoSqlTool,\nListTablesSqlTool,\nQueryCheckerTool,\nQuerySqlTool,\n} from \"../../../tools/sql.js\";\nimport { Toolkit } from \"../base.js\";\nimport { BaseLanguageModel } from \"../../../base_language/index.js\";\nimport { SQL_PREFIX, SQL_SUFFIX } from \"./prompt.js\";\nimport { renderTemplate } from \"../../../prompts/template.js\";\nimport { LLMChain } from \"../../../chains/llm_chain.js\";\nimport { ZeroShotAgent, ZeroShotCreatePromptArgs } from \"../../mrkl/index.js\";\nimport { AgentExecutor } from \"../../executor.js\";\nimport { SqlDatabase } from \"../../../sql_db.js\";\n\nexport interface SqlCreatePromptArgs extends ZeroShotCreatePromptArgs {\n/** Number of results to return. */\ntopK?: number;\n}\n\nexport class SqlToolkit extends Toolkit {\ntools: Tool[];\n\ndb: SqlDatabase;\n\ndialect = \"sqlite\";\n\nconstructor(db: SqlDatabase) {\nsuper();\nthis.db = db;\nthis.tools = [\nnew QuerySqlTool(db),\nnew InfoSqlTool(db),\nnew ListTablesSqlTool(db),\nnew QueryCheckerTool(),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/sql/sql.ts","loc":{"lines":{"from":1,"to":39}}}}],["31",{"pageContent":"function createSqlAgent(\nllm: BaseLanguageModel,\ntoolkit: SqlToolkit,\nargs?: SqlCreatePromptArgs\n) {\nconst {\nprefix = SQL_PREFIX,\nsuffix = SQL_SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\ntopK = 10,\n} = args ?? {};\nconst { tools } = toolkit;\nconst formattedPrefix = renderTemplate(prefix, \"f-string\", {\ndialect: toolkit.dialect,\ntop_k: topK,\n});\n\nconst prompt = ZeroShotAgent.createPrompt(tools, {\nprefix: formattedPrefix,\nsuffix,\ninputVariables,\n});\nconst chain = new LLMChain({ prompt, llm });\nconst agent = new ZeroShotAgent({\nllmChain: chain,\nallowedTools: tools.map((t) => t.name),\n});\nreturn AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/sql/sql.ts","loc":{"lines":{"from":77,"to":109}}}}],["32",{"pageContent":"export const VECTOR_PREFIX = `You are an agent designed to answer questions about sets of documents.\nYou have access to tools for interacting with the documents, and the inputs to the tools are questions.\nSometimes, you will be asked to provide sources for your questions, in which case you should use the appropriate tool to do so.\nIf the question does not seem relevant to any of the tools provided, just return \"I don't know\" as the answer.`;\n\nexport const VECTOR_ROUTER_PREFIX = `You are an agent designed to answer questions.\nYou have access to tools for interacting with different sources, and the inputs to the tools are questions.\nYour main task is to decide which of the tools is relevant for answering question at hand.\nFor complex questions, you can break the question down into sub questions and use tools to answers the sub questions.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/prompt.ts","loc":{"lines":{"from":1,"to":9}}}}],["33",{"pageContent":"import { Tool } from \"../../../tools/base.js\";\nimport { VectorStoreQATool } from \"../../../tools/vectorstore.js\";\nimport { VectorStore } from \"../../../vectorstores/base.js\";\nimport { Toolkit } from \"../base.js\";\nimport { BaseLanguageModel } from \"../../../base_language/index.js\";\nimport { ZeroShotCreatePromptArgs, ZeroShotAgent } from \"../../mrkl/index.js\";\nimport { VECTOR_PREFIX, VECTOR_ROUTER_PREFIX } from \"./prompt.js\";\nimport { SUFFIX } from \"../../mrkl/prompt.js\";\nimport { LLMChain } from \"../../../chains/llm_chain.js\";\nimport { AgentExecutor } from \"../../executor.js\";\n\nexport interface VectorStoreInfo {\nvectorStore: VectorStore;\nname: string;\ndescription: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/vectorstore.ts","loc":{"lines":{"from":1,"to":16}}}}],["34",{"pageContent":"class VectorStoreToolkit extends Toolkit {\ntools: Tool[];\n\nllm: BaseLanguageModel;\n\nconstructor(vectorStoreInfo: VectorStoreInfo, llm: BaseLanguageModel) {\nsuper();\nconst description = VectorStoreQATool.getDescription(\nvectorStoreInfo.name,\nvectorStoreInfo.description\n);\nthis.llm = llm;\nthis.tools = [\nnew VectorStoreQATool(vectorStoreInfo.name, description, {\nvectorStore: vectorStoreInfo.vectorStore,\nllm: this.llm,\n}),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/vectorstore.ts","loc":{"lines":{"from":118,"to":137}}}}],["35",{"pageContent":"class VectorStoreRouterToolkit extends Toolkit {\ntools: Tool[];\n\nvectorStoreInfos: VectorStoreInfo[];\n\nllm: BaseLanguageModel;\n\nconstructor(vectorStoreInfos: VectorStoreInfo[], llm: BaseLanguageModel) {\nsuper();\nthis.llm = llm;\nthis.vectorStoreInfos = vectorStoreInfos;\nthis.tools = vectorStoreInfos.map((vectorStoreInfo) => {\nconst description = VectorStoreQATool.getDescription(\nvectorStoreInfo.name,\nvectorStoreInfo.description\n);\nreturn new VectorStoreQATool(vectorStoreInfo.name, description, {\nvectorStore: vectorStoreInfo.vectorStore,\nllm: this.llm,\n});\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/vectorstore.ts","loc":{"lines":{"from":246,"to":268}}}}],["36",{"pageContent":"function createVectorStoreAgent(\nllm: BaseLanguageModel,\ntoolkit: VectorStoreToolkit,\nargs?: ZeroShotCreatePromptArgs\n) {\nconst {\nprefix = VECTOR_PREFIX,\nsuffix = SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\n} = args ?? {};\nconst { tools } = toolkit;\nconst prompt = ZeroShotAgent.createPrompt(tools, {\nprefix,\nsuffix,\ninputVariables,\n});\nconst chain = new LLMChain({ prompt, llm });\nconst agent = new ZeroShotAgent({\nllmChain: chain,\nallowedTools: tools.map((t) => t.name),\n});\nreturn AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/vectorstore.ts","loc":{"lines":{"from":375,"to":401}}}}],["37",{"pageContent":"function createVectorStoreRouterAgent(\nllm: BaseLanguageModel,\ntoolkit: VectorStoreRouterToolkit,\nargs?: ZeroShotCreatePromptArgs\n) {\nconst {\nprefix = VECTOR_ROUTER_PREFIX,\nsuffix = SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\n} = args ?? {};\nconst { tools } = toolkit;\nconst prompt = ZeroShotAgent.createPrompt(tools, {\nprefix,\nsuffix,\ninputVariables,\n});\nconst chain = new LLMChain({ prompt, llm });\nconst agent = new ZeroShotAgent({\nllmChain: chain,\nallowedTools: tools.map((t) => t.name),\n});\nreturn AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/vectorstore/vectorstore.ts","loc":{"lines":{"from":508,"to":534}}}}],["38",{"pageContent":"import { Toolkit } from \"../base.js\";\nimport { Tool } from \"../../../tools/base.js\";\nimport { ZapierNLARunAction, ZapierNLAWrapper } from \"../../../tools/zapier.js\";\n\nexport class ZapierToolKit extends Toolkit {\ntools: Tool[] = [];\n\nstatic async fromZapierNLAWrapper(\nzapierNLAWrapper: ZapierNLAWrapper\n): Promise<ZapierToolKit> {\nconst toolkit = new ZapierToolKit();\nconst actions = await zapierNLAWrapper.listActions();\nfor (const action of actions) {\nconst tool = new ZapierNLARunAction(\nzapierNLAWrapper,\naction.id,\naction.description,\naction.params\n);\ntoolkit.tools.push(tool);\n}\nreturn toolkit;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/agent_toolkits/zapier/zapier.ts","loc":{"lines":{"from":1,"to":24}}}}],["39",{"pageContent":"import { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { LLMChain } from \"../../chains/llm_chain.js\";\nimport {\nChatPromptTemplate,\nHumanMessagePromptTemplate,\nSystemMessagePromptTemplate,\n} from \"../../prompts/chat.js\";\nimport { AgentStep } from \"../../schema/index.js\";\nimport { Tool } from \"../../tools/base.js\";\nimport { Optional } from \"../../types/type-utils.js\";\nimport { Agent, AgentArgs, OutputParserArgs } from \"../agent.js\";\nimport { AgentInput } from \"../types.js\";\nimport { ChatAgentOutputParser } from \"./outputParser.js\";\nimport { FORMAT_INSTRUCTIONS, PREFIX, SUFFIX } from \"./prompt.js\";\n\nexport interface ChatCreatePromptArgs {\n/** String to put after the list of tools. */\nsuffix?: string;\n/** String to put before the list of tools. */\nprefix?: string;\n/** List of input variables the final prompt will expect. */\ninputVariables?: string[];\n}\n\nexport type ChatAgentInput = Optional<AgentInput, \"outputParser\">;\n\n/**\n* Agent for the MRKL chain.\n* @augments Agent\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/index.ts","loc":{"lines":{"from":1,"to":30}}}}],["40",{"pageContent":"class ChatAgent extends Agent {\nconstructor(input: ChatAgentInput) {\nconst outputParser =\ninput?.outputParser ?? ChatAgent.getDefaultOutputParser();\nsuper({ ...input, outputParser });\n}\n\n_agentType() {\nreturn \"chat-zero-shot-react-description\" as const;\n}\n\nobservationPrefix() {\nreturn \"Observation: \";\n}\n\nllmPrefix() {\nreturn \"Thought:\";\n}\n\n_stop(): string[] {\nreturn [\"Observation:\"];\n}\n\nstatic validateTools(tools: Tool[]) {\nconst invalidTool = tools.find((tool) => !tool.description);\nif (invalidTool) {\nconst msg =\n`Got a tool ${invalidTool.name} without a description.` +\n` This agent requires descriptions for all tools.`;\nthrow new Error(msg);\n}\n}\n\nstatic getDefaultOutputParser(_fields?: OutputParserArgs) {\nreturn new ChatAgentOutputParser();\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/index.ts","loc":{"lines":{"from":121,"to":156}}}}],["41",{"pageContent":"async constructScratchPad(steps: AgentStep[]): Promise<string> {\nconst agentScratchpad = await super.constructScratchPad(steps);\nif (agentScratchpad) {\nreturn `This was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\n${agentScratchpad}`;\n}\nreturn agentScratchpad;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/index.ts","loc":{"lines":{"from":257,"to":263}}}}],["42",{"pageContent":"/**\n* Create prompt in the style of the zero shot agent.\n*\n* @param tools - List of tools the agent will have access to, used to format the prompt.\n* @param args - Arguments to create the prompt with.\n* @param args.suffix - String to put after the list of tools.\n* @param args.prefix - String to put before the list of tools.\n*/\nstatic createPrompt(tools: Tool[], args?: ChatCreatePromptArgs) {\nconst { prefix = PREFIX, suffix = SUFFIX } = args ?? {};\nconst toolStrings = tools\n.map((tool) => `${tool.name}: ${tool.description}`)\n.join(\"\\n\");\nconst template = [prefix, toolStrings, FORMAT_INSTRUCTIONS, suffix].join(\n\"\\n\\n\"\n);\nconst messages = [\nSystemMessagePromptTemplate.fromTemplate(template),\nHumanMessagePromptTemplate.fromTemplate(\"{input}\\n\\n{agent_scratchpad}\"),\n];\nreturn ChatPromptTemplate.fromPromptMessages(messages);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/index.ts","loc":{"lines":{"from":374,"to":395}}}}],["43",{"pageContent":"static fromLLMAndTools(\nllm: BaseLanguageModel,\ntools: Tool[],\nargs?: ChatCreatePromptArgs & AgentArgs\n) {\nChatAgent.validateTools(tools);\nconst prompt = ChatAgent.createPrompt(tools, args);\nconst chain = new LLMChain({\nprompt,\nllm,\ncallbacks: args?.callbacks ?? args?.callbackManager,\n});\nconst outputParser =\nargs?.outputParser ?? ChatAgent.getDefaultOutputParser();\n\nreturn new ChatAgent({\nllmChain: chain,\noutputParser,\nallowedTools: tools.map((t) => t.name),\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/index.ts","loc":{"lines":{"from":494,"to":515}}}}],["44",{"pageContent":"import { AgentActionOutputParser } from \"../types.js\";\nimport { AgentFinish } from \"../../schema/index.js\";\nimport { FORMAT_INSTRUCTIONS } from \"./prompt.js\";\n\nexport const FINAL_ANSWER_ACTION = \"Final Answer:\";\nexport class ChatAgentOutputParser extends AgentActionOutputParser {\nasync parse(text: string) {\nif (text.includes(FINAL_ANSWER_ACTION)) {\nconst parts = text.split(FINAL_ANSWER_ACTION);\nconst output = parts[parts.length - 1].trim();\nreturn { returnValues: { output }, log: text } satisfies AgentFinish;\n}\n\n// eslint-disable-next-line @typescript-eslint/no-unused-vars\nconst [_, action, __] = text.split(/```(?:json)?/g);\ntry {\nconst response = JSON.parse(action.trim());\nreturn {\ntool: response.action,\ntoolInput: response.action_input,\nlog: text,\n};\n} catch {\nthrow new Error(\n`Unable to parse JSON response from chat agent.\\n\\n${text}`\n);\n}\n}\n\ngetFormatInstructions(): string {\nreturn FORMAT_INSTRUCTIONS;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/outputParser.ts","loc":{"lines":{"from":1,"to":33}}}}],["45",{"pageContent":"export const PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;\nexport const FORMAT_INSTRUCTIONS = `The way you use the tools is by specifying a json blob, denoted below by $JSON_BLOB\nSpecifically, this $JSON_BLOB should have a \"action\" key (with the name of the tool to use) and a \"action_input\" key (with the input to the tool going here). \nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n\n\\`\\`\\`\n{{\n\"action\": \"calculator\",\n\"action_input\": \"1 + 2\"\n}}\n\\`\\`\\`\n\nALWAYS use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: \n\\`\\`\\`\n$JSON_BLOB\n\\`\\`\\`\nObservation: the result of the action\n... (this Thought/Action/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/prompt.ts","loc":{"lines":{"from":1,"to":24}}}}],["46",{"pageContent":"const SUFFIX = `Begin! Reminder to always use the exact characters \\`Final Answer\\` when responding.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat/prompt.ts","loc":{"lines":{"from":25,"to":25}}}}],["47",{"pageContent":"import { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { LLMChain } from \"../../chains/llm_chain.js\";\nimport {\nChatPromptTemplate,\nHumanMessagePromptTemplate,\nMessagesPlaceholder,\nSystemMessagePromptTemplate,\n} from \"../../prompts/chat.js\";\nimport { renderTemplate } from \"../../prompts/template.js\";\nimport {\nAIChatMessage,\nAgentStep,\nBaseChatMessage,\nHumanChatMessage,\n} from \"../../schema/index.js\";\nimport { Tool } from \"../../tools/base.js\";\nimport { Optional } from \"../../types/type-utils.js\";\nimport { Agent, AgentArgs, OutputParserArgs } from \"../agent.js\";\nimport { AgentActionOutputParser, AgentInput } from \"../types.js\";\nimport { ChatConversationalAgentOutputParser } from \"./outputParser.js\";\nimport {\nPREFIX_END,\nDEFAULT_PREFIX,\nDEFAULT_SUFFIX,\nTEMPLATE_TOOL_RESPONSE,\n} from \"./prompt.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":1,"to":26}}}}],["48",{"pageContent":"interface ChatConversationalCreatePromptArgs {\n/** String to put after the list of tools. */\nsystemMessage?: string;\n/** String to put before the list of tools. */\nhumanMessage?: string;\n/** List of input variables the final prompt will expect. */\ninputVariables?: string[];\n/** Output parser to use for formatting. */\noutputParser?: AgentActionOutputParser;\n}\n\nexport type ChatConversationalAgentInput = Optional<AgentInput, \"outputParser\">;\n\n/**\n* Agent for the MRKL chain.\n* @augments Agent\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":157,"to":173}}}}],["49",{"pageContent":"class ChatConversationalAgent extends Agent {\nconstructor(input: ChatConversationalAgentInput) {\nconst outputParser =\ninput.outputParser ?? ChatConversationalAgent.getDefaultOutputParser();\nsuper({ ...input, outputParser });\n}\n\n_agentType() {\nreturn \"chat-conversational-react-description\" as const;\n}\n\nobservationPrefix() {\nreturn \"Observation: \";\n}\n\nllmPrefix() {\nreturn \"Thought:\";\n}\n\n_stop(): string[] {\nreturn [\"Observation:\"];\n}\n\nstatic validateTools(tools: Tool[]) {\nconst invalidTool = tools.find((tool) => !tool.description);\nif (invalidTool) {\nconst msg =\n`Got a tool ${invalidTool.name} without a description.` +\n` This agent requires descriptions for all tools.`;\nthrow new Error(msg);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":312,"to":343}}}}],["50",{"pageContent":"async constructScratchPad(steps: AgentStep[]): Promise<BaseChatMessage[]> {\nconst thoughts: BaseChatMessage[] = [];\nfor (const step of steps) {\nthoughts.push(new AIChatMessage(step.action.log));\nthoughts.push(\nnew HumanChatMessage(\nrenderTemplate(TEMPLATE_TOOL_RESPONSE, \"f-string\", {\nobservation: step.observation,\n})\n)\n);\n}\nreturn thoughts;\n}\n\nstatic getDefaultOutputParser(\n_fields?: OutputParserArgs\n): AgentActionOutputParser {\nreturn new ChatConversationalAgentOutputParser();\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":479,"to":498}}}}],["51",{"pageContent":"/**\n* Create prompt in the style of the ChatConversationAgent.\n*\n* @param tools - List of tools the agent will have access to, used to format the prompt.\n* @param args - Arguments to create the prompt with.\n* @param args.systemMessage - String to put before the list of tools.\n* @param args.humanMessage - String to put after the list of tools.\n*/\nstatic createPrompt(\ntools: Tool[],\nargs?: ChatConversationalCreatePromptArgs\n) {\nconst systemMessage = (args?.systemMessage ?? DEFAULT_PREFIX) + PREFIX_END;\nconst humanMessage = args?.humanMessage ?? DEFAULT_SUFFIX;\nconst outputParser =\nargs?.outputParser ?? new ChatConversationalAgentOutputParser();\nconst toolStrings = tools\n.map((tool) => `${tool.name}: ${tool.description}`)\n.join(\"\\n\");\nconst formatInstructions = renderTemplate(humanMessage, \"f-string\", {\nformat_instructions: outputParser.getFormatInstructions(),\n});\nconst toolNames = tools.map((tool) => tool.name).join(\"\\n\");\nconst finalPrompt = renderTemplate(formatInstructions, \"f-string\", {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":637,"to":660}}}}],["52",{"pageContent":"tools: toolStrings,\ntool_names: toolNames,\n});\nconst messages = [\nSystemMessagePromptTemplate.fromTemplate(systemMessage),\nnew MessagesPlaceholder(\"chat_history\"),\nHumanMessagePromptTemplate.fromTemplate(finalPrompt),\nnew MessagesPlaceholder(\"agent_scratchpad\"),\n];\nreturn ChatPromptTemplate.fromPromptMessages(messages);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":785,"to":795}}}}],["53",{"pageContent":"static fromLLMAndTools(\nllm: BaseLanguageModel,\ntools: Tool[],\nargs?: ChatConversationalCreatePromptArgs & AgentArgs\n) {\nChatConversationalAgent.validateTools(tools);\nconst prompt = ChatConversationalAgent.createPrompt(tools, args);\nconst chain = new LLMChain({\nprompt,\nllm,\ncallbacks: args?.callbacks ?? args?.callbackManager,\n});\nconst outputParser =\nargs?.outputParser ?? ChatConversationalAgent.getDefaultOutputParser();\n\nreturn new ChatConversationalAgent({\nllmChain: chain,\noutputParser,\nallowedTools: tools.map((t) => t.name),\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/index.ts","loc":{"lines":{"from":942,"to":963}}}}],["54",{"pageContent":"import { AgentActionOutputParser } from \"../types.js\";\nimport { FORMAT_INSTRUCTIONS } from \"./prompt.js\";\n\nexport class ChatConversationalAgentOutputParser extends AgentActionOutputParser {\nasync parse(text: string) {\nlet jsonOutput = text.trim();\nif (jsonOutput.includes(\"```json\")) {\njsonOutput = jsonOutput.split(\"```json\")[1].trimStart();\n} else if (jsonOutput.includes(\"```\")) {\nconst firstIndex = jsonOutput.indexOf(\"```\");\njsonOutput = jsonOutput.slice(firstIndex + 3).trimStart();\n}\nconst lastIndex = jsonOutput.lastIndexOf(\"```\");\nif (lastIndex !== -1) {\njsonOutput = jsonOutput.slice(0, lastIndex).trimEnd();\n}\n\nconst response = JSON.parse(jsonOutput);\n\nconst { action, action_input } = response;\n\nif (action === \"Final Answer\") {\nreturn { returnValues: { output: action_input }, log: text };\n}\nreturn { tool: action, toolInput: action_input, log: text };\n}\n\ngetFormatInstructions(): string {\nreturn FORMAT_INSTRUCTIONS;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/outputParser.ts","loc":{"lines":{"from":1,"to":31}}}}],["55",{"pageContent":"export const DEFAULT_PREFIX = `Assistant is a large language model trained by OpenAI.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/prompt.ts","loc":{"lines":{"from":1,"to":5}}}}],["56",{"pageContent":"Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/prompt.ts","loc":{"lines":{"from":7,"to":7}}}}],["57",{"pageContent":"const PREFIX_END = ` However, above all else, all responses must adhere to the format of RESPONSE FORMAT INSTRUCTIONS.`;\n\nexport const FORMAT_INSTRUCTIONS = `RESPONSE FORMAT INSTRUCTIONS\n----------------------------\n\nWhen responding to me please, please output a response in one of two formats:\n\n**Option 1:**\nUse this if you want the human to use a tool.\nMarkdown code snippet formatted in the following schema:\n\n\\`\\`\\`json\n{{{{\n\"action\": string \\\\ The action to take. Must be one of {tool_names}\n\"action_input\": string \\\\ The input to the action\n}}}}\n\\`\\`\\`\n\n**Option #2:**\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n\n\\`\\`\\`json\n{{{{\n\"action\": \"Final Answer\",\n\"action_input\": string \\\\ You should put what you want to return to use here and make sure to use valid json newline characters.\n}}}}\n\\`\\`\\``;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/prompt.ts","loc":{"lines":{"from":58,"to":84}}}}],["58",{"pageContent":"const DEFAULT_SUFFIX = `TOOLS\n------\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n\n{{tools}}\n\n{format_instructions}\n\nUSER'S INPUT\n--------------------\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n\n{{{{input}}}}`;\n\nexport const TEMPLATE_TOOL_RESPONSE = `TOOL RESPONSE:\n---------------------\n{observation}\n\nUSER'S INPUT\n--------------------\n\nOkay, so what is the response to my original question? If using information from tools, you must say it explicitly - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/chat_convo/prompt.ts","loc":{"lines":{"from":116,"to":137}}}}],["59",{"pageContent":"import { BaseChain, ChainInputs } from \"../chains/base.js\";\nimport { BaseMultiActionAgent, BaseSingleActionAgent } from \"./agent.js\";\nimport { Tool } from \"../tools/base.js\";\nimport { StoppingMethod } from \"./types.js\";\nimport { SerializedLLMChain } from \"../chains/serde.js\";\nimport {\nAgentAction,\nAgentFinish,\nAgentStep,\nChainValues,\n} from \"../schema/index.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\nexport interface AgentExecutorInput extends ChainInputs {\nagent: BaseSingleActionAgent | BaseMultiActionAgent;\ntools: Tool[];\nreturnIntermediateSteps?: boolean;\nmaxIterations?: number;\nearlyStoppingMethod?: StoppingMethod;\n}\n\n/**\n* A chain managing an agent using tools.\n* @augments BaseChain\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/executor.ts","loc":{"lines":{"from":1,"to":25}}}}],["60",{"pageContent":"class AgentExecutor extends BaseChain {\nagent: BaseSingleActionAgent | BaseMultiActionAgent;\n\ntools: Tool[];\n\nreturnIntermediateSteps = false;\n\nmaxIterations?: number = 15;\n\nearlyStoppingMethod: StoppingMethod = \"force\";\n\nget inputKeys() {\nreturn this.agent.inputKeys;\n}\n\nget outputKeys() {\nreturn this.agent.returnValues;\n}\n\nconstructor(input: AgentExecutorInput) {\nsuper(\ninput.memory,\ninput.verbose,\ninput.callbacks ?? input.callbackManager\n);\nthis.agent = input.agent;\nthis.tools = input.tools;\nif (this.agent._agentActionType() === \"multi\") {\nfor (const tool of this.tools) {\nif (tool.returnDirect) {\nthrow new Error(\n`Tool with return direct ${tool.name} not supported for multi-action agent.`\n);\n}\n}\n}\nthis.returnIntermediateSteps =\ninput.returnIntermediateSteps ?? this.returnIntermediateSteps;\nthis.maxIterations = input.maxIterations ?? this.maxIterations;\nthis.earlyStoppingMethod =\ninput.earlyStoppingMethod ?? this.earlyStoppingMethod;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/executor.ts","loc":{"lines":{"from":163,"to":204}}}}],["61",{"pageContent":"/** Create from agent and a list of tools. */\nstatic fromAgentAndTools(fields: AgentExecutorInput): AgentExecutor {\nreturn new AgentExecutor(fields);\n}\n\nprivate shouldContinue(iterations: number): boolean {\nreturn this.maxIterations === undefined || iterations < this.maxIterations;\n}\n\n/** @ignore */\nasync _call(\ninputs: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nconst toolsByName = Object.fromEntries(\nthis.tools.map((t) => [t.name.toLowerCase(), t])\n);\nconst steps: AgentStep[] = [];\nlet iterations = 0;\n\nconst getOutput = async (finishStep: AgentFinish) => {\nconst { returnValues } = finishStep;\nconst additional = await this.agent.prepareForOutput(returnValues, steps);\n\nif (this.returnIntermediateSteps) {\nreturn { ...returnValues, intermediateSteps: steps, ...additional };\n}\nawait runManager?.handleAgentEnd(finishStep);\nreturn { ...returnValues, ...additional };\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/executor.ts","loc":{"lines":{"from":331,"to":360}}}}],["62",{"pageContent":"while (this.shouldContinue(iterations)) {\nconst output = await this.agent.plan(\nsteps,\ninputs,\nrunManager?.getChild()\n);\n// Check if the agent has finished\nif (\"returnValues\" in output) {\nreturn getOutput(output);\n}\n\nlet actions: AgentAction[];\nif (Array.isArray(output)) {\nactions = output as AgentAction[];\n} else {\nactions = [output as AgentAction];\n}\n\nconst newSteps = await Promise.all(\nactions.map(async (action) => {\nawait runManager?.handleAgentAction(action);\n\nconst tool = toolsByName[action.tool?.toLowerCase()];\nconst observation = tool\n? await tool.call(action.toolInput, runManager?.getChild())\n: `${action.tool} is not a valid tool, try another one.`;\n\nreturn { action, observation };\n})\n);\n\nsteps.push(...newSteps);\n\nconst lastStep = steps[steps.length - 1];\nconst lastTool = toolsByName[lastStep.action.tool?.toLowerCase()];\n\nif (lastTool?.returnDirect) {\nreturn getOutput({\nreturnValues: { [this.agent.returnValues[0]]: lastStep.observation },\nlog: \"\",\n});\n}\n\niterations += 1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/executor.ts","loc":{"lines":{"from":489,"to":533}}}}],["63",{"pageContent":"const finish = await this.agent.returnStoppedResponse(\nthis.earlyStoppingMethod,\nsteps,\ninputs\n);\n\nreturn getOutput(finish);\n}\n\n_chainType() {\nreturn \"agent_executor\" as const;\n}\n\nserialize(): SerializedLLMChain {\nthrow new Error(\"Cannot serialize an AgentExecutor\");\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/executor.ts","loc":{"lines":{"from":658,"to":674}}}}],["64",{"pageContent":"import type { SerializedAgentT, AgentInput } from \"./types.js\";\nimport { Tool } from \"../tools/base.js\";\nimport { LLMChain } from \"../chains/llm_chain.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/helpers.ts","loc":{"lines":{"from":1,"to":4}}}}],["65",{"pageContent":"const deserializeHelper = async <\nT extends string,\nU extends Record<string, unknown>,\nV extends AgentInput,\nZ\n>(\nllm: BaseLanguageModel | undefined,\ntools: Tool[] | undefined,\ndata: SerializedAgentT<T, U, V>,\nfromLLMAndTools: (llm: BaseLanguageModel, tools: Tool[], args: U) => Z,\nfromConstructor: (args: V) => Z\n): Promise<Z> => {\nif (data.load_from_llm_and_tools) {\nif (!llm) {\nthrow new Error(\"Loading from llm and tools, llm must be provided.\");\n}\n\nif (!tools) {\nthrow new Error(\"Loading from llm and tools, tools must be provided.\");\n}\n\nreturn fromLLMAndTools(llm, tools, data);\n}\nif (!data.llm_chain) {\nthrow new Error(\"Loading from constructor, llm_chain must be provided.\");\n}\n\nconst llmChain = await LLMChain.deserialize(data.llm_chain);\nreturn fromConstructor({ ...data, llmChain });\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/helpers.ts","loc":{"lines":{"from":35,"to":64}}}}],["66",{"pageContent":"export {\nAgent,\nAgentArgs,\nBaseSingleActionAgent,\nLLMSingleActionAgent,\nLLMSingleActionAgentInput,\nOutputParserArgs,\n} from \"./agent.js\";\nexport {\nJsonToolkit,\nOpenApiToolkit,\nRequestsToolkit,\nSqlToolkit,\nVectorStoreInfo,\nVectorStoreRouterToolkit,\nVectorStoreToolkit,\nZapierToolKit,\ncreateJsonAgent,\ncreateOpenApiAgent,\ncreateSqlAgent,\nSqlCreatePromptArgs,\ncreateVectorStoreAgent,\n} from \"./agent_toolkits/index.js\";\nexport { Toolkit } from \"./agent_toolkits/base.js\";\nexport {\nChatAgent,\nChatAgentInput,\nChatCreatePromptArgs,\n} from \"./chat/index.js\";\nexport { ChatAgentOutputParser } from \"./chat/outputParser.js\";\nexport {\nChatConversationalAgent,\nChatConversationalAgentInput,\nChatConversationalCreatePromptArgs,\n} from \"./chat_convo/index.js\";\nexport { ChatConversationalAgentOutputParser } from \"./chat_convo/outputParser.js\";\nexport { AgentExecutor, AgentExecutorInput } from \"./executor.js\";\nexport {\ninitializeAgentExecutor,\ninitializeAgentExecutorWithOptions,\nInitializeAgentExecutorOptions,\n} from \"./initialize.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/index.ts","loc":{"lines":{"from":1,"to":42}}}}],["67",{"pageContent":"{\nZeroShotAgent,\nZeroShotAgentInput,\nZeroShotCreatePromptArgs,\n} from \"./mrkl/index.js\";\nexport { ZeroShotAgentOutputParser } from \"./mrkl/outputParser.js\";\nexport {\nAgentActionOutputParser,\nAgentInput,\nSerializedAgent,\nSerializedAgentT,\nSerializedZeroShotAgent,\nStoppingMethod,\n} from \"./types.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/index.ts","loc":{"lines":{"from":57,"to":70}}}}],["68",{"pageContent":"import { BaseLanguageModel } from \"../base_language/index.js\";\nimport { CallbackManager } from \"../callbacks/manager.js\";\nimport { BufferMemory } from \"../memory/buffer_memory.js\";\nimport { Tool } from \"../tools/base.js\";\nimport { ChatAgent } from \"./chat/index.js\";\nimport { ChatConversationalAgent } from \"./chat_convo/index.js\";\nimport { AgentExecutor, AgentExecutorInput } from \"./executor.js\";\nimport { ZeroShotAgent } from \"./mrkl/index.js\";\n\ntype AgentType =\n| \"zero-shot-react-description\"\n| \"chat-zero-shot-react-description\"\n| \"chat-conversational-react-description\";\n\n/**\n* @deprecated use initializeAgentExecutorWithOptions instead\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":1,"to":17}}}}],["69",{"pageContent":"const initializeAgentExecutor = async (\ntools: Tool[],\nllm: BaseLanguageModel,\n_agentType?: AgentType,\n_verbose?: boolean,\n_callbackManager?: CallbackManager\n): Promise<AgentExecutor> => {\nconst agentType = _agentType ?? \"zero-shot-react-description\";\nconst verbose = _verbose;\nconst callbackManager = _callbackManager;\nswitch (agentType) {\ncase \"zero-shot-react-description\":\nreturn AgentExecutor.fromAgentAndTools({\nagent: ZeroShotAgent.fromLLMAndTools(llm, tools),\ntools,\nreturnIntermediateSteps: true,\nverbose,\ncallbackManager,\n});\ncase \"chat-zero-shot-react-description\":\nreturn AgentExecutor.fromAgentAndTools({\nagent: ChatAgent.fromLLMAndTools(llm, tools),\ntools,\nreturnIntermediateSteps: true,\nverbose,\ncallbackManager,\n});\ncase \"chat-conversational-react-description\":\nreturn AgentExecutor.fromAgentAndTools({\nagent: ChatConversationalAgent.fromLLMAndTools(llm, tools),\ntools,\nverbose,\ncallbackManager,\n});\ndefault:\nthrow new Error(\"Unknown agent type\");\n}\n};\n\n/**\n* @interface\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":131,"to":172}}}}],["70",{"pageContent":"type InitializeAgentExecutorOptions =\n| ({\nagentType: \"zero-shot-react-description\";\nagentArgs?: Parameters<typeof ZeroShotAgent.fromLLMAndTools>[2];\nmemory?: never;\n} & Omit<AgentExecutorInput, \"agent\" | \"tools\">)\n| ({\nagentType: \"chat-zero-shot-react-description\";\nagentArgs?: Parameters<typeof ChatAgent.fromLLMAndTools>[2];\nmemory?: never;\n} & Omit<AgentExecutorInput, \"agent\" | \"tools\">)\n| ({\nagentType: \"chat-conversational-react-description\";\nagentArgs?: Parameters<typeof ChatConversationalAgent.fromLLMAndTools>[2];\n} & Omit<AgentExecutorInput, \"agent\" | \"tools\">);\n\n/**\n* Initialize an agent executor with options\n* @param tools Array of tools to use in the agent\n* @param llm LLM or ChatModel to use in the agent\n* @param options Options for the agent, including agentType, agentArgs, and other options for AgentExecutor.fromAgentAndTools\n* @returns AgentExecutor\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":275,"to":297}}}}],["71",{"pageContent":"const initializeAgentExecutorWithOptions = async (\ntools: Tool[],\nllm: BaseLanguageModel,\noptions: InitializeAgentExecutorOptions = {\nagentType:\nllm._modelType() === \"base_chat_model\"\n? \"chat-zero-shot-react-description\"\n: \"zero-shot-react-description\",\n}\n): Promise<AgentExecutor> => {\nswitch (options.agentType) {\ncase \"zero-shot-react-description\": {\nconst { agentArgs, ...rest } = options;\nreturn AgentExecutor.fromAgentAndTools({\nagent: ZeroShotAgent.fromLLMAndTools(llm, tools, agentArgs),\ntools,\n...rest,\n});\n}\ncase \"chat-zero-shot-react-description\": {\nconst { agentArgs, ...rest } = options;\nreturn AgentExecutor.fromAgentAndTools({\nagent: ChatAgent.fromLLMAndTools(llm, tools, agentArgs),\ntools,\n...rest,\n});\n}\ncase \"chat-conversational-react-description\": {\nconst { agentArgs, memory, ...rest } = options;\nconst executor = AgentExecutor.fromAgentAndTools({\nagent: ChatConversationalAgent.fromLLMAndTools(llm, tools, agentArgs),\ntools,\nmemory:\nmemory ??\nnew BufferMemory({\nreturnMessages: true,\nmemoryKey: \"chat_history\",","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":402,"to":438}}}}],["72",{"pageContent":"inputKey: \"input\",\n}),\n...rest,\n});\nreturn executor;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":539,"to":544}}}}],["73",{"pageContent":": {\nthrow new Error(\"Unknown agent type\");\n}\n}\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/initialize.ts","loc":{"lines":{"from":673,"to":677}}}}],["74",{"pageContent":"import { Agent } from \"./agent.js\";\nimport { Tool } from \"../tools/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { loadFromHub } from \"../util/hub.js\";\nimport { FileLoader, loadFromFile } from \"../util/load.js\";\nimport { parseFileConfig } from \"../util/parse.js\";\n\nconst loadAgentFromFile: FileLoader<Agent> = async (\nfile: string,\npath: string,\nllmAndTools?: { llm?: BaseLanguageModel; tools?: Tool[] }\n) => {\nconst serialized = parseFileConfig(file, path);\nreturn Agent.deserialize({ ...serialized, ...llmAndTools });\n};\n\nexport const loadAgent = async (\nuri: string,\nllmAndTools?: { llm?: BaseLanguageModel; tools?: Tool[] }\n): Promise<Agent> => {\nconst hubResult = await loadFromHub(\nuri,\nloadAgentFromFile,\n\"agents\",\nnew Set([\"json\", \"yaml\"]),\nllmAndTools\n);\nif (hubResult) {\nreturn hubResult;\n}\n\nreturn loadFromFile(uri, loadAgentFromFile, llmAndTools);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/load.ts","loc":{"lines":{"from":1,"to":33}}}}],["75",{"pageContent":"import { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { LLMChain } from \"../../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\nimport { renderTemplate } from \"../../prompts/template.js\";\nimport { Tool } from \"../../tools/base.js\";\nimport { Optional } from \"../../types/type-utils.js\";\nimport { Agent, AgentArgs, OutputParserArgs } from \"../agent.js\";\nimport { deserializeHelper } from \"../helpers.js\";\nimport {\nAgentInput,\nSerializedFromLLMAndTools,\nSerializedZeroShotAgent,\n} from \"../types.js\";\nimport { ZeroShotAgentOutputParser } from \"./outputParser.js\";\nimport { FORMAT_INSTRUCTIONS, PREFIX, SUFFIX } from \"./prompt.js\";\n\nexport interface ZeroShotCreatePromptArgs {\n/** String to put after the list of tools. */\nsuffix?: string;\n/** String to put before the list of tools. */\nprefix?: string;\n/** List of input variables the final prompt will expect. */\ninputVariables?: string[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/index.ts","loc":{"lines":{"from":1,"to":24}}}}],["76",{"pageContent":"type ZeroShotAgentInput = Optional<AgentInput, \"outputParser\">;\n\n/**\n* Agent for the MRKL chain.\n* @augments Agent\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/index.ts","loc":{"lines":{"from":144,"to":149}}}}],["77",{"pageContent":"class ZeroShotAgent extends Agent {\nconstructor(input: ZeroShotAgentInput) {\nconst outputParser =\ninput?.outputParser ?? ZeroShotAgent.getDefaultOutputParser();\nsuper({ ...input, outputParser });\n}\n\n_agentType() {\nreturn \"zero-shot-react-description\" as const;\n}\n\nobservationPrefix() {\nreturn \"Observation: \";\n}\n\nllmPrefix() {\nreturn \"Thought:\";\n}\n\nstatic getDefaultOutputParser(fields?: OutputParserArgs) {\nreturn new ZeroShotAgentOutputParser(fields);\n}\n\nstatic validateTools(tools: Tool[]) {\nconst invalidTool = tools.find((tool) => !tool.description);\nif (invalidTool) {\nconst msg =\n`Got a tool ${invalidTool.name} without a description.` +\n` This agent requires descriptions for all tools.`;\nthrow new Error(msg);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/index.ts","loc":{"lines":{"from":290,"to":321}}}}],["78",{"pageContent":"/**\n* Create prompt in the style of the zero shot agent.\n*\n* @param tools - List of tools the agent will have access to, used to format the prompt.\n* @param args - Arguments to create the prompt with.\n* @param args.suffix - String to put after the list of tools.\n* @param args.prefix - String to put before the list of tools.\n* @param args.inputVariables - List of input variables the final prompt will expect.\n*/\nstatic createPrompt(tools: Tool[], args?: ZeroShotCreatePromptArgs) {\nconst {\nprefix = PREFIX,\nsuffix = SUFFIX,\ninputVariables = [\"input\", \"agent_scratchpad\"],\n} = args ?? {};\nconst toolStrings = tools\n.map((tool) => `${tool.name}: ${tool.description}`)\n.join(\"\\n\");\n\nconst toolNames = tools.map((tool) => tool.name);\n\nconst formatInstructions = renderTemplate(FORMAT_INSTRUCTIONS, \"f-string\", {\ntool_names: toolNames,\n});\n\nconst template = [prefix, toolStrings, formatInstructions, suffix].join(\n\"\\n\\n\"\n);\n\nreturn new PromptTemplate({\ntemplate,\ninputVariables,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/index.ts","loc":{"lines":{"from":447,"to":480}}}}],["79",{"pageContent":"static fromLLMAndTools(\nllm: BaseLanguageModel,\ntools: Tool[],\nargs?: ZeroShotCreatePromptArgs & AgentArgs\n) {\nZeroShotAgent.validateTools(tools);\nconst prompt = ZeroShotAgent.createPrompt(tools, args);\nconst outputParser =\nargs?.outputParser ?? ZeroShotAgent.getDefaultOutputParser();\nconst chain = new LLMChain({\nprompt,\nllm,\ncallbacks: args?.callbacks ?? args?.callbackManager,\n});\n\nreturn new ZeroShotAgent({\nllmChain: chain,\nallowedTools: tools.map((t) => t.name),\noutputParser,\n});\n}\n\nstatic async deserialize(\ndata: SerializedZeroShotAgent & { llm?: BaseLanguageModel; tools?: Tool[] }\n): Promise<ZeroShotAgent> {\nconst { llm, tools, ...rest } = data;\nreturn deserializeHelper(\nllm,\ntools,\nrest,\n(\nllm: BaseLanguageModel,\ntools: Tool[],\nargs: SerializedFromLLMAndTools\n) =>\nZeroShotAgent.fromLLMAndTools(llm, tools, {\nprefix: args.prefix,\nsuffix: args.suffix,\ninputVariables: args.input_variables,\n}),\n(args) => new ZeroShotAgent(args)\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/index.ts","loc":{"lines":{"from":597,"to":640}}}}],["80",{"pageContent":"import { OutputParserArgs } from \"../agent.js\";\nimport { AgentActionOutputParser } from \"../types.js\";\n\nimport { FORMAT_INSTRUCTIONS } from \"./prompt.js\";\n\nexport const FINAL_ANSWER_ACTION = \"Final Answer:\";\nexport class ZeroShotAgentOutputParser extends AgentActionOutputParser {\nfinishToolName: string;\n\nconstructor(fields?: OutputParserArgs) {\nsuper();\nthis.finishToolName = fields?.finishToolName || FINAL_ANSWER_ACTION;\n}\n\nasync parse(text: string) {\nif (text.includes(this.finishToolName)) {\nconst parts = text.split(this.finishToolName);\nconst output = parts[parts.length - 1].trim();\nreturn {\nreturnValues: { output },\nlog: text,\n};\n}\n\nconst match = /Action: (.*)\\nAction Input: (.*)/s.exec(text);\nif (!match) {\nthrow new Error(`Could not parse LLM output: ${text}`);\n}\n\nreturn {\ntool: match[1].trim(),\ntoolInput: match[2].trim().replace(/^\"+|\"+$/g, \"\") ?? \"\",\nlog: text,\n};\n}\n\ngetFormatInstructions(): string {\nreturn FORMAT_INSTRUCTIONS;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/outputParser.ts","loc":{"lines":{"from":1,"to":40}}}}],["81",{"pageContent":"export const PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;\nexport const FORMAT_INSTRUCTIONS = `Use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question`;\nexport const SUFFIX = `Begin!\n\nQuestion: {input}\nThought:{agent_scratchpad}`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/mrkl/prompt.ts","loc":{"lines":{"from":1,"to":15}}}}],["82",{"pageContent":"/* eslint-disable no-process-env */\nimport { expect, test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { loadAgent } from \"../load.js\";\nimport { AgentExecutor } from \"../index.js\";\nimport { SerpAPI } from \"../../tools/serpapi.js\";\nimport { Calculator } from \"../../tools/calculator.js\";\nimport { initializeAgentExecutorWithOptions } from \"../initialize.js\";\nimport { WebBrowser } from \"../../tools/webbrowser.js\";\nimport { Tool } from \"../../tools/base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/agent.int.test.ts","loc":{"lines":{"from":1,"to":11}}}}],["83",{"pageContent":"test(\"Run agent from hub\", async () => {\nconst model = new OpenAI({ temperature: 0, modelName: \"text-babbage-001\" });\nconst tools: Tool[] = [\nnew SerpAPI(undefined, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n];\nconst agent = await loadAgent(\n\"lc://agents/zero-shot-react-description/agent.json\",\n{ llm: model, tools }\n);\nconst executor = AgentExecutor.fromAgentAndTools({\nagent,\ntools,\nreturnIntermediateSteps: true,\n});\nconst res = await executor.call({\ninput:\n\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\",\n});\nconsole.log(res);\n}, 30000);\n\ntest(\"Run agent locally\", async () => {\nconst model = new OpenAI({ temperature: 0, modelName: \"text-babbage-001\" });\nconst tools = [\nnew SerpAPI(undefined, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/agent.int.test.ts","loc":{"lines":{"from":124,"to":159}}}}],["84",{"pageContent":"const executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);\n}, 30000);\n\ntest(\"Run agent with incorrect api key should throw error\", async () => {\nconst model = new OpenAI({\ntemperature: 0,\nmodelName: \"text-babbage-001\",\nopenAIApiKey: \"invalid\",\n});\nconst tools = [\nnew SerpAPI(undefined, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/agent.int.test.ts","loc":{"lines":{"from":260,"to":293}}}}],["85",{"pageContent":"// Test that the model throws an error\nawait expect(() => model.call(input)).rejects.toThrowError(\n\"Request failed with status code 401\"\n);\n\n// Test that the agent throws the same error\nawait expect(() => executor.call({ input })).rejects.toThrowError(\n\"Request failed with status code 401\"\n);\n}, 10000);\n\ntest(\"Run tool web-browser\", async () => {\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\nnew SerpAPI(process.env.SERPAPI_API_KEY, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\nnew WebBrowser({ model, embeddings: new OpenAIEmbeddings() }),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"zero-shot-react-description\",\nreturnIntermediateSteps: true,\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `What is the word of the day on merriam webster`;\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/agent.int.test.ts","loc":{"lines":{"from":391,"to":425}}}}],["86",{"pageContent":"expect(result.intermediateSteps.length).toEqual(1);\nexpect(result.intermediateSteps[0].action.tool).toEqual(\"web-browser\");\nexpect(result.output).not.toEqual(\"\");\n}, 30000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/agent.int.test.ts","loc":{"lines":{"from":522,"to":525}}}}],["87",{"pageContent":"import { test, jest, expect } from \"@jest/globals\";\nimport LambdaClient from \"@aws-sdk/client-lambda\";\n\nimport { AWSLambda } from \"../../tools/aws_lambda.js\";\n\njest.mock(\"@aws-sdk/client-lambda\", () => ({\nLambdaClient: jest.fn().mockImplementation(() => ({\nsend: jest.fn().mockImplementation(() =>\nPromise.resolve({\nPayload: new TextEncoder().encode(\nJSON.stringify({ body: \"email sent.\" })\n),\n})\n),\n})),\nInvokeCommand: jest.fn().mockImplementation(() => ({})),\n}));\n\ntest(\"AWSLambda invokes the correct lambda function and returns the response.body contents\", async () => {\nif (!LambdaClient) {\n// this is to avoid a linting error. S3Client is mocked above.\n}\n\nconst lambda = new AWSLambda({\nname: \"email-sender\",\ndescription:\n\"Sends an email with the specified content to holtkam2@gmail.com\",\nregion: \"us-east-1\",\naccessKeyId: \"abc123\",\nsecretAccessKey: \"xyz456/1T+PzUZ2fd\",","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/aws_lambda.test.ts","loc":{"lines":{"from":1,"to":30}}}}],["88",{"pageContent":"Name: \"testFunction1\",\n});\n\nconst result = await lambda.call(\"Hello world! This is an email.\");\n\nexpect(result).toBe(\"email sent.\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/aws_lambda.test.ts","loc":{"lines":{"from":39,"to":45}}}}],["89",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { Calculator } from \"../../tools/calculator.js\";\n\ntest(\"Calculator tool, sum\", async () => {\nconst calculator = new Calculator();\nconst result = await calculator.call(\"1 + 1\");\nexpect(result).toBe(\"2\");\n});\n\ntest(\"Calculator tool, product\", async () => {\nconst calculator = new Calculator();\nconst result = await calculator.call(\"2 * 3\");\nexpect(result).toBe(\"6\");\n});\n\ntest(\"Calculator tool, division\", async () => {\nconst calculator = new Calculator();\nconst result = await calculator.call(\"7 /2\");\nexpect(result).toBe(\"3.5\");\n});\n\ntest(\"Calculator tool, exponentiation\", async () => {\nconst calculator = new Calculator();\nconst result = await calculator.call(\"2 ^ 8\");\nexpect(result).toBe(\"256\");\n});\n\ntest(\"Calculator tool, complicated expression\", async () => {\nconst calculator = new Calculator();\nconst result = await calculator.call(\"((2 + 3) * 4) / 2\");\nexpect(result).toBe(\"10\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/calculator.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["90",{"pageContent":"import { test } from \"@jest/globals\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\nimport { BufferMemory } from \"../../memory/index.js\";\nimport { Calculator } from \"../../tools/calculator.js\";\nimport { initializeAgentExecutorWithOptions } from \"../initialize.js\";\n\ntest(\"Run conversational agent with memory\", async () => {\nconst model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });\nconst tools = [new Calculator()];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"chat-conversational-react-description\",\nverbose: true,\nmemory: new BufferMemory({\nreturnMessages: true,\nmemoryKey: \"chat_history\",\ninputKey: \"input\",\n}),\n});\nconsole.log(\"Loaded agent.\");\n\nconst input0 = `\"how is your day going?\"`;\n\nconst result0 = await executor.call({ input: input0 });\n\nconsole.log(`Got output ${result0.output}`);\n\nconst input1 = `\"whats is 9 to the 2nd power?\"`;\n\nconst result1 = await executor.call({ input: input1 });\n\nconsole.log(`Got output ${result1.output}`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_convo_agent.int.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["91",{"pageContent":"const input2 = `\"whats is that result divided by 10?\"`;\n\nconst result2 = await executor.call({ input: input2 });\n\nconsole.log(`Got output ${result2.output}`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_convo_agent.int.test.ts","loc":{"lines":{"from":41,"to":46}}}}],["92",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { ChatConversationalAgentOutputParser } from \"../chat_convo/outputParser.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":1,"to":2}}}}],["93",{"pageContent":"test(\"Can parse JSON with text in front of it\", async () => {\nconst testCases = [\n{","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":64,"to":66}}}}],["94",{"pageContent":"input: `Based on the information from the search, I can provide you with a query to get all the orders for the email \\`example@gmail.com\\`. Here's the query:\\n\\n\\`\\`\\`sql\\nSELECT * FROM orders\\nJOIN users ON users.id = orders.user_id\\nWHERE users.email = 'example@gmail.com'\\n\\`\\`\\`\\n\\nPlease make any necessary modifications depending on your database schema and table structures. Run this query on your database to retrieve the orders made by the specified user.\\n\\n\\`\\`\\`json\\n{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"To get all the orders for a user with the email \\`example@gmail.com\\`, you can use the following query:\\\\n\\\\n\\`\\`\\`\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = 'example@gmail.com'\\\\n\\`\\`\\`\\\\n\\\\nPlease make any necessary modifications depending on your database schema and table structures. Run this query on your database to retrieve the orders made by the specified user.\"\\n}\\n\\`\\`\\``,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":71,"to":71}}}}],["95",{"pageContent":"output: `{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"To get all the orders for a user with the email \\`example@gmail.com\\`, you can use the following query:\\\\n\\\\n\\`\\`\\`\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = 'example@gmail.com'\\\\n\\`\\`\\`\\\\n\\\\nPlease make any necessary modifications depending on your database schema and table structures. Run this query on your database to retrieve the  made by the specifsredroied user.\"\\n}`,\ntool: \"Final Answer\",\ntoolInput: \"To get all the orders for a user with the email \",\n},","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":128,"to":131}}}}],["96",{"pageContent":"{\ninput:\n'Here is an example of a valid JSON object matching the provided spec:\\n\\n```json\\n{\\n  \"action\": \"metabase\",\\n  \"action_input\": [\"GET\", \"/api/table/1\"]\\n}\\n```\\n\\nIn this example, the \"action\" key has a string value of \"metabase\", and the \"action_input\" key has an array value containing two elements: a string value of \"GET\" and a string value of \"/api/table/1\". This JSON object could be used to make a request to a Metabase API endpoint with the specified method and arguments.',\noutput: `{ \"action\": \"metabase\", \"action_input\": [\"GET\", \"/api/table/1\"] } `,\ntool: \"metabase\",\ntoolInput: [\"GET\", \"/api/table/1\"],\n},\n{\ninput:\n'```\\n{\\n  \"action\": \"metabase\",\\n  \"action_input\": [\"GET\", \"/api/table/1\"]\\n}\\n```',\noutput: `{ \"action\": \"metabase\", \"action_input\": [\"GET\", \"/api/table/1\"] } `,\ntool: \"metabase\",\ntoolInput: [\"GET\", \"/api/table/1\"],\n},\n{\ninput:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":188,"to":203}}}}],["97",{"pageContent":"'Here we have some boilerplate nonsense```\\n{\\n \"action\": \"blogpost\",\\n  \"action_input\": \"```sql\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = \\'bud\\'```\"\\n}\\n``` and at the end there is more nonsense',\noutput:\n'{\"action\":\"blogpost\",\"action_input\":\"```sql\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = \\'bud\\'```\"}',\ntool: \"blogpost\",\ntoolInput:\n\"```sql\\nSELECT * FROM orders\\nJOIN users ON users.id = orders.user_id\\nWHERE users.email = 'bud'```\",\n},\n{\ninput:\n'Here we have some boilerplate nonsense```json\\n{\\n \\t\\r\\n\"action\": \"blogpost\",\\n\\t\\r  \"action_input\": \"```sql\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = \\'bud\\'```\"\\n\\t\\r}\\n\\n\\n\\t\\r``` and at the end there is more nonsense',\noutput:\n'{\"action\":\"blogpost\",\"action_input\":\"```sql\\\\nSELECT * FROM orders\\\\nJOIN users ON users.id = orders.user_id\\\\nWHERE users.email = \\'bud\\'```\"}',\ntool: \"blogpost\",\ntoolInput:","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":260,"to":273}}}}],["98",{"pageContent":"\"```sql\\nSELECT * FROM orders\\nJOIN users ON users.id = orders.user_id\\nWHERE users.email = 'bud'```\",\n},\n];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":330,"to":332}}}}],["99",{"pageContent":"const p = new ChatConversationalAgentOutputParser();\nfor (const message of testCases) {\nconst parsed = await p.parse(message.input);\nexpect(parsed).toBeDefined();\nif (message.tool === \"Final Answer\") {\nexpect(parsed.returnValues).toBeDefined();\n} else {\nexpect(parsed.tool).toEqual(message.tool);\n\nif (typeof message.toolInput === \"object\") {\nexpect(message.toolInput).toEqual(parsed.toolInput);\n}\nif (typeof message.toolInput === \"string\") {\nexpect(message.toolInput).toContain(parsed.toolInput);\n}\n}\n}\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/chat_output_parser.test.ts","loc":{"lines":{"from":394,"to":411}}}}],["100",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport {\nJsonListKeysTool,\nJsonSpec,\nJsonGetValueTool,\n} from \"../../tools/json.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/json.test.ts","loc":{"lines":{"from":1,"to":6}}}}],["101",{"pageContent":"test(\"JsonListKeysTool\", async () => {\nconst jsonSpec = new JsonSpec({\nfoo: \"bar\",\nbaz: { test: { foo: [1, 2, 3], qux: [{ x: 1, y: 2, z: 3 }, { a: 1 }] } },\n});\nconst jsonListKeysTool = new JsonListKeysTool(jsonSpec);\nexpect(await jsonListKeysTool.call(\"\")).toBe(\"foo, baz\");\nexpect(await jsonListKeysTool.call(\"/foo\")).toContain(\"not a dictionary\");\nexpect(await jsonListKeysTool.call(\"/baz\")).toBe(\"test\");\nexpect(await jsonListKeysTool.call(\"/baz/test\")).toBe(\"foo, qux\");\nexpect(await jsonListKeysTool.call(\"/baz/test/foo\")).toContain(\n\"not a dictionary\"\n);\nexpect(await jsonListKeysTool.call(\"/baz/test/foo/0\")).toContain(\n\"not a dictionary\"\n);\nexpect(await jsonListKeysTool.call(\"/baz/test/qux\")).toContain(\n\"not a dictionary\"\n);\nexpect(await jsonListKeysTool.call(\"/baz/test/qux/0\")).toBe(\"x, y, z\");\nexpect(await jsonListKeysTool.call(\"/baz/test/qux/1\")).toBe(\"a\");\nexpect(await jsonListKeysTool.call(\"/bar\")).toContain(\"not a dictionary\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/json.test.ts","loc":{"lines":{"from":75,"to":97}}}}],["102",{"pageContent":"test(\"JsonGetValueTool\", async () => {\nconst jsonSpec = new JsonSpec({\nfoo: \"bar\",\nbaz: { test: { foo: [1, 2, 3], qux: [{ x: 1, y: 2, z: 3 }, { a: 1 }] } },\n});\nconst jsonGetValueTool = new JsonGetValueTool(jsonSpec);\nexpect(await jsonGetValueTool.call(\"\")).toBe(\n`{\"foo\":\"bar\",\"baz\":{\"test\":{\"foo\":[1,2,3],\"qux\":[{\"x\":1,\"y\":2,\"z\":3},{\"a\":1}]}}}`\n);\nexpect(await jsonGetValueTool.call(\"/foo\")).toBe(\"bar\");\nexpect(await jsonGetValueTool.call(\"/baz\")).toBe(\n`{\"test\":{\"foo\":[1,2,3],\"qux\":[{\"x\":1,\"y\":2,\"z\":3},{\"a\":1}]}}`\n);\nexpect(await jsonGetValueTool.call(\"/baz/test\")).toBe(\n`{\"foo\":[1,2,3],\"qux\":[{\"x\":1,\"y\":2,\"z\":3},{\"a\":1}]}`\n);\nexpect(await jsonGetValueTool.call(\"/baz/test/foo\")).toBe(\"[1,2,3]\");\nexpect(await jsonGetValueTool.call(\"/baz/test/foo/0\")).toBe(\"1\");\nexpect(await jsonGetValueTool.call(\"/baz/test/qux\")).toBe(\n`[{\"x\":1,\"y\":2,\"z\":3},{\"a\":1}]`\n);\nexpect(await jsonGetValueTool.call(\"/baz/test/qux/0\")).toBe(\n`{\"x\":1,\"y\":2,\"z\":3}`\n);\nexpect(await jsonGetValueTool.call(\"/baz/test/qux/0/x\")).toBe(\"1\");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/json.test.ts","loc":{"lines":{"from":145,"to":169}}}}],["103",{"pageContent":"expect(await jsonGetValueTool.call(\"/baz/test/qux/1\")).toBe(`{\"a\":1}`);\nexpect(await jsonGetValueTool.call(\"/bar\")).toContain(`null`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/json.test.ts","loc":{"lines":{"from":216,"to":218}}}}],["104",{"pageContent":"test(\"JsonGetValueTool, large values\", async () => {\nconst jsonSpec = new JsonSpec(\n{ foo: \"bar\", baz: { test: { foo: [1, 2, 3, 4] } } },\n5\n);\nconst jsonGetValueTool = new JsonGetValueTool(jsonSpec);\nexpect(await jsonGetValueTool.call(\"\")).toContain(\"large dictionary\");\nexpect(await jsonGetValueTool.call(\"/foo\")).toBe(\"bar\");\nexpect(await jsonGetValueTool.call(\"/baz\")).toContain(\"large dictionary\");\nexpect(await jsonGetValueTool.call(\"/baz/test\")).toContain(\n\"large dictionary\"\n);\nexpect(await jsonGetValueTool.call(\"/baz/test/foo\")).toBe(\"[1,2,...\");\nexpect(await jsonGetValueTool.call(\"/baz/test/foo/0\")).toBe(\"1\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/json.test.ts","loc":{"lines":{"from":286,"to":300}}}}],["105",{"pageContent":"/* eslint-disable no-process-env */\nimport { test, expect, beforeEach, afterEach } from \"@jest/globals\";\nimport { DataSource } from \"typeorm\";\nimport {\nInfoSqlTool,\nQuerySqlTool,\nListTablesSqlTool,\nQueryCheckerTool,\n} from \"../../tools/sql.js\";\nimport { SqlDatabase } from \"../../sql_db.js\";\n\nconst previousEnv = process.env;\n\nlet db: SqlDatabase;\n\nbeforeEach(async () => {\nconst datasource = new DataSource({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/sql.test.ts","loc":{"lines":{"from":1,"to":17}}}}],["106",{"pageContent":": \"sqlite\",\ndatabase: \":memory:\",\nsynchronize: true,\n});\n\nawait datasource.initialize();\n\nawait datasource.query(`\nCREATE TABLE products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Apple', 100);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Banana', 200);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Orange', 300);\n`);\nawait datasource.query(`\nCREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, age INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Alice', 20);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Bob', 21);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Charlie', 22);\n`);\n\ndb = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\n\nprocess.env = { ...previousEnv, OPENAI_API_KEY: \"test\" };\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/sql.test.ts","loc":{"lines":{"from":115,"to":152}}}}],["107",{"pageContent":"afterEach(async () => {\nprocess.env = previousEnv;\nawait db.appDataSource.destroy();\n});\n\ntest(\"QuerySqlTool\", async () => {\nconst querySqlTool = new QuerySqlTool(db);\nconst result = await querySqlTool.call(\"SELECT * FROM users\");\nexpect(result).toBe(\n`[{\"id\":1,\"name\":\"Alice\",\"age\":20},{\"id\":2,\"name\":\"Bob\",\"age\":21},{\"id\":3,\"name\":\"Charlie\",\"age\":22}]`\n);\n});\n\ntest(\"QuerySqlTool with error\", async () => {\nconst querySqlTool = new QuerySqlTool(db);\nconst result = await querySqlTool.call(\"SELECT * FROM userss\");\nexpect(result).toBe(`QueryFailedError: SQLITE_ERROR: no such table: userss`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/sql.test.ts","loc":{"lines":{"from":231,"to":248}}}}],["108",{"pageContent":"test(\"InfoSqlTool\", async () => {\nconst infoSqlTool = new InfoSqlTool(db);\nconst result = await infoSqlTool.call(\"users, products\");\nconst expectStr = `\nCREATE TABLE products (\nid INTEGER , name TEXT , price INTEGER ) \nSELECT * FROM \"products\" LIMIT 3;\nid name price\n1 Apple 100\n2 Banana 200\n3 Orange 300\nCREATE TABLE users (\nid INTEGER , name TEXT , age INTEGER ) \nSELECT * FROM \"users\" LIMIT 3;\nid name age\n1 Alice 20\n2 Bob 21\n3 Charlie 22`;\nexpect(result.trim()).toBe(expectStr.trim());\n});\n\ntest(\"InfoSqlTool with error\", async () => {\nconst infoSqlTool = new InfoSqlTool(db);\nconst result = await infoSqlTool.call(\"userss, products\");\nexpect(result).toBe(\n`Error: Wrong target table name: the table userss was not found in the database`\n);\n});\n\ntest(\"ListTablesSqlTool\", async () => {\nconst listSqlTool = new ListTablesSqlTool(db);\nconst result = await listSqlTool.call(\"\");\nexpect(result).toBe(`products, users`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/sql.test.ts","loc":{"lines":{"from":337,"to":370}}}}],["109",{"pageContent":"test(\"QueryCheckerTool\", async () => {\nconst queryCheckerTool = new QueryCheckerTool();\nexpect(queryCheckerTool.llmChain).not.toBeNull();\nexpect(queryCheckerTool.llmChain.inputKeys).toEqual([\"query\"]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/sql.test.ts","loc":{"lines":{"from":450,"to":454}}}}],["110",{"pageContent":"import { beforeEach, describe, expect, test } from \"@jest/globals\";\nimport { ZapierToolKit } from \"../agent_toolkits/zapier/zapier.js\";\nimport { ZapierNLAWrapper, ZapierValues } from \"../../tools/zapier.js\";\n\ndescribe(\"ZapierNLAWrapper\", () => {\nlet actions: ZapierValues[] = [];\nlet zapier: ZapierNLAWrapper;\n\nbeforeEach(async () => {\nzapier = new ZapierNLAWrapper();\nactions = await zapier.listActions();\n});\n\ntest(\"loads ZapierToolKit\", async () => {\nconst toolkit = await ZapierToolKit.fromZapierNLAWrapper(zapier);\n\nexpect(toolkit).toBeDefined();\n});\n\ntest(\"Zapier NLA has connected actions\", async () => {\nexpect(actions.length).toBeGreaterThan(0);\n});\n\ndescribe(\"Giphy action\", () => {\ntest(\"returns a GIF\", async () => {\nconst giphy = actions.find(\n(action) => action.description === \"Giphy: Find GIF\"\n);\nconst result = await zapier.runAction(giphy?.id, \"cats\");\n\nexpect(result).toMatchObject({\nkeyword: \"cats\",\nsize: expect.any(String),\nurl: expect.stringContaining(\"https://\"),\n});\n});\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/tests/zapier_toolkit.int.test.ts","loc":{"lines":{"from":1,"to":38}}}}],["111",{"pageContent":"import { LLMChain } from \"../chains/llm_chain.js\";\nimport { SerializedLLMChain } from \"../chains/serde.js\";\nimport { AgentAction, AgentFinish } from \"../schema/index.js\";\nimport { BaseOutputParser } from \"../schema/output_parser.js\";\n\nexport interface AgentInput {\nllmChain: LLMChain;\noutputParser: AgentActionOutputParser;\nallowedTools?: string[];\n}\n\nexport abstract class AgentActionOutputParser extends BaseOutputParser<\nAgentAction | AgentFinish\n> {}\n\nexport type StoppingMethod = \"force\" | \"generate\";\n\nexport type SerializedAgentT<\nTType extends string = string,\nFromLLMInput extends Record<string, unknown> = Record<string, unknown>,\nConstructorInput extends AgentInput = AgentInput\n> = {\n_type: TType;\nllm_chain?: SerializedLLMChain;\n} & (\n| ({ load_from_llm_and_tools: true } & FromLLMInput)\n| ({ load_from_llm_and_tools?: false } & ConstructorInput)\n);\n\nexport type SerializedFromLLMAndTools = {\nsuffix?: string;\nprefix?: string;\ninput_variables?: string[];\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/types.ts","loc":{"lines":{"from":1,"to":34}}}}],["112",{"pageContent":"type SerializedZeroShotAgent = SerializedAgentT<\n\"zero-shot-react-description\",\nSerializedFromLLMAndTools,\nAgentInput\n>;\n\nexport type SerializedAgent = SerializedZeroShotAgent;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/agents/types.ts","loc":{"lines":{"from":43,"to":49}}}}],["113",{"pageContent":"import type { TiktokenModel } from \"@dqbd/tiktoken\";\n\n// https://www.npmjs.com/package/@dqbd/tiktoken\n\nexport const getModelNameForTiktoken = (modelName: string): TiktokenModel => {\nif (modelName.startsWith(\"gpt-3.5-turbo-\")) {\nreturn \"gpt-3.5-turbo\";\n}\n\nif (modelName.startsWith(\"gpt-4-32k-\")) {\nreturn \"gpt-4-32k\";\n}\n\nif (modelName.startsWith(\"gpt-4-\")) {\nreturn \"gpt-4\";\n}\n\nreturn modelName as TiktokenModel;\n};\n\nexport const getEmbeddingContextSize = (modelName?: string): number => {\nswitch (modelName) {\ncase \"text-embedding-ada-002\":\nreturn 8191;\ndefault:\nreturn 2046;\n}\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/count_tokens.ts","loc":{"lines":{"from":1,"to":28}}}}],["114",{"pageContent":"const getModelContextSize = (modelName: string): number => {\nswitch (getModelNameForTiktoken(modelName)) {\ncase \"gpt-3.5-turbo\":\nreturn 4096;\ncase \"gpt-4-32k\":\nreturn 32768;\ncase \"gpt-4\":\nreturn 8192;\ncase \"text-davinci-003\":\nreturn 4097;\ncase \"text-curie-001\":\nreturn 2048;\ncase \"text-babbage-001\":\nreturn 2048;\ncase \"text-ada-001\":\nreturn 2048;\ncase \"code-davinci-002\":\nreturn 8000;\ncase \"code-cushman-001\":\nreturn 2048;\ndefault:\nreturn 4097;\n}\n};\n\ninterface CalculateMaxTokenProps {\nprompt: string;\nmodelName: TiktokenModel;\n}\n\nexport const importTiktoken = async () => {\ntry {\nconst { encoding_for_model } = await import(\"@dqbd/tiktoken\");\nreturn { encoding_for_model };\n} catch (error) {\nconsole.log(error);\nreturn { encoding_for_model: null };\n}\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/count_tokens.ts","loc":{"lines":{"from":103,"to":141}}}}],["115",{"pageContent":"const calculateMaxTokens = async ({\nprompt,\nmodelName,\n}: CalculateMaxTokenProps) => {\nconst { encoding_for_model } = await importTiktoken();\n\n// fallback to approximate calculation if tiktoken is not available\nlet numTokens = Math.ceil(prompt.length / 4);\n\ntry {\nif (encoding_for_model) {\nconst encoding = encoding_for_model(getModelNameForTiktoken(modelName));\n\nconst tokenized = encoding.encode(prompt);\n\nnumTokens = tokenized.length;\n\nencoding.free();\n}\n} catch (error) {\nconsole.warn(\n\"Failed to calculate number of tokens with tiktoken, falling back to approximate count\",\nerror\n);\n}\n\nconst maxTokens = getModelContextSize(modelName);\n\nreturn maxTokens - numTokens;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/count_tokens.ts","loc":{"lines":{"from":208,"to":237}}}}],["116",{"pageContent":"import type { Tiktoken } from \"@dqbd/tiktoken\";\nimport { BasePromptValue, LLMResult } from \"../schema/index.js\";\nimport { CallbackManager, Callbacks } from \"../callbacks/manager.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../util/async_caller.js\";\nimport { getModelNameForTiktoken, importTiktoken } from \"./count_tokens.js\";\n\nconst getVerbosity = () => false;\n\nexport type SerializedLLM = {\n_model: string;\n_type: string;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\nexport interface BaseLangChainParams {\nverbose?: boolean;\ncallbacks?: Callbacks;\n}\n\n/**\n* Base class for language models, chains, tools.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/index.ts","loc":{"lines":{"from":1,"to":22}}}}],["117",{"pageContent":"abstract class BaseLangChain implements BaseLangChainParams {\n/**\n* Whether to print out response text.\n*/\nverbose: boolean;\n\ncallbacks?: Callbacks;\n\nconstructor(params: BaseLangChainParams) {\nthis.verbose = params.verbose ?? getVerbosity();\nthis.callbacks = params.callbacks;\n}\n}\n\n/**\n* Base interface for language model parameters.\n* A subclass of {@link BaseLanguageModel} should have a constructor that\n* takes in a parameter that extends this interface.\n*/\nexport interface BaseLanguageModelParams\nextends AsyncCallerParams,\nBaseLangChainParams {\n/**\n* @deprecated Use `callbacks` instead\n*/\ncallbackManager?: CallbackManager;\n}\n\nexport interface BaseLanguageModelCallOptions {}\n\n/**\n* Base class for language models.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/index.ts","loc":{"lines":{"from":161,"to":193}}}}],["118",{"pageContent":"abstract class BaseLanguageModel\nextends BaseLangChain\nimplements BaseLanguageModelParams\n{\ndeclare CallOptions: BaseLanguageModelCallOptions;\n\n/**\n* The async caller should be used by subclasses to make any async calls,\n* which will thus benefit from the concurrency and retry logic.\n*/\ncaller: AsyncCaller;\n\nconstructor(params: BaseLanguageModelParams) {\nsuper({\nverbose: params.verbose,\ncallbacks: params.callbacks ?? params.callbackManager,\n});\nthis.caller = new AsyncCaller(params ?? {});\n}\n\nabstract generatePrompt(\npromptValues: BasePromptValue[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult>;\n\nabstract _modelType(): string;\n\nabstract _llmType(): string;\n\nprivate _encoding?: Tiktoken;\n\nprivate _registry?: FinalizationRegistry<Tiktoken>;\n\nasync getNumTokens(text: string) {\n// fallback to approximate calculation if tiktoken is not available\nlet numTokens = Math.ceil(text.length / 4);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/index.ts","loc":{"lines":{"from":330,"to":366}}}}],["119",{"pageContent":"try {\nif (!this._encoding) {\nconst { encoding_for_model } = await importTiktoken();\n// modelName only exists in openai subclasses, but tiktoken only supports\n// openai tokenisers anyway, so for other subclasses we default to gpt2\nif (encoding_for_model) {\nthis._encoding = encoding_for_model(\n\"modelName\" in this\n? getModelNameForTiktoken(this.modelName as string)\n: \"gpt2\"\n);\n// We need to register a finalizer to free the tokenizer when the\n// model is garbage collected.\nthis._registry = new FinalizationRegistry((t) => t.free());\nthis._registry.register(this, this._encoding);\n}\n}\n\nif (this._encoding) {\nnumTokens = this._encoding.encode(text).length;\n}\n} catch (error) {\nconsole.warn(\n\"Failed to calculate number of tokens with tiktoken, falling back to approximate count\",\nerror\n);\n}\n\nreturn numTokens;\n}\n\n/**\n* Get the identifying parameters of the LLM.\n*/\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_identifyingParams(): Record<string, any> {\nreturn {};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/index.ts","loc":{"lines":{"from":494,"to":531}}}}],["120",{"pageContent":"/**\n* Return a json-like object representing this LLM.\n*/\nserialize(): SerializedLLM {\nreturn {\n...this._identifyingParams(),\n_type: this._llmType(),\n_model: this._modelType(),\n};\n}\n\n/**\n* Load an LLM from a json-like object describing it.\n*/\nstatic async deserialize(data: SerializedLLM): Promise<BaseLanguageModel> {\nconst { _type, _model, ...rest } = data;\nif (_model && _model !== \"base_chat_model\") {\nthrow new Error(`Cannot load LLM with model ${_model}`);\n}\nconst Cls = {\nopenai: (await import(\"../chat_models/openai.js\")).ChatOpenAI,\n}[_type];\nif (Cls === undefined) {\nthrow new Error(`Cannot load  LLM with type ${_type}`);\n}\nreturn new Cls(rest);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/base_language/index.ts","loc":{"lines":{"from":656,"to":683}}}}],["121",{"pageContent":"import hash from \"object-hash\";\n\n/**\n* This cache key should be consistent across all versions of langchain.\n* It is currently NOT consistent across versions of langchain.\n*\n* A huge benefit of having a remote cache (like redis) is that you can\n* access the cache from different processes/machines. The allows you to\n* seperate concerns and scale horizontally.\n*\n* TODO: Make cache key consistent across versions of langchain.\n*/\nexport const getCacheKey = (...strings: string[]): string =>\nhash(strings.join(\"_\"));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/cache/base.ts","loc":{"lines":{"from":1,"to":14}}}}],["122",{"pageContent":"import { getCacheKey } from \"./base.js\";\nimport { Generation, BaseCache } from \"../schema/index.js\";\n\nconst GLOBAL_MAP = new Map();\n\nexport class InMemoryCache<T = Generation[]> extends BaseCache<T> {\nprivate cache: Map<string, T>;\n\nconstructor(map?: Map<string, T>) {\nsuper();\nthis.cache = map ?? new Map();\n}\n\nlookup(prompt: string, llmKey: string): Promise<T | null> {\nreturn Promise.resolve(this.cache.get(getCacheKey(prompt, llmKey)) ?? null);\n}\n\nasync update(prompt: string, llmKey: string, value: T): Promise<void> {\nthis.cache.set(getCacheKey(prompt, llmKey), value);\n}\n\nstatic global(): InMemoryCache {\nreturn new InMemoryCache(GLOBAL_MAP);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/cache/index.ts","loc":{"lines":{"from":1,"to":25}}}}],["123",{"pageContent":"import type { RedisClientType } from \"redis\";\n\nimport { BaseCache, Generation } from \"../schema/index.js\";\nimport { getCacheKey } from \"./base.js\";\n\nexport class RedisCache extends BaseCache {\nprivate redisClient: RedisClientType;\n\nconstructor(redisClient: RedisClientType) {\nsuper();\nthis.redisClient = redisClient;\n}\n\npublic async lookup(prompt: string, llmKey: string) {\nlet idx = 0;\nlet key = getCacheKey(prompt, llmKey, String(idx));\nlet value = await this.redisClient.get(key);\nconst generations: Generation[] = [];\n\nwhile (value) {\nif (!value) {\nbreak;\n}\n\ngenerations.push({ text: value });\nidx += 1;\nkey = getCacheKey(prompt, llmKey, String(idx));\nvalue = await this.redisClient.get(key);\n}\n\nreturn generations.length > 0 ? generations : null;\n}\n\npublic async update(prompt: string, llmKey: string, value: Generation[]) {\nfor (let i = 0; i < value.length; i += 1) {\nconst key = getCacheKey(prompt, llmKey, String(i));\nawait this.redisClient.set(key, value[i].text);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/cache/redis.ts","loc":{"lines":{"from":1,"to":40}}}}],["124",{"pageContent":"import { test, expect } from \"@jest/globals\";\n\nimport { InMemoryCache } from \"../index.js\";\n\ntest(\"InMemoryCache\", async () => {\nconst cache = new InMemoryCache();\nawait cache.update(\"foo\", \"bar\", [{ text: \"baz\" }]);\nexpect(await cache.lookup(\"foo\", \"bar\")).toEqual([{ text: \"baz\" }]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/cache/tests/cache.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["125",{"pageContent":"import { test, expect, jest } from \"@jest/globals\";\nimport hash from \"object-hash\";\n\nimport { RedisCache } from \"../redis.js\";\n\nconst sha256 = (str: string) => hash(str);\n\ntest(\"RedisCache\", async () => {\nconst redis = {\nget: jest.fn(async (key: string) => {\nif (key === sha256(\"foo_bar_0\")) {\nreturn \"baz\";\n}\nreturn null;\n}),\n};\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconst cache = new RedisCache(redis as any);\nexpect(await cache.lookup(\"foo\", \"bar\")).toEqual([{ text: \"baz\" }]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/cache/tests/redis.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["126",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\nimport {\nAgentAction,\nAgentFinish,\nChainValues,\nLLMResult,\n} from \"../schema/index.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ntype Error = any;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/base.ts","loc":{"lines":{"from":1,"to":10}}}}],["127",{"pageContent":"interface BaseCallbackHandlerInput {\nignoreLLM?: boolean;\nignoreChain?: boolean;\nignoreAgent?: boolean;\n}\n\nabstract class BaseCallbackHandlerMethodsClass {\n/**\n* Called at the start of an LLM or Chat Model run, with the prompt(s)\n* and the run ID.\n*/\nhandleLLMStart?(\nllm: { name: string },\nprompts: string[],\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called when an LLM/ChatModel in `streaming` mode produces a new token\n*/\nhandleLLMNewToken?(\ntoken: string,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called if an LLM/ChatModel run encounters an error\n*/\nhandleLLMError?(\nerr: Error,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called at the end of an LLM/ChatModel run, with the output and the run ID.\n*/\nhandleLLMEnd?(\noutput: LLMResult,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/base.ts","loc":{"lines":{"from":191,"to":234}}}}],["128",{"pageContent":"/**\n* Called at the start of a Chain run, with the chain name and inputs\n* and the run ID.\n*/\nhandleChainStart?(\nchain: { name: string },\ninputs: ChainValues,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called if a Chain run encounters an error\n*/\nhandleChainError?(\nerr: Error,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called at the end of a Chain run, with the outputs and the run ID.\n*/\nhandleChainEnd?(\noutputs: ChainValues,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called at the start of a Tool run, with the tool name and input\n* and the run ID.\n*/\nhandleToolStart?(\ntool: { name: string },\ninput: string,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called if a Tool run encounters an error\n*/\nhandleToolError?(\nerr: Error,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/base.ts","loc":{"lines":{"from":385,"to":432}}}}],["129",{"pageContent":"/**\n* Called at the end of a Tool run, with the tool output and the run ID.\n*/\nhandleToolEnd?(\noutput: string,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\nhandleText?(\ntext: string,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called when an agent is about to execute an action,\n* with the action and the run ID.\n*/\nhandleAgentAction?(\naction: AgentAction,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n\n/**\n* Called when an agent finishes execution, before it exits.\n* with the final output and the run ID.\n*/\nhandleAgentEnd?(\naction: AgentFinish,\nrunId: string,\nparentRunId?: string\n): Promise<void> | void;\n}\n\n/**\n* Base interface for callbacks. All methods are optional. If a method is not\n* implemented, it will be ignored. If a method is implemented, it will be\n* called at the appropriate time. All methods are called with the run ID of\n* the LLM/ChatModel/Chain that is running, which is generated by the\n* CallbackManager.\n*\n* @interface\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/base.ts","loc":{"lines":{"from":583,"to":627}}}}],["130",{"pageContent":"type CallbackHandlerMethods = BaseCallbackHandlerMethodsClass;\n\nexport abstract class BaseCallbackHandler\nextends BaseCallbackHandlerMethodsClass\nimplements BaseCallbackHandlerInput\n{\nabstract name: string;\n\nignoreLLM = false;\n\nignoreChain = false;\n\nignoreAgent = false;\n\nconstructor(input?: BaseCallbackHandlerInput) {\nsuper();\nif (input) {\nthis.ignoreLLM = input.ignoreLLM ?? this.ignoreLLM;\nthis.ignoreChain = input.ignoreChain ?? this.ignoreChain;\nthis.ignoreAgent = input.ignoreAgent ?? this.ignoreAgent;\n}\n}\n\ncopy(): BaseCallbackHandler {\nreturn new (this.constructor as new (\ninput?: BaseCallbackHandlerInput\n) => BaseCallbackHandler)(this);\n}\n\nstatic fromMethods(methods: CallbackHandlerMethods) {\nclass Handler extends BaseCallbackHandler {\nname = uuidv4();\n\nconstructor() {\nsuper();\nObject.assign(this, methods);\n}\n}\nreturn new Handler();\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/base.ts","loc":{"lines":{"from":772,"to":812}}}}],["131",{"pageContent":"import type { CSPair } from \"ansi-styles\";\nimport styles from \"ansi-styles\";\nimport {\nAgentRun,\nBaseTracer,\nBaseTracerSession,\nChainRun,\nLLMRun,\nRun,\nToolRun,\n} from \"./tracers.js\";\n\nfunction wrap(style: CSPair, text: string) {\nreturn `${style.open}${text}${style.close}`;\n}\n\nfunction tryJsonStringify(obj: unknown, fallback: string) {\ntry {\nreturn JSON.stringify(obj, null, 2);\n} catch (err) {\nreturn fallback;\n}\n}\n\nfunction elapsed(run: Run): string {\nconst elapsed = run.end_time - run.start_time;\nif (elapsed < 1000) {\nreturn `${elapsed}ms`;\n}\nreturn `${(elapsed / 1000).toFixed(2)}s`;\n}\n\nconst { color } = styles;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/console.ts","loc":{"lines":{"from":1,"to":33}}}}],["132",{"pageContent":"class ConsoleCallbackHandler extends BaseTracer {\nname = \"console_callback_handler\" as const;\n\n// boilerplate to work with the base tracer class\n\nconstructor() {\nsuper();\n}\n\ni = 0;\n\nprotected persistSession(session: BaseTracerSession) {\n// eslint-disable-next-line no-plusplus\nreturn Promise.resolve({ ...session, id: this.i++ });\n}\n\nprotected persistRun(_run: Run) {\nreturn Promise.resolve();\n}\n\nloadDefaultSession() {\nreturn this.newSession();\n}\n\nloadSession(sessionName: string) {\nreturn this.newSession(sessionName);\n}\n\n// utility methods\n\ngetParents(run: Run) {\nconst parents: Run[] = [];\nlet currentRun = run;\nwhile (currentRun.parent_uuid) {\nconst parent = this.runMap.get(currentRun.parent_uuid);\nif (parent) {\nparents.push(parent);\ncurrentRun = parent;\n} else {\nbreak;\n}\n}\nreturn parents;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/console.ts","loc":{"lines":{"from":210,"to":253}}}}],["133",{"pageContent":"getBreadcrumbs(run: Run) {\nconst parents = this.getParents(run).reverse();\nconst string = [...parents, run]\n.map((parent, i, arr) => {\nconst name = `${parent.execution_order}:${parent.type}:${parent.serialized?.name}`;\nreturn i === arr.length - 1 ? wrap(styles.bold, name) : name;\n})\n.join(\" > \");\nreturn wrap(color.grey, string);\n}\n\n// logging methods\n\nonChainStart(run: ChainRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(\ncolor.green,\n\"[chain/start]\"\n)} [${crumbs}] Entering Chain run with input: ${tryJsonStringify(\nrun.inputs,\n\"[inputs]\"\n)}`\n);\n}\n\nonChainEnd(run: ChainRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.cyan, \"[chain/end]\")} [${crumbs}] [${elapsed(\nrun\n)}] Exiting Chain run with output: ${tryJsonStringify(\nrun.outputs,\n\"[outputs]\"\n)}`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/console.ts","loc":{"lines":{"from":422,"to":458}}}}],["134",{"pageContent":"onChainError(run: ChainRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.red, \"[chain/error]\")} [${crumbs}] [${elapsed(\nrun\n)}] Chain run errored with error: ${tryJsonStringify(\nrun.error,\n\"[error]\"\n)}`\n);\n}\n\nonLLMStart(run: LLMRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(\ncolor.green,\n\"[llm/start]\"\n)} [${crumbs}] Entering LLM run with input: ${tryJsonStringify(\n{ prompts: run.prompts.map((p) => p.trim()) },\n\"[inputs]\"\n)}`\n);\n}\n\nonLLMEnd(run: LLMRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.cyan, \"[llm/end]\")} [${crumbs}] [${elapsed(\nrun\n)}] Exiting LLM run with output: ${tryJsonStringify(\nrun.response,\n\"[response]\"\n)}`\n);\n}\n\nonLLMError(run: LLMRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.red, \"[llm/error]\")} [${crumbs}] [${elapsed(\nrun\n)}] LLM run errored with error: ${tryJsonStringify(run.error, \"[error]\")}`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/console.ts","loc":{"lines":{"from":627,"to":671}}}}],["135",{"pageContent":"onToolStart(run: ToolRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(\ncolor.green,\n\"[tool/start]\"\n)} [${crumbs}] Entering Tool run with input: \"${run.tool_input?.trim()}\"`\n);\n}\n\nonToolEnd(run: ToolRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.cyan, \"[tool/end]\")} [${crumbs}] [${elapsed(\nrun\n)}] Exiting Tool run with output: \"${run.output?.trim()}\"`\n);\n}\n\nonToolError(run: ToolRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(color.red, \"[tool/error]\")} [${crumbs}] [${elapsed(\nrun\n)}] Tool run errored with error: ${tryJsonStringify(\nrun.error,\n\"[error]\"\n)}`\n);\n}\n\nonAgentAction(run: AgentRun) {\nconst crumbs = this.getBreadcrumbs(run);\nconsole.log(\n`${wrap(\ncolor.blue,\n\"[agent/action]\"\n)} [${crumbs}] Agent selected action: ${tryJsonStringify(\nrun.actions[run.actions.length - 1],\n\"[action]\"\n)}`\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/console.ts","loc":{"lines":{"from":831,"to":874}}}}],["136",{"pageContent":"import { LangChainTracer } from \"./tracers.js\";\n\nexport async function getTracingCallbackHandler(\nsession?: string\n): Promise<LangChainTracer> {\nconst tracer = new LangChainTracer();\nif (session) {\nawait tracer.loadSession(session);\n} else {\nawait tracer.loadDefaultSession();\n}\nreturn tracer;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/initialize.ts","loc":{"lines":{"from":1,"to":13}}}}],["137",{"pageContent":"import { AgentAction, ChainValues, LLMResult } from \"../../schema/index.js\";\nimport { BaseCallbackHandler } from \"../base.js\";\n\nexport type RunType = \"llm\" | \"chain\" | \"tool\";\n\nexport interface BaseTracerSession {\nstart_time: number;\nname?: string;\n}\n\nexport type TracerSessionCreate = BaseTracerSession;\n\nexport interface TracerSession extends BaseTracerSession {\nid: number;\n}\n\nexport interface BaseRun {\nuuid: string;\nparent_uuid?: string;\nstart_time: number;\nend_time: number;\nexecution_order: number;\nchild_execution_order: number;\nserialized: { name: string };\nsession_id: number;\nerror?: string;\ntype: RunType;\n}\n\nexport interface LLMRun extends BaseRun {\nprompts: string[];\nresponse?: LLMResult;\n}\n\nexport interface ChainRun extends BaseRun {\ninputs: ChainValues;\noutputs?: ChainValues;\nchild_llm_runs: LLMRun[];\nchild_chain_runs: ChainRun[];\nchild_tool_runs: ToolRun[];\n}\n\nexport interface AgentRun extends ChainRun {\nactions: AgentAction[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":1,"to":45}}}}],["138",{"pageContent":"interface ToolRun extends BaseRun {\ntool_input: string;\noutput?: string;\naction: string;\nchild_llm_runs: LLMRun[];\nchild_chain_runs: ChainRun[];\nchild_tool_runs: ToolRun[];\n}\n\nexport type Run = LLMRun | ChainRun | ToolRun;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":462,"to":471}}}}],["139",{"pageContent":"abstract class BaseTracer extends BaseCallbackHandler {\nprotected session?: TracerSession;\n\nprotected runMap: Map<string, Run> = new Map();\n\nprotected constructor() {\nsuper();\n}\n\ncopy(): this {\nreturn this;\n}\n\nabstract loadSession(sessionName: string): Promise<TracerSession>;\n\nabstract loadDefaultSession(): Promise<TracerSession>;\n\nprotected abstract persistRun(run: Run): Promise<void>;\n\nprotected abstract persistSession(\nsession: TracerSessionCreate\n): Promise<TracerSession>;\n\nasync newSession(sessionName?: string): Promise<TracerSession> {\nconst sessionCreate: TracerSessionCreate = {\nstart_time: Date.now(),\nname: sessionName,\n};\nconst session = await this.persistSession(sessionCreate);\nthis.session = session;\nreturn session;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":924,"to":955}}}}],["140",{"pageContent":"protected _addChildRun(parentRun: ChainRun | ToolRun, childRun: Run) {\nif (childRun.type === \"llm\") {\nparentRun.child_llm_runs.push(childRun as LLMRun);\n} else if (childRun.type === \"chain\") {\nparentRun.child_chain_runs.push(childRun as ChainRun);\n} else if (childRun.type === \"tool\") {\nparentRun.child_tool_runs.push(childRun as ToolRun);\n} else {\nthrow new Error(\"Invalid run type\");\n}\n}\n\nprotected _startTrace(run: Run) {\nif (run.parent_uuid) {\nconst parentRun = this.runMap.get(run.parent_uuid);\nif (parentRun) {\nif (!(parentRun.type === \"tool\" || parentRun.type === \"chain\")) {\nthrow new Error(\"Caller run can only be a tool or chain\");\n} else {\nthis._addChildRun(parentRun as ChainRun | ToolRun, run);\n}\n} else {\nthrow new Error(`Caller run ${run.parent_uuid} not found`);\n}\n}\nthis.runMap.set(run.uuid, run);\n}\n\nprotected async _endTrace(run: Run): Promise<void> {\nif (!run.parent_uuid) {\nawait this.persistRun(run);\n} else {\nconst parentRun = this.runMap.get(run.parent_uuid);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":1380,"to":1412}}}}],["141",{"pageContent":"if (parentRun === undefined) {\nthrow new Error(`Parent run ${run.parent_uuid} not found`);\n}\n\nparentRun.child_execution_order = Math.max(\nparentRun.child_execution_order,\nrun.child_execution_order\n);\n}\nthis.runMap.delete(run.uuid);\n}\n\nprotected _getExecutionOrder(parentRunId: string | undefined): number {\n// If a run has no parent then execution order is 1\nif (parentRunId === undefined) {\nreturn 1;\n}\n\nconst parentRun = this.runMap.get(parentRunId);\n\nif (parentRun === undefined) {\nthrow new Error(`Parent run ${parentRunId} not found`);\n}\n\nreturn parentRun.child_execution_order + 1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":1828,"to":1853}}}}],["142",{"pageContent":"async handleLLMStart(\nllm: { name: string },\nprompts: string[],\nrunId: string,\nparentRunId?: string\n): Promise<void> {\nif (this.session === undefined) {\nthis.session = await this.loadDefaultSession();\n}\nconst execution_order = this._getExecutionOrder(parentRunId);\nconst run: LLMRun = {\nuuid: runId,\nparent_uuid: parentRunId,\nstart_time: Date.now(),\nend_time: 0,\nserialized: llm,\nprompts,\nsession_id: this.session.id,\nexecution_order,\nchild_execution_order: execution_order,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":2288,"to":2307}}}}],["143",{"pageContent":": \"llm\",\n};\n\nthis._startTrace(run);\nawait this.onLLMStart?.(run);\n}\n\nasync handleLLMEnd(output: LLMResult, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"llm\") {\nthrow new Error(\"No LLM run to end.\");\n}\nconst llmRun = run as LLMRun;\nllmRun.end_time = Date.now();\nllmRun.response = output;\nawait this.onLLMEnd?.(llmRun);\nawait this._endTrace(llmRun);\n}\n\nasync handleLLMError(error: Error, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"llm\") {\nthrow new Error(\"No LLM run to end.\");\n}\nconst llmRun = run as LLMRun;\nllmRun.end_time = Date.now();\nllmRun.error = error.message;\nawait this.onLLMError?.(llmRun);\nawait this._endTrace(llmRun);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":2747,"to":2776}}}}],["144",{"pageContent":"async handleChainStart(\nchain: { name: string },\ninputs: ChainValues,\nrunId: string,\nparentRunId?: string\n): Promise<void> {\nif (this.session === undefined) {\nthis.session = await this.loadDefaultSession();\n}\nconst execution_order = this._getExecutionOrder(parentRunId);\nconst run: ChainRun = {\nuuid: runId,\nparent_uuid: parentRunId,\nstart_time: Date.now(),\nend_time: 0,\nserialized: chain,\ninputs,\nsession_id: this.session.id,\nexecution_order,\nchild_execution_order: execution_order,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":3204,"to":3223}}}}],["145",{"pageContent":": \"chain\",\nchild_llm_runs: [],\nchild_chain_runs: [],\nchild_tool_runs: [],\n};\n\nthis._startTrace(run);\nawait this.onChainStart?.(run);\n}\n\nasync handleChainEnd(outputs: ChainValues, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"chain\") {\nthrow new Error(\"No chain run to end.\");\n}\nconst chainRun = run as ChainRun;\nchainRun.end_time = Date.now();\nchainRun.outputs = outputs;\nawait this.onChainEnd?.(chainRun);\nawait this._endTrace(chainRun);\n}\n\nasync handleChainError(error: Error, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"chain\") {\nthrow new Error(\"No chain run to end.\");\n}\nconst chainRun = run as ChainRun;\nchainRun.end_time = Date.now();\nchainRun.error = error.message;\nawait this.onChainError?.(chainRun);\nawait this._endTrace(chainRun);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":3662,"to":3694}}}}],["146",{"pageContent":"async handleToolStart(\ntool: { name: string },\ninput: string,\nrunId: string,\nparentRunId?: string\n): Promise<void> {\nif (this.session === undefined) {\nthis.session = await this.loadDefaultSession();\n}\nconst execution_order = this._getExecutionOrder(parentRunId);\nconst run: ToolRun = {\nuuid: runId,\nparent_uuid: parentRunId,\nstart_time: Date.now(),\nend_time: 0,\nserialized: tool,\ntool_input: input,\nsession_id: this.session.id,\nexecution_order,\nchild_execution_order: execution_order,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":4116,"to":4135}}}}],["147",{"pageContent":": \"tool\",\naction: JSON.stringify(tool), // TODO: this is duplicate info, not needed\nchild_llm_runs: [],\nchild_chain_runs: [],\nchild_tool_runs: [],\n};\n\nthis._startTrace(run);\nawait this.onToolStart?.(run);\n}\n\nasync handleToolEnd(output: string, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"tool\") {\nthrow new Error(\"No tool run to end\");\n}\nconst toolRun = run as ToolRun;\ntoolRun.end_time = Date.now();\ntoolRun.output = output;\nawait this.onToolEnd?.(toolRun);\nawait this._endTrace(toolRun);\n}\n\nasync handleToolError(error: Error, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"tool\") {\nthrow new Error(\"No tool run to end\");\n}\nconst toolRun = run as ToolRun;\ntoolRun.end_time = Date.now();\ntoolRun.error = error.message;\nawait this.onToolError?.(toolRun);\nawait this._endTrace(toolRun);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":4574,"to":4607}}}}],["148",{"pageContent":"async handleAgentAction(action: AgentAction, runId: string): Promise<void> {\nconst run = this.runMap.get(runId);\nif (!run || run?.type !== \"chain\") {\nreturn;\n}\nconst agentRun = run as AgentRun;\nagentRun.actions = agentRun.actions || [];\nagentRun.actions.push(action);\nawait this.onAgentAction?.(run as AgentRun);\n}\n\n// custom event handlers\n\nonLLMStart?(run: LLMRun): void | Promise<void>;\n\nonLLMEnd?(run: LLMRun): void | Promise<void>;\n\nonLLMError?(run: LLMRun): void | Promise<void>;\n\nonChainStart?(run: ChainRun): void | Promise<void>;\n\nonChainEnd?(run: ChainRun): void | Promise<void>;\n\nonChainError?(run: ChainRun): void | Promise<void>;\n\nonToolStart?(run: ToolRun): void | Promise<void>;\n\nonToolEnd?(run: ToolRun): void | Promise<void>;\n\nonToolError?(run: ToolRun): void | Promise<void>;\n\nonAgentAction?(run: AgentRun): void | Promise<void>;\n\n// TODO Implement handleAgentEnd, handleText\n\n// onAgentEnd?(run: ChainRun): void | Promise<void>;\n\n// onText?(run: Run): void | Promise<void>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":5028,"to":5066}}}}],["149",{"pageContent":"class LangChainTracer extends BaseTracer {\nname = \"langchain_tracer\";\n\nprotected endpoint =\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.LANGCHAIN_ENDPOINT\n: undefined) || \"http://localhost:8000\";\n\nprotected headers: Record<string, string> = {\n\"Content-Type\": \"application/json\",\n};\n\nconstructor() {\nsuper();\n// eslint-disable-next-line no-process-env\nif (typeof process !== \"undefined\" && process.env?.LANGCHAIN_API_KEY) {\n// eslint-disable-next-line no-process-env\nthis.headers[\"x-api-key\"] = process.env?.LANGCHAIN_API_KEY;\n}\n}\n\nprotected async persistRun(run: LLMRun | ChainRun | ToolRun): Promise<void> {\nlet endpoint;\nif (run.type === \"llm\") {\nendpoint = `${this.endpoint}/llm-runs`;\n} else if (run.type === \"chain\") {\nendpoint = `${this.endpoint}/chain-runs`;\n} else {\nendpoint = `${this.endpoint}/tool-runs`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":5481,"to":5511}}}}],["150",{"pageContent":"const response = await fetch(endpoint, {\nmethod: \"POST\",\nheaders: this.headers,\nbody: JSON.stringify(run),\n});\nif (!response.ok) {\nconsole.error(\n`Failed to persist run: ${response.status} ${response.statusText}`\n);\n}\n}\n\nprotected async persistSession(\nsessionCreate: TracerSessionCreate\n): Promise<TracerSession> {\nconst endpoint = `${this.endpoint}/sessions`;\nconst response = await fetch(endpoint, {\nmethod: \"POST\",\nheaders: this.headers,\nbody: JSON.stringify(sessionCreate),\n});\nif (!response.ok) {\nconsole.error(\n`Failed to persist session: ${response.status} ${response.statusText}, using default session.`\n);\nreturn {\nid: 1,\n...sessionCreate,\n};\n}\nreturn {\nid: (await response.json()).id,\n...sessionCreate,\n};\n}\n\nasync loadSession(sessionName: string): Promise<TracerSession> {\nconst endpoint = `${this.endpoint}/sessions?name=${sessionName}`;\nreturn this._handleSessionResponse(endpoint);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":5932,"to":5971}}}}],["151",{"pageContent":"async loadDefaultSession(): Promise<TracerSession> {\nconst endpoint = `${this.endpoint}/sessions?name=default`;\nreturn this._handleSessionResponse(endpoint);\n}\n\nprivate async _handleSessionResponse(endpoint: string) {\nconst response = await fetch(endpoint, {\nmethod: \"GET\",\nheaders: this.headers,\n});\nlet tracerSession: TracerSession;\nif (!response.ok) {\nconsole.error(\n`Failed to load session: ${response.status} ${response.statusText}`\n);\ntracerSession = {\nid: 1,\nstart_time: Date.now(),\n};\nthis.session = tracerSession;\nreturn tracerSession;\n}\nconst resp = (await response.json()) as TracerSession[];\nif (resp.length === 0) {\ntracerSession = {\nid: 1,\nstart_time: Date.now(),\n};\nthis.session = tracerSession;\nreturn tracerSession;\n}\n[tracerSession] = resp;\nthis.session = tracerSession;\nreturn tracerSession;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/handlers/tracers.ts","loc":{"lines":{"from":6391,"to":6426}}}}],["152",{"pageContent":"export {\nBaseCallbackHandler,\nCallbackHandlerMethods,\nBaseCallbackHandlerInput,\n} from \"./base.js\";\n\nexport {\nLangChainTracer,\nBaseRun,\nLLMRun,\nChainRun,\nToolRun,\n} from \"./handlers/tracers.js\";\n\nexport { getTracingCallbackHandler } from \"./handlers/initialize.js\";\n\nexport {\nCallbackManager,\nCallbackManagerForChainRun,\nCallbackManagerForLLMRun,\nCallbackManagerForToolRun,\nCallbackManagerOptions,\nCallbacks,\n} from \"./manager.js\";\nexport { ConsoleCallbackHandler } from \"./handlers/console.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/index.ts","loc":{"lines":{"from":1,"to":25}}}}],["153",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\nimport {\nAgentAction,\nAgentFinish,\nChainValues,\nLLMResult,\n} from \"../schema/index.js\";\nimport { BaseCallbackHandler, CallbackHandlerMethods } from \"./base.js\";\nimport { ConsoleCallbackHandler } from \"./handlers/console.js\";\nimport { getTracingCallbackHandler } from \"./handlers/initialize.js\";\n\ntype BaseCallbackManagerMethods = {\n[K in keyof CallbackHandlerMethods]?: (\n...args: Parameters<Required<CallbackHandlerMethods>[K]>\n) => Promise<unknown>;\n};\n\nexport interface CallbackManagerOptions {\nverbose?: boolean;\ntracing?: boolean;\n}\n\nexport type Callbacks =\n| CallbackManager\n| (BaseCallbackHandler | CallbackHandlerMethods)[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":1,"to":25}}}}],["154",{"pageContent":"abstract class BaseCallbackManager {\nabstract addHandler(handler: BaseCallbackHandler): void;\n\nabstract removeHandler(handler: BaseCallbackHandler): void;\n\nabstract setHandlers(handlers: BaseCallbackHandler[]): void;\n\nsetHandler(handler: BaseCallbackHandler): void {\nreturn this.setHandlers([handler]);\n}\n}\n\nclass BaseRunManager {\nconstructor(\npublic readonly runId: string,\nprotected readonly handlers: BaseCallbackHandler[],\nprotected readonly inheritableHandlers: BaseCallbackHandler[],\nprotected readonly _parentRunId?: string\n) {}\n\nasync handleText(text: string): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\ntry {\nawait handler.handleText?.(text, this.runId, this._parentRunId);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleText: ${err}`\n);\n}\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":502,"to":535}}}}],["155",{"pageContent":"class CallbackManagerForLLMRun\nextends BaseRunManager\nimplements BaseCallbackManagerMethods\n{\nasync handleLLMNewToken(token: string): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreLLM) {\ntry {\nawait handler.handleLLMNewToken?.(\ntoken,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleLLMNewToken: ${err}`\n);\n}\n}\n})\n);\n}\n\nasync handleLLMError(err: Error | unknown): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreLLM) {\ntry {\nawait handler.handleLLMError?.(err, this.runId, this._parentRunId);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleLLMError: ${err}`\n);\n}\n}\n})\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":1007,"to":1045}}}}],["156",{"pageContent":"async handleLLMEnd(output: LLMResult): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreLLM) {\ntry {\nawait handler.handleLLMEnd?.(output, this.runId, this._parentRunId);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleLLMEnd: ${err}`\n);\n}\n}\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":1519,"to":1534}}}}],["157",{"pageContent":"class CallbackManagerForChainRun\nextends BaseRunManager\nimplements BaseCallbackManagerMethods\n{\ngetChild(): CallbackManager {\n// eslint-disable-next-line @typescript-eslint/no-use-before-define\nconst manager = new CallbackManager(this.runId);\nmanager.setHandlers(this.inheritableHandlers);\nreturn manager;\n}\n\nasync handleChainError(err: Error | unknown): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreChain) {\ntry {\nawait handler.handleChainError?.(\nerr,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleChainError: ${err}`\n);\n}\n}\n})\n);\n}\n\nasync handleChainEnd(output: ChainValues): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreChain) {\ntry {\nawait handler.handleChainEnd?.(\noutput,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleChainEnd: ${err}`\n);\n}\n}\n})\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":2026,"to":2075}}}}],["158",{"pageContent":"async handleAgentAction(action: AgentAction): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreAgent) {\ntry {\nawait handler.handleAgentAction?.(\naction,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleAgentAction: ${err}`\n);\n}\n}\n})\n);\n}\n\nasync handleAgentEnd(action: AgentFinish): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreAgent) {\ntry {\nawait handler.handleAgentEnd?.(\naction,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleAgentEnd: ${err}`\n);\n}\n}\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":2542,"to":2581}}}}],["159",{"pageContent":"class CallbackManagerForToolRun\nextends BaseRunManager\nimplements BaseCallbackManagerMethods\n{\ngetChild(): CallbackManager {\n// eslint-disable-next-line @typescript-eslint/no-use-before-define\nconst manager = new CallbackManager(this.runId);\nmanager.setHandlers(this.inheritableHandlers);\nreturn manager;\n}\n\nasync handleToolError(err: Error | unknown): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreAgent) {\ntry {\nawait handler.handleToolError?.(err, this.runId, this._parentRunId);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleToolError: ${err}`\n);\n}\n}\n})\n);\n}\n\nasync handleToolEnd(output: string): Promise<void> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreAgent) {\ntry {\nawait handler.handleToolEnd?.(\noutput,\nthis.runId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleToolEnd: ${err}`\n);\n}\n}\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":3058,"to":3104}}}}],["160",{"pageContent":"class CallbackManager\nextends BaseCallbackManager\nimplements BaseCallbackManagerMethods\n{\nhandlers: BaseCallbackHandler[];\n\ninheritableHandlers: BaseCallbackHandler[];\n\nname = \"callback_manager\";\n\nprivate readonly _parentRunId?: string;\n\nconstructor(parentRunId?: string) {\nsuper();\nthis.handlers = [];\nthis.inheritableHandlers = [];\nthis._parentRunId = parentRunId;\n}\n\nasync handleLLMStart(\nllm: { name: string },\nprompts: string[],\nrunId: string = uuidv4()\n): Promise<CallbackManagerForLLMRun> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreLLM) {\ntry {\nawait handler.handleLLMStart?.(\nllm,\nprompts,\nrunId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleLLMStart: ${err}`\n);\n}\n}\n})\n);\nreturn new CallbackManagerForLLMRun(\nrunId,\nthis.handlers,\nthis.inheritableHandlers,\nthis._parentRunId\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":3571,"to":3619}}}}],["161",{"pageContent":"async handleChainStart(\nchain: { name: string },\ninputs: ChainValues,\nrunId = uuidv4()\n): Promise<CallbackManagerForChainRun> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreChain) {\ntry {\nawait handler.handleChainStart?.(\nchain,\ninputs,\nrunId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleChainStart: ${err}`\n);\n}\n}\n})\n);\nreturn new CallbackManagerForChainRun(\nrunId,\nthis.handlers,\nthis.inheritableHandlers,\nthis._parentRunId\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":4089,"to":4118}}}}],["162",{"pageContent":"async handleToolStart(\ntool: { name: string },\ninput: string,\nrunId = uuidv4()\n): Promise<CallbackManagerForToolRun> {\nawait Promise.all(\nthis.handlers.map(async (handler) => {\nif (!handler.ignoreAgent) {\ntry {\nawait handler.handleToolStart?.(\ntool,\ninput,\nrunId,\nthis._parentRunId\n);\n} catch (err) {\nconsole.error(\n`Error in handler ${handler.constructor.name}, handleToolStart: ${err}`\n);\n}\n}\n})\n);\nreturn new CallbackManagerForToolRun(\nrunId,\nthis.handlers,\nthis.inheritableHandlers,\nthis._parentRunId\n);\n}\n\naddHandler(handler: BaseCallbackHandler, inherit = true): void {\nthis.handlers.push(handler);\nif (inherit) {\nthis.inheritableHandlers.push(handler);\n}\n}\n\nremoveHandler(handler: BaseCallbackHandler): void {\nthis.handlers = this.handlers.filter((_handler) => _handler !== handler);\nthis.inheritableHandlers = this.inheritableHandlers.filter(\n(_handler) => _handler !== handler\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":4602,"to":4645}}}}],["163",{"pageContent":"setHandlers(handlers: BaseCallbackHandler[], inherit = true): void {\nthis.handlers = [];\nthis.inheritableHandlers = [];\nfor (const handler of handlers) {\nthis.addHandler(handler, inherit);\n}\n}\n\ncopy(\nadditionalHandlers: BaseCallbackHandler[] = [],\ninherit = true\n): CallbackManager {\nconst manager = new CallbackManager(this._parentRunId);\nfor (const handler of this.handlers) {\nconst inheritable = this.inheritableHandlers.includes(handler);\nconst copied = handler.copy();\nmanager.addHandler(copied, inheritable);\n}\nfor (const handler of additionalHandlers) {\nif (\n// Prevent multiple copies of console_callback_handler\nmanager.handlers\n.filter((h) => h.name === \"console_callback_handler\")\n.some((h) => h.name === handler.name)\n) {\ncontinue;\n}\nmanager.addHandler(handler.copy(), inherit);\n}\nreturn manager;\n}\n\nstatic fromHandlers(handlers: CallbackHandlerMethods) {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":5115,"to":5147}}}}],["164",{"pageContent":"Handler extends BaseCallbackHandler {\nname = uuidv4();\n\nconstructor() {\nsuper();\nObject.assign(this, handlers);\n}\n}\n\nconst manager = new this();\nmanager.addHandler(new Handler());\nreturn manager;\n}\n\nstatic async configure(\ninheritableHandlers?: Callbacks,\nlocalHandlers?: Callbacks,\noptions?: CallbackManagerOptions\n): Promise<CallbackManager | undefined> {\nlet callbackManager: CallbackManager | undefined;\nif (inheritableHandlers || localHandlers) {\nif (Array.isArray(inheritableHandlers) || !inheritableHandlers) {\ncallbackManager = new CallbackManager();\ncallbackManager.setHandlers(\ninheritableHandlers?.map(ensureHandler) ?? [],\ntrue\n);\n} else {\ncallbackManager = inheritableHandlers;\n}\ncallbackManager = callbackManager.copy(\nArray.isArray(localHandlers)\n? localHandlers.map(ensureHandler)\n: localHandlers?.handlers,\nfalse\n);\n}\nconst tracingEnabled =","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":5617,"to":5654}}}}],["165",{"pageContent":"of process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.LANGCHAIN_TRACING !== undefined\n: false;\nif (options?.verbose || tracingEnabled) {\nif (!callbackManager) {\ncallbackManager = new CallbackManager();\n}\nif (\noptions?.verbose &&\n!callbackManager.handlers.some(\n(handler) => handler.name === ConsoleCallbackHandler.prototype.name\n)\n) {\nconst consoleHandler = new ConsoleCallbackHandler();\ncallbackManager.addHandler(consoleHandler, true);\n}\nif (\ntracingEnabled &&\n!callbackManager.handlers.some(\n(handler) => handler.name === \"langchain_tracer\"\n)\n) {\nconst session =\ntypeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.LANGCHAIN_SESSION\n: undefined;\ncallbackManager.addHandler(\nawait getTracingCallbackHandler(session),\ntrue\n);\n}\n}\nreturn callbackManager;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":6126,"to":6162}}}}],["166",{"pageContent":"ensureHandler(\nhandler: BaseCallbackHandler | CallbackHandlerMethods\n): BaseCallbackHandler {\nif (\"name\" in handler) {\nreturn handler;\n}\n\nreturn BaseCallbackHandler.fromMethods(handler);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/manager.ts","loc":{"lines":{"from":6634,"to":6642}}}}],["167",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { CallbackManager } from \"../manager.js\";\nimport { BaseCallbackHandler, BaseCallbackHandlerInput } from \"../base.js\";\nimport {\nAgentAction,\nAgentFinish,\nChainValues,\nLLMResult,\n} from \"../../schema/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":1,"to":10}}}}],["168",{"pageContent":"FakeCallbackHandler extends BaseCallbackHandler {\nname = `fake-${uuidv4()}`;\n\nstarts = 0;\n\nends = 0;\n\nerrors = 0;\n\nchainStarts = 0;\n\nchainEnds = 0;\n\nllmStarts = 0;\n\nllmEnds = 0;\n\nllmStreams = 0;\n\ntoolStarts = 0;\n\ntoolEnds = 0;\n\nagentEnds = 0;\n\ntexts = 0;\n\nconstructor(inputs?: BaseCallbackHandlerInput) {\nsuper(inputs);\n}\n\nasync handleLLMStart(\n_llm: { name: string },\n_prompts: string[]\n): Promise<void> {\nthis.starts += 1;\nthis.llmStarts += 1;\n}\n\nasync handleLLMEnd(_output: LLMResult): Promise<void> {\nthis.ends += 1;\nthis.llmEnds += 1;\n}\n\nasync handleLLMNewToken(_token: string): Promise<void> {\nthis.llmStreams += 1;\n}\n\nasync handleLLMError(_err: Error): Promise<void> {\nthis.errors += 1;\n}\n\nasync handleChainStart(\n_chain: { name: string },\n_inputs: ChainValues\n): Promise<void> {\nthis.starts += 1;\nthis.chainStarts += 1;\n}\n\nasync handleChainEnd(_outputs: ChainValues): Promise<void> {\nthis.ends += 1;\nthis.chainEnds += 1;\n}\n\nasync handleChainError(_err: Error): Promise<void> {\nthis.errors += 1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":374,"to":441}}}}],["169",{"pageContent":"async handleToolStart(\n_tool: { name: string },\n_input: string\n): Promise<void> {\nthis.starts += 1;\nthis.toolStarts += 1;\n}\n\nasync handleToolEnd(_output: string): Promise<void> {\nthis.ends += 1;\nthis.toolEnds += 1;\n}\n\nasync handleToolError(_err: Error): Promise<void> {\nthis.errors += 1;\n}\n\nasync handleText(_text: string): Promise<void> {\nthis.texts += 1;\n}\n\nasync handleAgentAction(_action: AgentAction): Promise<void> {\nthis.starts += 1;\nthis.toolStarts += 1;\n}\n\nasync handleAgentEnd(_action: AgentFinish): Promise<void> {\nthis.ends += 1;\nthis.agentEnds += 1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":758,"to":787}}}}],["170",{"pageContent":"copy(): FakeCallbackHandler {\nconst newInstance = new FakeCallbackHandler();\nnewInstance.name = this.name;\nnewInstance.starts = this.starts;\nnewInstance.ends = this.ends;\nnewInstance.errors = this.errors;\nnewInstance.chainStarts = this.chainStarts;\nnewInstance.chainEnds = this.chainEnds;\nnewInstance.llmStarts = this.llmStarts;\nnewInstance.llmEnds = this.llmEnds;\nnewInstance.llmStreams = this.llmStreams;\nnewInstance.toolStarts = this.toolStarts;\nnewInstance.toolEnds = this.toolEnds;\nnewInstance.agentEnds = this.agentEnds;\nnewInstance.texts = this.texts;\n\nreturn newInstance;\n}\n}\n\ntest(\"CallbackManager\", async () => {\nconst manager = new CallbackManager();\nconst handler1 = new FakeCallbackHandler();\nconst handler2 = new FakeCallbackHandler();\nmanager.addHandler(handler1);\nmanager.addHandler(handler2);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":1127,"to":1152}}}}],["171",{"pageContent":"const llmCb = await manager.handleLLMStart({ name: \"test\" }, [\"test\"]);\nawait llmCb.handleLLMEnd({ generations: [] });\nawait llmCb.handleLLMNewToken(\"test\");\nawait llmCb.handleLLMError(new Error(\"test\"));\nconst chainCb = await manager.handleChainStart(\n{ name: \"test\" },\n{ test: \"test\" }\n);\nawait chainCb.handleChainEnd({ test: \"test\" });\nawait chainCb.handleChainError(new Error(\"test\"));\nconst toolCb = await manager.handleToolStart({ name: \"test\" }, \"test\");\nawait toolCb.handleToolEnd(\"test\");\nawait toolCb.handleToolError(new Error(\"test\"));\nawait chainCb.handleText(\"test\");\nawait chainCb.handleAgentAction({\ntool: \"test\",\ntoolInput: \"test\",\nlog: \"test\",\n});\nawait chainCb.handleAgentEnd({ returnValues: { test: \"test\" }, log: \"test\" });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":1478,"to":1497}}}}],["172",{"pageContent":"for (const handler of [handler1, handler2]) {\nexpect(handler.starts).toBe(4);\nexpect(handler.ends).toBe(4);\nexpect(handler.errors).toBe(3);\nexpect(handler.llmStarts).toBe(1);\nexpect(handler.llmEnds).toBe(1);\nexpect(handler.llmStreams).toBe(1);\nexpect(handler.chainStarts).toBe(1);\nexpect(handler.chainEnds).toBe(1);\nexpect(handler.toolStarts).toBe(2);\nexpect(handler.toolEnds).toBe(1);\nexpect(handler.agentEnds).toBe(1);\nexpect(handler.texts).toBe(1);\n}\n});\n\ntest(\"CallbackHandler with ignoreLLM\", async () => {\nconst handler = new FakeCallbackHandler({\nignoreLLM: true,\n});\nconst manager = new CallbackManager();\nmanager.addHandler(handler);\nconst llmCb = await manager.handleLLMStart({ name: \"test\" }, [\"test\"]);\nawait llmCb.handleLLMEnd({ generations: [] });\nawait llmCb.handleLLMNewToken(\"test\");\nawait llmCb.handleLLMError(new Error(\"test\"));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":1826,"to":1851}}}}],["173",{"pageContent":"expect(handler.starts).toBe(0);\nexpect(handler.ends).toBe(0);\nexpect(handler.errors).toBe(0);\nexpect(handler.llmStarts).toBe(0);\nexpect(handler.llmEnds).toBe(0);\nexpect(handler.llmStreams).toBe(0);\n});\n\ntest(\"CallbackHandler with ignoreChain\", async () => {\nconst handler = new FakeCallbackHandler({\nignoreChain: true,\n});\nconst manager = new CallbackManager();\nmanager.addHandler(handler);\nconst chainCb = await manager.handleChainStart(\n{ name: \"test\" },\n{ test: \"test\" }\n);\nawait chainCb.handleChainEnd({ test: \"test\" });\nawait chainCb.handleChainError(new Error(\"test\"));\n\nexpect(handler.starts).toBe(0);\nexpect(handler.ends).toBe(0);\nexpect(handler.errors).toBe(0);\nexpect(handler.chainStarts).toBe(0);\nexpect(handler.chainEnds).toBe(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":2174,"to":2200}}}}],["174",{"pageContent":"test(\"CallbackHandler with ignoreAgent\", async () => {\nconst handler = new FakeCallbackHandler({\nignoreAgent: true,\n});\nconst manager = new CallbackManager();\nmanager.addHandler(handler);\nconst toolCb = await manager.handleToolStart({ name: \"test\" }, \"test\");\nawait toolCb.handleToolEnd(\"test\");\nawait toolCb.handleToolError(new Error(\"test\"));\nconst chainCb = await manager.handleChainStart(\n{ name: \"agent_executor\" },\n{}\n);\nawait chainCb.handleAgentAction({\ntool: \"test\",\ntoolInput: \"test\",\nlog: \"test\",\n});\nawait chainCb.handleAgentEnd({ returnValues: { test: \"test\" }, log: \"test\" });\n\nexpect(handler.starts).toBe(1);\nexpect(handler.ends).toBe(0);\nexpect(handler.errors).toBe(0);\nexpect(handler.toolStarts).toBe(0);\nexpect(handler.toolEnds).toBe(0);\nexpect(handler.agentEnds).toBe(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":2529,"to":2555}}}}],["175",{"pageContent":"test(\"CallbackManager with child manager\", async () => {\nconst llmRunId = \"llmRunId\";\nconst chainRunId = \"chainRunId\";\nlet llmWasCalled = false;\nlet chainWasCalled = false;\nconst manager = CallbackManager.fromHandlers({\nasync handleLLMStart(\n_llm: { name: string },\n_prompts: string[],\nrunId?: string,\nparentRunId?: string\n) {\nexpect(runId).toBe(llmRunId);\nexpect(parentRunId).toBe(chainRunId);\nllmWasCalled = true;\n},\nasync handleChainStart(\n_chain: { name: string },\n_inputs: ChainValues,\nrunId?: string,\nparentRunId?: string\n) {\nexpect(runId).toBe(chainRunId);\nexpect(parentRunId).toBe(undefined);\nchainWasCalled = true;\n},\n});\nconst chainCb = await manager.handleChainStart(\n{ name: \"test\" },\n{ test: \"test\" },\nchainRunId\n);\nawait chainCb.getChild().handleLLMStart({ name: \"test\" }, [\"test\"], llmRunId);\nexpect(llmWasCalled).toBe(true);\nexpect(chainWasCalled).toBe(true);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":2881,"to":2916}}}}],["176",{"pageContent":"test(\"CallbackManager with child manager inherited handlers\", async () => {\nconst callbackManager1 = new CallbackManager();\nconst handler1 = new FakeCallbackHandler();\nconst handler2 = new FakeCallbackHandler();\nconst handler3 = new FakeCallbackHandler();\nconst handler4 = new FakeCallbackHandler();\n\ncallbackManager1.setHandlers([handler1, handler2]);\nexpect(callbackManager1.handlers).toEqual([handler1, handler2]);\nexpect(callbackManager1.inheritableHandlers).toEqual([handler1, handler2]);\n\nconst callbackManager2 = callbackManager1.copy([handler3, handler4]);\nexpect(callbackManager2.handlers).toEqual([\nhandler1,\nhandler2,\nhandler3,\nhandler4,\n]);\nexpect(callbackManager2.inheritableHandlers).toEqual([\nhandler1,\nhandler2,\nhandler3,\nhandler4,\n]);\n\nconst callbackManager3 = callbackManager1.copy([handler3, handler4], false);\nexpect(callbackManager3.handlers).toEqual([\nhandler1,\nhandler2,\nhandler3,\nhandler4,\n]);\nexpect(callbackManager3.inheritableHandlers).toEqual([handler1, handler2]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":3238,"to":3270}}}}],["177",{"pageContent":"const chainCb = await callbackManager3.handleChainStart(\n{ name: \"test\" },\n{ test: \"test\" }\n);\nconst childManager = chainCb.getChild();\nexpect(childManager.handlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\n]);\nexpect(childManager.inheritableHandlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\n]);\n\nconst toolCb = await childManager.handleToolStart({ name: \"test\" }, \"test\");\nconst childManager2 = toolCb.getChild();\nexpect(childManager2.handlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\n]);\nexpect(childManager2.inheritableHandlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\n]);\n});\n\ntest(\"CallbackManager.copy()\", () => {\nconst callbackManager1 = new CallbackManager();\nconst handler1 = new FakeCallbackHandler();\nconst handler2 = new FakeCallbackHandler();\nconst handler3 = new FakeCallbackHandler();\nconst handler4 = new FakeCallbackHandler();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":3587,"to":3618}}}}],["178",{"pageContent":"callbackManager1.addHandler(handler1, true);\ncallbackManager1.addHandler(handler2, false);\nexpect(callbackManager1.handlers).toEqual([handler1, handler2]);\nexpect(callbackManager1.inheritableHandlers).toEqual([handler1]);\n\nconst callbackManager2 = callbackManager1.copy([handler3]);\nexpect(callbackManager2.handlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\nhandler3.name,\n]);\nexpect(callbackManager2.inheritableHandlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler3.name,\n]);\n\nconst callbackManager3 = callbackManager2.copy([handler4], false);\nexpect(callbackManager3.handlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler2.name,\nhandler3.name,\nhandler4.name,\n]);\nexpect(callbackManager3.inheritableHandlers.map((h) => h.name)).toEqual([\nhandler1.name,\nhandler3.name,\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/callbacks.test.ts","loc":{"lines":{"from":3939,"to":3966}}}}],["179",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\n/* eslint-disable no-process-env */\nimport { test } from \"@jest/globals\";\n\nimport { LangChainTracer } from \"../handlers/tracers.js\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { SerpAPI } from \"../../tools/index.js\";\nimport { Calculator } from \"../../tools/calculator.js\";\nimport { initializeAgentExecutorWithOptions } from \"../../agents/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/langchain_tracer.int.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["180",{"pageContent":"test(\"Test LangChain tracer\", async () => {\nconst tracer = new LangChainTracer();\nconst chainRunId = uuidv4();\nconst toolRunId = uuidv4();\nconst llmRunId = uuidv4();\nawait tracer.handleChainStart({ name: \"test\" }, { foo: \"bar\" }, chainRunId);\nawait tracer.handleToolStart({ name: \"test\" }, \"test\", toolRunId, chainRunId);\nawait tracer.handleLLMStart({ name: \"test\" }, [\"test\"], llmRunId, toolRunId);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId);\nawait tracer.handleToolEnd(\"output\", toolRunId);\nconst llmRunId2 = uuidv4();\nawait tracer.handleLLMStart(\n{ name: \"test2\" },\n[\"test\"],\nllmRunId2,\nchainRunId\n);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId2);\nawait tracer.handleChainEnd({ foo: \"bar\" }, chainRunId);\n\nconst llmRunId3 = uuidv4();\nawait tracer.handleLLMStart({ name: \"test\" }, [\"test\"], llmRunId3);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId3);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/langchain_tracer.int.test.ts","loc":{"lines":{"from":66,"to":89}}}}],["181",{"pageContent":"test(\"Test Traced Agent with concurrency\", async () => {\nprocess.env.LANGCHAIN_TRACING = \"true\";\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\nnew SerpAPI(process.env.SERPAPI_API_KEY, {\nlocation: \"Austin,Texas,United States\",\nhl: \"en\",\ngl: \"us\",\n}),\nnew Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\nagentType: \"zero-shot-react-description\",\nverbose: true,\n});\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst [resultA, resultB, resultC] = await Promise.all([\nexecutor.call({ input }),\nexecutor.call({ input }),\nexecutor.call({ input }),\n]);\n\nconsole.log(`Got output ${resultA.output}`);\nconsole.log(`Got output ${resultB.output}`);\nconsole.log(`Got output ${resultC.output}`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/langchain_tracer.int.test.ts","loc":{"lines":{"from":135,"to":165}}}}],["182",{"pageContent":"import { test, expect, jest } from \"@jest/globals\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport {\nBaseTracer,\nLLMRun,\nChainRun,\nToolRun,\nTracerSession,\nTracerSessionCreate,\n} from \"../handlers/tracers.js\";\n\nconst TEST_SESSION_ID = 2023;\nconst _DATE = 1620000000000;\n\nDate.now = jest.fn(() => _DATE);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":1,"to":15}}}}],["183",{"pageContent":"FakeTracer extends BaseTracer {\nname = \"fake_tracer\";\n\nruns: (LLMRun | ChainRun | ToolRun)[] = [];\n\nconstructor() {\nsuper();\n}\n\nprotected persistRun(run: LLMRun | ChainRun | ToolRun): Promise<void> {\nthis.runs.push(run);\nreturn Promise.resolve();\n}\n\nprotected persistSession(\nsession: TracerSessionCreate\n): Promise<TracerSession> {\nreturn Promise.resolve({\nid: TEST_SESSION_ID,\n...session,\n});\n}\n\nasync loadSession(sessionName: string): Promise<TracerSession> {\nreturn Promise.resolve({\nid: TEST_SESSION_ID,\nname: sessionName,\nstart_time: _DATE,\n});\n}\n\nasync loadDefaultSession(): Promise<TracerSession> {\nreturn Promise.resolve({\nid: TEST_SESSION_ID,\nname: \"default\",\nstart_time: _DATE,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":246,"to":284}}}}],["184",{"pageContent":"test(\"Test LLMRun\", async () => {\nconst tracer = new FakeTracer();\nawait tracer.newSession();\nconst runId = uuidv4();\nawait tracer.handleLLMStart({ name: \"test\" }, [\"test\"], runId);\nawait tracer.handleLLMEnd({ generations: [] }, runId);\nexpect(tracer.runs.length).toBe(1);\nconst run = tracer.runs[0];\nconst compareRun: LLMRun = {\nuuid: runId,\nstart_time: _DATE,\nend_time: _DATE,\nexecution_order: 1,\nchild_execution_order: 1,\nserialized: { name: \"test\" },\nsession_id: TEST_SESSION_ID,\nprompts: [\"test\"],","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":496,"to":512}}}}],["185",{"pageContent":": \"llm\",\nresponse: { generations: [] },\n};\nexpect(run).toEqual(compareRun);\n});\n\ntest(\"Test LLM Run no start\", async () => {\nconst tracer = new FakeTracer();\nawait tracer.newSession();\nconst runId = uuidv4();\nawait expect(tracer.handleLLMEnd({ generations: [] }, runId)).rejects.toThrow(\n\"No LLM run to end\"\n);\n});\n\ntest(\"Test Chain Run\", async () => {\nconst tracer = new FakeTracer();\nawait tracer.newSession();\nconst runId = uuidv4();\nconst compareRun: ChainRun = {\nuuid: runId,\nstart_time: _DATE,\nend_time: _DATE,\nexecution_order: 1,\nchild_execution_order: 1,\nserialized: { name: \"test\" },\nsession_id: TEST_SESSION_ID,\ninputs: { foo: \"bar\" },\noutputs: { foo: \"bar\" },","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":732,"to":760}}}}],["186",{"pageContent":": \"chain\",\nchild_llm_runs: [],\nchild_chain_runs: [],\nchild_tool_runs: [],\n};\nawait tracer.handleChainStart({ name: \"test\" }, { foo: \"bar\" }, runId);\nawait tracer.handleChainEnd({ foo: \"bar\" }, runId);\nexpect(tracer.runs.length).toBe(1);\nconst run = tracer.runs[0];\nexpect(run).toEqual(compareRun);\n});\n\ntest(\"Test Tool Run\", async () => {\nconst tracer = new FakeTracer();\nawait tracer.newSession();\nconst runId = uuidv4();\nconst compareRun: ToolRun = {\nuuid: runId,\nstart_time: _DATE,\nend_time: _DATE,\nexecution_order: 1,\nchild_execution_order: 1,\nserialized: { name: \"test\" },\nsession_id: TEST_SESSION_ID,\ntool_input: \"test\",\noutput: \"output\",","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":973,"to":998}}}}],["187",{"pageContent":": \"tool\",\naction: JSON.stringify({ name: \"test\" }),\nchild_llm_runs: [],\nchild_chain_runs: [],\nchild_tool_runs: [],\n};\nawait tracer.handleToolStart({ name: \"test\" }, \"test\", runId);\nawait tracer.handleToolEnd(\"output\", runId);\nexpect(tracer.runs.length).toBe(1);\nconst run = tracer.runs[0];\nexpect(run).toEqual(compareRun);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":1212,"to":1223}}}}],["188",{"pageContent":"test(\"Test nested runs\", async () => {\nconst tracer = new FakeTracer();\nawait tracer.newSession();\nconst chainRunId = uuidv4();\nconst toolRunId = uuidv4();\nconst llmRunId = uuidv4();\nawait tracer.handleChainStart({ name: \"test\" }, { foo: \"bar\" }, chainRunId);\nawait tracer.handleToolStart({ name: \"test\" }, \"test\", toolRunId, chainRunId);\nawait tracer.handleLLMStart({ name: \"test\" }, [\"test\"], llmRunId, toolRunId);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId);\nawait tracer.handleToolEnd(\"output\", toolRunId);\nconst llmRunId2 = uuidv4();\nawait tracer.handleLLMStart(\n{ name: \"test2\" },\n[\"test\"],\nllmRunId2,\nchainRunId\n);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId2);\nawait tracer.handleChainEnd({ foo: \"bar\" }, chainRunId);\nconst compareRun: ChainRun = {\nchild_chain_runs: [],\nchild_llm_runs: [\n{\nuuid: llmRunId2,\nparent_uuid: chainRunId,\nend_time: 1620000000000,\nexecution_order: 4,\nchild_execution_order: 4,\nprompts: [\"test\"],\nresponse: {\ngenerations: [[]],\n},\nserialized: {\nname: \"test2\",\n},","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":1452,"to":1487}}}}],["189",{"pageContent":"session_id: 2023,\nstart_time: 1620000000000,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":1684,"to":1685}}}}],["190",{"pageContent":": \"llm\",\n},\n],\nchild_tool_runs: [\n{\nuuid: toolRunId,\nparent_uuid: chainRunId,\naction: '{\"name\":\"test\"}',\nchild_chain_runs: [],\nchild_llm_runs: [\n{\nuuid: llmRunId,\nparent_uuid: toolRunId,\nend_time: 1620000000000,\nexecution_order: 3,\nchild_execution_order: 3,\nprompts: [\"test\"],\nresponse: {\ngenerations: [[]],\n},\nserialized: {\nname: \"test\",\n},\nsession_id: 2023,\nstart_time: 1620000000000,\ntype: \"llm\",\n},\n],\nchild_tool_runs: [],\nend_time: 1620000000000,\nexecution_order: 2,\nchild_execution_order: 3,\noutput: \"output\",\nserialized: {\nname: \"test\",\n},\nsession_id: 2023,\nstart_time: 1620000000000,\ntool_input: \"test\",\ntype: \"tool\",\n},\n],\nuuid: chainRunId,\nend_time: 1620000000000,\nexecution_order: 1,\nchild_execution_order: 4,\ninputs: {\nfoo: \"bar\",\n},\noutputs: {\nfoo: \"bar\",\n},\nserialized: {\nname: \"test\",\n},\nsession_id: 2023,\nstart_time: 1620000000000,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":1930,"to":1986}}}}],["191",{"pageContent":": \"chain\",\n};\nexpect(tracer.runs.length).toBe(1);\nexpect(tracer.runs[0]).toEqual(compareRun);\n\nconst llmRunId3 = uuidv4();\nawait tracer.handleLLMStart({ name: \"test\" }, [\"test\"], llmRunId3);\nawait tracer.handleLLMEnd({ generations: [[]] }, llmRunId3);\nexpect(tracer.runs.length).toBe(2);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/callbacks/tests/tracer.test.ts","loc":{"lines":{"from":2191,"to":2200}}}}],["192",{"pageContent":"import { BaseChain, ChainInputs } from \"./base.js\";\nimport {\nTextSplitter,\nRecursiveCharacterTextSplitter,\n} from \"../text_splitter.js\";\nimport { ChainValues } from \"../schema/index.js\";\nimport { SerializedAnalyzeDocumentChain } from \"./serde.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type LoadValues = Record<string, any>;\n\nexport interface AnalyzeDocumentChainInput extends Omit<ChainInputs, \"memory\"> {\ncombineDocumentsChain: BaseChain;\ntextSplitter?: TextSplitter;\ninputKey?: string;\n}\n\n/**\n* Chain that combines documents by stuffing into context.\n* @augments BaseChain\n* @augments StuffDocumentsChainInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/analyze_documents_chain.ts","loc":{"lines":{"from":1,"to":23}}}}],["193",{"pageContent":"class AnalyzeDocumentChain\nextends BaseChain\nimplements AnalyzeDocumentChainInput\n{\ninputKey = \"input_document\";\n\ncombineDocumentsChain: BaseChain;\n\ntextSplitter: TextSplitter;\n\nconstructor(fields: AnalyzeDocumentChainInput) {\nsuper(fields);\nthis.combineDocumentsChain = fields.combineDocumentsChain;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.textSplitter =\nfields.textSplitter ?? new RecursiveCharacterTextSplitter();\n}\n\nget inputKeys(): string[] {\nreturn [this.inputKey];\n}\n\nget outputKeys(): string[] {\nreturn this.combineDocumentsChain.outputKeys;\n}\n\n/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Document key ${this.inputKey} not found.`);\n}\nconst { [this.inputKey]: doc, ...rest } = values;\n\nconst currentDoc = doc as string;\nconst currentDocs = await this.textSplitter.createDocuments([currentDoc]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/analyze_documents_chain.ts","loc":{"lines":{"from":107,"to":144}}}}],["194",{"pageContent":"const newInputs = { input_documents: currentDocs, ...rest };\nconst result = await this.combineDocumentsChain.call(\nnewInputs,\nrunManager?.getChild()\n);\nreturn result;\n}\n\n_chainType() {\nreturn \"analyze_document_chain\" as const;\n}\n\nstatic async deserialize(\ndata: SerializedAnalyzeDocumentChain,\nvalues: LoadValues\n) {\nif (!(\"text_splitter\" in values)) {\nthrow new Error(\n`Need to pass in a text_splitter to deserialize AnalyzeDocumentChain.`\n);\n}\nconst { text_splitter } = values;\n\nif (!data.combine_document_chain) {\nthrow new Error(\n`Need to pass in a combine_document_chain to deserialize AnalyzeDocumentChain.`\n);\n}\n\nreturn new AnalyzeDocumentChain({\ncombineDocumentsChain: await BaseChain.deserialize(\ndata.combine_document_chain\n),\ntextSplitter: text_splitter,\n});\n}\n\nserialize(): SerializedAnalyzeDocumentChain {\nreturn {\n_type: this._chainType(),\ncombine_document_chain: this.combineDocumentsChain.serialize(),\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/analyze_documents_chain.ts","loc":{"lines":{"from":216,"to":259}}}}],["195",{"pageContent":"import { BaseMemory } from \"../memory/base.js\";\nimport { ChainValues, RUN_KEY } from \"../schema/index.js\";\nimport {\nCallbackManagerForChainRun,\nCallbackManager,\nCallbacks,\n} from \"../callbacks/manager.js\";\nimport { SerializedBaseChain } from \"./serde.js\";\nimport { BaseLangChain, BaseLangChainParams } from \"../base_language/index.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type LoadValues = Record<string, any>;\n\nexport interface ChainInputs extends BaseLangChainParams {\nmemory?: BaseMemory;\n\n/**\n* @deprecated Use `callbacks` instead\n*/\ncallbackManager?: CallbackManager;\n}\n\n/**\n* Base interface that all chains must implement.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1,"to":25}}}}],["196",{"pageContent":"abstract class BaseChain extends BaseLangChain implements ChainInputs {\ndeclare memory?: BaseMemory;\n\nconstructor(\nfields?: BaseMemory | ChainInputs,\n/** @deprecated */\nverbose?: boolean,\n/** @deprecated */\ncallbacks?: Callbacks\n) {\nif (\narguments.length === 1 &&","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":201,"to":212}}}}],["197",{"pageContent":"of fields === \"object\" &&\n!(\"saveContext\" in fields)\n) {\n// fields is not a BaseMemory\nconst { memory, callbackManager, ...rest } = fields;\nsuper({ ...rest, callbacks: callbackManager ?? rest.callbacks });\nthis.memory = memory;\n} else {\n// fields is a BaseMemory\nsuper({ verbose, callbacks });\nthis.memory = fields as BaseMemory;\n}\n}\n\n/**\n* Run the core logic of this chain and return the output\n*/\nabstract _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues>;\n\n/**\n* Return the string type key uniquely identifying this class of chain.\n*/\nabstract _chainType(): string;\n\n/**\n* Return a json-like object representing this chain.\n*/\nserialize(): SerializedBaseChain {\nthrow new Error(\"Method not implemented.\");\n}\n\nabstract get inputKeys(): string[];\n\nabstract get outputKeys(): string[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":403,"to":439}}}}],["198",{"pageContent":"async run(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ninput: any,\ncallbacks?: Callbacks\n): Promise<string> {\nconst isKeylessInput = this.inputKeys.length <= 1;\nif (!isKeylessInput) {\nthrow new Error(\n`Chain ${this._chainType()} expects multiple inputs, cannot use 'run' `\n);\n}\nconst values = this.inputKeys.length ? { [this.inputKeys[0]]: input } : {};\nconst returnValues = await this.call(values, callbacks);\nconst keys = Object.keys(returnValues);\n\nif (keys.length === 1) {\nreturn returnValues[keys[0]];\n}\nthrow new Error(\n\"return values have multiple keys, `run` only supported when one key currently\"\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":609,"to":630}}}}],["199",{"pageContent":"/**\n* Run the core logic of this chain and add to output if desired.\n*\n* Wraps _call and handles memory.\n*/\nasync call(values: ChainValues, callbacks?: Callbacks): Promise<ChainValues> {\nconst fullValues = { ...values } as typeof values;\nif (!(this.memory == null)) {\nconst newValues = await this.memory.loadMemoryVariables(values);\nfor (const [key, value] of Object.entries(newValues)) {\nfullValues[key] = value;\n}\n}\nconst callbackManager_ = await CallbackManager.configure(\ncallbacks,\nthis.callbacks,\n{ verbose: this.verbose }\n);\nconst runManager = await callbackManager_?.handleChainStart(\n{ name: this._chainType() },\nfullValues\n);\nlet outputValues;\ntry {\noutputValues = await this._call(fullValues, runManager);\n} catch (e) {\nawait runManager?.handleChainError(e);\nthrow e;\n}\nawait runManager?.handleChainEnd(outputValues);\nif (!(this.memory == null)) {\nawait this.memory.saveContext(values, outputValues);\n}\n// add the runManager's currentRunId to the outputValues\nObject.defineProperty(outputValues, RUN_KEY, {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":808,"to":842}}}}],["200",{"pageContent":"value: runManager ? { runId: runManager?.runId } : undefined,\nconfigurable: true,\n});\nreturn outputValues;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1004,"to":1008}}}}],["201",{"pageContent":"/**\n* Call the chain on all inputs in the list\n*/\nasync apply(\ninputs: ChainValues[],\ncallbacks?: Callbacks[]\n): Promise<ChainValues> {\nreturn Promise.all(\ninputs.map(async (i, idx) => this.call(i, callbacks?.[idx]))\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1205,"to":1215}}}}],["202",{"pageContent":"/**\n* Load a chain from a json-like object describing it.\n*/\nstatic async deserialize(\ndata: SerializedBaseChain,\nvalues: LoadValues = {}\n): Promise<BaseChain> {\nswitch (data._type) {\ncase \"llm_chain\": {\nconst { LLMChain } = await import(\"./llm_chain.js\");\nreturn LLMChain.deserialize(data);\n}\ncase \"sequential_chain\": {\nconst { SequentialChain } = await import(\"./sequential_chain.js\");\nreturn SequentialChain.deserialize(data);\n}\ncase \"simple_sequential_chain\": {\nconst { SimpleSequentialChain } = await import(\"./sequential_chain.js\");\nreturn SimpleSequentialChain.deserialize(data);\n}\ncase \"stuff_documents_chain\": {\nconst { StuffDocumentsChain } = await import(\"./combine_docs_chain.js\");\nreturn StuffDocumentsChain.deserialize(data);\n}\ncase \"map_reduce_documents_chain\": {\nconst { MapReduceDocumentsChain } = await import(\n\"./combine_docs_chain.js\"\n);\nreturn MapReduceDocumentsChain.deserialize(data);\n}\ncase \"refine_documents_chain\": {\nconst { RefineDocumentsChain } = await import(\n\"./combine_docs_chain.js\"\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1407,"to":1440}}}}],["203",{"pageContent":"return RefineDocumentsChain.deserialize(data);\n}\ncase \"vector_db_qa\": {\nconst { VectorDBQAChain } = await import(\"./vector_db_qa.js\");\nreturn VectorDBQAChain.deserialize(data, values);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1602,"to":1607}}}}],["204",{"pageContent":":\nthrow new Error(\n`Invalid prompt type in config: ${\n(data as SerializedBaseChain)._type\n}`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/base.ts","loc":{"lines":{"from":1800,"to":1808}}}}],["205",{"pageContent":"import { PromptTemplate } from \"../prompts/prompt.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { VectorStore } from \"../vectorstores/base.js\";\nimport { SerializedChatVectorDBQAChain } from \"./serde.js\";\nimport { ChainValues } from \"../schema/index.js\";\nimport { BaseChain, ChainInputs } from \"./base.js\";\nimport { LLMChain } from \"./llm_chain.js\";\nimport { loadQAStuffChain } from \"./question_answering/load.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":1,"to":11}}}}],["206",{"pageContent":"type LoadValues = Record<string, any>;\n\nconst question_generator_template = `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:`;\n\nconst qa_template = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:`;\n\nexport interface ChatVectorDBQAChainInput extends ChainInputs {\nvectorstore: VectorStore;\ncombineDocumentsChain: BaseChain;\nquestionGeneratorChain: LLMChain;\nreturnSourceDocuments?: boolean;\noutputKey?: string;\ninputKey?: string;\nk?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":196,"to":220}}}}],["207",{"pageContent":"class ChatVectorDBQAChain\nextends BaseChain\nimplements ChatVectorDBQAChainInput\n{\nk = 4;\n\ninputKey = \"question\";\n\nchatHistoryKey = \"chat_history\";\n\nget inputKeys() {\nreturn [this.inputKey, this.chatHistoryKey];\n}\n\noutputKey = \"result\";\n\nget outputKeys() {\nreturn [this.outputKey];\n}\n\nvectorstore: VectorStore;\n\ncombineDocumentsChain: BaseChain;\n\nquestionGeneratorChain: LLMChain;\n\nreturnSourceDocuments = false;\n\nconstructor(fields: ChatVectorDBQAChainInput) {\nsuper(fields);\nthis.vectorstore = fields.vectorstore;\nthis.combineDocumentsChain = fields.combineDocumentsChain;\nthis.questionGeneratorChain = fields.questionGeneratorChain;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.outputKey = fields.outputKey ?? this.outputKey;\nthis.k = fields.k ?? this.k;\nthis.returnSourceDocuments =\nfields.returnSourceDocuments ?? this.returnSourceDocuments;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":402,"to":440}}}}],["208",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Question key ${this.inputKey} not found.`);\n}\nif (!(this.chatHistoryKey in values)) {\nthrow new Error(`chat history key ${this.inputKey} not found.`);\n}\nconst question: string = values[this.inputKey];\nconst chatHistory: string = values[this.chatHistoryKey];\nlet newQuestion = question;\nif (chatHistory.length > 0) {\nconst result = await this.questionGeneratorChain.call(\n{\nquestion,\nchat_history: chatHistory,\n},\nrunManager?.getChild()\n);\nconst keys = Object.keys(result);\nconsole.log(\"_call\", values, keys);\nif (keys.length === 1) {\nnewQuestion = result[keys[0]];\n} else {\nthrow new Error(\n\"Return from llm chain has multiple values, only single values supported.\"\n);\n}\n}\nconst docs = await this.vectorstore.similaritySearch(newQuestion, this.k);\nconst inputs = {\nquestion: newQuestion,\ninput_documents: docs,\nchat_history: chatHistory,\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":617,"to":654}}}}],["209",{"pageContent":"const result = await this.combineDocumentsChain.call(\ninputs,\nrunManager?.getChild()\n);\nif (this.returnSourceDocuments) {\nreturn {\n...result,\nsourceDocuments: docs,\n};\n}\nreturn result;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":829,"to":840}}}}],["210",{"pageContent":"_chainType() {\nreturn \"chat-vector-db\" as const;\n}\n\nstatic async deserialize(\ndata: SerializedChatVectorDBQAChain,\nvalues: LoadValues\n) {\nif (!(\"vectorstore\" in values)) {\nthrow new Error(\n`Need to pass in a vectorstore to deserialize VectorDBQAChain`\n);\n}\nconst { vectorstore } = values;\n\nreturn new ChatVectorDBQAChain({\ncombineDocumentsChain: await BaseChain.deserialize(\ndata.combine_documents_chain\n),\nquestionGeneratorChain: await LLMChain.deserialize(\ndata.question_generator\n),\nk: data.k,\nvectorstore,\n});\n}\n\nserialize(): SerializedChatVectorDBQAChain {\nreturn {\n_type: this._chainType(),\ncombine_documents_chain: this.combineDocumentsChain.serialize(),\nquestion_generator: this.questionGeneratorChain.serialize(),\nk: this.k,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":1032,"to":1066}}}}],["211",{"pageContent":"static fromLLM(\nllm: BaseLanguageModel,\nvectorstore: VectorStore,\noptions: {\ninputKey?: string;\noutputKey?: string;\nk?: number;\nreturnSourceDocuments?: boolean;\nquestionGeneratorTemplate?: string;\nqaTemplate?: string;\n} = {}\n): ChatVectorDBQAChain {\nconst { questionGeneratorTemplate, qaTemplate, ...rest } = options;\nconst question_generator_prompt = PromptTemplate.fromTemplate(\nquestionGeneratorTemplate || question_generator_template\n);\nconst qa_prompt = PromptTemplate.fromTemplate(qaTemplate || qa_template);\n\nconst qaChain = loadQAStuffChain(llm, { prompt: qa_prompt });\nconst questionGeneratorChain = new LLMChain({\nprompt: question_generator_prompt,\nllm,\n});\nconst instance = new this({\nvectorstore,\ncombineDocumentsChain: qaChain,\nquestionGeneratorChain,\n...rest,\n});\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/chat_vector_db_chain.ts","loc":{"lines":{"from":1248,"to":1279}}}}],["212",{"pageContent":"import type {\nSerializedStuffDocumentsChain,\nSerializedMapReduceDocumentsChain,\nSerializedRefineDocumentsChain,\n} from \"./serde.js\";\nimport { BaseChain, ChainInputs } from \"./base.js\";\nimport { LLMChain } from \"./llm_chain.js\";\n\nimport { Document } from \"../document.js\";\n\nimport { ChainValues } from \"../schema/index.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { PromptTemplate } from \"../prompts/prompt.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\nexport interface StuffDocumentsChainInput extends ChainInputs {\n/** LLM Wrapper to use after formatting documents */\nllmChain: LLMChain;\ninputKey?: string;\n/** Variable name in the LLM chain to put the documents in */\ndocumentVariableName?: string;\n}\n\n/**\n* Chain that combines documents by stuffing into context.\n* @augments BaseChain\n* @augments StuffDocumentsChainInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":1,"to":28}}}}],["213",{"pageContent":"class StuffDocumentsChain\nextends BaseChain\nimplements StuffDocumentsChainInput\n{\nllmChain: LLMChain;\n\ninputKey = \"input_documents\";\n\ndocumentVariableName = \"context\";\n\nget inputKeys() {\nreturn [this.inputKey, ...this.llmChain.inputKeys];\n}\n\nget outputKeys() {\nreturn this.llmChain.outputKeys;\n}\n\nconstructor(fields: StuffDocumentsChainInput) {\nsuper(fields);\nthis.llmChain = fields.llmChain;\nthis.documentVariableName =\nfields.documentVariableName ?? this.documentVariableName;\nthis.inputKey = fields.inputKey ?? this.inputKey;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":396,"to":420}}}}],["214",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Document key ${this.inputKey} not found.`);\n}\nconst { [this.inputKey]: docs, ...rest } = values;\nconst texts = (docs as Document[]).map(({ pageContent }) => pageContent);\nconst text = texts.join(\"\\n\\n\");\nconst result = await this.llmChain.call(\n{\n...rest,\n[this.documentVariableName]: text,\n},\nrunManager?.getChild()\n);\nreturn result;\n}\n\n_chainType() {\nreturn \"stuff_documents_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedStuffDocumentsChain) {\nif (!data.llm_chain) {\nthrow new Error(\"Missing llm_chain\");\n}\n\nreturn new StuffDocumentsChain({\nllmChain: await LLMChain.deserialize(data.llm_chain),\n});\n}\n\nserialize(): SerializedStuffDocumentsChain {\nreturn {\n_type: this._chainType(),\nllm_chain: this.llmChain.serialize(),\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":799,"to":840}}}}],["215",{"pageContent":"interface MapReduceDocumentsChainInput extends StuffDocumentsChainInput {\nmaxTokens?: number;\nmaxIterations?: number;\nensureMapStep?: boolean;\ncombineDocumentChain: BaseChain;\n}\n\n/**\n* Combine documents by mapping a chain over them, then combining results.\n* @augments BaseChain\n* @augments StuffDocumentsChainInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":1206,"to":1217}}}}],["216",{"pageContent":"class MapReduceDocumentsChain\nextends BaseChain\nimplements MapReduceDocumentsChainInput\n{\nllmChain: LLMChain;\n\ninputKey = \"input_documents\";\n\ndocumentVariableName = \"context\";\n\nget inputKeys() {\nreturn [this.inputKey, ...this.combineDocumentChain.inputKeys];\n}\n\nget outputKeys() {\nreturn this.combineDocumentChain.outputKeys;\n}\n\nmaxTokens = 3000;\n\nmaxIterations = 10;\n\nensureMapStep = false;\n\ncombineDocumentChain: BaseChain;\n\nconstructor(fields: MapReduceDocumentsChainInput) {\nsuper(fields);\nthis.llmChain = fields.llmChain;\nthis.combineDocumentChain = fields.combineDocumentChain;\nthis.documentVariableName =\nfields.documentVariableName ?? this.documentVariableName;\nthis.ensureMapStep = fields.ensureMapStep ?? this.ensureMapStep;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.maxTokens = fields.maxTokens ?? this.maxTokens;\nthis.maxIterations = fields.maxIterations ?? this.maxIterations;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":1601,"to":1637}}}}],["217",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Document key ${this.inputKey} not found.`);\n}\nconst { [this.inputKey]: docs, ...rest } = values;\n\nlet currentDocs = docs as Document[];\n\nfor (let i = 0; i < this.maxIterations; i += 1) {\nconst inputs = currentDocs.map((d) => ({\n[this.documentVariableName]: d.pageContent,\n...rest,\n}));\nconst promises = inputs.map(async (i) => {\nconst prompt = await this.llmChain.prompt.format(i);\nreturn this.llmChain.llm.getNumTokens(prompt);\n});\n\nconst length = await Promise.all(promises).then((results) =>\nresults.reduce((a, b) => a + b, 0)\n);\n\nconst canSkipMapStep = i !== 0 || !this.ensureMapStep;\nconst withinTokenLimit = length < this.maxTokens;\nif (canSkipMapStep && withinTokenLimit) {\nbreak;\n}\n\nconst results = await this.llmChain.apply(\ninputs,\nrunManager ? [runManager.getChild()] : undefined\n);\nconst { outputKey } = this.llmChain;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":2003,"to":2039}}}}],["218",{"pageContent":"currentDocs = results.map((r: ChainValues) => ({\npageContent: r[outputKey],\n}));\n}\nconst newInputs = { input_documents: currentDocs, ...rest };\nconst result = await this.combineDocumentChain.call(\nnewInputs,\nrunManager?.getChild()\n);\nreturn result;\n}\n\n_chainType() {\nreturn \"map_reduce_documents_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedMapReduceDocumentsChain) {\nif (!data.llm_chain) {\nthrow new Error(\"Missing llm_chain\");\n}\n\nif (!data.combine_document_chain) {\nthrow new Error(\"Missing combine_document_chain\");\n}\n\nreturn new MapReduceDocumentsChain({\nllmChain: await LLMChain.deserialize(data.llm_chain),\ncombineDocumentChain: await BaseChain.deserialize(\ndata.combine_document_chain\n),\n});\n}\n\nserialize(): SerializedMapReduceDocumentsChain {\nreturn {\n_type: this._chainType(),\nllm_chain: this.llmChain.serialize(),\ncombine_document_chain: this.combineDocumentChain.serialize(),\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":2401,"to":2441}}}}],["219",{"pageContent":"interface RefineDocumentsChainInput extends StuffDocumentsChainInput {\nrefineLLMChain: LLMChain;\ndocumentPrompt?: BasePromptTemplate;\ninitialResponseName?: string;\ndocumentVariableName?: string;\noutputKey?: string;\n}\n\n/**\n* Combine documents by doing a first pass and then refining on more documents.\n* @augments BaseChain\n* @augments RefineDocumentsChainInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":2807,"to":2819}}}}],["220",{"pageContent":"class RefineDocumentsChain\nextends BaseChain\nimplements RefineDocumentsChainInput\n{\nllmChain: LLMChain;\n\ninputKey = \"input_documents\";\n\noutputKey = \"output_text\";\n\ndocumentVariableName = \"context\";\n\ninitialResponseName = \"existing_answer\";\n\nrefineLLMChain: LLMChain;\n\nget defaultDocumentPrompt(): BasePromptTemplate {\nreturn new PromptTemplate({\ninputVariables: [\"page_content\"],\ntemplate: \"{page_content}\",\n});\n}\n\ndocumentPrompt = this.defaultDocumentPrompt;\n\nget inputKeys() {\nreturn [this.inputKey, ...this.refineLLMChain.inputKeys];\n}\n\nget outputKeys() {\nreturn [this.outputKey];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":3202,"to":3233}}}}],["221",{"pageContent":"constructor(fields: RefineDocumentsChainInput) {\nsuper(fields);\nthis.llmChain = fields.llmChain;\nthis.refineLLMChain = fields.refineLLMChain;\nthis.documentVariableName =\nfields.documentVariableName ?? this.documentVariableName;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.outputKey = fields.outputKey ?? this.outputKey;\nthis.documentPrompt = fields.documentPrompt ?? this.documentPrompt;\nthis.initialResponseName =\nfields.initialResponseName ?? this.initialResponseName;\n}\n\n/** @ignore */\nasync _constructInitialInputs(doc: Document, rest: Record<string, unknown>) {\nconst baseInfo: Record<string, unknown> = {\npage_content: doc.pageContent,\n...doc.metadata,\n};\nconst documentInfo: Record<string, unknown> = {};\nthis.documentPrompt.inputVariables.forEach((value) => {\ndocumentInfo[value] = baseInfo[value];\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":3611,"to":3633}}}}],["222",{"pageContent":"const baseInputs: Record<string, unknown> = {\n[this.documentVariableName]: await this.documentPrompt.format({\n...documentInfo,\n}),\n};\nconst inputs = { ...baseInputs, ...rest };\nreturn inputs;\n}\n\n/** @ignore */\nasync _constructRefineInputs(doc: Document, res: string) {\nconst baseInfo: Record<string, unknown> = {\npage_content: doc.pageContent,\n...doc.metadata,\n};\nconst documentInfo: Record<string, unknown> = {};\nthis.documentPrompt.inputVariables.forEach((value) => {\ndocumentInfo[value] = baseInfo[value];\n});\nconst baseInputs: Record<string, unknown> = {\n[this.documentVariableName]: await this.documentPrompt.format({\n...documentInfo,\n}),\n};\nconst inputs = { [this.initialResponseName]: res, ...baseInputs };\nreturn inputs;\n}\n\n/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Document key ${this.inputKey} not found.`);\n}\nconst { [this.inputKey]: docs, ...rest } = values;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":4003,"to":4039}}}}],["223",{"pageContent":"const currentDocs = docs as Document[];\n\nconst initialInputs = await this._constructInitialInputs(\ncurrentDocs[0],\nrest\n);\nlet res = await this.llmChain.predict(\n{ ...initialInputs },\nrunManager?.getChild()\n);\n\nconst refineSteps = [res];\n\nfor (let i = 1; i < currentDocs.length; i += 1) {\nconst refineInputs = await this._constructRefineInputs(\ncurrentDocs[i],\nres\n);\nconst inputs = { ...refineInputs, ...rest };\nres = await this.refineLLMChain.predict(\n{ ...inputs },\nrunManager?.getChild()\n);\nrefineSteps.push(res);\n}\n\nreturn { [this.outputKey]: res };\n}\n\n_chainType() {\nreturn \"refine_documents_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedRefineDocumentsChain) {\nconst SerializedLLMChain = data.llm_chain;\n\nif (!SerializedLLMChain) {\nthrow new Error(\"Missing llm_chain\");\n}\n\nconst SerializedRefineDocumentChain = data.refine_llm_chain;\n\nif (!SerializedRefineDocumentChain) {\nthrow new Error(\"Missing refine_llm_chain\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":4402,"to":4446}}}}],["224",{"pageContent":"return new RefineDocumentsChain({\nllmChain: await LLMChain.deserialize(SerializedLLMChain),\nrefineLLMChain: await LLMChain.deserialize(SerializedRefineDocumentChain),\n});\n}\n\nserialize(): SerializedRefineDocumentsChain {\nreturn {\n_type: this._chainType(),\nllm_chain: this.llmChain.serialize(),\nrefine_llm_chain: this.refineLLMChain.serialize(),\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/combine_docs_chain.ts","loc":{"lines":{"from":4811,"to":4824}}}}],["225",{"pageContent":"import { LLMChain, LLMChainInput } from \"./llm_chain.js\";\nimport { PromptTemplate } from \"../prompts/prompt.js\";\nimport { BufferMemory } from \"../memory/buffer_memory.js\";\nimport { Optional } from \"../types/type-utils.js\";\n\nconst defaultTemplate = `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n{history}\nHuman: {input}\nAI:`;\n\nexport class ConversationChain extends LLMChain {\nconstructor({\nprompt,\noutputKey,\nmemory,\n...rest\n}: Optional<LLMChainInput, \"prompt\">) {\nsuper({\nprompt:\nprompt ??\nnew PromptTemplate({\ntemplate: defaultTemplate,\ninputVariables: [\"history\", \"input\"],\n}),\noutputKey: outputKey ?? \"response\",\nmemory: memory ?? new BufferMemory(),\n...rest,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversation.ts","loc":{"lines":{"from":1,"to":32}}}}],["226",{"pageContent":"import { PromptTemplate } from \"../prompts/prompt.js\";\nimport { BaseLLM } from \"../llms/base.js\";\nimport { SerializedChatVectorDBQAChain } from \"./serde.js\";\nimport { ChainValues, BaseRetriever } from \"../schema/index.js\";\nimport { BaseChain, ChainInputs } from \"./base.js\";\nimport { LLMChain } from \"./llm_chain.js\";\nimport { loadQAStuffChain } from \"./question_answering/load.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":1,"to":10}}}}],["227",{"pageContent":"type LoadValues = Record<string, any>;\n\nconst question_generator_template = `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:`;\n\nconst qa_template = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:`;\n\nexport interface ConversationalRetrievalQAChainInput\nextends Omit<ChainInputs, \"memory\"> {\nretriever: BaseRetriever;\ncombineDocumentsChain: BaseChain;\nquestionGeneratorChain: LLMChain;\nreturnSourceDocuments?: boolean;\ninputKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":167,"to":190}}}}],["228",{"pageContent":"class ConversationalRetrievalQAChain\nextends BaseChain\nimplements ConversationalRetrievalQAChainInput\n{\ninputKey = \"question\";\n\nchatHistoryKey = \"chat_history\";\n\nget inputKeys() {\nreturn [this.inputKey, this.chatHistoryKey];\n}\n\nget outputKeys() {\nreturn this.combineDocumentsChain.outputKeys.concat(\nthis.returnSourceDocuments ? [\"sourceDocuments\"] : []\n);\n}\n\nretriever: BaseRetriever;\n\ncombineDocumentsChain: BaseChain;\n\nquestionGeneratorChain: LLMChain;\n\nreturnSourceDocuments = false;\n\nconstructor(fields: ConversationalRetrievalQAChainInput) {\nsuper(fields);\nthis.retriever = fields.retriever;\nthis.combineDocumentsChain = fields.combineDocumentsChain;\nthis.questionGeneratorChain = fields.questionGeneratorChain;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.returnSourceDocuments =\nfields.returnSourceDocuments ?? this.returnSourceDocuments;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":340,"to":374}}}}],["229",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Question key ${this.inputKey} not found.`);\n}\nif (!(this.chatHistoryKey in values)) {\nthrow new Error(`chat history key ${this.inputKey} not found.`);\n}\nconst question: string = values[this.inputKey];\nconst chatHistory: string = values[this.chatHistoryKey];\nlet newQuestion = question;\nif (chatHistory.length > 0) {\nconst result = await this.questionGeneratorChain.call(\n{\nquestion,\nchat_history: chatHistory,\n},\nrunManager?.getChild()\n);\nconst keys = Object.keys(result);\nif (keys.length === 1) {\nnewQuestion = result[keys[0]];\n} else {\nthrow new Error(\n\"Return from llm chain has multiple values, only single values supported.\"\n);\n}\n}\nconst docs = await this.retriever.getRelevantDocuments(newQuestion);\nconst inputs = {\nquestion: newQuestion,\ninput_documents: docs,\nchat_history: chatHistory,\n};\nconst result = await this.combineDocumentsChain.call(\ninputs,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":521,"to":559}}}}],["230",{"pageContent":"runManager?.getChild()\n);\nif (this.returnSourceDocuments) {\nreturn {\n...result,\nsourceDocuments: docs,\n};\n}\nreturn result;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":701,"to":710}}}}],["231",{"pageContent":"_chainType(): string {\nreturn \"conversational_retrieval_chain\";\n}\n\nstatic async deserialize(\n_data: SerializedChatVectorDBQAChain,\n_values: LoadValues\n): Promise<ConversationalRetrievalQAChain> {\nthrow new Error(\"Not implemented.\");\n}\n\nserialize(): SerializedChatVectorDBQAChain {\nthrow new Error(\"Not implemented.\");\n}\n\nstatic fromLLM(\nllm: BaseLLM,\nretriever: BaseRetriever,\noptions: {\ninputKey?: string;\noutputKey?: string;\nreturnSourceDocuments?: boolean;\nquestionGeneratorTemplate?: string;\nqaTemplate?: string;\n} = {}\n): ConversationalRetrievalQAChain {\nconst { questionGeneratorTemplate, qaTemplate, ...rest } = options;\nconst question_generator_prompt = PromptTemplate.fromTemplate(\nquestionGeneratorTemplate || question_generator_template\n);\nconst qa_prompt = PromptTemplate.fromTemplate(qaTemplate || qa_template);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":874,"to":904}}}}],["232",{"pageContent":"const qaChain = loadQAStuffChain(llm, { prompt: qa_prompt });\nconst questionGeneratorChain = new LLMChain({\nprompt: question_generator_prompt,\nllm,\n});\nconst instance = new this({\nretriever,\ncombineDocumentsChain: qaChain,\nquestionGeneratorChain,\n...rest,\n});\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/conversational_retrieval_chain.ts","loc":{"lines":{"from":1051,"to":1064}}}}],["233",{"pageContent":"export { BaseChain, ChainInputs } from \"./base.js\";\nexport { LLMChain, LLMChainInput } from \"./llm_chain.js\";\nexport { ConversationChain } from \"./conversation.js\";\nexport {\nSequentialChain,\nSequentialChainInput,\nSimpleSequentialChain,\nSimpleSequentialChainInput,\n} from \"./sequential_chain.js\";\nexport {\nStuffDocumentsChain,\nStuffDocumentsChainInput,\nMapReduceDocumentsChain,\nMapReduceDocumentsChainInput,\nRefineDocumentsChain,\nRefineDocumentsChainInput,\n} from \"./combine_docs_chain.js\";\nexport {\nChatVectorDBQAChain,\nChatVectorDBQAChainInput,\n} from \"./chat_vector_db_chain.js\";\nexport {\nAnalyzeDocumentChain,\nAnalyzeDocumentChainInput,\n} from \"./analyze_documents_chain.js\";\nexport { VectorDBQAChain, VectorDBQAChainInput } from \"./vector_db_qa.js\";\nexport {\nloadQAChain,\nloadQAStuffChain,\nloadQAMapReduceChain,\nloadQARefineChain,\n} from \"./question_answering/load.js\";\nexport {\nloadSummarizationChain,\nSummarizationChainParams,\n} from \"./summarization/load.js\";\nexport {\nSqlDatabaseChain,\nSqlDatabaseChainInput,\n} from \"./sql_db/sql_db_chain.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/index.ts","loc":{"lines":{"from":1,"to":40}}}}],["234",{"pageContent":"{\nConversationalRetrievalQAChain,\nConversationalRetrievalQAChainInput,\n} from \"./conversational_retrieval_chain.js\";\nexport { RetrievalQAChain, RetrievalQAChainInput } from \"./retrieval_qa.js\";\nexport {\nSerializedLLMChain,\nSerializedSequentialChain,\nSerializedSimpleSequentialChain,\nSerializedSqlDatabaseChain,\nSerializedAnalyzeDocumentChain,\nSerializedBaseChain,\nSerializedChatVectorDBQAChain,\nSerializedMapReduceDocumentsChain,\nSerializedStuffDocumentsChain,\nSerializedVectorDBQAChain,\nSerializedRefineDocumentsChain,\n} from \"./serde.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/index.ts","loc":{"lines":{"from":59,"to":76}}}}],["235",{"pageContent":"import { BaseChain, ChainInputs } from \"./base.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { ChainValues, Generation, BasePromptValue } from \"../schema/index.js\";\nimport { BaseOutputParser } from \"../schema/output_parser.js\";\nimport { SerializedLLMChain } from \"./serde.js\";\nimport { CallbackManager } from \"../callbacks/index.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/llm_chain.ts","loc":{"lines":{"from":1,"to":8}}}}],["236",{"pageContent":"interface LLMChainInput extends ChainInputs {\n/** Prompt object to use */\nprompt: BasePromptTemplate;\n/** LLM Wrapper to use */\nllm: BaseLanguageModel;\n/** OutputParser to use */\noutputParser?: BaseOutputParser;\n/** Key to use for output, defaults to `text` */\noutputKey?: string;\n}\n\n/**\n* Chain to run queries against LLMs.\n*\n* @example\n* ```ts\n* import { LLMChain } from \"langchain/chains\";\n* import { OpenAI } from \"langchain/llms/openai\";\n* import { PromptTemplate } from \"langchain/prompts\";\n*\n* const prompt = PromptTemplate.fromTemplate(\"Tell me a {adjective} joke\");\n* const llm = new LLMChain({ llm: new OpenAI(), prompt });\n* ```\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/llm_chain.ts","loc":{"lines":{"from":155,"to":178}}}}],["237",{"pageContent":"class LLMChain extends BaseChain implements LLMChainInput {\nprompt: BasePromptTemplate;\n\nllm: BaseLanguageModel;\n\noutputKey = \"text\";\n\noutputParser?: BaseOutputParser;\n\nget inputKeys() {\nreturn this.prompt.inputVariables;\n}\n\nget outputKeys() {\nreturn [this.outputKey];\n}\n\nconstructor(fields: LLMChainInput) {\nsuper(fields);\nthis.prompt = fields.prompt;\nthis.llm = fields.llm;\nthis.outputKey = fields.outputKey ?? this.outputKey;\nthis.outputParser = fields.outputParser ?? this.outputParser;\nif (this.prompt.outputParser) {\nif (this.outputParser) {\nthrow new Error(\"Cannot set both outputParser and prompt.outputParser\");\n}\nthis.outputParser = this.prompt.outputParser;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/llm_chain.ts","loc":{"lines":{"from":319,"to":348}}}}],["238",{"pageContent":"/** @ignore */\nasync _getFinalOutput(\ngenerations: Generation[],\npromptValue: BasePromptValue,\nrunManager?: CallbackManagerForChainRun\n): Promise<unknown> {\nconst completion = generations[0].text;\nlet finalCompletion: unknown;\nif (this.outputParser) {\nfinalCompletion = await this.outputParser.parseWithPrompt(\ncompletion,\npromptValue,\nrunManager?.getChild()\n);\n} else {\nfinalCompletion = completion;\n}\nreturn finalCompletion;\n}\n\n/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nlet stop;\nif (\"stop\" in values && Array.isArray(values.stop)) {\nstop = values.stop;\n}\nconst promptValue = await this.prompt.formatPromptValue(values);\nconst { generations } = await this.llm.generatePrompt(\n[promptValue],\nstop,\nrunManager?.getChild()\n);\nreturn {\n[this.outputKey]: await this._getFinalOutput(\ngenerations[0],\npromptValue,\nrunManager\n),\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/llm_chain.ts","loc":{"lines":{"from":488,"to":530}}}}],["239",{"pageContent":"/**\n* Format prompt with values and pass to LLM\n*\n* @param values - keys to pass to prompt template\n* @param callbackManager - CallbackManager to use\n* @returns Completion from LLM.\n*\n* @example\n* ```ts\n* llm.predict({ adjective: \"funny\" })\n* ```\n*/\nasync predict(\nvalues: ChainValues,\ncallbackManager?: CallbackManager\n): Promise<string> {\nconst output = await this.call(values, callbackManager);\nreturn output[this.outputKey];\n}\n\n_chainType() {\nreturn \"llm_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedLLMChain) {\nconst { llm, prompt } = data;\nif (!llm) {\nthrow new Error(\"LLMChain must have llm\");\n}\nif (!prompt) {\nthrow new Error(\"LLMChain must have prompt\");\n}\n\nreturn new LLMChain({\nllm: await BaseLanguageModel.deserialize(llm),\nprompt: await BasePromptTemplate.deserialize(prompt),\n});\n}\n\nserialize(): SerializedLLMChain {\nreturn {\n_type: this._chainType(),\nllm: this.llm.serialize(),\nprompt: this.prompt.serialize(),\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/llm_chain.ts","loc":{"lines":{"from":659,"to":705}}}}],["240",{"pageContent":"import { BaseChain } from \"./base.js\";\nimport { loadFromHub } from \"../util/hub.js\";\nimport { FileLoader, LoadValues, loadFromFile } from \"../util/load.js\";\nimport { parseFileConfig } from \"../util/parse.js\";\n\nconst loadChainFromFile: FileLoader<BaseChain> = async (\nfile: string,\npath: string,\nvalues: LoadValues = {}\n) => {\nconst serialized = parseFileConfig(file, path);\nreturn BaseChain.deserialize(serialized, values);\n};\n\n/**\n* Load a chain from {@link https://github.com/hwchase17/langchain-hub | LangchainHub} or local filesystem.\n*\n* @example\n* Loading from LangchainHub:\n* ```ts\n* import { loadChain } from \"langchain/chains/load\";\n* const chain = await loadChain(\"lc://chains/hello-world/chain.json\");\n* const res = await chain.call({ topic: \"my favorite color\" });\n* ```\n*\n* @example\n* Loading from local filesystem:\n* ```ts\n* import { loadChain } from \"langchain/chains/load\";\n* const chain = await loadChain(\"/path/to/chain.json\");\n* ```\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/load.ts","loc":{"lines":{"from":1,"to":32}}}}],["241",{"pageContent":"const loadChain = async (\nuri: string,\nvalues: LoadValues = {}\n): Promise<BaseChain> => {\nconst hubResult = await loadFromHub(\nuri,\nloadChainFromFile,\n\"chains\",\nnew Set([\"json\", \"yaml\"]),\nvalues\n);\nif (hubResult) {\nreturn hubResult;\n}\n\nreturn loadFromFile(uri, loadChainFromFile, values);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/load.ts","loc":{"lines":{"from":51,"to":67}}}}],["242",{"pageContent":"import { BaseChatModel } from \"../chat_models/base.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { BaseLLM } from \"../llms/base.js\";\n\nexport abstract class BasePromptSelector {\nabstract getPrompt(llm: BaseLanguageModel): BasePromptTemplate;\n}\n\nexport class ConditionalPromptSelector extends BasePromptSelector {\ndefaultPrompt: BasePromptTemplate;\n\nconditionals: Array<\n[condition: (llm: BaseLanguageModel) => boolean, prompt: BasePromptTemplate]\n>;\n\nconstructor(\ndefault_prompt: BasePromptTemplate,\nconditionals: Array<\n[\ncondition: (llm: BaseLanguageModel) => boolean,\nprompt: BasePromptTemplate\n]\n> = []\n) {\nsuper();\nthis.defaultPrompt = default_prompt;\nthis.conditionals = conditionals;\n}\n\ngetPrompt(llm: BaseLanguageModel): BasePromptTemplate {\nfor (const [condition, prompt] of this.conditionals) {\nif (condition(llm)) {\nreturn prompt;\n}\n}\nreturn this.defaultPrompt;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/prompt_selector.ts","loc":{"lines":{"from":1,"to":39}}}}],["243",{"pageContent":"function isLLM(llm: BaseLanguageModel): llm is BaseLLM {\nreturn llm._modelType() === \"base_llm\";\n}\n\nexport function isChatModel(llm: BaseLanguageModel): llm is BaseChatModel {\nreturn llm._modelType() === \"base_chat_model\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/prompt_selector.ts","loc":{"lines":{"from":54,"to":60}}}}],["244",{"pageContent":"import { LLMChain } from \"../llm_chain.js\";\nimport { BasePromptTemplate } from \"../../prompts/base.js\";\nimport {\nStuffDocumentsChain,\nMapReduceDocumentsChain,\nRefineDocumentsChain,\n} from \"../combine_docs_chain.js\";\nimport { QA_PROMPT_SELECTOR, DEFAULT_QA_PROMPT } from \"./stuff_prompts.js\";\nimport {\nCOMBINE_PROMPT,\nDEFAULT_COMBINE_QA_PROMPT,\nCOMBINE_PROMPT_SELECTOR,\nCOMBINE_QA_PROMPT_SELECTOR,\n} from \"./map_reduce_prompts.js\";\nimport { BaseLanguageModel } from \"../../base_language/index.js\";\nimport {\nQUESTION_PROMPT_SELECTOR,\nREFINE_PROMPT_SELECTOR,\n} from \"./refine_prompts.js\";\n\ninterface qaChainParams {\nprompt?: BasePromptTemplate;\ncombineMapPrompt?: BasePromptTemplate;\ncombinePrompt?: BasePromptTemplate;\nquestionPrompt?: BasePromptTemplate;\nrefinePrompt?: BasePromptTemplate;\ntype?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/load.ts","loc":{"lines":{"from":1,"to":28}}}}],["245",{"pageContent":"const loadQAChain = (\nllm: BaseLanguageModel,\nparams: qaChainParams = {}\n) => {\nconst {\nprompt = DEFAULT_QA_PROMPT,\ncombineMapPrompt = DEFAULT_COMBINE_QA_PROMPT,\ncombinePrompt = COMBINE_PROMPT,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/load.ts","loc":{"lines":{"from":137,"to":144}}}}],["246",{"pageContent":"= \"stuff\",\n} = params;\nif (type === \"stuff\") {\nconst llmChain = new LLMChain({ prompt, llm });\nconst chain = new StuffDocumentsChain({ llmChain });\nreturn chain;\n}\nif (type === \"map_reduce\") {\nconst llmChain = new LLMChain({ prompt: combineMapPrompt, llm });\nconst combineLLMChain = new LLMChain({ prompt: combinePrompt, llm });\nconst combineDocumentChain = new StuffDocumentsChain({\nllmChain: combineLLMChain,\ndocumentVariableName: \"summaries\",\n});\nconst chain = new MapReduceDocumentsChain({\nllmChain,\ncombineDocumentChain,\n});\nreturn chain;\n}\nif (type === \"refine\") {\nconst {\nquestionPrompt = QUESTION_PROMPT_SELECTOR.getPrompt(llm),\nrefinePrompt = REFINE_PROMPT_SELECTOR.getPrompt(llm),\n} = params;\nconst llmChain = new LLMChain({ prompt: questionPrompt, llm });\nconst refineLLMChain = new LLMChain({ prompt: refinePrompt, llm });\n\nconst chain = new RefineDocumentsChain({\nllmChain,\nrefineLLMChain,\n});\nreturn chain;\n}\nthrow new Error(`Invalid _type: ${type}`);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/load.ts","loc":{"lines":{"from":272,"to":307}}}}],["247",{"pageContent":"StuffQAChainParams {\nprompt?: BasePromptTemplate;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/load.ts","loc":{"lines":{"from":408,"to":410}}}}],["248",{"pageContent":"const loadQAStuffChain = (\nllm: BaseLanguageModel,\nparams: StuffQAChainParams = {}\n) => {\nconst { prompt = QA_PROMPT_SELECTOR.getPrompt(llm) } = params;\nconst llmChain = new LLMChain({ prompt, llm });\nconst chain = new StuffDocumentsChain({ llmChain });\nreturn chain;\n};\n\ninterface MapReduceQAChainParams {\ncombineMapPrompt?: BasePromptTemplate;\ncombinePrompt?: BasePromptTemplate;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/load.ts","loc":{"lines":{"from":543,"to":556}}}}],["249",{"pageContent":"const loadQAMapReduceChain = (\nllm: BaseLanguageModel,\nparams: MapReduceQAChainParams = {}\n) => {\nconst {\ncombineMapPrompt = COMBINE_QA_PROMPT_SELECTOR.getPrompt(llm),\ncombinePrompt = COMBINE_PROMPT_SELECTOR.getPrompt(llm),\n} = params;\nconst llmChain = new LLMChain({ prompt: combineMapPrompt, llm });\nconst combineLLMChain = new LLMChain({ prompt: combinePrompt, llm });\nconst combineDocumentChain = new StuffDocumentsChain({\nllmChain: combineLLMChain,\ndocumentVariableName: \"summaries\",\n});\nconst chain = new MapReduceDocumentsChain({\nllmChain,\ncombineDocumentChain,\n});\nreturn chain;\n};\n\ninterface RefineQAChainParams {\nquestionPrompt?: BasePromptTemplate;\nrefinePrompt?: BasePromptTemplate;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/load.ts","loc":{"lines":{"from":678,"to":702}}}}],["250",{"pageContent":"const loadQARefineChain = (\nllm: BaseLanguageModel,\nparams: RefineQAChainParams = {}\n) => {\nconst {\nquestionPrompt = QUESTION_PROMPT_SELECTOR.getPrompt(llm),\nrefinePrompt = REFINE_PROMPT_SELECTOR.getPrompt(llm),\n} = params;\nconst llmChain = new LLMChain({ prompt: questionPrompt, llm });\nconst refineLLMChain = new LLMChain({ prompt: refinePrompt, llm });\n\nconst chain = new RefineDocumentsChain({\nllmChain,\nrefineLLMChain,\n});\nreturn chain;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/load.ts","loc":{"lines":{"from":814,"to":830}}}}],["251",{"pageContent":"/* eslint-disable spaced-comment */\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\nimport {\nChatPromptTemplate,\nSystemMessagePromptTemplate,\nHumanMessagePromptTemplate,\n} from \"../../prompts/chat.js\";\nimport { ConditionalPromptSelector, isChatModel } from \"../prompt_selector.js\";\n\nconst qa_template = `Use the following portion of a long document to see if any of the text is relevant to answer the question. \nReturn any relevant text verbatim.\n{context}\nQuestion: {question}\nRelevant text, if any:`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":1,"to":14}}}}],["252",{"pageContent":"const DEFAULT_COMBINE_QA_PROMPT =\n/*#__PURE__*/\nPromptTemplate.fromTemplate(qa_template);\n\nconst system_template = `Use the following portion of a long document to see if any of the text is relevant to answer the question. \nReturn any relevant text verbatim.\n----------------\n{context}`;\nconst messages = [\n/*#__PURE__*/ SystemMessagePromptTemplate.fromTemplate(system_template),\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nconst CHAT_QA_PROMPT =\n/*#__PURE__*/ ChatPromptTemplate.fromPromptMessages(messages);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":84,"to":97}}}}],["253",{"pageContent":"const COMBINE_QA_PROMPT_SELECTOR =\n/*#__PURE__*/ new ConditionalPromptSelector(DEFAULT_COMBINE_QA_PROMPT, [\n[isChatModel, CHAT_QA_PROMPT],\n]);\n\nconst combine_prompt = `Given the following extracted parts of a long document and a question, create a final answer. \nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\n\nQUESTION: Which state/country's law governs the interpretation of the contract?\n=========\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":166,"to":176}}}}],["254",{"pageContent":"Content: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\n\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\n=========\nFINAL ANSWER: This Agreement is governed by English law.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":198,"to":202}}}}],["255",{"pageContent":"QUESTION: What did the president say about Michael Jackson?\n=========","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":204,"to":205}}}}],["256",{"pageContent":"Content: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russias Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":206,"to":206}}}}],["257",{"pageContent":"Content: And we wont stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLets use this moment to reset. Lets stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLets stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe cant change how divided weve been. But we can change how we move forwardon COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans whod grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":208,"to":208}}}}],["258",{"pageContent":"Content: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as Ive always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd Im taking robust action to make sure the pain of our sanctions  is targeted at Russias economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about whats happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":210,"to":210}}}}],["259",{"pageContent":"Content: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIts based on DARPAthe Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purposeto drive breakthroughs in cancer, Alzheimers, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americanstonight , we have gathered in a sacred spacethe citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":212,"to":212}}}}],["260",{"pageContent":"=========\nFINAL ANSWER: The president did not mention Michael Jackson.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":213,"to":214}}}}],["261",{"pageContent":"QUESTION: {question}\n=========\n{summaries}\n=========\nFINAL ANSWER:`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":216,"to":220}}}}],["262",{"pageContent":"const COMBINE_PROMPT =\n/*#__PURE__*/ PromptTemplate.fromTemplate(combine_prompt);\n\nconst system_combine_template = `Given the following extracted parts of a long document and a question, create a final answer. \nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\n----------------\n{summaries}`;\nconst combine_messages = [\n/*#__PURE__*/ SystemMessagePromptTemplate.fromTemplate(\nsystem_combine_template\n),\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nconst CHAT_COMBINE_PROMPT =\n/*#__PURE__*/ ChatPromptTemplate.fromPromptMessages(combine_messages);\n\nexport const COMBINE_PROMPT_SELECTOR =\n/*#__PURE__*/ new ConditionalPromptSelector(COMBINE_PROMPT, [\n[isChatModel, CHAT_COMBINE_PROMPT],\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/map_reduce_prompts.ts","loc":{"lines":{"from":240,"to":259}}}}],["263",{"pageContent":"/* eslint-disable spaced-comment */\nimport {\nPromptTemplate,\nChatPromptTemplate,\nSystemMessagePromptTemplate,\nHumanMessagePromptTemplate,\nAIMessagePromptTemplate,\n} from \"../../prompts/index.js\";\nimport { ConditionalPromptSelector, isChatModel } from \"../prompt_selector.js\";\n\nexport const DEFAULT_REFINE_PROMPT_TMPL = `The original question is as follows: {question}\nWe have provided an existing answer: {existing_answer}\nWe have the opportunity to refine the existing answer\n(only if needed) with some more context below.\n------------\n{context}\n------------\nGiven the new context, refine the original answer to better answer the question. \nIf the context isn't useful, return the original answer.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/refine_prompts.ts","loc":{"lines":{"from":1,"to":19}}}}],["264",{"pageContent":"const DEFAULT_REFINE_PROMPT = /*#__PURE__*/ new PromptTemplate({\ninputVariables: [\"question\", \"existing_answer\", \"context\"],\ntemplate: DEFAULT_REFINE_PROMPT_TMPL,\n});\n\nconst refineTemplate = `The original question is as follows: {question}\nWe have provided an existing answer: {existing_answer}\nWe have the opportunity to refine the existing answer\n(only if needed) with some more context below.\n------------\n{context}\n------------\nGiven the new context, refine the original answer to better answer the question. \nIf the context isn't useful, return the original answer.`;\n\nconst messages = [\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n/*#__PURE__*/ AIMessagePromptTemplate.fromTemplate(\"{existing_answer}\"),\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(refineTemplate),\n];\n\nexport const CHAT_REFINE_PROMPT =\n/*#__PURE__*/ ChatPromptTemplate.fromPromptMessages(messages);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/refine_prompts.ts","loc":{"lines":{"from":75,"to":97}}}}],["265",{"pageContent":"const REFINE_PROMPT_SELECTOR =\n/*#__PURE__*/ new ConditionalPromptSelector(DEFAULT_REFINE_PROMPT, [\n[isChatModel, CHAT_REFINE_PROMPT],\n]);\n\nexport const DEFAULT_TEXT_QA_PROMPT_TMPL = `Context information is below. \n---------------------\n{context}\n---------------------\nGiven the context information and not prior knowledge, answer the question: {question}`;\nexport const DEFAULT_TEXT_QA_PROMPT = /*#__PURE__*/ new PromptTemplate({\ninputVariables: [\"context\", \"question\"],\ntemplate: DEFAULT_TEXT_QA_PROMPT_TMPL,\n});\n\nconst chat_qa_prompt_template = `Context information is below. \n---------------------\n{context}\n---------------------\nGiven the context information and not prior knowledge, answer any questions`;\nconst chat_messages = [\n/*#__PURE__*/ SystemMessagePromptTemplate.fromTemplate(\nchat_qa_prompt_template\n),\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nexport const CHAT_QUESTION_PROMPT =\n/*#__PURE__*/ ChatPromptTemplate.fromPromptMessages(chat_messages);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/refine_prompts.ts","loc":{"lines":{"from":147,"to":174}}}}],["266",{"pageContent":"const QUESTION_PROMPT_SELECTOR =\n/*#__PURE__*/ new ConditionalPromptSelector(DEFAULT_TEXT_QA_PROMPT, [\n[isChatModel, CHAT_QUESTION_PROMPT],\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/refine_prompts.ts","loc":{"lines":{"from":223,"to":226}}}}],["267",{"pageContent":"/* eslint-disable spaced-comment */\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\nimport {\nChatPromptTemplate,\nSystemMessagePromptTemplate,\nHumanMessagePromptTemplate,\n} from \"../../prompts/chat.js\";\nimport { ConditionalPromptSelector, isChatModel } from \"../prompt_selector.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/stuff_prompts.ts","loc":{"lines":{"from":1,"to":8}}}}],["268",{"pageContent":"const DEFAULT_QA_PROMPT = /*#__PURE__*/ new PromptTemplate({\ntemplate:\n\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\",\ninputVariables: [\"context\", \"question\"],\n});\n\nconst system_template = `Use the following pieces of context to answer the users question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}`;\nconst messages = [\n/*#__PURE__*/ SystemMessagePromptTemplate.fromTemplate(system_template),\n/*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nconst CHAT_PROMPT =\n/*#__PURE__*/ ChatPromptTemplate.fromPromptMessages(messages);\n\nexport const QA_PROMPT_SELECTOR = /*#__PURE__*/ new ConditionalPromptSelector(\nDEFAULT_QA_PROMPT,\n[[isChatModel, CHAT_PROMPT]]\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/stuff_prompts.ts","loc":{"lines":{"from":30,"to":50}}}}],["269",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../../llms/openai.js\";\nimport {\nloadQAMapReduceChain,\nloadQARefineChain,\nloadQAStuffChain,\n} from \"../load.js\";\nimport { Document } from \"../../../document.js\";\n\ntest(\"Test loadQAStuffChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadQAStuffChain(model);\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"Whats up\" });\nconsole.log({ res });\n});\n\ntest(\"Test loadQAMapReduceChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadQAMapReduceChain(model);\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"Whats up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/tests/load.int.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["270",{"pageContent":"test(\"Test loadQARefineChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadQARefineChain(model);\nconst docs = [\nnew Document({ pageContent: \"Harrison went to Harvard.\" }),\nnew Document({ pageContent: \"Ankush went to Princeton.\" }),\n];\nconst res = await chain.call({\ninput_documents: docs,\nquestion: \"Where did Harrison go to college?\",\n});\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/question_answering/tests/load.int.test.ts","loc":{"lines":{"from":48,"to":60}}}}],["271",{"pageContent":"import { BaseChain, ChainInputs } from \"./base.js\";\nimport { BaseLLM } from \"../llms/base.js\";\nimport { SerializedVectorDBQAChain } from \"./serde.js\";\nimport { ChainValues, BaseRetriever } from \"../schema/index.js\";\nimport { loadQAStuffChain } from \"./question_answering/load.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type LoadValues = Record<string, any>;\n\nexport interface RetrievalQAChainInput extends Omit<ChainInputs, \"memory\"> {\nretriever: BaseRetriever;\ncombineDocumentsChain: BaseChain;\ninputKey?: string;\nreturnSourceDocuments?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/retrieval_qa.ts","loc":{"lines":{"from":1,"to":16}}}}],["272",{"pageContent":"class RetrievalQAChain\nextends BaseChain\nimplements RetrievalQAChainInput\n{\ninputKey = \"query\";\n\nget inputKeys() {\nreturn [this.inputKey];\n}\n\nget outputKeys() {\nreturn this.combineDocumentsChain.outputKeys.concat(\nthis.returnSourceDocuments ? [\"sourceDocuments\"] : []\n);\n}\n\nretriever: BaseRetriever;\n\ncombineDocumentsChain: BaseChain;\n\nreturnSourceDocuments = false;\n\nconstructor(fields: RetrievalQAChainInput) {\nsuper(fields);\nthis.retriever = fields.retriever;\nthis.combineDocumentsChain = fields.combineDocumentsChain;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.returnSourceDocuments =\nfields.returnSourceDocuments ?? this.returnSourceDocuments;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/retrieval_qa.ts","loc":{"lines":{"from":103,"to":132}}}}],["273",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Question key ${this.inputKey} not found.`);\n}\nconst question: string = values[this.inputKey];\nconst docs = await this.retriever.getRelevantDocuments(question);\nconst inputs = { question, input_documents: docs };\nconst result = await this.combineDocumentsChain.call(\ninputs,\nrunManager?.getChild()\n);\nif (this.returnSourceDocuments) {\nreturn {\n...result,\nsourceDocuments: docs,\n};\n}\nreturn result;\n}\n\n_chainType() {\nreturn \"retrieval_qa\" as const;\n}\n\nstatic async deserialize(\n_data: SerializedVectorDBQAChain,\n_values: LoadValues\n): Promise<RetrievalQAChain> {\nthrow new Error(\"Not implemented\");\n}\n\nserialize(): SerializedVectorDBQAChain {\nthrow new Error(\"Not implemented\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/retrieval_qa.ts","loc":{"lines":{"from":217,"to":254}}}}],["274",{"pageContent":"static fromLLM(\nllm: BaseLLM,\nretriever: BaseRetriever,\noptions?: Partial<\nOmit<RetrievalQAChainInput, \"combineDocumentsChain\" | \"index\">\n>\n): RetrievalQAChain {\nconst qaChain = loadQAStuffChain(llm);\nreturn new this({\nretriever,\ncombineDocumentsChain: qaChain,\n...options,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/retrieval_qa.ts","loc":{"lines":{"from":328,"to":342}}}}],["275",{"pageContent":"import { BaseChain, ChainInputs } from \"./base.js\";\nimport { ChainValues } from \"../schema/index.js\";\nimport {\nSerializedBaseChain,\nSerializedSequentialChain,\nSerializedSimpleSequentialChain,\n} from \"./serde.js\";\nimport { intersection, union, difference } from \"../util/set.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\n\nfunction formatSet(input: Set<string>) {\nreturn Array.from(input)\n.map((i) => `\"${i}\"`)\n.join(\", \");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":1,"to":15}}}}],["276",{"pageContent":"interface SequentialChainInput extends ChainInputs {\n/** Array of chains to run as a sequence. The chains are run in order they appear in the array. */\nchains: BaseChain[];\n/** Defines which variables should be passed as initial input to the first chain. */\ninputVariables: string[];\n/** Which variables should be returned as a result of executing the chain. If not specified, output of the last of the chains is used. */\noutputVariables?: string[];\n/** Whether or not to return all intermediate outputs and variables (excluding initial input variables). */\nreturnAll?: boolean;\n}\n\n/**\n* Chain where the outputs of one chain feed directly into next.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":313,"to":326}}}}],["277",{"pageContent":"class SequentialChain extends BaseChain implements SequentialChainInput {\nchains: BaseChain[];\n\ninputVariables: string[];\n\noutputVariables: string[];\n\nreturnAll?: boolean | undefined;\n\nget inputKeys() {\nreturn this.inputVariables;\n}\n\nget outputKeys(): string[] {\nreturn this.outputVariables;\n}\n\nconstructor(fields: SequentialChainInput) {\nsuper(fields);\nthis.chains = fields.chains;\nthis.inputVariables = fields.inputVariables;\nthis.outputVariables = fields.outputVariables ?? [];\nif (this.outputVariables.length > 0 && fields.returnAll) {\nthrow new Error(\n\"Either specify variables to return using `outputVariables` or use `returnAll` param. Cannot apply both conditions at the same time.\"\n);\n}\nthis.returnAll = fields.returnAll ?? false;\nthis._validateChains();\n}\n\n/** @ignore */\n_validateChains() {\nif (this.chains.length === 0) {\nthrow new Error(\"Sequential chain must have at least one chain.\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":619,"to":654}}}}],["278",{"pageContent":"const memoryKeys = this.memory?.memoryKeys ?? [];\nconst inputKeysSet = new Set(this.inputKeys);\nconst memoryKeysSet = new Set(memoryKeys);\nconst keysIntersection = intersection(inputKeysSet, memoryKeysSet);\nif (keysIntersection.size > 0) {\nthrow new Error(\n`The following keys: ${formatSet(\nkeysIntersection\n)} are overlapping between memory and input keys of the chain variables. This can lead to unexpected behaviour. Please use input and memory keys that don't overlap.`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":944,"to":954}}}}],["279",{"pageContent":"const availableKeys = union(inputKeysSet, memoryKeysSet);\nfor (const chain of this.chains) {\nconst missingKeys = difference(new Set(chain.inputKeys), availableKeys);\nif (missingKeys.size > 0) {\nthrow new Error(\n`Missing variables for chain \"${chain._chainType()}\": ${formatSet(\nmissingKeys\n)}. Only got the following variables: ${formatSet(availableKeys)}.`\n);\n}\nconst outputKeysSet = new Set(chain.outputKeys);\nconst overlappinOutputKeys = intersection(availableKeys, outputKeysSet);\nif (overlappinOutputKeys.size > 0) {\nthrow new Error(\n`The following output variables for chain \"${chain._chainType()}\" are overlapping: ${formatSet(\noverlappinOutputKeys\n)}. This can lead to unexpected behaviour.`\n);\n}\n\nfor (const outputKey of outputKeysSet) {\navailableKeys.add(outputKey);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":1249,"to":1272}}}}],["280",{"pageContent":"if (this.outputVariables.length === 0) {\nif (this.returnAll) {\nconst outputKeys = difference(availableKeys, inputKeysSet);\nthis.outputVariables = Array.from(outputKeys);\n} else {\nthis.outputVariables = this.chains[this.chains.length - 1].outputKeys;\n}\n} else {\nconst missingKeys = difference(\nnew Set(this.outputVariables),\nnew Set(availableKeys)\n);\nif (missingKeys.size > 0) {\nthrow new Error(\n`The following output variables were expected to be in the final chain output but were not found: ${formatSet(\nmissingKeys\n)}.`\n);\n}\n}\n}\n\n/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nlet input: ChainValues = values;\nconst allChainValues: ChainValues = {};\nfor (const chain of this.chains) {\ninput = await chain.call(input, runManager?.getChild());\nfor (const key of Object.keys(input)) {\nallChainValues[key] = input[key];\n}\n}\nconst output: ChainValues = {};\nfor (const key of this.outputVariables) {\noutput[key] = allChainValues[key];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":1562,"to":1600}}}}],["281",{"pageContent":"return output;\n}\n\n_chainType() {\nreturn \"sequential_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedSequentialChain) {\nconst chains: BaseChain[] = [];\nconst inputVariables: string[] = data.input_variables;\nconst outputVariables: string[] = data.output_variables;\nconst serializedChains = data.chains;\nfor (const serializedChain of serializedChains) {\nconst deserializedChain = await BaseChain.deserialize(serializedChain);\nchains.push(deserializedChain);\n}\nreturn new SequentialChain({ chains, inputVariables, outputVariables });\n}\n\nserialize(): SerializedSequentialChain {\nconst chains: SerializedBaseChain[] = [];\nfor (const chain of this.chains) {\nchains.push(chain.serialize());\n}\nreturn {\n_type: this._chainType(),\ninput_variables: this.inputVariables,\noutput_variables: this.outputVariables,\nchains,\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":1888,"to":1919}}}}],["282",{"pageContent":"interface SimpleSequentialChainInput extends ChainInputs {\n/** Array of chains to run as a sequence. The chains are run in order they appear in the array. */\nchains: Array<BaseChain>;\n/** Whether or not to trim the intermediate outputs. */\ntrimOutputs?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":2209,"to":2214}}}}],["283",{"pageContent":"/**\n* Simple chain where a single string output of one chain is fed directly into the next.\n* @augments BaseChain\n* @augments SimpleSequentialChainInput\n*\n* @example\n* ```ts\n* import { SimpleSequentialChain, LLMChain } from \"langchain/chains\";\n* import { OpenAI } from \"langchain/llms/openai\";\n* import { PromptTemplate } from \"langchain/prompts\";\n*\n* // This is an LLMChain to write a synopsis given a title of a play.\n* const llm = new OpenAI({ temperature: 0 });\n* const template = `You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n*\n* Title: {title}\n* Playwright: This is a synopsis for the above play:`\n* const promptTemplate = new PromptTemplate({ template, inputVariables: [\"title\"] });\n* const synopsisChain = new LLMChain({ llm, prompt: promptTemplate });\n*\n*\n* // This is an LLMChain to write a review of a play given a synopsis.\n* const reviewLLM = new OpenAI({ temperature: 0 })","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":2518,"to":2540}}}}],["284",{"pageContent":"* const reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n*\n* Play Synopsis:\n* {synopsis}\n* Review from a New York Times play critic of the above play:`\n* const reviewPromptTempalte = new PromptTemplate({ template: reviewTemplate, inputVariables: [\"synopsis\"] });\n* const reviewChain = new LLMChain({ llm: reviewLLM, prompt: reviewPromptTempalte });\n*\n* const overallChain = new SimpleSequentialChain({chains: [synopsisChain, reviewChain], verbose:true})\n* const review = await overallChain.run(\"Tragedy at sunset on the beach\")\n* // the variable review contains resulting play review.\n* ```\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":2829,"to":2841}}}}],["285",{"pageContent":"class SimpleSequentialChain\nextends BaseChain\nimplements SimpleSequentialChainInput\n{\nchains: Array<BaseChain>;\n\ninputKey = \"input\";\n\noutputKey = \"output\";\n\ntrimOutputs: boolean;\n\nget inputKeys() {\nreturn [this.inputKey];\n}\n\nget outputKeys(): string[] {\nreturn [this.outputKey];\n}\n\nconstructor(fields: SimpleSequentialChainInput) {\nsuper(\nfields.memory,\nfields.verbose,\nfields.callbacks ?? fields.callbackManager\n);\nthis.chains = fields.chains;\nthis.trimOutputs = fields.trimOutputs ?? false;\nthis._validateChains();\n}\n\n/** @ignore */\n_validateChains() {\nfor (const chain of this.chains) {\nif (chain.inputKeys.length !== 1) {\nthrow new Error(\n`Chains used in SimpleSequentialChain should all have one input, got ${\nchain.inputKeys.length\n} for ${chain._chainType()}.`\n);\n}\nif (chain.outputKeys.length !== 1) {\nthrow new Error(\n`Chains used in SimpleSequentialChain should all have one output, got ${\nchain.outputKeys.length\n} for ${chain._chainType()}.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":3133,"to":3182}}}}],["286",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nlet input: string = values[this.inputKey];\nfor (const chain of this.chains) {\ninput = await chain.run(input, runManager?.getChild());\nif (this.trimOutputs) {\ninput = input.trim();\n}\nawait runManager?.handleText(input);\n}\nreturn { [this.outputKey]: input };\n}\n\n_chainType() {\nreturn \"simple_sequential_chain\" as const;\n}\n\nstatic async deserialize(data: SerializedSimpleSequentialChain) {\nconst chains: Array<BaseChain> = [];\nconst serializedChains = data.chains;\nfor (const serializedChain of serializedChains) {\nconst deserializedChain = await BaseChain.deserialize(serializedChain);\nchains.push(deserializedChain);\n}\nreturn new SimpleSequentialChain({ chains });\n}\n\nserialize(): SerializedSimpleSequentialChain {\nconst chains: Array<SerializedBaseChain> = [];\nfor (const chain of this.chains) {\nchains.push(chain.serialize());\n}\nreturn {\n_type: this._chainType(),\nchains,\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sequential_chain.ts","loc":{"lines":{"from":3470,"to":3510}}}}],["287",{"pageContent":"import type { SerializedLLM } from \"../llms/base.js\";\nimport type { SerializedBasePromptTemplate } from \"../prompts/serde.js\";\nimport type { SerializedSqlDatabase } from \"../util/sql_utils.js\";\n\nexport type SerializedLLMChain = {\n_type: \"llm_chain\";\nllm?: SerializedLLM;\nprompt?: SerializedBasePromptTemplate;\n};\n\nexport type SerializedSequentialChain = {\n_type: \"sequential_chain\";\ninput_variables: string[];\noutput_variables: string[];\nchains: SerializedBaseChain[];\n};\n\nexport type SerializedSimpleSequentialChain = {\n_type: \"simple_sequential_chain\";\nchains: Array<SerializedBaseChain>;\n};\n\nexport type SerializedSqlDatabaseChain = {\n_type: \"sql_database_chain\";\nsql_database: SerializedSqlDatabase;\nllm: SerializedLLM;\n};\n\nexport type SerializedVectorDBQAChain = {\n_type: \"vector_db_qa\";\nk: number;\ncombine_documents_chain: SerializedBaseChain;\n};\n\nexport type SerializedStuffDocumentsChain = {\n_type: \"stuff_documents_chain\";\nllm_chain?: SerializedLLMChain;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/serde.ts","loc":{"lines":{"from":1,"to":38}}}}],["288",{"pageContent":"type SerializedChatVectorDBQAChain = {\n_type: \"chat-vector-db\";\nk: number;\ncombine_documents_chain: SerializedBaseChain;\nquestion_generator: SerializedLLMChain;\n};\n\nexport type SerializedMapReduceDocumentsChain = {\n_type: \"map_reduce_documents_chain\";\nllm_chain?: SerializedLLMChain;\ncombine_document_chain?: SerializedBaseChain;\n};\n\nexport type SerializedRefineDocumentsChain = {\n_type: \"refine_documents_chain\";\nllm_chain?: SerializedLLMChain;\nrefine_llm_chain?: SerializedLLMChain;\n};\n\nexport type SerializedAnalyzeDocumentChain = {\n_type: \"analyze_document_chain\";\ncombine_document_chain?: SerializedBaseChain;\n};\n\nexport type SerializedBaseChain =\n| SerializedLLMChain\n| SerializedSequentialChain\n| SerializedSimpleSequentialChain\n| SerializedVectorDBQAChain\n| SerializedStuffDocumentsChain\n| SerializedSqlDatabaseChain\n| SerializedChatVectorDBQAChain\n| SerializedMapReduceDocumentsChain\n| SerializedAnalyzeDocumentChain\n| SerializedRefineDocumentsChain;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/serde.ts","loc":{"lines":{"from":75,"to":109}}}}],["289",{"pageContent":"import type { TiktokenModel } from \"@dqbd/tiktoken\";\nimport { DEFAULT_SQL_DATABASE_PROMPT } from \"./sql_db_prompt.js\";\nimport { BaseChain, ChainInputs } from \"../base.js\";\nimport type { OpenAI } from \"../../llms/openai.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport type { SqlDatabase } from \"../../sql_db.js\";\nimport { ChainValues } from \"../../schema/index.js\";\nimport { SerializedSqlDatabaseChain } from \"../serde.js\";\nimport { BaseLanguageModel } from \"../../base_language/index.js\";\nimport {\ncalculateMaxTokens,\ngetModelContextSize,\n} from \"../../base_language/count_tokens.js\";\nimport { CallbackManagerForChainRun } from \"../../callbacks/manager.js\";\n\nexport interface SqlDatabaseChainInput extends ChainInputs {\nllm: BaseLanguageModel;\ndatabase: SqlDatabase;\ntopK?: number;\ninputKey?: string;\noutputKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":1,"to":22}}}}],["290",{"pageContent":"class SqlDatabaseChain extends BaseChain {\n// LLM wrapper to use\nllm: BaseLanguageModel;\n\n// SQL Database to connect to.\ndatabase: SqlDatabase;\n\n// Prompt to use to translate natural language to SQL.\nprompt = DEFAULT_SQL_DATABASE_PROMPT;\n\n// Number of results to return from the query\ntopK = 5;\n\ninputKey = \"query\";\n\noutputKey = \"result\";\n\n// Whether to return the result of querying the SQL table directly.\nreturnDirect = false;\n\nconstructor(fields: SqlDatabaseChainInput) {\nsuper(fields);\nthis.llm = fields.llm;\nthis.database = fields.database;\nthis.topK = fields.topK ?? this.topK;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.outputKey = fields.outputKey ?? this.outputKey;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":175,"to":202}}}}],["291",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nconst llmChain = new LLMChain({\nprompt: this.prompt,\nllm: this.llm,\noutputKey: this.outputKey,\nmemory: this.memory,\n});\nif (!(this.inputKey in values)) {\nthrow new Error(`Question key ${this.inputKey} not found.`);\n}\nconst question: string = values[this.inputKey];\nlet inputText = `${question}\\nSQLQuery:`;\nconst tablesToUse = values.table_names_to_use;\nconst tableInfo = await this.database.getTableInfo(tablesToUse);\n\nconst llmInputs = {\ninput: inputText,\ntop_k: this.topK,\ndialect: this.database.appDataSourceOptions.type,\ntable_info: tableInfo,\nstop: [\"\\nSQLResult:\"],\n};\nawait this.verifyNumberOfTokens(inputText, tableInfo);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":360,"to":386}}}}],["292",{"pageContent":"const intermediateStep: string[] = [];\nconst sqlCommand = await llmChain.predict(\nllmInputs,\nrunManager?.getChild()\n);\nintermediateStep.push(sqlCommand);\nlet queryResult = \"\";\ntry {\nqueryResult = await this.database.appDataSource.query(sqlCommand);\nintermediateStep.push(queryResult);\n} catch (error) {\nconsole.error(error);\n}\n\nlet finalResult;\nif (this.returnDirect) {\nfinalResult = { [this.outputKey]: queryResult };\n} else {\ninputText += `${sqlCommand}\\nSQLResult: ${JSON.stringify(\nqueryResult\n)}\\nAnswer:`;\nllmInputs.input = inputText;\nfinalResult = {\n[this.outputKey]: await llmChain.predict(\nllmInputs,\nrunManager?.getChild()\n),\n};\n}\n\nreturn finalResult;\n}\n\n_chainType() {\nreturn \"sql_database_chain\" as const;\n}\n\nget inputKeys(): string[] {\nreturn [this.inputKey];\n}\n\nget outputKeys(): string[] {\nreturn [this.outputKey];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":543,"to":586}}}}],["293",{"pageContent":"static async deserialize(\ndata: SerializedSqlDatabaseChain,\nSqlDatabaseFromOptionsParams: (typeof SqlDatabase)[\"fromOptionsParams\"]\n) {\nconst llm = await BaseLanguageModel.deserialize(data.llm);\nconst sqlDataBase = await SqlDatabaseFromOptionsParams(data.sql_database);\n\nreturn new SqlDatabaseChain({\nllm,\ndatabase: sqlDataBase,\n});\n}\n\nserialize(): SerializedSqlDatabaseChain {\nreturn {\n_type: this._chainType(),\nllm: this.llm.serialize(),\nsql_database: this.database.serialize(),\n};\n}\n\nprivate async verifyNumberOfTokens(\ninputText: string,\ntableinfo: string\n): Promise<void> {\n// We verify it only for OpenAI for the moment\nif (this.llm._llmType() !== \"openai\") {\nreturn;\n}\nconst llm = this.llm as OpenAI;\nconst promptTemplate = this.prompt.template;\nconst stringWeSend = `${inputText}${promptTemplate}${tableinfo}`;\n\nconst maxToken = await calculateMaxTokens({\nprompt: stringWeSend,\n// Cast here to allow for other models that may not fit the union\nmodelName: llm.modelName as TiktokenModel,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":739,"to":776}}}}],["294",{"pageContent":"if (maxToken < llm.maxTokens) {\nthrow new Error(`The combination of the database structure and your question is too big for the model ${\nllm.modelName\n} which can compute only a max tokens of ${getModelContextSize(\nllm.modelName\n)}.\nWe suggest you to use the includeTables parameters when creating the SqlDatabase object to select only a subset of the tables. You can also use a model which can handle more tokens.`);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_chain.ts","loc":{"lines":{"from":921,"to":930}}}}],["295",{"pageContent":"/* eslint-disable spaced-comment */\nimport { PromptTemplate } from \"../../prompts/prompt.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":1,"to":2}}}}],["296",{"pageContent":"const DEFAULT_SQL_DATABASE_PROMPT = /*#__PURE__*/ new PromptTemplate({\ntemplate: `Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n\nUse the following format:\n\nQuestion: \"Question here\"\nSQLQuery: \"SQL Query to run\"\nSQLResult: \"Result of the SQLQuery\"\nAnswer: \"Final answer here\"\n\nOnly use the tables listed below.\n\n{table_info}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":24,"to":40}}}}],["297",{"pageContent":"Question: {input}`,\ninputVariables: [\"dialect\", \"table_info\", \"input\", \"top_k\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/sql_db/sql_db_prompt.ts","loc":{"lines":{"from":49,"to":51}}}}],["298",{"pageContent":"import { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { BasePromptTemplate } from \"../../prompts/base.js\";\nimport {\nStuffDocumentsChain,\nMapReduceDocumentsChain,\nRefineDocumentsChain,\n} from \"../combine_docs_chain.js\";\nimport { DEFAULT_PROMPT } from \"./stuff_prompts.js\";\nimport { REFINE_PROMPT } from \"./refine_prompts.js\";\n\nexport type SummarizationChainParams =\n| {\ntype?: \"stuff\";\nprompt?: BasePromptTemplate;\n}\n| {\ntype?: \"map_reduce\";\ncombineMapPrompt?: BasePromptTemplate;\ncombinePrompt?: BasePromptTemplate;\n}\n| {\ntype?: \"refine\";\nrefinePrompt?: BasePromptTemplate;\nquestionPrompt?: BasePromptTemplate;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/load.ts","loc":{"lines":{"from":1,"to":26}}}}],["299",{"pageContent":"const loadSummarizationChain = (\nllm: BaseLanguageModel,\nparams: SummarizationChainParams = { type: \"map_reduce\" }\n) => {\nif (params.type === \"stuff\") {\nconst { prompt = DEFAULT_PROMPT } = params;\nconst llmChain = new LLMChain({ prompt, llm });\nconst chain = new StuffDocumentsChain({\nllmChain,\ndocumentVariableName: \"text\",\n});\nreturn chain;\n}\nif (params.type === \"map_reduce\") {\nconst {\ncombineMapPrompt = DEFAULT_PROMPT,\ncombinePrompt = DEFAULT_PROMPT,\n} = params;\nconst llmChain = new LLMChain({ prompt: combineMapPrompt, llm });\nconst combineLLMChain = new LLMChain({ prompt: combinePrompt, llm });\nconst combineDocumentChain = new StuffDocumentsChain({\nllmChain: combineLLMChain,\ndocumentVariableName: \"text\",\n});\nconst chain = new MapReduceDocumentsChain({\nllmChain,\ncombineDocumentChain,\ndocumentVariableName: \"text\",\n});\nreturn chain;\n}\nif (params.type === \"refine\") {\nconst { refinePrompt = REFINE_PROMPT, questionPrompt = DEFAULT_PROMPT } =\nparams;\nconst llmChain = new LLMChain({ prompt: questionPrompt, llm });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/load.ts","loc":{"lines":{"from":74,"to":108}}}}],["300",{"pageContent":"const refineLLMChain = new LLMChain({ prompt: refinePrompt, llm });\nconst chain = new RefineDocumentsChain({\nllmChain,\nrefineLLMChain,\ndocumentVariableName: \"text\",\n});\nreturn chain;\n}\nthrow new Error(`Invalid _type: ${params.type}`);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/load.ts","loc":{"lines":{"from":145,"to":154}}}}],["301",{"pageContent":"import { PromptTemplate } from \"../../prompts/prompt.js\";\n\nconst refinePromptTemplate = `Your job is to produce a final summary\nWe have provided an existing summary up to a certain point: \"{existing_answer}\"\nWe have the opportunity to refine the existing summary\n(only if needed) with some more context below.\n------------\n\"{text}\"\n------------\n\nGiven the new context, refine the original summary\nIf the context isn't useful, return the original summary.\n\nREFINED SUMMARY:`;\n\nexport const REFINE_PROMPT = /* #__PURE__ */ new PromptTemplate({\ntemplate: refinePromptTemplate,\ninputVariables: [\"existing_answer\", \"text\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/refine_prompts.ts","loc":{"lines":{"from":1,"to":19}}}}],["302",{"pageContent":"/* eslint-disable spaced-comment */\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\n\nconst template = `Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:`;\n\nexport const DEFAULT_PROMPT = /*#__PURE__*/ new PromptTemplate({\ntemplate,\ninputVariables: [\"text\"],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/stuff_prompts.ts","loc":{"lines":{"from":1,"to":15}}}}],["303",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../../llms/openai.js\";\nimport { loadSummarizationChain } from \"../load.js\";\nimport { Document } from \"../../../document.js\";\n\ntest(\"Test loadSummzationChain stuff\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadSummarizationChain(model, { type: \"stuff\" });\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"Whats up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/tests/load.int.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["304",{"pageContent":"test(\"Test loadSummarizationChain map_reduce\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadSummarizationChain(model, { type: \"map_reduce\" });\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"Whats up\" });\nconsole.log({ res });\n});\n\ntest(\"Test loadSummarizationChain refine\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = loadSummarizationChain(model, { type: \"refine\" });\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"Whats up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/summarization/tests/load.int.test.ts","loc":{"lines":{"from":41,"to":63}}}}],["305",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { StuffDocumentsChain } from \"../combine_docs_chain.js\";\nimport { ChatVectorDBQAChain } from \"../chat_vector_db_chain.js\";\nimport { HNSWLib } from \"../../vectorstores/hnswlib.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["306",{"pageContent":"test(\"Test ChatVectorDBQAChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = PromptTemplate.fromTemplate(\n\"Print {question}, and ignore {chat_history}\"\n);\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst llmChain = new LLMChain({ prompt, llm: model });\nconst combineDocsChain = new StuffDocumentsChain({\nllmChain,\ndocumentVariableName: \"foo\",\n});\nconst chain = new ChatVectorDBQAChain({\ncombineDocumentsChain: combineDocsChain,\nvectorstore: vectorStore,\nquestionGeneratorChain: llmChain,\n});\nconst res = await chain.call({ question: \"foo\", chat_history: \"bar\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":120,"to":142}}}}],["307",{"pageContent":"test(\"Test ChatVectorDBQAChain with returnSourceDocuments\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = PromptTemplate.fromTemplate(\n\"Print {question}, and ignore {chat_history}\"\n);\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst llmChain = new LLMChain({ prompt, llm: model });\nconst combineDocsChain = new StuffDocumentsChain({\nllmChain,\ndocumentVariableName: \"foo\",\n});\nconst chain = new ChatVectorDBQAChain({\ncombineDocumentsChain: combineDocsChain,\nvectorstore: vectorStore,\nquestionGeneratorChain: llmChain,\nreturnSourceDocuments: true,\n});\nconst res = await chain.call({ question: \"foo\", chat_history: \"bar\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":245,"to":268}}}}],["308",{"pageContent":"test(\"Test ChatVectorDBQAChain from LLM\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst chain = ChatVectorDBQAChain.fromLLM(model, vectorStore);\nconst res = await chain.call({ question: \"foo\", chat_history: \"bar\" });\nconsole.log({ res });\n});\ntest(\"Test ChatVectorDBQAChain from LLM with flag option to return source\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst chain = ChatVectorDBQAChain.fromLLM(model, vectorStore, {\nreturnSourceDocuments: true,\n});\nconst res = await chain.call({ question: \"foo\", chat_history: \"bar\" });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":370,"to":391}}}}],["309",{"pageContent":"expect(res).toEqual(\nexpect.objectContaining({\ntext: expect.any(String),\nsourceDocuments: expect.arrayContaining([\nexpect.objectContaining({\nmetadata: expect.objectContaining({\nid: expect.any(Number),\n}),\npageContent: expect.any(String),\n}),\n]),\n})\n);\n});\n\ntest(\"Test ChatVectorDBQAChain from LLM with override default prompts\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\", temperature: 0 });\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\n\nconst qa_template = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say \"Sorry I dont know, I am learning from Aliens\", don't try to make up an answer.\n{context}\n\nQuestion: {question}\nHelpful Answer:`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":490,"to":517}}}}],["310",{"pageContent":"const chain = ChatVectorDBQAChain.fromLLM(model, vectorStore, {\nqaTemplate: qa_template,\n});\nconst res = await chain.call({\nquestion: \"What is better programming Language Python or Javascript \",\nchat_history: \"bar\",\n});\nexpect(res.text).toContain(\"I am learning from Aliens\");\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/chat_vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":617,"to":626}}}}],["311",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { loadChain } from \"../load.js\";\nimport { StuffDocumentsChain } from \"../combine_docs_chain.js\";\nimport { Document } from \"../../document.js\";\nimport {\nloadQAMapReduceChain,\nloadQARefineChain,\n} from \"../question_answering/load.js\";\n\ntest(\"Test StuffDocumentsChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst llmChain = new LLMChain({ prompt, llm: model });\nconst chain = new StuffDocumentsChain({\nllmChain,\ndocumentVariableName: \"foo\",\n});\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.int.test.ts","loc":{"lines":{"from":1,"to":31}}}}],["312",{"pageContent":"test(\"Test MapReduceDocumentsChain with QA chain\", async () => {\nconst model = new OpenAI({ temperature: 0, modelName: \"text-ada-001\" });\nconst chain = loadQAMapReduceChain(model);\nconst docs = [\nnew Document({ pageContent: \"harrison went to harvard\" }),\nnew Document({ pageContent: \"ankush went to princeton\" }),\n];\nconst res = await chain.call({\ninput_documents: docs,\nquestion: \"Where did harrison go to college\",\n});\nconsole.log({ res });\n});\n\ntest(\"Test RefineDocumentsChain with QA chain\", async () => {\nconst model = new OpenAI({ temperature: 0, modelName: \"text-ada-001\" });\nconst chain = loadQARefineChain(model);\nconst docs = [\nnew Document({ pageContent: \"harrison went to harvard\" }),\nnew Document({ pageContent: \"ankush went to princeton\" }),\n];\nconst res = await chain.call({\ninput_documents: docs,\nquestion: \"Where did harrison go to college\",\n});\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.int.test.ts","loc":{"lines":{"from":74,"to":100}}}}],["313",{"pageContent":"test(\"Load chain from hub\", async () => {\nconst chain = await loadChain(\n\"lc://chains/question_answering/stuff/chain.json\"\n);\nconst docs = [\nnew Document({ pageContent: \"foo\" }),\nnew Document({ pageContent: \"bar\" }),\nnew Document({ pageContent: \"baz\" }),\n];\nconst res = await chain.call({ input_documents: docs, question: \"what up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.int.test.ts","loc":{"lines":{"from":146,"to":157}}}}],["314",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { Document } from \"../../document.js\";\nimport { BaseLLM } from \"../../llms/base.js\";\nimport { loadQAMapReduceChain } from \"../question_answering/load.js\";\nimport { LLMResult } from \"../../schema/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.test.ts","loc":{"lines":{"from":1,"to":5}}}}],["315",{"pageContent":"FakeLLM extends BaseLLM {\nnrMapCalls = 0;\n\nnrReduceCalls = 0;\n\n_llmType(): string {\nreturn \"fake\";\n}\n\nasync _generate(prompts: string[], _?: string[]): Promise<LLMResult> {\nreturn {\ngenerations: prompts.map((prompt) => {\nlet completion = \"\";\nif (prompt.startsWith(\"Use the following portion\")) {\nthis.nrMapCalls += 1;\ncompletion = \"a portion of context\";\n} else if (prompt.startsWith(\"Given the following extracted\")) {\nthis.nrReduceCalls += 1;\ncompletion = \"a final answer\";\n}\nreturn [\n{\ntext: completion,\nscore: 0,\n},\n];\n}),\n};\n}\n}\n\ntest(\"Test MapReduceDocumentsChain\", async () => {\nconst model = new FakeLLM({});\nconst chain = loadQAMapReduceChain(model);\nconst docs = [\nnew Document({ pageContent: \"harrison went to harvard\" }),\nnew Document({ pageContent: \"ankush went to princeton\" }),\n];\n\nconst res = await chain.call({\ninput_documents: docs,\nquestion: \"Where did harrison go to college\",\n});\nconsole.log({ res });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.test.ts","loc":{"lines":{"from":80,"to":123}}}}],["316",{"pageContent":"expect(res).toEqual({\ntext: \"a final answer\",\n});\nexpect(model.nrMapCalls).toBe(0); // below maxTokens\nexpect(model.nrReduceCalls).toBe(1);\n});\n\ntest(\"Test MapReduceDocumentsChain with content above maxTokens\", async () => {\nconst model = new FakeLLM({});\nconst chain = loadQAMapReduceChain(model);\nconst aString = \"a\".repeat(10000);\nconst bString = \"b\".repeat(10000);\nconst docs = [\nnew Document({ pageContent: aString }),\nnew Document({ pageContent: bString }),\n];\n\nconst res = await chain.call({\ninput_documents: docs,\nquestion: \"Is the letter c present in the document\",\n});\nconsole.log({ res });\n\nexpect(res).toEqual({\ntext: \"a final answer\",\n});\nexpect(model.nrMapCalls).toBe(2); // above maxTokens\nexpect(model.nrReduceCalls).toBe(1);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/combine_docs_chain.test.ts","loc":{"lines":{"from":172,"to":200}}}}],["317",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { ConversationChain } from \"../conversation.js\";\n\ntest(\"Test ConversationChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst chain = new ConversationChain({ llm: model });\nconst res = await chain.call({ input: \"my favorite color\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/conversation_chain.int.test.ts","loc":{"lines":{"from":1,"to":10}}}}],["318",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\nimport {\nChatPromptTemplate,\nHumanMessagePromptTemplate,\nPromptTemplate,\n} from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { loadChain } from \"../load.js\";\n\ntest(\"Test OpenAI\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst chain = new LLMChain({ prompt, llm: model });\nconst res = await chain.call({ foo: \"my favorite color\" });\nconsole.log({ res });\n});\n\ntest(\"Test run method\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst chain = new LLMChain({ prompt, llm: model });\nconst res = await chain.run(\"my favorite color\");\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/llm_chain.int.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["319",{"pageContent":"test(\"Test apply\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst chain = new LLMChain({ prompt, llm: model });\nconst res = await chain.apply([{ foo: \"my favorite color\" }]);\nconsole.log({ res });\n});\n\ntest(\"Load chain from hub\", async () => {\nconst chain = await loadChain(\"lc://chains/hello-world/chain.json\");\nconst res = await chain.call({ topic: \"my favorite color\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/llm_chain.int.test.ts","loc":{"lines":{"from":82,"to":97}}}}],["320",{"pageContent":"test(\"Test LLMChain with ChatOpenAI\", async () => {\nconst model = new ChatOpenAI({ temperature: 0.9 });\nconst template = \"What is a good name for a company that makes {product}?\";\nconst prompt = new PromptTemplate({ template, inputVariables: [\"product\"] });\nconst humanMessagePrompt = new HumanMessagePromptTemplate(prompt);\nconst chatPromptTemplate = ChatPromptTemplate.fromPromptMessages([\nhumanMessagePrompt,\n]);\nconst chatChain = new LLMChain({ llm: model, prompt: chatPromptTemplate });\nconst res = await chatChain.call({ product: \"colorful socks\" });\nconsole.log({ res });\n});\n\ntest(\"Test deserialize\", async () => {\nconst model = new ChatOpenAI();\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst chain = new LLMChain({ prompt, llm: model });\n\nconst serialized = chain.serialize();\n// console.log(serialized)\nconst chain2 = await LLMChain.deserialize({ ...serialized });\n\nconst res = await chain2.run(\"my favorite color\");\nconsole.log({ res });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/llm_chain.int.test.ts","loc":{"lines":{"from":161,"to":187}}}}],["321",{"pageContent":"// chain === chain2?\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/llm_chain.int.test.ts","loc":{"lines":{"from":235,"to":236}}}}],["322",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { SequentialChain } from \"../sequential_chain.js\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\n\ntest(\"Test SequentialChain example usage\", async () => {\n// This is an LLMChain to write a synopsis given a title of a play and the era it is set in.\nconst llm = new OpenAI({ temperature: 0 });\nconst template = `You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.\n\nTitle: {title}\nEra: {era}\nPlaywright: This is a synopsis for the above play:`;\nconst promptTemplate = new PromptTemplate({\ntemplate,\ninputVariables: [\"title\", \"era\"],\n});\nconst synopsisChain = new LLMChain({\nllm,\nprompt: promptTemplate,\noutputKey: \"synopsis\",\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.int.test.ts","loc":{"lines":{"from":1,"to":24}}}}],["323",{"pageContent":"// This is an LLMChain to write a review of a play given a synopsis.\nconst reviewLLM = new OpenAI({ temperature: 0 });\nconst reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n\nPlay Synopsis:\n{synopsis}\nReview from a New York Times play critic of the above play:`;\nconst reviewPromptTempalte = new PromptTemplate({\ntemplate: reviewTemplate,\ninputVariables: [\"synopsis\"],\n});\nconst reviewChain = new LLMChain({\nllm: reviewLLM,\nprompt: reviewPromptTempalte,\noutputKey: \"review\",\n});\n\nconst overallChain = new SequentialChain({\nchains: [synopsisChain, reviewChain],\ninputVariables: [\"era\", \"title\"],\n// Here we return multiple variables\noutputVariables: [\"synopsis\", \"review\"],\nverbose: true,\n});\nconst review = await overallChain.call({\ntitle: \"Tragedy at sunset on the beach\",\nera: \"Victorian England\",\n});\nexpect(review.review.toLowerCase()).toContain(\n\"tragedy at sunset on the beach\"\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.int.test.ts","loc":{"lines":{"from":100,"to":131}}}}],["324",{"pageContent":"test.skip(\"Test SequentialChain serialize/deserialize\", async () => {\nconst llm1 = new ChatOpenAI();\nconst template1 = `Echo back \"{foo} {bar}\"`;\nconst promptTemplate1 = new PromptTemplate({\ntemplate: template1,\ninputVariables: [\"foo\", \"bar\"],\n});\nconst chain1 = new LLMChain({\nllm: llm1,\nprompt: promptTemplate1,\noutputKey: \"baz\",\n});\n\nconst llm2 = new ChatOpenAI();\nconst template2 = `Echo back \"{baz}\"`;\nconst promptTemplate2 = new PromptTemplate({\ntemplate: template2,\ninputVariables: [\"baz\"],\n});\nconst chain2 = new LLMChain({\nllm: llm2,\nprompt: promptTemplate2,\n});\n\nconst sampleSequentialChain = new SequentialChain({\nchains: [chain1, chain2],\ninputVariables: [\"foo\", \"bar\"],\noutputVariables: [\"text\"],\nverbose: true,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.int.test.ts","loc":{"lines":{"from":203,"to":232}}}}],["325",{"pageContent":"const serializedChain = sampleSequentialChain.serialize();\nexpect(serializedChain._type).toEqual(\"sequential_chain\");\nexpect(serializedChain.chains.length).toEqual(2);\nconst deserializedChain = await SequentialChain.deserialize(serializedChain);\nexpect(deserializedChain.chains.length).toEqual(2);\nexpect(deserializedChain._chainType).toEqual(\"sequential_chain\");\nconst review = await deserializedChain.call({ foo: \"test1\", bar: \"test2\" });\nexpect(review.trim()).toMatchInlineSnapshot(`\"test1 test2\"`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.int.test.ts","loc":{"lines":{"from":315,"to":323}}}}],["326",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { BaseLLM } from \"../../llms/base.js\";\nimport { LLMResult } from \"../../schema/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { SequentialChain } from \"../sequential_chain.js\";\n\nclass FakeLLM1 extends BaseLLM {\nnrMapCalls = 0;\n\nnrReduceCalls = 0;\n\n_llmType(): string {\nreturn \"fake_1\";\n}\n\nasync _generate(_prompts: string[], _?: string[]): Promise<LLMResult> {\nreturn {\ngenerations: [\n[\n{\ntext: \"The answer is XXX.\",\n},\n],\n],\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":1,"to":28}}}}],["327",{"pageContent":"FakeLLM2 extends BaseLLM {\nnrMapCalls = 0;\n\nnrReduceCalls = 0;\n\n_llmType(): string {\nreturn \"fake_2\";\n}\n\nasync _generate(prompts: string[], _?: string[]): Promise<LLMResult> {\nlet response = \"I don't know what you are talking about.\";\nif (prompts[0].includes(\"XXX\")) {\nresponse = \"final answer\";\n}\nreturn {\ngenerations: [\n[\n{\ntext: response,\n},\n],\n],\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":169,"to":193}}}}],["328",{"pageContent":"test(\"Test SequentialChain\", async () => {\nconst model1 = new FakeLLM1({});\nconst model2 = new FakeLLM2({});\nconst template1 = \"Some arbitrary template with fake {input1} and {input2}.\";\nconst template2 = \"Some arbitrary template with fake {input3}.\";\nconst prompt1 = new PromptTemplate({\ntemplate: template1,\ninputVariables: [\"input1\", \"input2\"],\n});\nconst prompt2 = new PromptTemplate({\ntemplate: template2,\ninputVariables: [\"input3\"],\n});\nconst chain1 = new LLMChain({\nllm: model1,\nprompt: prompt1,\noutputKey: \"input3\",\n});\nconst chain2 = new LLMChain({ llm: model2, prompt: prompt2 });\nconst combinedChain = new SequentialChain({\nchains: [chain1, chain2],\ninputVariables: [\"input1\", \"input2\"],\noutputVariables: [\"text\"],\n});\nconst response = await combinedChain.call({\ninput1: \"test1\",\ninput2: \"test2\",\n});\nexpect(response).toMatchInlineSnapshot(`\n{\n\"text\": \"final answer\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":345,"to":378}}}}],["329",{"pageContent":"test(\"Test SequentialChain input/output chains' validation\", () => {\nconst model1 = new FakeLLM1({});\nconst template1 = \"Some arbitrary template with fake {input1} and {input2}.\";\nconst prompt1 = new PromptTemplate({\ntemplate: template1,\ninputVariables: [\"input1\", \"input2\"],\n});\nconst chain1 = new LLMChain({\nllm: model1,\nprompt: prompt1,\noutputKey: \"input3\",\n});\nconst model2 = new FakeLLM2({});\nconst template2 = \"Some arbitrary template with fake {input3}.\";\nconst prompt2 = new PromptTemplate({\ntemplate: template2,\ninputVariables: [\"input3\"],\n});\nconst chain2 = new LLMChain({ llm: model2, prompt: prompt2 });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":501,"to":519}}}}],["330",{"pageContent":"expect(() => {\n/* eslint-disable no-new */\nnew SequentialChain({\nchains: [chain1, chain2],\ninputVariables: [\"input1\"],\noutputVariables: [\"text\"],\n});\n}).toThrowErrorMatchingInlineSnapshot(\n`\"Missing variables for chain \"llm_chain\": \"input2\". Only got the following variables: \"input1\".\"`\n);\nexpect(() => {\n/* eslint-disable no-new */\nnew SequentialChain({\nchains: [chain1, chain2],\ninputVariables: [\"input1\", \"input2\"],\noutputVariables: [\"nonexistent\"],\n});\n}).toThrowErrorMatchingInlineSnapshot(\n`\"The following output variables were expected to be in the final chain output but were not found: \"nonexistent\".\"`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":658,"to":678}}}}],["331",{"pageContent":"test(\"Test SequentialChain chains' intermediate variables validation\", () => {\nconst model1 = new FakeLLM1({});\nconst template1 = \"Some arbitrary template with fake {input1} and {input2}.\";\nconst prompt1 = new PromptTemplate({\ntemplate: template1,\ninputVariables: [\"input1\", \"input2\"],\n});\nconst chain1 = new LLMChain({\nllm: model1,\nprompt: prompt1,\noutputKey: \"nonexistent\",\n});\nconst model2 = new FakeLLM2({});\nconst template2 = \"Some arbitrary template with fake {input3}.\";\nconst prompt2 = new PromptTemplate({\ntemplate: template2,\ninputVariables: [\"input3\"],\n});\nconst chain2 = new LLMChain({ llm: model2, prompt: prompt2 });\n\nexpect(() => {\n/* eslint-disable no-new */\nnew SequentialChain({\nchains: [chain1, chain2],\ninputVariables: [\"input1\", \"input2\"],\noutputVariables: [\"text\"],\n});\n}).toThrowErrorMatchingInlineSnapshot(\n`\"Missing variables for chain \"llm_chain\": \"input3\". Only got the following variables: \"input1\", \"input2\", \"nonexistent\".\"`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sequential_chain.test.ts","loc":{"lines":{"from":817,"to":847}}}}],["332",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { SimpleSequentialChain } from \"../sequential_chain.js\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\n\ntest(\"Test SimpleSequentialChain example usage\", async () => {\n// This is an LLMChain to write a synopsis given a title of a play.\nconst llm = new OpenAI({ temperature: 0 });\nconst template = `You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n\nTitle: {title}\nPlaywright: This is a synopsis for the above play:`;\nconst promptTemplate = new PromptTemplate({\ntemplate,\ninputVariables: [\"title\"],\n});\nconst synopsisChain = new LLMChain({ llm, prompt: promptTemplate });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.int.test.ts","loc":{"lines":{"from":1,"to":19}}}}],["333",{"pageContent":"// This is an LLMChain to write a review of a play given a synopsis.\nconst reviewLLM = new OpenAI({ temperature: 0 });\nconst reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n\nPlay Synopsis:\n{synopsis}\nReview from a New York Times play critic of the above play:`;\nconst reviewPromptTempalte = new PromptTemplate({\ntemplate: reviewTemplate,\ninputVariables: [\"synopsis\"],\n});\nconst reviewChain = new LLMChain({\nllm: reviewLLM,\nprompt: reviewPromptTempalte,\n});\n\nconst overallChain = new SimpleSequentialChain({\nchains: [synopsisChain, reviewChain],\nverbose: true,\n});\nconst review = await overallChain.run(\"Tragedy at sunset on the beach\");\nexpect(review.trim().toLowerCase()).toContain(\n\"tragedy at sunset on the beach\"\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.int.test.ts","loc":{"lines":{"from":81,"to":105}}}}],["334",{"pageContent":"test(\"Test SimpleSequentialChain serialize/deserialize\", async () => {\nconst llm1 = new ChatOpenAI();\nconst template1 = `Echo back \"{foo}\"`;\nconst promptTemplate1 = new PromptTemplate({\ntemplate: template1,\ninputVariables: [\"foo\"],\n});\nconst chain1 = new LLMChain({ llm: llm1, prompt: promptTemplate1 });\n\nconst llm2 = new ChatOpenAI();\nconst template2 = `Echo back \"{bar}\"`;\nconst promptTemplate2 = new PromptTemplate({\ntemplate: template2,\ninputVariables: [\"bar\"],\n});\nconst chain2 = new LLMChain({\nllm: llm2,\nprompt: promptTemplate2,\n});\n\nconst sampleSequentialChain = new SimpleSequentialChain({\nchains: [chain1, chain2],\nverbose: true,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.int.test.ts","loc":{"lines":{"from":167,"to":190}}}}],["335",{"pageContent":"const serializedChain = sampleSequentialChain.serialize();\nexpect(serializedChain._type).toEqual(\"simple_sequential_chain\");\nexpect(serializedChain.chains.length).toEqual(2);\nconst deserializedChain = await SimpleSequentialChain.deserialize(\nserializedChain\n);\nexpect(deserializedChain.chains.length).toEqual(2);\nexpect(deserializedChain._chainType()).toEqual(\"simple_sequential_chain\");\nawait deserializedChain.run(\"test\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.int.test.ts","loc":{"lines":{"from":257,"to":266}}}}],["336",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { BaseLLM } from \"../../llms/base.js\";\nimport { LLMResult } from \"../../schema/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { SimpleSequentialChain } from \"../sequential_chain.js\";\nimport { AnalyzeDocumentChain } from \"../analyze_documents_chain.js\";\nimport { ConversationalRetrievalQAChain } from \"../conversational_retrieval_chain.js\";\nimport { VectorStoreRetriever } from \"../../vectorstores/base.js\";\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\n\nclass FakeLLM1 extends BaseLLM {\nnrMapCalls = 0;\n\nnrReduceCalls = 0;\n\n_llmType(): string {\nreturn \"fake_1\";\n}\n\nasync _generate(_prompts: string[], _?: string[]): Promise<LLMResult> {\nreturn {\ngenerations: [\n[\n{\ntext: \"The answer is XXX.\",\n},\n],\n],\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.test.ts","loc":{"lines":{"from":1,"to":33}}}}],["337",{"pageContent":"FakeLLM2 extends BaseLLM {\nnrMapCalls = 0;\n\nnrReduceCalls = 0;\n\n_llmType(): string {\nreturn \"fake_2\";\n}\n\nasync _generate(prompts: string[], _?: string[]): Promise<LLMResult> {\nlet response = \"I don't know what you are talking about.\";\nif (prompts[0].includes(\"XXX\")) {\nresponse = \"final answer\";\n}\nreturn {\ngenerations: [\n[\n{\ntext: response,\n},\n],\n],\n};\n}\n}\n\ntest(\"Test SimpleSequentialChain\", async () => {\nconst model1 = new FakeLLM1({});\nconst model2 = new FakeLLM2({});\nconst template = \"Some arbitrary template with fake {input}.\";\nconst prompt = new PromptTemplate({ template, inputVariables: [\"input\"] });\nconst chain1 = new LLMChain({ llm: model1, prompt });\nconst chain2 = new LLMChain({ llm: model2, prompt });\nconst combinedChain = new SimpleSequentialChain({ chains: [chain1, chain2] });\nconst response = await combinedChain.run(\"initial question\");\nexpect(response).toEqual(\"final answer\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.test.ts","loc":{"lines":{"from":124,"to":160}}}}],["338",{"pageContent":"test(\"Test SimpleSequentialChain input chains' single input validation\", async () => {\nconst model1 = new FakeLLM1({});\nconst model2 = new FakeLLM2({});\nconst template = \"Some arbitrary template with fake {input1} and {input2}.\";\nconst prompt = new PromptTemplate({\ntemplate,\ninputVariables: [\"input1\", \"input2\"],\n});\nconst chain1 = new LLMChain({ llm: model1, prompt });\nconst chain2 = new LLMChain({ llm: model2, prompt });\nexpect(() => {\n/* eslint-disable no-new */\nnew SimpleSequentialChain({ chains: [chain1, chain2] });\n}).toThrowErrorMatchingInlineSnapshot(\n`\"Chains used in SimpleSequentialChain should all have one input, got 2 for llm_chain.\"`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.test.ts","loc":{"lines":{"from":252,"to":268}}}}],["339",{"pageContent":"test(\"Test SimpleSequentialChain input chains' single ouput validation\", async () => {\nconst model1 = new FakeLLM1({});\nconst fakeEmbeddings = new FakeEmbeddings();\nconst anyStore = new MemoryVectorStore(fakeEmbeddings);\nconst retriever = new VectorStoreRetriever({\nvectorStore: anyStore,\n});\nconst template = \"Some arbitrary template with fake {input}.\";\nconst prompt = new PromptTemplate({ template, inputVariables: [\"input\"] });\nconst chain1 = new LLMChain({ llm: model1, prompt });\nconst chain2 = new ConversationalRetrievalQAChain({\nretriever,\ncombineDocumentsChain: chain1,\nquestionGeneratorChain: chain1,\nreturnSourceDocuments: true,\n});\n// Chain below is is not meant to work in a real-life scenario.\n// It's only combined this way to get one input/multiple outputs chain.\nconst multipleOutputChain = new AnalyzeDocumentChain({\ncombineDocumentsChain: chain2,\n});\nexpect(() => {\n/* eslint-disable no-new */\nnew SimpleSequentialChain({ chains: [chain1, multipleOutputChain] });\n}).toThrowErrorMatchingInlineSnapshot(","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.test.ts","loc":{"lines":{"from":373,"to":397}}}}],["340",{"pageContent":"`\"Chains used in SimpleSequentialChain should all have one output, got 2 for analyze_document_chain.\"`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/simple_sequential_chain.test.ts","loc":{"lines":{"from":480,"to":482}}}}],["341",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { DataSource } from \"typeorm\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { SqlDatabaseChain } from \"../sql_db/sql_db_chain.js\";\nimport { SqlDatabase } from \"../../sql_db.js\";\n\ntest(\"Test SqlDatabaseChain\", async () => {\nconst datasource = new DataSource({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["342",{"pageContent":": \"sqlite\",\ndatabase: \":memory:\",\nsynchronize: true,\n});\n\nawait datasource.initialize();\nawait datasource.query(`\nCREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, age INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Alice', 20);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Bob', 21);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Charlie', 22);\n`);\n\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\n\nconst chain = new SqlDatabaseChain({\nllm: new OpenAI({ temperature: 0 }),\ndatabase: db,\n});\n\nconst res = await chain.run(\"How many users are there?\");\nconsole.log(res);\n\nawait datasource.destroy();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":104,"to":136}}}}],["343",{"pageContent":"// We create this string to reach the token limit of the query built to describe the database and get the SQL query.\nconst veryLongString = `Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam orci nisi, vulputate ac pulvinar eu, maximus a tortor. Duis suscipit, nibh vel fermentum vehicula, mauris ante convallis metus, et feugiat turpis mauris non felis. Interdum et malesuada fames ac ante ipsum primis in faucibus. Maecenas efficitur nibh in nisi sagittis ultrices. Donec id velit nunc. Nam a lacus risus. Vestibulum molestie massa eget convallis pellentesque.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":157,"to":158}}}}],["344",{"pageContent":"Mauris a nisl eget velit finibus blandit ac a odio. Sed sagittis consequat urna a egestas. Curabitur pretium convallis nibh, in ullamcorper odio tempus nec. Curabitur laoreet nec nisl sed accumsan. Sed elementum eleifend molestie. Aenean ullamcorper interdum risus, eget pharetra est volutpat ut. Aenean maximus consequat justo rutrum finibus. Mauris consequat facilisis consectetur. Vivamus rutrum dignissim libero, non aliquam lectus tempus id. In hac habitasse platea dictumst. Sed at magna dignissim, tincidunt lectus in, malesuada risus. Phasellus placerat blandit ligula. Integer posuere id elit at commodo. Sed consequat sagittis odio eget congue.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":159,"to":159}}}}],["345",{"pageContent":"Aliquam ultricies, sapien a porta luctus, dolor nibh dignissim erat, dictum luctus orci lorem non quam. Quisque dapibus tempus mattis. Suspendisse gravida consequat mi at viverra. Quisque sed est purus. Fusce tincidunt semper massa eu blandit. Donec in lacus a tortor facilisis facilisis. Interdum et malesuada fames ac ante ipsum primis in faucibus. In aliquam dignissim eros ac consectetur. Aliquam fringilla magna erat. Nullam tincidunt maximus nulla, quis gravida est varius vel. Aliquam cursus, diam non facilisis mollis, nunc diam convallis enim, ac tempus diam tortor in dui. Nunc feugiat ligula odio, eleifend fermentum quam tincidunt sed. Duis pellentesque quam eget volutpat commodo.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":160,"to":160}}}}],["346",{"pageContent":"Aliquam ex velit, porta sit amet augue vulputate, rhoncus fermentum magna. Integer non elementum augue. Phasellus rhoncus nisl nec magna lacinia vulputate. Suspendisse diam nibh, egestas a porta a, pellentesque ut nisl. Donec tempus ligula at leo convallis consequat. Duis sapien lorem, lobortis ac nisl dapibus, bibendum mollis lorem. Sed congue porttitor ex, eget scelerisque ligula consectetur quis. Mauris felis mauris, sodales quis nunc non, condimentum eleifend quam. Ut vitae viverra lorem. Vivamus lacinia et dolor vitae cursus. Proin faucibus venenatis enim vitae tincidunt. Sed sed venenatis leo.\nDonec eu erat ullamcorper, consectetur dui sed, cursus tellus. Phasellus consectetur felis augue, quis auctor odio semper ac. In scelerisque gravida ligula eget lobortis. Sed tristique ultricies fringilla. Nunc in ultrices purus. Curabitur dictum cursus ante, at tempus est interdum at. Donec gravida lectus ut purus vestibulum, eu accumsan nisl pharetra.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":161,"to":162}}}}],["347",{"pageContent":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam orci nisi, vulputate ac pulvinar eu, maximus a tortor. Duis suscipit, nibh vel fermentum vehicula, mauris ante convallis metus, et feugiat turpis mauris non felis. Interdum et malesuada fames ac ante ipsum primis in faucibus. Maecenas efficitur nibh in nisi sagittis ultrices. Donec id velit nunc. Nam a lacus risus. Vestibulum molestie massa eget convallis pellentesque.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":162,"to":162}}}}],["348",{"pageContent":"Mauris a nisl eget velit finibus blandit ac a odio. Sed sagittis consequat urna a egestas. Curabitur pretium convallis nibh, in ullamcorper odio tempus nec. Curabitur laoreet nec nisl sed accumsan. Sed elementum eleifend molestie. Aenean ullamcorper interdum risus, eget pharetra est volutpat ut. Aenean maximus consequat justo rutrum finibus. Mauris consequat facilisis consectetur. Vivamus rutrum dignissim libero, non aliquam lectus tempus id. In hac habitasse platea dictumst. Sed at magna dignissim, tincidunt lectus in, malesuada risus. Phasellus placerat blandit ligula. Integer posuere id elit at commodo. Sed consequat sagittis odio eget congue.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":163,"to":163}}}}],["349",{"pageContent":"Aliquam ultricies, sapien a porta luctus, dolor nibh dignissim erat, dictum luctus orci lorem non quam. Quisque dapibus tempus mattis. Suspendisse gravida consequat mi at viverra. Quisque sed est purus. Fusce tincidunt semper massa eu blandit. Donec in lacus a tortor facilisis facilisis. Interdum et malesuada fames ac ante ipsum primis in faucibus. In aliquam dignissim eros ac consectetur. Aliquam fringilla magna erat. Nullam tincidunt maximus nulla, quis gravida est varius vel. Aliquam cursus, diam non facilisis mollis, nunc diam convallis enim, ac tempus diam tortor in dui. Nunc feugiat ligula odio, eleifend fermentum quam tincidunt sed. Duis pellentesque quam eget volutpat commodo.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":164,"to":164}}}}],["350",{"pageContent":"Aliquam ex velit, porta sit amet augue vulputate, rhoncus fermentum magna. Integer non elementum augue. Phasellus rhoncus nisl nec magna lacinia vulputate. Suspendisse diam nibh, egestas a porta a, pellentesque ut nisl. Donec tempus ligula at leo convallis consequat. Duis sapien lorem, lobortis ac nisl dapibus, bibendum mollis lorem. Sed congue porttitor ex, eget scelerisque ligula consectetur quis. Mauris felis mauris, sodales quis nunc non, condimentum eleifend quam. Ut vitae viverra lorem. Vivamus lacinia et dolor vitae cursus. Proin faucibus venenatis enim vitae tincidunt. Sed sed venenatis leo.\nDonec eu erat ullamcorper, consectetur dui sed, cursus tellus. Phasellus consectetur felis augue, quis auctor odio semper ac. In scelerisque gravida ligula eget lobortis. Sed tristique ultricies fringilla. Nunc in ultrices purus. Curabitur dictum cursus ante, at tempus est interdum at. Donec gravida lectus ut purus vestibulum, eu accumsan nisl pharetra.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":165,"to":166}}}}],["351",{"pageContent":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam orci nisi, vulputate ac pulvinar eu, maximus a tortor. Duis suscipit, nibh vel fermentum vehicula, mauris ante convallis metus, et feugiat turpis mauris non felis. Interdum et malesuada fames ac ante ipsum primis in faucibus. Maecenas efficitur nibh in nisi sagittis ultrices. Donec id velit nunc. Nam a lacus risus. Vestibulum molestie massa eget convallis pellentesque.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":166,"to":166}}}}],["352",{"pageContent":"Mauris a nisl eget velit finibus blandit ac a odio. Sed sagittis consequat urna a egestas. Curabitur pretium convallis nibh, in ullamcorper odio tempus nec. Curabitur laoreet nec nisl sed accumsan. Sed elementum eleifend molestie. Aenean ullamcorper interdum risus, eget pharetra est volutpat ut. Aenean maximus consequat justo rutrum finibus. Mauris consequat facilisis consectetur. Vivamus rutrum dignissim libero, non aliquam lectus tempus id. In hac habitasse platea dictumst. Sed at magna dignissim, tincidunt lectus in, malesuada risus. Phasellus placerat blandit ligula. Integer posuere id elit at commodo. Sed consequat sagittis odio eget congue.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":167,"to":167}}}}],["353",{"pageContent":"Aliquam ultricies, sapien a porta luctus, dolor nibh dignissim erat, dictum luctus orci lorem non quam. Quisque dapibus tempus mattis. Suspendisse gravida consequat mi at viverra. Quisque sed est purus. Fusce tincidunt semper massa eu blandit. Donec in lacus a tortor facilisis facilisis. Interdum et malesuada fames ac ante ipsum primis in faucibus. In aliquam dignissim eros ac consectetur. Aliquam fringilla magna erat. Nullam tincidunt maximus nulla, quis gravida est varius vel. Aliquam cursus, diam non facilisis mollis, nunc diam convallis enim, ac tempus diam tortor in dui. Nunc feugiat ligula odio, eleifend fermentum quam tincidunt sed. Duis pellentesque quam eget volutpat commodo.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":168,"to":168}}}}],["354",{"pageContent":"Aliquam ex velit, porta sit amet augue vulputate, rhoncus fermentum magna. Integer non elementum augue. Phasellus rhoncus nisl nec magna lacinia vulputate. Suspendisse diam nibh, egestas a porta a, pellentesque ut nisl. Donec tempus ligula at leo convallis consequat. Duis sapien lorem, lobortis ac nisl dapibus, bibendum mollis lorem. Sed congue porttitor ex, eget scelerisque ligula consectetur quis. Mauris felis mauris, sodales quis nunc non, condimentum eleifend quam. Ut vitae viverra lorem. Vivamus lacinia et dolor vitae cursus. Proin faucibus venenatis enim vitae tincidunt. Sed sed venenatis leo.\nDonec eu erat ullamcorper, consectetur dui sed, cursus tellus. Phasellus consectetur felis augue, quis auctor odio semper ac. In scelerisque gravida ligula eget lobortis. Sed tristique ultricies fringilla. Nunc in ultrices purus. Curabitur dictum cursus ante, at tempus est interdum at. Donec gravida lectus ut purus vestibulum, eu accumsan nisl pharetra.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":169,"to":170}}}}],["355",{"pageContent":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam orci nisi, vulputate ac pulvinar eu, maximus a tortor. Duis suscipit, nibh vel fermentum vehicula, mauris ante convallis metus, et feugiat turpis mauris non felis. Interdum et malesuada fames ac ante ipsum primis in faucibus. Maecenas efficitur nibh in nisi sagittis ultrices. Donec id velit nunc. Nam a lacus risus. Vestibulum molestie massa eget convallis pellentesque.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":170,"to":170}}}}],["356",{"pageContent":"Mauris a nisl eget velit finibus blandit ac a odio. Sed sagittis consequat urna a egestas. Curabitur pretium convallis nibh, in ullamcorper odio tempus nec. Curabitur laoreet nec nisl sed accumsan. Sed elementum eleifend molestie. Aenean ullamcorper interdum risus, eget pharetra est volutpat ut. Aenean maximus consequat justo rutrum finibus. Mauris consequat facilisis consectetur. Vivamus rutrum dignissim libero, non aliquam lectus tempus id. In hac habitasse platea dictumst. Sed at magna dignissim, tincidunt lectus in, malesuada risus. Phasellus placerat blandit ligula. Integer posuere id elit at commodo. Sed consequat sagittis odio eget congue.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":171,"to":171}}}}],["357",{"pageContent":"Aliquam ultricies, sapien a porta luctus, dolor nibh dignissim erat, dictum luctus orci lorem non quam. Quisque dapibus tempus mattis. Suspendisse gravida consequat mi at viverra. Quisque sed est purus. Fusce tincidunt semper massa eu blandit. Donec in lacus a tortor facilisis facilisis. Interdum et malesuada fames ac ante ipsum primis in faucibus. In aliquam dignissim eros ac consectetur. Aliquam fringilla magna erat. Nullam tincidunt maximus nulla, quis gravida est varius vel. Aliquam cursus, diam non facilisis mollis, nunc diam convallis enim, ac tempus diam tortor in dui. Nunc feugiat ligula odio, eleifend fermentum quam tincidunt sed. Duis pellentesque quam eget volutpat commodo.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":172,"to":172}}}}],["358",{"pageContent":"Aliquam ex velit, porta sit amet augue vulputate, rhoncus fermentum magna. Integer non elementum augue. Phasellus rhoncus nisl nec magna lacinia vulputate. Suspendisse diam nibh, egestas a porta a, pellentesque ut nisl. Donec tempus ligula at leo convallis consequat. Duis sapien lorem, lobortis ac nisl dapibus, bibendum mollis lorem. Sed congue porttitor ex, eget scelerisque ligula consectetur quis. Mauris felis mauris, sodales quis nunc non, condimentum eleifend quam. Ut vitae viverra lorem. Vivamus lacinia et dolor vitae cursus. Proin faucibus venenatis enim vitae tincidunt. Sed sed venenatis leo.\n`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":188,"to":189}}}}],["359",{"pageContent":"test(\"Test token limite SqlDatabaseChain\", async () => {\nconst datasource = new DataSource({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":230,"to":231}}}}],["360",{"pageContent":": \"sqlite\",\ndatabase: \":memory:\",\nsynchronize: true,\n});\n\nawait datasource.initialize();\nawait datasource.query(`\nCREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, age INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Alice', 20);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Bob', 21);\n`);\n\n// eslint-disable-next-line @typescript-eslint/no-use-before-define\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('${veryLongString}', 22);\n`);\n\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\n\nconst chain = new SqlDatabaseChain({\nllm: new OpenAI({ temperature: 0 }),\ndatabase: db,\n});\n\nconst runChain = async () => {\nawait chain.run(\"How many users are there?\");\n};\n\nawait expect(runChain()).rejects.toThrow();\n\nawait datasource.destroy();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/sql_db_chain.int.test.ts","loc":{"lines":{"from":332,"to":369}}}}],["361",{"pageContent":"import { test } from \"@jest/globals\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { PromptTemplate } from \"../../prompts/index.js\";\nimport { LLMChain } from \"../llm_chain.js\";\nimport { loadChain } from \"../load.js\";\nimport { StuffDocumentsChain } from \"../combine_docs_chain.js\";\nimport { VectorDBQAChain } from \"../vector_db_qa.js\";\nimport { HNSWLib } from \"../../vectorstores/hnswlib.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["362",{"pageContent":"test(\"Test VectorDBQAChain\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst prompt = new PromptTemplate({\ntemplate: \"Print {foo}\",\ninputVariables: [\"foo\"],\n});\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst llmChain = new LLMChain({ prompt, llm: model });\nconst combineDocsChain = new StuffDocumentsChain({\nllmChain,\ndocumentVariableName: \"foo\",\n});\nconst chain = new VectorDBQAChain({\ncombineDocumentsChain: combineDocsChain,\nvectorstore: vectorStore,\n});\nconst res = await chain.call({ query: \"What up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":58,"to":80}}}}],["363",{"pageContent":"test(\"Test VectorDBQAChain from LLM\", async () => {\nconst model = new OpenAI({ modelName: \"text-ada-001\" });\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore);\nconst res = await chain.call({ query: \"What up\" });\nconsole.log({ res });\n});\n\ntest(\"Load chain from hub\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew OpenAIEmbeddings()\n);\nconst chain = await loadChain(\"lc://chains/vector-db-qa/stuff/chain.json\", {\nvectorstore: vectorStore,\n});\nconst res = await chain.call({ query: \"what up\" });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/tests/vector_db_qa_chain.int.test.ts","loc":{"lines":{"from":121,"to":144}}}}],["364",{"pageContent":"import { BaseChain, ChainInputs } from \"./base.js\";\nimport { VectorStore } from \"../vectorstores/base.js\";\nimport { SerializedVectorDBQAChain } from \"./serde.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { CallbackManagerForChainRun } from \"../callbacks/manager.js\";\nimport { ChainValues } from \"../schema/index.js\";\nimport { loadQAStuffChain } from \"./question_answering/load.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type LoadValues = Record<string, any>;\n\nexport interface VectorDBQAChainInput extends Omit<ChainInputs, \"memory\"> {\nvectorstore: VectorStore;\ncombineDocumentsChain: BaseChain;\nreturnSourceDocuments?: boolean;\nk?: number;\ninputKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/vector_db_qa.ts","loc":{"lines":{"from":1,"to":18}}}}],["365",{"pageContent":"class VectorDBQAChain extends BaseChain implements VectorDBQAChainInput {\nk = 4;\n\ninputKey = \"query\";\n\nget inputKeys() {\nreturn [this.inputKey];\n}\n\nget outputKeys() {\nreturn this.combineDocumentsChain.outputKeys.concat(\nthis.returnSourceDocuments ? [\"sourceDocuments\"] : []\n);\n}\n\nvectorstore: VectorStore;\n\ncombineDocumentsChain: BaseChain;\n\nreturnSourceDocuments = false;\n\nconstructor(fields: VectorDBQAChainInput) {\nsuper(fields);\nthis.vectorstore = fields.vectorstore;\nthis.combineDocumentsChain = fields.combineDocumentsChain;\nthis.inputKey = fields.inputKey ?? this.inputKey;\nthis.k = fields.k ?? this.k;\nthis.returnSourceDocuments =\nfields.returnSourceDocuments ?? this.returnSourceDocuments;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/vector_db_qa.ts","loc":{"lines":{"from":127,"to":156}}}}],["366",{"pageContent":"/** @ignore */\nasync _call(\nvalues: ChainValues,\nrunManager?: CallbackManagerForChainRun\n): Promise<ChainValues> {\nif (!(this.inputKey in values)) {\nthrow new Error(`Question key ${this.inputKey} not found.`);\n}\nconst question: string = values[this.inputKey];\nconst docs = await this.vectorstore.similaritySearch(question, this.k);\nconst inputs = { question, input_documents: docs };\nconst result = await this.combineDocumentsChain.call(\ninputs,\nrunManager?.getChild()\n);\nif (this.returnSourceDocuments) {\nreturn {\n...result,\nsourceDocuments: docs,\n};\n}\nreturn result;\n}\n\n_chainType() {\nreturn \"vector_db_qa\" as const;\n}\n\nstatic async deserialize(\ndata: SerializedVectorDBQAChain,\nvalues: LoadValues\n) {\nif (!(\"vectorstore\" in values)) {\nthrow new Error(\n`Need to pass in a vectorstore to deserialize VectorDBQAChain`\n);\n}\nconst { vectorstore } = values;\nif (!data.combine_documents_chain) {\nthrow new Error(\n`VectorDBQAChain must have combine_documents_chain in serialized data`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/vector_db_qa.ts","loc":{"lines":{"from":266,"to":308}}}}],["367",{"pageContent":"return new VectorDBQAChain({\ncombineDocumentsChain: await BaseChain.deserialize(\ndata.combine_documents_chain\n),\nk: data.k,\nvectorstore,\n});\n}\n\nserialize(): SerializedVectorDBQAChain {\nreturn {\n_type: this._chainType(),\ncombine_documents_chain: this.combineDocumentsChain.serialize(),\nk: this.k,\n};\n}\n\nstatic fromLLM(\nllm: BaseLanguageModel,\nvectorstore: VectorStore,\noptions?: Partial<\nOmit<VectorDBQAChainInput, \"combineDocumentsChain\" | \"vectorstore\">\n>\n): VectorDBQAChain {\nconst qaChain = loadQAStuffChain(llm);\nreturn new this({\nvectorstore,\ncombineDocumentsChain: qaChain,\n...options,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chains/vector_db_qa.ts","loc":{"lines":{"from":403,"to":434}}}}],["368",{"pageContent":"import {\nAI_PROMPT,\nHUMAN_PROMPT,\nClient as AnthropicApi,\nCompletionResponse,\nSamplingParameters,\n} from \"@anthropic-ai/sdk\";\nimport { BaseChatModel, BaseChatModelParams } from \"./base.js\";\nimport {\nAIChatMessage,\nBaseChatMessage,\nChatGeneration,\nChatResult,\nMessageType,\n} from \"../schema/index.js\";\nimport { CallbackManagerForLLMRun } from \"../callbacks/manager.js\";\n\nfunction getAnthropicPromptFromMessage(type: MessageType): string {\nswitch (type) {\ncase \"ai\":\nreturn AI_PROMPT;\ncase \"human\":\nreturn HUMAN_PROMPT;\ncase \"system\":\nreturn \"\";\ndefault:\nthrow new Error(`Unknown message type: ${type}`);\n}\n}\n\nconst DEFAULT_STOP_SEQUENCES = [HUMAN_PROMPT];\n\n/**\n* Input to AnthropicChat class.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":1,"to":35}}}}],["369",{"pageContent":"interface AnthropicInput {\n/** Amount of randomness injected into the response. Ranges\n* from 0 to 1. Use temp closer to 0 for analytical /\n* multiple choice, and temp closer to 1 for creative\n* and generative tasks.\n*/\ntemperature?: number;\n\n/** Only sample from the top K options for each subsequent\n* token. Used to remove \"long tail\" low probability\n* responses. Defaults to -1, which disables it.\n*/\ntopK?: number;\n\n/** Does nucleus sampling, in which we compute the\n* cumulative distribution over all the options for each\n* subsequent token in decreasing probability order and\n* cut it off once it reaches a particular probability\n* specified by top_p. Defaults to -1, which disables it.\n* Note that you should either alter temperature or top_p,\n* but not both.\n*/\ntopP?: number;\n\n/** A maximum number of tokens to generate before stopping. */\nmaxTokensToSample: number;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":292,"to":317}}}}],["370",{"pageContent":"/** A list of strings upon which to stop generating.\n* You probably want `[\"\\n\\nHuman:\"]`, as that's the cue for\n* the next turn in the dialog agent.\n*/\nstopSequences?: string[];\n\n/** Whether to stream the results or not */\nstreaming?: boolean;\n\n/** Anthropic API key */\napiKey?: string;\n\n/** Model name to use */\nmodelName: string;\n\n/** Holds any additional parameters that are valid to pass to {@link\n* https://console.anthropic.com/docs/api/reference |\n* `anthropic.complete`} that are not explicitly specified on this class.\n*/\ninvocationKwargs?: Kwargs;\n}\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":567,"to":589}}}}],["371",{"pageContent":"Kwargs = Record<string, any>;\n\n/**\n* Wrapper around Anthropic large language models.\n*\n* To use you should have the `@anthropic-ai/sdk` package installed, with the\n* `ANTHROPIC_API_KEY` environment variable set.\n*\n* @remarks\n* Any parameters that are valid to be passed to {@link\n* https://console.anthropic.com/docs/api/reference |\n* `anthropic.complete`} can be passed through {@link invocationKwargs},\n* even if not explicitly available on this class.\n*\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":850,"to":864}}}}],["372",{"pageContent":"class ChatAnthropic extends BaseChatModel implements AnthropicInput {\napiKey?: string;\n\ntemperature = 1;\n\ntopK = -1;\n\ntopP = -1;\n\nmaxTokensToSample = 2048;\n\nmodelName = \"claude-v1\";\n\ninvocationKwargs?: Kwargs;\n\nstopSequences?: string[];\n\nstreaming = false;\n\n// Used for non-streaming requests\nprivate batchClient: AnthropicApi;\n\n// Used for streaming requests\nprivate streamingClient: AnthropicApi;\n\nconstructor(\nfields?: Partial<AnthropicInput> &\nBaseChatModelParams & {\nanthropicApiKey?: string;\n}\n) {\nsuper(fields ?? {});\n\nthis.apiKey =\nfields?.anthropicApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.ANTHROPIC_API_KEY\n: undefined);\nif (!this.apiKey) {\nthrow new Error(\"Anthropic API key not found\");\n}\n\nthis.modelName = fields?.modelName ?? this.modelName;\nthis.invocationKwargs = fields?.invocationKwargs ?? {};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":1133,"to":1177}}}}],["373",{"pageContent":"this.temperature = fields?.temperature ?? this.temperature;\nthis.topK = fields?.topK ?? this.topK;\nthis.topP = fields?.topP ?? this.topP;\nthis.maxTokensToSample =\nfields?.maxTokensToSample ?? this.maxTokensToSample;\nthis.stopSequences = fields?.stopSequences ?? this.stopSequences;\n\nthis.streaming = fields?.streaming ?? false;\n}\n\n/**\n* Get the parameters used to invoke the model\n*/\ninvocationParams(): Omit<SamplingParameters, \"prompt\"> & Kwargs {\nreturn {\nmodel: this.modelName,\ntemperature: this.temperature,\ntop_k: this.topK,\ntop_p: this.topP,\nstop_sequences: this.stopSequences ?? DEFAULT_STOP_SEQUENCES,\nmax_tokens_to_sample: this.maxTokensToSample,\nstream: this.streaming,\n...this.invocationKwargs,\n};\n}\n\n/** @ignore */\n_identifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n};\n}\n\n/**\n* Get the identifying parameters for the model\n*/\nidentifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":1427,"to":1469}}}}],["374",{"pageContent":"private formatMessagesAsPrompt(messages: BaseChatMessage[]): string {\nreturn (\nmessages\n.map((message) => {\nconst messagePrompt = getAnthropicPromptFromMessage(\nmessage._getType()\n);\nreturn `${messagePrompt} ${message.text}`;\n})\n.join(\"\") + AI_PROMPT\n);\n}\n\n/** @ignore */\nasync _generate(\nmessages: BaseChatMessage[],\nstopSequences?: string[],\nrunManager?: CallbackManagerForLLMRun\n): Promise<ChatResult> {\nif (this.stopSequences && stopSequences) {\nthrow new Error(\n`\"stopSequence\" parameter found in input and default params`\n);\n}\n\nconst params = this.invocationParams();\nparams.stop_sequences = stopSequences\n? stopSequences.concat(DEFAULT_STOP_SEQUENCES)\n: params.stop_sequences;\n\nconst response = await this.completionWithRetry(\n{\n...params,\nprompt: this.formatMessagesAsPrompt(messages),\n},\nrunManager\n);\n\nconst generations: ChatGeneration[] = response.completion\n.split(AI_PROMPT)\n.map((message) => ({\ntext: message,\nmessage: new AIChatMessage(message),\n}));\n\nreturn {\ngenerations,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":1718,"to":1766}}}}],["375",{"pageContent":"/** @ignore */\nasync completionWithRetry(\nrequest: SamplingParameters & Kwargs,\nrunManager?: CallbackManagerForLLMRun\n): Promise<CompletionResponse> {\nif (!this.apiKey) {\nthrow new Error(\"Missing Anthropic API key.\");\n}\nlet makeCompletionRequest;\nif (request.stream) {\nif (!this.streamingClient) {\nthis.streamingClient = new AnthropicApi(this.apiKey);\n}\nmakeCompletionRequest = async () => {\nlet currentCompletion = \"\";\nreturn this.streamingClient.completeStream(request, {\nonUpdate: (data: CompletionResponse) => {\nif (data.stop_reason) {\nreturn;\n}\nconst part = data.completion;\nif (part) {\nconst delta = part.slice(currentCompletion.length);\ncurrentCompletion += delta ?? \"\";\n// eslint-disable-next-line no-void\nvoid runManager?.handleLLMNewToken(delta ?? \"\");\n}\n},\n});\n};\n} else {\nif (!this.batchClient) {\nthis.batchClient = new AnthropicApi(this.apiKey);\n}\nmakeCompletionRequest = async () => this.batchClient.complete(request);\n}\nreturn this.caller.call(makeCompletionRequest);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":2014,"to":2051}}}}],["376",{"pageContent":"_llmType() {\nreturn \"anthropic\";\n}\n\n/** @ignore */\n_combineLLMOutput() {\nreturn [];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/anthropic.ts","loc":{"lines":{"from":2299,"to":2307}}}}],["377",{"pageContent":"import {\nAIChatMessage,\nBaseChatMessage,\nBasePromptValue,\nChatGeneration,\nChatResult,\nLLMResult,\nRUN_KEY,\n} from \"../schema/index.js\";\nimport {\nBaseLanguageModel,\nBaseLanguageModelCallOptions,\nBaseLanguageModelParams,\n} from \"../base_language/index.js\";\nimport { getBufferString } from \"../memory/base.js\";\nimport {\nCallbackManager,\nCallbackManagerForLLMRun,\nCallbacks,\n} from \"../callbacks/manager.js\";\n\nexport type SerializedChatModel = {\n_model: string;\n_type: string;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\n// todo?\nexport type SerializedLLM = {\n_model: string;\n_type: string;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\nexport type BaseChatModelParams = BaseLanguageModelParams;\n\nexport type BaseChatModelCallOptions = BaseLanguageModelCallOptions;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":1,"to":37}}}}],["378",{"pageContent":"abstract class BaseChatModel extends BaseLanguageModel {\ndeclare CallOptions: BaseChatModelCallOptions;\n\nconstructor(fields: BaseChatModelParams) {\nsuper(fields);\n}\n\nabstract _combineLLMOutput?(\n...llmOutputs: LLMResult[\"llmOutput\"][]\n): LLMResult[\"llmOutput\"];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":166,"to":175}}}}],["379",{"pageContent":"async generate(\nmessages: BaseChatMessage[][],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult> {\nconst generations: ChatGeneration[][] = [];\nconst llmOutputs: LLMResult[\"llmOutput\"][] = [];\nconst messageStrings: string[] = messages.map((messageList) =>\ngetBufferString(messageList)\n);\nconst callbackManager_ = await CallbackManager.configure(\ncallbacks,\nthis.callbacks,\n{ verbose: this.verbose }\n);\nconst runManager = await callbackManager_?.handleLLMStart(\n{ name: this._llmType() },\nmessageStrings\n);\ntry {\nconst results = await Promise.all(\nmessages.map((messageList) =>\nthis._generate(messageList, stop, runManager)\n)\n);\nfor (const result of results) {\nif (result.llmOutput) {\nllmOutputs.push(result.llmOutput);\n}\ngenerations.push(result.generations);\n}\n} catch (err) {\nawait runManager?.handleLLMError(err);\nthrow err;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":327,"to":361}}}}],["380",{"pageContent":"const output: LLMResult = {\ngenerations,\nllmOutput: llmOutputs.length\n? this._combineLLMOutput?.(...llmOutputs)\n: undefined,\n};\nawait runManager?.handleLLMEnd(output);\nObject.defineProperty(output, RUN_KEY, {\nvalue: runManager ? { runId: runManager?.runId } : undefined,\nconfigurable: true,\n});\nreturn output;\n}\n\n_modelType(): string {\nreturn \"base_chat_model\" as const;\n}\n\nabstract _llmType(): string;\n\nasync generatePrompt(\npromptValues: BasePromptValue[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult> {\nconst promptMessages: BaseChatMessage[][] = promptValues.map(\n(promptValue) => promptValue.toChatMessages()\n);\nreturn this.generate(promptMessages, stop, callbacks);\n}\n\nabstract _generate(\nmessages: BaseChatMessage[],\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<ChatResult>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":490,"to":525}}}}],["381",{"pageContent":"async call(\nmessages: BaseChatMessage[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<BaseChatMessage> {\nconst result = await this.generate([messages], stop, callbacks);\nconst generations = result.generations as ChatGeneration[][];\nreturn generations[0][0].message;\n}\n\nasync callPrompt(\npromptValue: BasePromptValue,\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<BaseChatMessage> {\nconst promptMessages: BaseChatMessage[] = promptValue.toChatMessages();\nreturn this.call(promptMessages, stop, callbacks);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":654,"to":672}}}}],["382",{"pageContent":"abstract class SimpleChatModel extends BaseChatModel {\nabstract _call(\nmessages: BaseChatMessage[],\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<string>;\n\nasync _generate(\nmessages: BaseChatMessage[],\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<ChatResult> {\nconst text = await this._call(messages, stop, runManager);\nconst message = new AIChatMessage(text);\nreturn {\ngenerations: [\n{\ntext: message.text,\nmessage,\n},\n],\n};\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/base.ts","loc":{"lines":{"from":813,"to":836}}}}],["383",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/chat_models' is deprecated. Import from eg. 'langchain/chat_models/openai' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport { BaseChatModel, BaseChatModelParams, SimpleChatModel } from \"./base.js\";\nexport { ChatOpenAI } from \"./openai.js\";\nexport { ChatAnthropic } from \"./anthropic.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/index.ts","loc":{"lines":{"from":1,"to":7}}}}],["384",{"pageContent":"import {\nConfiguration,\nOpenAIApi,\nCreateChatCompletionRequest,\nConfigurationParameters,\nCreateChatCompletionResponse,\nChatCompletionResponseMessageRoleEnum,\nChatCompletionRequestMessage,\n} from \"openai\";\nimport type { AxiosRequestConfig } from \"axios\";\nimport type { StreamingAxiosConfiguration } from \"../util/axios-types.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport {\nBaseChatModel,\nBaseChatModelCallOptions,\nBaseChatModelParams,\n} from \"./base.js\";\nimport {\nAIChatMessage,\nBaseChatMessage,\nChatGeneration,\nChatMessage,\nChatResult,\nHumanChatMessage,\nMessageType,\nSystemChatMessage,\n} from \"../schema/index.js\";\nimport { getModelNameForTiktoken } from \"../base_language/count_tokens.js\";\nimport { CallbackManagerForLLMRun } from \"../callbacks/manager.js\";\n\ninterface TokenUsage {\ncompletionTokens?: number;\npromptTokens?: number;\ntotalTokens?: number;\n}\n\ninterface OpenAILLMOutput {\ntokenUsage: TokenUsage;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":1,"to":39}}}}],["385",{"pageContent":"messageTypeToOpenAIRole(\ntype: MessageType\n): ChatCompletionResponseMessageRoleEnum {\nswitch (type) {\ncase \"system\":\nreturn \"system\";\ncase \"ai\":\nreturn \"assistant\";\ncase \"human\":\nreturn \"user\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":489,"to":498}}}}],["386",{"pageContent":":\nthrow new Error(`Unknown message type: ${type}`);\n}\n}\n\nfunction openAIResponseToChatMessage(\nrole: ChatCompletionResponseMessageRoleEnum | undefined,\ntext: string\n): BaseChatMessage {\nswitch (role) {\ncase \"user\":\nreturn new HumanChatMessage(text);\ncase \"assistant\":\nreturn new AIChatMessage(text);\ncase \"system\":\nreturn new SystemChatMessage(text);\ndefault:\nreturn new ChatMessage(text, role ?? \"unknown\");\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":977,"to":996}}}}],["387",{"pageContent":"interface OpenAIInput {\n/** Sampling temperature to use, between 0 and 2, defaults to 1 */\ntemperature: number;\n\n/** Total probability mass of tokens to consider at each step, between 0 and 1, defaults to 1 */\ntopP: number;\n\n/** Penalizes repeated tokens according to frequency */\nfrequencyPenalty: number;\n\n/** Penalizes repeated tokens */\npresencePenalty: number;\n\n/** Number of chat completions to generate for each prompt */\nn: number;\n\n/** Dictionary used to adjust the probability of specific tokens being generated */\nlogitBias?: Record<string, number>;\n\n/** Whether to stream the results or not. Enabling disables tokenUsage reporting */\nstreaming: boolean;\n\n/**\n* Maximum number of tokens to generate in the completion. If not specified,\n* defaults to the maximum number of tokens allowed by the model.\n*/\nmaxTokens?: number;\n\n/** Model name to use */\nmodelName: string;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":1469,"to":1498}}}}],["388",{"pageContent":"/** Holds any additional parameters that are valid to pass to {@link\n* https://platform.openai.com/docs/api-reference/completions/create |\n* `openai.create`} that are not explicitly specified on this class.\n*/\nmodelKwargs?: Kwargs;\n\n/** List of stop words to use when generating */\nstop?: string[];\n\n/**\n* Timeout to use when making requests to OpenAI.\n*/\ntimeout?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":1952,"to":1965}}}}],["389",{"pageContent":"interface ChatOpenAICallOptions extends BaseChatModelCallOptions {\n/**\n* List of stop words to use when generating\n*/\nstop?: string[];\n\n/**\n* Additional options to pass to the underlying axios request.\n*/\noptions?: AxiosRequestConfig;\n}\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ntype Kwargs = Record<string, any>;\n\n/**\n* Wrapper around OpenAI large language models that use the Chat endpoint.\n*\n* To use you should have the `openai` package installed, with the\n* `OPENAI_API_KEY` environment variable set.\n*\n* @remarks\n* Any parameters that are valid to be passed to {@link\n* https://platform.openai.com/docs/api-reference/chat/create |\n* `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n* if not explicitly available on this class.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":2440,"to":2466}}}}],["390",{"pageContent":"class ChatOpenAI extends BaseChatModel implements OpenAIInput {\ndeclare CallOptions: ChatOpenAICallOptions;\n\ntemperature = 1;\n\ntopP = 1;\n\nfrequencyPenalty = 0;\n\npresencePenalty = 0;\n\nn = 1;\n\nlogitBias?: Record<string, number>;\n\nmodelName = \"gpt-3.5-turbo\";\n\nmodelKwargs?: Kwargs;\n\nstop?: string[];\n\ntimeout?: number;\n\nstreaming = false;\n\nmaxTokens?: number;\n\nprivate client: OpenAIApi;\n\nprivate clientConfig: ConfigurationParameters;\n\nconstructor(\nfields?: Partial<OpenAIInput> &\nBaseChatModelParams & {\nconcurrency?: number;\ncache?: boolean;\nopenAIApiKey?: string;\n},\nconfiguration?: ConfigurationParameters\n) {\nsuper(fields ?? {});\n\nconst apiKey =\nfields?.openAIApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.OPENAI_API_KEY\n: undefined);\nif (!apiKey) {\nthrow new Error(\"OpenAI API key not found\");\n}\n\nthis.modelName = fields?.modelName ?? this.modelName;\nthis.modelKwargs = fields?.modelKwargs ?? {};\nthis.timeout = fields?.timeout;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":2924,"to":2978}}}}],["391",{"pageContent":"this.temperature = fields?.temperature ?? this.temperature;\nthis.topP = fields?.topP ?? this.topP;\nthis.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\nthis.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\nthis.maxTokens = fields?.maxTokens;\nthis.n = fields?.n ?? this.n;\nthis.logitBias = fields?.logitBias;\nthis.stop = fields?.stop;\n\nthis.streaming = fields?.streaming ?? false;\n\nif (this.streaming && this.n > 1) {\nthrow new Error(\"Cannot stream results when n > 1\");\n}\n\nthis.clientConfig = {\napiKey,\n...configuration,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":3424,"to":3443}}}}],["392",{"pageContent":"/**\n* Get the parameters used to invoke the model\n*/\ninvocationParams(): Omit<CreateChatCompletionRequest, \"messages\"> & Kwargs {\nreturn {\nmodel: this.modelName,\ntemperature: this.temperature,\ntop_p: this.topP,\nfrequency_penalty: this.frequencyPenalty,\npresence_penalty: this.presencePenalty,\nmax_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\nn: this.n,\nlogit_bias: this.logitBias,\nstop: this.stop,\nstream: this.streaming,\n...this.modelKwargs,\n};\n}\n\n/** @ignore */\n_identifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n...this.clientConfig,\n};\n}\n\n/**\n* Get the identifying parameters for the model\n*/\nidentifyingParams() {\nreturn this._identifyingParams();\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":3908,"to":3941}}}}],["393",{"pageContent":"/** @ignore */\nasync _generate(\nmessages: BaseChatMessage[],\nstopOrOptions?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<ChatResult> {\nconst stop = Array.isArray(stopOrOptions)\n? stopOrOptions\n: stopOrOptions?.stop;\nconst options = Array.isArray(stopOrOptions)\n? {}\n: stopOrOptions?.options ?? {};\nconst tokenUsage: TokenUsage = {};\nif (this.stop && stop) {\nthrow new Error(\"Stop found in input and default params\");\n}\n\nconst params = this.invocationParams();\nparams.stop = stop ?? params.stop;\nconst messagesMapped: ChatCompletionRequestMessage[] = messages.map(\n(message) => ({\nrole: messageTypeToOpenAIRole(message._getType()),\ncontent: message.text,\nname: message.name,\n})\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":4400,"to":4425}}}}],["394",{"pageContent":"const data = params.stream\n? await new Promise<CreateChatCompletionResponse>((resolve, reject) => {\nlet response: CreateChatCompletionResponse;\nlet rejected = false;\nthis.completionWithRetry(\n{\n...params,\nmessages: messagesMapped,\n},\n{\n...options,\nresponseType: \"stream\",\nonmessage: (event) => {\nif (event.data?.trim?.() === \"[DONE]\") {\nresolve(response);\n} else {\nconst message = JSON.parse(event.data) as {\nid: string;\nobject: string;\ncreated: number;\nmodel: string;\nchoices: Array<{\nindex: number;\nfinish_reason: string | null;\ndelta: { content?: string; role?: string };\n}>;\n};\n\n// on the first message set the response properties\nif (!response) {\nresponse = {\nid: message.id,\nobject: message.object,\ncreated: message.created,\nmodel: message.model,\nchoices: [],\n};\n}\n\n// on all messages, update choice\nconst part = message.choices[0];\nif (part != null) {\nlet choice = response.choices.find(\n(c) => c.index === part.index\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":4884,"to":4928}}}}],["395",{"pageContent":"if (!choice) {\nchoice = {\nindex: part.index,\nfinish_reason: part.finish_reason ?? undefined,\n};\nresponse.choices.push(choice);\n}\n\nif (!choice.message) {\nchoice.message = {\nrole: part.delta\n?.role as ChatCompletionResponseMessageRoleEnum,\ncontent: part.delta?.content ?? \"\",\n};\n}\n\nchoice.message.content += part.delta?.content ?? \"\";\n// eslint-disable-next-line no-void\nvoid runManager?.handleLLMNewToken(\npart.delta?.content ?? \"\"\n);\n}\n}\n},\n}\n).catch((error) => {\nif (!rejected) {\nrejected = true;\nreject(error);\n}\n});\n})\n: await this.completionWithRetry(\n{\n...params,\nmessages: messagesMapped,\n},\noptions\n);\n\nconst {\ncompletion_tokens: completionTokens,\nprompt_tokens: promptTokens,\ntotal_tokens: totalTokens,\n} = data.usage ?? {};\n\nif (completionTokens) {\ntokenUsage.completionTokens =\n(tokenUsage.completionTokens ?? 0) + completionTokens;\n}\n\nif (promptTokens) {\ntokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":5378,"to":5431}}}}],["396",{"pageContent":"if (totalTokens) {\ntokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n}\n\nconst generations: ChatGeneration[] = [];\nfor (const part of data.choices) {\nconst role = part.message?.role ?? undefined;\nconst text = part.message?.content ?? \"\";\ngenerations.push({\ntext,\nmessage: openAIResponseToChatMessage(role, text),\n});\n}\nreturn {\ngenerations,\nllmOutput: { tokenUsage },\n};\n}\n\nasync getNumTokensFromMessages(messages: BaseChatMessage[]): Promise<{\ntotalCount: number;\ncountPerMessage: number[];\n}> {\nlet totalCount = 0;\nlet tokensPerMessage = 0;\nlet tokensPerName = 0;\n\n// From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\nif (getModelNameForTiktoken(this.modelName) === \"gpt-3.5-turbo\") {\ntokensPerMessage = 4;\ntokensPerName = -1;\n} else if (getModelNameForTiktoken(this.modelName).startsWith(\"gpt-4\")) {\ntokensPerMessage = 3;\ntokensPerName = 1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":5881,"to":5915}}}}],["397",{"pageContent":"const countPerMessage = await Promise.all(\nmessages.map(async (message) => {\nconst textCount = await this.getNumTokens(message.text);\nconst count =\ntextCount + tokensPerMessage + (message.name ? tokensPerName : 0);\n\ntotalCount += count;\nreturn count;\n})\n);\n\nreturn { totalCount, countPerMessage };\n}\n\n/** @ignore */\nasync completionWithRetry(\nrequest: CreateChatCompletionRequest,\noptions?: StreamingAxiosConfiguration\n) {\nif (!this.client) {\nconst clientConfig = new Configuration({\n...this.clientConfig,\nbaseOptions: {\ntimeout: this.timeout,\nadapter: fetchAdapter,\n...this.clientConfig.baseOptions,\n},\n});\nthis.client = new OpenAIApi(clientConfig);\n}\nreturn this.caller\n.call(\nthis.client.createChatCompletion.bind(this.client),\nrequest,\noptions\n)\n.then((res) => res.data);\n}\n\n_llmType() {\nreturn \"openai\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":6365,"to":6406}}}}],["398",{"pageContent":"/** @ignore */\n_combineLLMOutput(...llmOutputs: OpenAILLMOutput[]): OpenAILLMOutput {\nreturn llmOutputs.reduce<{\n[key in keyof OpenAILLMOutput]: Required<OpenAILLMOutput[key]>;\n}>(\n(acc, llmOutput) => {\nif (llmOutput && llmOutput.tokenUsage) {\nacc.tokenUsage.completionTokens +=\nllmOutput.tokenUsage.completionTokens ?? 0;\nacc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\nacc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n}\nreturn acc;\n},\n{\ntokenUsage: {\ncompletionTokens: 0,\npromptTokens: 0,\ntotalTokens: 0,\n},\n}\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/openai.ts","loc":{"lines":{"from":6864,"to":6887}}}}],["399",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { HumanChatMessage } from \"../../schema/index.js\";\nimport { ChatPromptValue } from \"../../prompts/chat.js\";\nimport {\nPromptTemplate,\nChatPromptTemplate,\nAIMessagePromptTemplate,\nHumanMessagePromptTemplate,\nSystemMessagePromptTemplate,\n} from \"../../prompts/index.js\";\nimport { ChatAnthropic } from \"../anthropic.js\";\nimport { CallbackManager } from \"../../callbacks/index.js\";\n\ntest(\"Test ChatAnthropic\", async () => {\nconst chat = new ChatAnthropic({ modelName: \"claude-instant-v1\" });\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.call([message]);\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatanthropic.int.test.ts","loc":{"lines":{"from":1,"to":19}}}}],["400",{"pageContent":"test(\"Test ChatAnthropic Generate\", async () => {\nconst chat = new ChatAnthropic({\nmodelName: \"claude-instant-v1\",\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.generate([[message], [message]]);\nexpect(res.generations.length).toBe(2);\nfor (const generation of res.generations) {\nexpect(generation.length).toBe(1);\nfor (const message of generation) {\nconsole.log(message.text);\n}\n}\nconsole.log({ res });\n});\n\ntest(\"Test ChatAnthropic tokenUsage with a batch\", async () => {\nconst model = new ChatAnthropic({\ntemperature: 0,\nmodelName: \"claude-instant-v1\",\n});\nconst res = await model.generate([\n[new HumanChatMessage(`Hello!`)],\n[new HumanChatMessage(`Hi!`)],\n]);\nconsole.log({ res });\n});\n\ntest(\"Test ChatAnthropic in streaming mode\", async () => {\nlet nrNewTokens = 0;\nlet streamedCompletion = \"\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatanthropic.int.test.ts","loc":{"lines":{"from":132,"to":162}}}}],["401",{"pageContent":"const model = new ChatAnthropic({\nmodelName: \"claude-instant-v1\",\nstreaming: true,\ncallbacks: CallbackManager.fromHandlers({\nasync handleLLMNewToken(token: string) {\nnrNewTokens += 1;\nstreamedCompletion += token;\n},\n}),\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await model.call([message]);\nconsole.log({ res });\n\nexpect(nrNewTokens > 0).toBe(true);\nexpect(res.text).toBe(streamedCompletion);\n});\n\ntest(\"Test ChatAnthropic prompt value\", async () => {\nconst chat = new ChatAnthropic({\nmodelName: \"claude-instant-v1\",\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.generatePrompt([new ChatPromptValue([message])]);\nexpect(res.generations.length).toBe(1);\nfor (const generation of res.generations) {\nfor (const g of generation) {\nconsole.log(g.text);\n}\n}\nconsole.log({ res });\n});\n\ntest(\"ChatAnthropic, docs, prompt templates\", async () => {\nconst chat = new ChatAnthropic({\nmodelName: \"claude-instant-v1\",\ntemperature: 0,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatanthropic.int.test.ts","loc":{"lines":{"from":268,"to":305}}}}],["402",{"pageContent":"const systemPrompt = PromptTemplate.fromTemplate(\n\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n);\n\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nnew SystemMessagePromptTemplate(systemPrompt),\nHumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n]);\n\nconst responseA = await chat.generatePrompt([\nawait chatPrompt.formatPromptValue({\ninput_language: \"English\",\noutput_language: \"French\",\ntext: \"I love programming.\",\n}),\n]);\n\nconsole.log(responseA.generations);\n});\n\ntest(\"ChatAnthropic, longer chain of messages\", async () => {\nconst chat = new ChatAnthropic({\nmodelName: \"claude-instant-v1\",\ntemperature: 0,\n});\n\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nHumanMessagePromptTemplate.fromTemplate(`Hi, my name is Joe!`),\nAIMessagePromptTemplate.fromTemplate(`Nice to meet you, Joe!`),\nHumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatanthropic.int.test.ts","loc":{"lines":{"from":408,"to":438}}}}],["403",{"pageContent":"const responseA = await chat.generatePrompt([\nawait chatPrompt.formatPromptValue({\ntext: \"What did I say my name was?\",\n}),\n]);\n\nconsole.log(responseA.generations);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatanthropic.int.test.ts","loc":{"lines":{"from":543,"to":550}}}}],["404",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { ChatOpenAI } from \"../openai.js\";\nimport {\nHumanChatMessage,\nLLMResult,\nSystemChatMessage,\n} from \"../../schema/index.js\";\nimport { ChatPromptValue } from \"../../prompts/chat.js\";\nimport {\nPromptTemplate,\nChatPromptTemplate,\nHumanMessagePromptTemplate,\nSystemMessagePromptTemplate,\n} from \"../../prompts/index.js\";\nimport { CallbackManager } from \"../../callbacks/index.js\";\n\ntest(\"Test ChatOpenAI\", async () => {\nconst chat = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\", maxTokens: 10 });\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.call([message]);\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":1,"to":22}}}}],["405",{"pageContent":"test(\"Test ChatOpenAI with SystemChatMessage\", async () => {\nconst chat = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\", maxTokens: 10 });\nconst system_message = new SystemChatMessage(\"You are to chat with a user.\");\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.call([system_message, message]);\nconsole.log({ res });\n});\n\ntest(\"Test ChatOpenAI Generate\", async () => {\nconst chat = new ChatOpenAI({\nmodelName: \"gpt-3.5-turbo\",\nmaxTokens: 10,\nn: 2,\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.generate([[message], [message]]);\nexpect(res.generations.length).toBe(2);\nfor (const generation of res.generations) {\nexpect(generation.length).toBe(2);\nfor (const message of generation) {\nconsole.log(message.text);\n}\n}\nconsole.log({ res });\n});\n\ntest(\"Test ChatOpenAI tokenUsage\", async () => {\nlet tokenUsage = {\ncompletionTokens: 0,\npromptTokens: 0,\ntotalTokens: 0,\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":226,"to":257}}}}],["406",{"pageContent":"const model = new ChatOpenAI({\nmodelName: \"gpt-3.5-turbo\",\nmaxTokens: 10,\ncallbackManager: CallbackManager.fromHandlers({\nasync handleLLMEnd(output: LLMResult) {\ntokenUsage = output.llmOutput?.tokenUsage;\n},\n}),\n});\nconst message = new HumanChatMessage(\"Hello\");\nconst res = await model.call([message]);\nconsole.log({ res });\n\nexpect(tokenUsage.promptTokens).toBeGreaterThan(0);\n});\n\ntest(\"Test ChatOpenAI tokenUsage with a batch\", async () => {\nlet tokenUsage = {\ncompletionTokens: 0,\npromptTokens: 0,\ntotalTokens: 0,\n};\n\nconst model = new ChatOpenAI({\ntemperature: 0,\nmodelName: \"gpt-3.5-turbo\",\ncallbackManager: CallbackManager.fromHandlers({\nasync handleLLMEnd(output: LLMResult) {\ntokenUsage = output.llmOutput?.tokenUsage;\n},\n}),\n});\nconst res = await model.generate([\n[new HumanChatMessage(\"Hello\")],\n[new HumanChatMessage(\"Hi\")],\n]);\nconsole.log(res);\n\nexpect(tokenUsage.promptTokens).toBeGreaterThan(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":455,"to":494}}}}],["407",{"pageContent":"test(\"Test ChatOpenAI in streaming mode\", async () => {\nlet nrNewTokens = 0;\nlet streamedCompletion = \"\";\n\nconst model = new ChatOpenAI({\nmodelName: \"gpt-3.5-turbo\",\nstreaming: true,\nmaxTokens: 10,\ncallbacks: [\n{\nasync handleLLMNewToken(token: string) {\nnrNewTokens += 1;\nstreamedCompletion += token;\n},\n},\n],\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await model.call([message]);\nconsole.log({ res });\n\nexpect(nrNewTokens > 0).toBe(true);\nexpect(res.text).toBe(streamedCompletion);\n});\n\ntest(\"Test ChatOpenAI prompt value\", async () => {\nconst chat = new ChatOpenAI({\nmodelName: \"gpt-3.5-turbo\",\nmaxTokens: 10,\nn: 2,\n});\nconst message = new HumanChatMessage(\"Hello!\");\nconst res = await chat.generatePrompt([new ChatPromptValue([message])]);\nexpect(res.generations.length).toBe(1);\nfor (const generation of res.generations) {\nexpect(generation.length).toBe(2);\nfor (const g of generation) {\nconsole.log(g.text);\n}\n}\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":692,"to":733}}}}],["408",{"pageContent":"test(\"OpenAI Chat, docs, prompt templates\", async () => {\nconst chat = new ChatOpenAI({ temperature: 0, maxTokens: 10 });\n\nconst systemPrompt = PromptTemplate.fromTemplate(\n\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n);\n\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nnew SystemMessagePromptTemplate(systemPrompt),\nHumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n]);\n\nconst responseA = await chat.generatePrompt([\nawait chatPrompt.formatPromptValue({\ninput_language: \"English\",\noutput_language: \"French\",\ntext: \"I love programming.\",\n}),\n]);\n\nconsole.log(responseA.generations);\n}, 50000);\n\ntest(\"Test OpenAI with stop\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5 });\nconst res = await model.call(\n[new HumanChatMessage(\"Print hello world\")],\n[\"world\"]\n);\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":930,"to":960}}}}],["409",{"pageContent":"test(\"Test OpenAI with stop in object\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5 });\nconst res = await model.call([new HumanChatMessage(\"Print hello world\")], {\nstop: [\"world\"],\n});\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with timeout in call options\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5 });\nawait expect(() =>\nmodel.call([new HumanChatMessage(\"Print hello world\")], {\noptions: { timeout: 10 },\n})\n).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with timeout in call options and node adapter\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5 });\nawait expect(() =>\nmodel.call([new HumanChatMessage(\"Print hello world\")], {\noptions: { timeout: 10, adapter: undefined },\n})\n).rejects.toThrow();\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":1159,"to":1183}}}}],["410",{"pageContent":"test(\"Test OpenAI with signal in call options\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5 });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call([new HumanChatMessage(\"Print hello world\")], {\noptions: { signal: controller.signal },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with signal in call options and node adapter\", async () => {\nconst model = new ChatOpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call([new HumanChatMessage(\"Print hello world\")], {\noptions: { signal: controller.signal, adapter: undefined },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/chat_models/tests/chatopenai.int.test.ts","loc":{"lines":{"from":1383,"to":1409}}}}],["411",{"pageContent":"import { Document } from \"../document.js\";\n\nexport abstract class Docstore {\nabstract search(search: string): Document | string;\n\nabstract add(texts: Record<string, Document>): void;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/docstore/base.ts","loc":{"lines":{"from":1,"to":7}}}}],["412",{"pageContent":"export { Document } from \"../document.js\";\nexport { Docstore } from \"./base.js\";\nexport { InMemoryDocstore } from \"./in_memory.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/docstore/index.ts","loc":{"lines":{"from":1,"to":3}}}}],["413",{"pageContent":"import { Document } from \"../document.js\";\nimport { Docstore } from \"./base.js\";\n\nexport class InMemoryDocstore extends Docstore {\n_docs: Map<string, Document>;\n\nconstructor(docs?: Map<string, Document>) {\nsuper();\nthis._docs = docs ?? new Map();\n}\n\n/** Method for getting count of documents in _docs */\nget count() {\nreturn this._docs.size;\n}\n\nsearch(search: string): Document | string {\nreturn this._docs.get(search) ?? `ID ${search} not found.`;\n}\n\nadd(texts: Record<string, Document>): void {\nconst keys = [...this._docs.keys()];\nconst overlapping = Object.keys(texts).filter((x) => keys.includes(x));\n\nif (overlapping.length > 0) {\nthrow new Error(`Tried to add ids that already exist: ${overlapping}`);\n}\n\nfor (const [key, value] of Object.entries(texts)) {\nthis._docs.set(key, value);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/docstore/in_memory.ts","loc":{"lines":{"from":1,"to":33}}}}],["414",{"pageContent":"export interface DocumentInput<\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nMetadata extends Record<string, any> = Record<string, any>\n> {\npageContent: string;\n\nmetadata?: Metadata;\n}\n\n/**\n* Interface for interacting with a document.\n*/\nexport class Document<\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nMetadata extends Record<string, any> = Record<string, any>\n> implements DocumentInput\n{\npageContent: string;\n\nmetadata: Metadata;\n\nconstructor(fields: DocumentInput<Metadata>) {\nthis.pageContent = fields.pageContent\n? fields.pageContent.toString()\n: this.pageContent;\nthis.metadata = fields.metadata ?? ({} as Metadata);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document.ts","loc":{"lines":{"from":1,"to":28}}}}],["415",{"pageContent":"import {\nRecursiveCharacterTextSplitter,\nTextSplitter,\n} from \"../text_splitter.js\";\nimport { Document } from \"../document.js\";\n\nexport interface DocumentLoader {\nload(): Promise<Document[]>;\nloadAndSplit(textSplitter?: TextSplitter): Promise<Document[]>;\n}\n\nexport abstract class BaseDocumentLoader implements DocumentLoader {\nabstract load(): Promise<Document[]>;\n\nasync loadAndSplit(\nsplitter: TextSplitter = new RecursiveCharacterTextSplitter()\n): Promise<Document[]> {\nconst docs = await this.load();\nreturn splitter.splitDocuments(docs);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/base.ts","loc":{"lines":{"from":1,"to":21}}}}],["416",{"pageContent":"import type { readFile as ReadFileT } from \"node:fs/promises\";\nimport { Document } from \"../../document.js\";\nimport { getEnv } from \"../../util/env.js\";\nimport { BaseDocumentLoader } from \"../base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/buffer.ts","loc":{"lines":{"from":1,"to":4}}}}],["417",{"pageContent":"abstract class BufferLoader extends BaseDocumentLoader {\nconstructor(public filePathOrBlob: string | Blob) {\nsuper();\n}\n\nprotected abstract parse(\nraw: Buffer,\nmetadata: Document[\"metadata\"]\n): Promise<Document[]>;\n\npublic async load(): Promise<Document[]> {\nlet buffer: Buffer;\nlet metadata: Record<string, string>;\nif (typeof this.filePathOrBlob === \"string\") {\nconst { readFile } = await BufferLoader.imports();\nbuffer = await readFile(this.filePathOrBlob);\nmetadata = { source: this.filePathOrBlob };\n} else {\nbuffer = await this.filePathOrBlob\n.arrayBuffer()\n.then((ab) => Buffer.from(ab));\nmetadata = { source: \"blob\", blobType: this.filePathOrBlob.type };\n}\nreturn this.parse(buffer, metadata);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/buffer.ts","loc":{"lines":{"from":45,"to":69}}}}],["418",{"pageContent":"static async imports(): Promise<{\nreadFile: typeof ReadFileT;\n}> {\ntry {\nconst { readFile } = await import(\"node:fs/promises\");\nreturn { readFile };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n`Failed to load fs/promises. TextLoader available only on environment 'node'. It appears you are running environment '${getEnv()}'. See https://<link to docs> for alternatives.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/buffer.ts","loc":{"lines":{"from":93,"to":106}}}}],["419",{"pageContent":"import { TextLoader } from \"./text.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/csv.ts","loc":{"lines":{"from":1,"to":1}}}}],["420",{"pageContent":"/**\n* Loads a CSV file into a list of documents.\n* Each document represents one row of the CSV file.\n*\n* When `column` is not specified, each row is converted into a key/value pair\n* with each key/value pair outputted to a new line in the document's pageContent.\n*\n* @example\n* // CSV file:\n* // id,html\n* // 1,<i>Corruption discovered at the core of the Banking Clan!</i>\n* // 2,<i>Corruption discovered at the core of the Banking Clan!</i>\n*\n* const loader = new CSVLoader(\"path/to/file.csv\");\n* const docs = await loader.load();\n*\n* // docs[0].pageContent:\n* // id: 1\n* // html: <i>Corruption discovered at the core of the Banking Clan!</i>\n*\n* When `column` is specified, one document is created for each row, and the\n* value of the specified column is used as the document's pageContent.\n*\n* @example\n* // CSV file:\n* // id,html\n* // 1,<i>Corruption discovered at the core of the Banking Clan!</i>\n* // 2,<i>Corruption discovered at the core of the Banking Clan!</i>\n*","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/csv.ts","loc":{"lines":{"from":76,"to":104}}}}],["421",{"pageContent":"* const loader = new CSVLoader(\"path/to/file.csv\", \"html\");\n* const docs = await loader.load();\n*\n* // docs[0].pageContent:\n* // <i>Corruption discovered at the core of the Banking Clan!</i>\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/csv.ts","loc":{"lines":{"from":150,"to":155}}}}],["422",{"pageContent":"class CSVLoader extends TextLoader {\nconstructor(filePathOrBlob: string | Blob, public column?: string) {\nsuper(filePathOrBlob);\n}\n\nprotected async parse(raw: string): Promise<string[]> {\nconst { csvParse } = await CSVLoaderImports();\nconst parsed = csvParse(raw.trim());\nconst { column } = this;\n\nif (column !== undefined) {\nif (!parsed.columns.includes(column)) {\nthrow new Error(`Column ${column} not found in CSV file.`);\n}\n\n// Note TextLoader will raise an exception if the value is null.\n// eslint-disable-next-line @typescript-eslint/no-non-null-assertion\nreturn parsed.map((row) => row[column]!);\n}\n\nreturn parsed.map((row) =>\nObject.keys(row)\n.map((key) => `${key.trim()}: ${row[key]?.trim()}`)\n.join(\"\\n\")\n);\n}\n}\n\nasync function CSVLoaderImports() {\ntry {\nconst { csvParse } = await import(\"d3-dsv\");\nreturn { csvParse };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Please install d3-dsv as a dependency with, e.g. `yarn add d3-dsv@2`\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/csv.ts","loc":{"lines":{"from":224,"to":262}}}}],["423",{"pageContent":"import type { extname as ExtnameT, resolve as ResolveT } from \"node:path\";\nimport type { readdir as ReaddirT } from \"node:fs/promises\";\nimport { Document } from \"../../document.js\";\nimport { getEnv } from \"../../util/env.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\n\n// TypeScript enums are not tree-shakeable, so doing this instead\n// See https://bargsten.org/jsts/enums/\nexport const UnknownHandling = {\nIgnore: \"ignore\",\nWarn: \"warn\",\nError: \"error\",\n} as const;\n// eslint-disable-next-line @typescript-eslint/no-redeclare\nexport type UnknownHandling =\n(typeof UnknownHandling)[keyof typeof UnknownHandling];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/directory.ts","loc":{"lines":{"from":1,"to":16}}}}],["424",{"pageContent":"class DirectoryLoader extends BaseDocumentLoader {\nconstructor(\npublic directoryPath: string,\npublic loaders: {\n[extension: string]: (filePath: string) => BaseDocumentLoader;\n},\npublic recursive: boolean = true,\npublic unknown: UnknownHandling = UnknownHandling.Warn\n) {\nsuper();\n\nif (Object.keys(loaders).length === 0) {\nthrow new Error(\"Must provide at least one loader\");\n}\nfor (const extension in loaders) {\nif (Object.hasOwn(loaders, extension)) {\nif (extension[0] !== \".\") {\nthrow new Error(`Extension must start with a dot: ${extension}`);\n}\n}\n}\n}\n\npublic async load(): Promise<Document[]> {\nconst { readdir, extname, resolve } = await DirectoryLoader.imports();\nconst files = await readdir(this.directoryPath, { withFileTypes: true });\n\nconst documents: Document[] = [];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/directory.ts","loc":{"lines":{"from":101,"to":128}}}}],["425",{"pageContent":"for (const file of files) {\nconst fullPath = resolve(this.directoryPath, file.name);\nif (file.isDirectory()) {\nif (this.recursive) {\nconst loader = new DirectoryLoader(\nfullPath,\nthis.loaders,\nthis.recursive,\nthis.unknown\n);\ndocuments.push(...(await loader.load()));\n}\n} else {\n// I'm aware some things won't be files,\n// but they will be caught by the \"unknown\" handling below.\nconst loaderFactory = this.loaders[extname(file.name)];\nif (loaderFactory) {\nconst loader = loaderFactory(fullPath);\ndocuments.push(...(await loader.load()));\n} else {\nswitch (this.unknown) {\ncase UnknownHandling.Ignore:\nbreak;\ncase UnknownHandling.Warn:\nconsole.warn(`Unknown file type: ${file.name}`);\nbreak;\ncase UnknownHandling.Error:\nthrow new Error(`Unknown file type: ${file.name}`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/directory.ts","loc":{"lines":{"from":207,"to":234}}}}],["426",{"pageContent":":\nthrow new Error(`Unknown unknown handling: ${this.unknown}`);\n}\n}\n}\n}\n\nreturn documents;\n}\n\nstatic async imports(): Promise<{\nreaddir: typeof ReaddirT;\nextname: typeof ExtnameT;\nresolve: typeof ResolveT;\n}> {\ntry {\nconst { extname, resolve } = await import(\"node:path\");\nconst { readdir } = await import(\"node:fs/promises\");\nreturn { readdir, extname, resolve };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n`Failed to load fs/promises. DirectoryLoader available only on environment 'node'. It appears you are running environment '${getEnv()}'. See https://<link to docs> for alternatives.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/directory.ts","loc":{"lines":{"from":313,"to":339}}}}],["427",{"pageContent":"import { Document } from \"../../document.js\";\nimport { BufferLoader } from \"./buffer.js\";\n\nexport class DocxLoader extends BufferLoader {\nconstructor(filePathOrBlob: string | Blob) {\nsuper(filePathOrBlob);\n}\n\npublic async parse(\nraw: Buffer,\nmetadata: Document[\"metadata\"]\n): Promise<Document[]> {\nconst { extractRawText } = await DocxLoaderImports();\nconst docx = await extractRawText({\nbuffer: raw,\n});\n\nif (!docx.value) return [];\n\nreturn [\nnew Document({\npageContent: docx.value,\nmetadata,\n}),\n];\n}\n}\n\nasync function DocxLoaderImports() {\ntry {\nconst { default: mod } = await import(\"mammoth\");\nconst { extractRawText } = mod;\nreturn { extractRawText };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Failed to load mammoth. Please install it with eg. `npm install mammoth`.\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/docx.ts","loc":{"lines":{"from":1,"to":40}}}}],["428",{"pageContent":"import type { EPub } from \"epub2\";\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/epub.ts","loc":{"lines":{"from":1,"to":3}}}}],["429",{"pageContent":"class EPubLoader extends BaseDocumentLoader {\nprivate splitChapters: boolean;\n\nconstructor(public filePath: string, { splitChapters = true } = {}) {\nsuper();\nthis.splitChapters = splitChapters;\n}\n\nprotected async parse(\nepub: EPub\n): Promise<{ pageContent: string; metadata?: object }[]> {\nconst { htmlToText } = await HtmlToTextImport();\nconst chapters = await Promise.all(\nepub.flow.map(async (chapter) => {\nif (!chapter.id) return null as never;\nconst html: string = await epub.getChapterRawAsync(chapter.id);\nif (!html) return null as never;\nreturn {\nhtml,\ntitle: chapter.title,\n};\n})\n);\nreturn chapters.filter(Boolean).map((chapter) => ({\npageContent: htmlToText(chapter.html),\nmetadata: {\n...(chapter.title && { chapter: chapter.title }),\n},\n}));\n}\n\npublic async load(): Promise<Document[]> {\nconst { EPub } = await EpubImport();\nconst epub = await EPub.createAsync(this.filePath);\n\nconst parsed = await this.parse(epub);\nconst metadata = { source: this.filePath };\n\nif (parsed.length === 0) return [];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/epub.ts","loc":{"lines":{"from":83,"to":121}}}}],["430",{"pageContent":"return this.splitChapters\n? parsed.map(\n(chapter) =>\nnew Document({\npageContent: chapter.pageContent,\nmetadata: {\n...metadata,\n...chapter.metadata,\n},\n})\n)\n: [\nnew Document({\npageContent: parsed\n.map((chapter) => chapter.pageContent)\n.join(\"\\n\\n\"),\nmetadata,\n}),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/epub.ts","loc":{"lines":{"from":173,"to":193}}}}],["431",{"pageContent":"EpubImport() {\nconst { EPub } = await import(\"epub2\").catch(() => {\nthrow new Error(\n\"Failed to load epub2. Please install it with eg. `npm install epub2`.\"\n);\n});\nreturn { EPub };\n}\n\nasync function HtmlToTextImport() {\nconst { htmlToText } = await import(\"html-to-text\").catch(() => {\nthrow new Error(\n\"Failed to load html-to-text. Please install it with eg. `npm install html-to-text`.\"\n);\n});\nreturn { htmlToText };\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/epub.ts","loc":{"lines":{"from":268,"to":284}}}}],["432",{"pageContent":"import jsonpointer from \"jsonpointer\";\nimport { TextLoader } from \"./text.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":1,"to":2}}}}],["433",{"pageContent":"class JSONLoader extends TextLoader {\npublic pointers: string[];\n\nconstructor(filePathOrBlob: string | Blob, pointers: string | string[] = []) {\nsuper(filePathOrBlob);\nthis.pointers = Array.isArray(pointers) ? pointers : [pointers];\n}\n\nprotected async parse(raw: string): Promise<string[]> {\nconst json = JSON.parse(raw.trim());\n// If there is no pointers specified we extract all strings we found\nconst extractAllStrings = !(this.pointers.length > 0);\nconst compiledPointers = this.pointers.map((pointer) =>\njsonpointer.compile(pointer)\n);\n\nreturn this.extractArrayStringsFromObject(\njson,\ncompiledPointers,\nextractAllStrings\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":133,"to":154}}}}],["434",{"pageContent":"/**\n* If JSON pointers are specified, return all strings below any of them\n* and exclude all other nodes expect if they match a JSON pointer (to allow to extract strings from different levels)\n*\n* If no JSON pointer is specified then return all string in the object\n*/\nprivate extractArrayStringsFromObject(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\njson: any,\npointers: jsonpointer[],\nextractAllStrings = false,\nkeyHasBeenFound = false\n): string[] {\nif (!json) {\nreturn [];\n}\n\nif (typeof json === \"string\" && extractAllStrings) {\nreturn [json];\n}\n\nif (Array.isArray(json) && extractAllStrings) {\nlet extractedString: string[] = [];\nfor (const element of json) {\nextractedString = extractedString.concat(\nthis.extractArrayStringsFromObject(element, pointers, true)\n);\n}\n\nreturn extractedString;\n}\n\nif (typeof json === \"object\") {\nif (extractAllStrings) {\nreturn this.extractArrayStringsFromObject(\nObject.values(json),\npointers,\ntrue\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":270,"to":309}}}}],["435",{"pageContent":"const targetedEntries = this.getTargetedEntries(json, pointers);\nconst thisLevelEntries = Object.values(json) as object[];\nconst notTargetedEntries = thisLevelEntries.filter(\n(entry: object) => !targetedEntries.includes(entry)\n);\n\nlet extractedStrings: string[] = [];\n// If we found a targeted entry, we extract all strings from it\nif (targetedEntries.length > 0) {\nfor (const oneEntry of targetedEntries) {\nextractedStrings = extractedStrings.concat(\nthis.extractArrayStringsFromObject(oneEntry, pointers, true, true)\n);\n}\n\nfor (const oneEntry of notTargetedEntries) {\nextractedStrings = extractedStrings.concat(\nthis.extractArrayStringsFromObject(oneEntry, pointers, false, true)\n);\n}\n} else if (extractAllStrings || !keyHasBeenFound) {\nfor (const oneEntry of notTargetedEntries) {\nextractedStrings = extractedStrings.concat(\nthis.extractArrayStringsFromObject(\noneEntry,\npointers,\nextractAllStrings\n)\n);\n}\n}\n\nreturn extractedStrings;\n}\n\nreturn [];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":413,"to":449}}}}],["436",{"pageContent":"private getTargetedEntries(json: object, pointers: jsonpointer[]): object[] {\nconst targetEntries = [];\nfor (const pointer of pointers) {\nconst targetedEntry = pointer.get(json);\nif (targetedEntry) {\ntargetEntries.push(targetedEntry);\n}\n}\n\nreturn targetEntries;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":553,"to":564}}}}],["437",{"pageContent":"class JSONLinesLoader extends TextLoader {\nconstructor(filePathOrBlob: string | Blob, public pointer: string) {\nsuper(filePathOrBlob);\n}\n\nprotected async parse(raw: string): Promise<string[]> {\nconst lines = raw.split(\"\\n\");\nconst jsons = lines\n.map((line) => line.trim())\n.filter(Boolean)\n.map((line) => JSON.parse(line));\nconst pointer = jsonpointer.compile(this.pointer);\nreturn jsons.map((json) => pointer.get(json));\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/json.ts","loc":{"lines":{"from":688,"to":702}}}}],["438",{"pageContent":"import { DirectoryLoader, UnknownHandling } from \"./directory.js\";\nimport { TextLoader } from \"./text.js\";\n\nexport class NotionLoader extends DirectoryLoader {\nconstructor(directoryPath: string) {\nsuper(\ndirectoryPath,\n{\n\".md\": (filePath) => new TextLoader(filePath),\n},\ntrue,\nUnknownHandling.Ignore\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/notion.ts","loc":{"lines":{"from":1,"to":15}}}}],["439",{"pageContent":"import type { TextItem } from \"pdf-parse/lib/pdf.js/v1.10.100/build/pdf.js\";\nimport { Document } from \"../../document.js\";\nimport { BufferLoader } from \"./buffer.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/pdf.ts","loc":{"lines":{"from":1,"to":3}}}}],["440",{"pageContent":"class PDFLoader extends BufferLoader {\nprivate splitPages: boolean;\n\nprivate pdfjs: typeof PDFLoaderImports;\n\nconstructor(\nfilePathOrBlob: string | Blob,\n{ splitPages = true, pdfjs = PDFLoaderImports } = {}\n) {\nsuper(filePathOrBlob);\nthis.splitPages = splitPages;\nthis.pdfjs = pdfjs;\n}\n\npublic async parse(\nraw: Buffer,\nmetadata: Document[\"metadata\"]\n): Promise<Document[]> {\nconst { getDocument, version } = await this.pdfjs();\nconst pdf = await getDocument({\ndata: new Uint8Array(raw.buffer),\nuseWorkerFetch: false,\nisEvalSupported: false,\nuseSystemFonts: true,\n}).promise;\nconst meta = await pdf.getMetadata().catch(() => null);\n\nconst documents: Document[] = [];\n\nfor (let i = 1; i <= pdf.numPages; i += 1) {\nconst page = await pdf.getPage(i);\nconst content = await page.getTextContent();\n\nif (content.items.length === 0) {\ncontinue;\n}\n\nconst text = content.items\n.map((item) => (item as TextItem).str)\n.join(\"\\n\");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/pdf.ts","loc":{"lines":{"from":103,"to":142}}}}],["441",{"pageContent":"documents.push(\nnew Document({\npageContent: text,\nmetadata: {\n...metadata,\npdf: {\nversion,\ninfo: meta?.info,\nmetadata: meta?.metadata,\ntotalPages: pdf.numPages,\n},\nloc: {\npageNumber: i,\n},\n},\n})\n);\n}\n\nif (this.splitPages) {\nreturn documents;\n}\n\nif (documents.length === 0) {\nreturn [];\n}\n\nreturn [\nnew Document({\npageContent: documents.map((doc) => doc.pageContent).join(\"\\n\\n\"),\nmetadata: {\n...metadata,\npdf: {\nversion,\ninfo: meta?.info,\nmetadata: meta?.metadata,\ntotalPages: pdf.numPages,\n},\n},\n}),\n];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/pdf.ts","loc":{"lines":{"from":213,"to":255}}}}],["442",{"pageContent":"PDFLoaderImports() {\ntry {\nconst { default: mod } = await import(\n\"pdf-parse/lib/pdf.js/v1.10.100/build/pdf.js\"\n);\nconst { getDocument, version } = mod;\nreturn { getDocument, version };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Failed to load pdf-parse. Please install it with eg. `npm install pdf-parse`.\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/pdf.ts","loc":{"lines":{"from":339,"to":352}}}}],["443",{"pageContent":"import type SRTParserT from \"srt-parser-2\";\nimport { TextLoader } from \"./text.js\";\n\nexport class SRTLoader extends TextLoader {\nconstructor(filePathOrBlob: string | Blob) {\nsuper(filePathOrBlob);\n}\n\nprotected async parse(raw: string): Promise<string[]> {\nconst { SRTParser2 } = await SRTLoaderImports();\nconst parser = new SRTParser2();\nconst srts = parser.fromSrt(raw);\nreturn [\nsrts\n.map((srt) => srt.text)\n.filter(Boolean)\n.join(\" \"),\n];\n}\n}\n\nasync function SRTLoaderImports(): Promise<{\nSRTParser2: typeof SRTParserT.default;\n}> {\ntry {\nconst SRTParser2 = (await import(\"srt-parser-2\")).default.default;\nreturn { SRTParser2 };\n} catch (e) {\nthrow new Error(\n\"Please install srt-parser-2 as a dependency with, e.g. `yarn add srt-parser-2`\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/srt.ts","loc":{"lines":{"from":1,"to":33}}}}],["444",{"pageContent":"import type { readFile as ReadFileT } from \"node:fs/promises\";\nimport { Document } from \"../../document.js\";\nimport { getEnv } from \"../../util/env.js\";\nimport { BaseDocumentLoader } from \"../base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/text.ts","loc":{"lines":{"from":1,"to":4}}}}],["445",{"pageContent":"class TextLoader extends BaseDocumentLoader {\nconstructor(public filePathOrBlob: string | Blob) {\nsuper();\n}\n\nprotected async parse(raw: string): Promise<string[]> {\nreturn [raw];\n}\n\npublic async load(): Promise<Document[]> {\nlet text: string;\nlet metadata: Record<string, string>;\nif (typeof this.filePathOrBlob === \"string\") {\nconst { readFile } = await TextLoader.imports();\ntext = await readFile(this.filePathOrBlob, \"utf8\");\nmetadata = { source: this.filePathOrBlob };\n} else {\ntext = await this.filePathOrBlob.text();\nmetadata = { source: \"blob\", blobType: this.filePathOrBlob.type };\n}\nconst parsed = await this.parse(text);\nparsed.forEach((pageContent, i) => {\nif (typeof pageContent !== \"string\") {\nthrow new Error(\n`Expected string, at position ${i} got ${typeof pageContent}`\n);\n}\n});\nreturn parsed.map(\n(pageContent, i) =>\nnew Document({\npageContent,\nmetadata:\nparsed.length === 1\n? metadata\n: {\n...metadata,\nline: i + 1,\n},\n})\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/text.ts","loc":{"lines":{"from":62,"to":103}}}}],["446",{"pageContent":"static async imports(): Promise<{\nreadFile: typeof ReadFileT;\n}> {\ntry {\nconst { readFile } = await import(\"node:fs/promises\");\nreturn { readFile };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n`Failed to load fs/promises. TextLoader available only on environment 'node'. It appears you are running environment '${getEnv()}'. See https://<link to docs> for alternatives.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/text.ts","loc":{"lines":{"from":138,"to":151}}}}],["447",{"pageContent":"import type { basename as BasenameT } from \"node:path\";\nimport type { readFile as ReaFileT } from \"node:fs/promises\";\nimport { DirectoryLoader, UnknownHandling } from \"./directory.js\";\nimport { getEnv } from \"../../util/env.js\";\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\n\ninterface Element {\ntype: string;\ntext: string;\n// this is purposefully loosely typed\nmetadata: {\n[key: string]: unknown;\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":1,"to":15}}}}],["448",{"pageContent":"class UnstructuredLoader extends BaseDocumentLoader {\nconstructor(public webPath: string, public filePath: string) {\nsuper();\nthis.filePath = filePath;\n\nthis.webPath = webPath;\n}\n\nasync _partition() {\nconst { readFile, basename } = await this.imports();\n\nconst buffer = await readFile(this.filePath);\nconst fileName = basename(this.filePath);\n\n// I'm aware this reads the file into memory first, but we have lots of work\n// to do on then consuming Documents in a streaming fashion anyway, so not\n// worried about this for now.\nconst formData = new FormData();\nformData.append(\"files\", new Blob([buffer]), fileName);\n\nconst response = await fetch(this.webPath, {\nmethod: \"POST\",\nbody: formData,\n});\n\nif (!response.ok) {\nthrow new Error(\n`Failed to partition file ${this.filePath} with error ${\nresponse.status\n} and message ${await response.text()}`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":124,"to":155}}}}],["449",{"pageContent":"const elements = await response.json();\nif (!Array.isArray(elements)) {\nthrow new Error(\n`Expected partitioning request to return an array, but got ${elements}`\n);\n}\nreturn elements.filter((el) => typeof el.text === \"string\") as Element[];\n}\n\nasync load(): Promise<Document[]> {\nconst elements = await this._partition();\n\nconst documents: Document[] = [];\nfor (const element of elements) {\nconst { metadata, text } = element;\ndocuments.push(\nnew Document({\npageContent: text,\nmetadata: {\n...metadata,\ncategory: element.type,\n},\n})\n);\n}\n\nreturn documents;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":246,"to":273}}}}],["450",{"pageContent":"async imports(): Promise<{\nreadFile: typeof ReaFileT;\nbasename: typeof BasenameT;\n}> {\ntry {\nconst { readFile } = await import(\"node:fs/promises\");\nconst { basename } = await import(\"node:path\");\nreturn { readFile, basename };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n`Failed to load fs/promises. TextLoader available only on environment 'node'. It appears you are running environment '${getEnv()}'. See https://<link to docs> for alternatives.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":377,"to":392}}}}],["451",{"pageContent":"class UnstructuredDirectoryLoader extends DirectoryLoader {\nconstructor(\npublic webPath: string,\npublic directoryPath: string,\npublic recursive: boolean = true,\npublic unknown: UnknownHandling = UnknownHandling.Warn\n) {\nconst loaders = {\n\".txt\": (p: string) => new UnstructuredLoader(webPath, p),\n\".text\": (p: string) => new UnstructuredLoader(webPath, p),\n\".pdf\": (p: string) => new UnstructuredLoader(webPath, p),\n\".docx\": (p: string) => new UnstructuredLoader(webPath, p),\n\".doc\": (p: string) => new UnstructuredLoader(webPath, p),\n\".jpg\": (p: string) => new UnstructuredLoader(webPath, p),\n\".jpeg\": (p: string) => new UnstructuredLoader(webPath, p),\n\".eml\": (p: string) => new UnstructuredLoader(webPath, p),\n\".html\": (p: string) => new UnstructuredLoader(webPath, p),\n\".md\": (p: string) => new UnstructuredLoader(webPath, p),\n\".pptx\": (p: string) => new UnstructuredLoader(webPath, p),\n\".ppt\": (p: string) => new UnstructuredLoader(webPath, p),\n\".msg\": (p: string) => new UnstructuredLoader(webPath, p),\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":500,"to":521}}}}],["452",{"pageContent":"super(directoryPath, loaders, recursive, unknown);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":610,"to":612}}}}],["453",{"pageContent":"{ UnknownHandling };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/fs/unstructured.ts","loc":{"lines":{"from":733,"to":733}}}}],["454",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/document_loaders' is deprecated. Import from eg. 'langchain/document_loaders/fs/text' or 'langchain/document_loaders/web/cheerio' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport type { DocumentLoader } from \"./base.js\";\nexport { BaseDocumentLoader } from \"./base.js\";\nexport { CheerioWebBaseLoader } from \"./web/cheerio.js\";\nexport { PuppeteerWebBaseLoader, PuppeteerEvaluate } from \"./web/puppeteer.js\";\nexport { CollegeConfidentialLoader } from \"./web/college_confidential.js\";\nexport { GitbookLoader } from \"./web/gitbook.js\";\nexport { HNLoader } from \"./web/hn.js\";\nexport { IMSDBLoader } from \"./web/imsdb.js\";\nexport { DirectoryLoader, UnknownHandling } from \"./fs/directory.js\";\nexport { SRTLoader } from \"./fs/srt.js\";\nexport { PDFLoader } from \"./fs/pdf.js\";\nexport { DocxLoader } from \"./fs/docx.js\";\nexport { EPubLoader } from \"./fs/epub.js\";\nexport { TextLoader } from \"./fs/text.js\";\nexport { JSONLoader, JSONLinesLoader } from \"./fs/json.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/index.ts","loc":{"lines":{"from":1,"to":19}}}}],["455",{"pageContent":"{ CSVLoader } from \"./fs/csv.js\";\nexport { NotionLoader } from \"./fs/notion.js\";\nexport { GithubRepoLoader, GithubRepoLoaderParams } from \"./web/github.js\";\nexport { UnstructuredLoader } from \"./fs/unstructured.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/index.ts","loc":{"lines":{"from":20,"to":23}}}}],["456",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { CheerioWebBaseLoader } from \"../web/cheerio.js\";\n\ntest(\"Test cheerio web scraper loader\", async () => {\nconst loader = new CheerioWebBaseLoader(\n\"https://news.ycombinator.com/item?id=34817881\"\n);\nawait loader.load();\n});\n\ntest(\"Test cheerio web scraper loader with selector\", async () => {\nconst selectH1 = \"h1\";\nconst loader = new CheerioWebBaseLoader(\"https://about.google/commitments/\", {\nselector: selectH1,\n});\n\nconst doc = await loader.load();\nexpect(doc[0].pageContent.trim()).toBe(\n\"Committed to significantly improving the lives of as many people as possible.\"\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/cheerio.int.test.ts","loc":{"lines":{"from":1,"to":21}}}}],["457",{"pageContent":"import { test } from \"@jest/globals\";\nimport { CollegeConfidentialLoader } from \"../web/college_confidential.js\";\n\ntest(\"Test College confidential loader\", async () => {\nconst loader = new CollegeConfidentialLoader(\n\"https://www.collegeconfidential.com/colleges/brown-university/\"\n);\nawait loader.load();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/college_confidential.int.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["458",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport * as fs from \"node:fs/promises\";\nimport { test, expect } from \"@jest/globals\";\nimport { CSVLoader } from \"../fs/csv.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test CSV loader from blob\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.csv\"\n);\nconst loader = new CSVLoader(\nnew Blob([await fs.readFile(filePath)], { type: \"text/csv\" }),\n\"html\"\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: \"blob\", blobType: \"text/csv\", line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\nexpect(docs[1]).toEqual(\nnew Document({\nmetadata: { source: \"blob\", blobType: \"text/csv\", line: 2 },\npageContent: \"<i>Reunited, Rush Clovis and Senator Amidala</i>\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/csv-blob.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["459",{"pageContent":"test(\"Test CSV loader from blob\", async () => {\nconst loader = new CSVLoader(\nnew Blob(\n[\n`id,text\n1,This is a sentence.\n2,This is another sentence.`,\n],\n{ type: \"text/csv\" }\n)\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(2);\nexpect(docs[0]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"text/csv\",\n\"line\": 1,\n\"source\": \"blob\",\n},\n\"pageContent\": \"id: 1\ntext: This is a sentence.\",\n}\n`);\nexpect(docs[1]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"text/csv\",\n\"line\": 2,\n\"source\": \"blob\",\n},\n\"pageContent\": \"id: 2\ntext: This is another sentence.\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/csv-blob.test.ts","loc":{"lines":{"from":72,"to":107}}}}],["460",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport { CSVLoader } from \"../fs/csv.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test CSV loader from file with column arg\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.csv\"\n);\nconst loader = new CSVLoader(filePath, \"html\");\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\nexpect(docs[1]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 2 },\npageContent: \"<i>Reunited, Rush Clovis and Senator Amidala</i>\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/csv.test.ts","loc":{"lines":{"from":1,"to":28}}}}],["461",{"pageContent":"test(\"Test CSV loader without column arg\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.csv\"\n);\nconst loader = new CSVLoader(filePath);\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 1 },\npageContent: `id: 1\nhtml: <i>Corruption discovered at the core of the Banking Clan!</i>`,\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/csv.test.ts","loc":{"lines":{"from":48,"to":63}}}}],["462",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport { DirectoryLoader, UnknownHandling } from \"../fs/directory.js\";\nimport { CSVLoader } from \"../fs/csv.js\";\nimport { PDFLoader } from \"../fs/pdf.js\";\nimport { TextLoader } from \"../fs/text.js\";\nimport { JSONLoader } from \"../fs/json.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/directory.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["463",{"pageContent":"test(\"Test Directory loader\", async () => {\nconst directoryPath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data\"\n);\nconst loader = new DirectoryLoader(\ndirectoryPath,\n{\n\".csv\": (p) => new CSVLoader(p, \"html\"),\n\".pdf\": (p) => new PDFLoader(p),\n\".txt\": (p) => new TextLoader(p),\n\".json\": (p) => new JSONLoader(p),\n},\ntrue,\nUnknownHandling.Ignore\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(90);\nexpect(docs.map((d) => d.metadata.source).sort()).toEqual([\n// PDF\n...Array.from({ length: 15 }, (_) =>\npath.resolve(directoryPath, \"1706.03762.pdf\")\n),\n// CSV\n...Array.from({ length: 32 }, (_) =>\npath.resolve(\ndirectoryPath,\n\"Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.csv\"\n)\n),\n// JSON\n...Array.from({ length: 32 }, (_) =>\npath.resolve(\ndirectoryPath,\n\"Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.json\"\n)\n),\n...Array.from({ length: 10 }, (_) =>\npath.resolve(directoryPath, \"complex.json\")\n),\n// TXT\npath.resolve(directoryPath, \"example.txt\"),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/directory.test.ts","loc":{"lines":{"from":53,"to":96}}}}],["464",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { DocxLoader } from \"../fs/docx.js\";\n\ntest(\"Test Word doc loader from file\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/attention.docx\"\n);\nconst loader = new DocxLoader(filePath);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1); // not much text in the example\nexpect(docs[0].pageContent).toContain(\"an interesting activity\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/docx.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["465",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { EPubLoader } from \"../fs/epub.js\";\n\ntest(\"Test EPub loader from file\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/attention.epub\"\n);\nconst loader = new EPubLoader(filePath);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(3);\nexpect(docs[0].pageContent).toContain(\"Attention Is All You Need\");\n});\n\ntest(\"Test EPub loader from file to single document\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/attention.epub\"\n);\nconst loader = new EPubLoader(filePath, { splitChapters: false });\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1);\nexpect(docs[0].pageContent).toContain(\"Attention Is All You Need\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/epub.test.ts","loc":{"lines":{"from":1,"to":28}}}}],["466",{"pageContent":"import { test } from \"@jest/globals\";\nimport { GithubRepoLoader } from \"../web/github.js\";\n\ntest(\"Test GithubRepoLoader\", async () => {\nconst loader = new GithubRepoLoader(\n\"https://github.com/hwchase17/langchainjs\",\n{ branch: \"main\", recursive: false, unknown: \"warn\" }\n);\nconst documents = await loader.load();\nconsole.log(documents[0].pageContent);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/github.int.test.ts","loc":{"lines":{"from":1,"to":11}}}}],["467",{"pageContent":"import { test } from \"@jest/globals\";\nimport { HNLoader } from \"../web/hn.js\";\n\ntest(\"Test Hacker News loader\", async () => {\nconst loader = new HNLoader(\"https://news.ycombinator.com/item?id=34817881\");\nawait loader.load();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/hn.int.test.ts","loc":{"lines":{"from":1,"to":7}}}}],["468",{"pageContent":"import { test } from \"@jest/globals\";\nimport { IMSDBLoader } from \"../web/imsdb.js\";\n\ntest(\"Test IMSDB loader\", async () => {\nconst loader = new IMSDBLoader(\n\"https://imsdb.com/scripts/BlacKkKlansman.html\"\n);\nawait loader.load();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/imsdb.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["469",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport * as fs from \"node:fs/promises\";\nimport { test, expect } from \"@jest/globals\";\nimport { JSONLoader } from \"../fs/json.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test JSON loader from blob\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.json\"\n);\nconst loader = new JSONLoader(\nnew Blob([await fs.readFile(filePath)], { type: \"application/json\" })\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: \"blob\", blobType: \"application/json\", line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json-blob.test.ts","loc":{"lines":{"from":1,"to":25}}}}],["470",{"pageContent":"test(\"Test JSON loader from blob\", async () => {\nconst loader = new JSONLoader(\nnew Blob(\n[\n`{\n\"texts\": [\"This is a sentence.\", \"This is another sentence.\"]\n}`,\n],\n{ type: \"application/json\" }\n)\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(2);\nexpect(docs[0]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/json\",\n\"line\": 1,\n\"source\": \"blob\",\n},\n\"pageContent\": \"This is a sentence.\",\n}\n`);\nexpect(docs[1]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/json\",\n\"line\": 2,\n\"source\": \"blob\",\n},\n\"pageContent\": \"This is another sentence.\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json-blob.test.ts","loc":{"lines":{"from":111,"to":144}}}}],["471",{"pageContent":"test(\"Test JSON loader from blob\", async () => {\nconst loader = new JSONLoader(\nnew Blob(\n[\n`{\n\"1\": {\n\"body\": \"BD 2023 SUMMER\",\n\"from\": \"LinkedIn Job\",\n\"labels\": [\"IMPORTANT\", \"CATEGORY_UPDATES\", \"INBOX\"]\n},\n\"2\": {\n\"body\": \"Intern, Treasury and other roles are available\",\n\"from\": \"LinkedIn Job2\",\n\"labels\": [\"IMPORTANT\"],\n\"other\": {\n\"name\": \"plop\",\n\"surname\": \"bob\"\n}\n}\n}`,\n],\n{ type: \"application/json\" }\n)\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(10);\nexpect(docs[0]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/json\",\n\"line\": 1,\n\"source\": \"blob\",\n},\n\"pageContent\": \"BD 2023 SUMMER\",\n}\n`);\nexpect(docs[1]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/json\",\n\"line\": 2,\n\"source\": \"blob\",\n},\n\"pageContent\": \"LinkedIn Job\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json-blob.test.ts","loc":{"lines":{"from":235,"to":281}}}}],["472",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport { Document } from \"../../document.js\";\nimport { JSONLoader } from \"../fs/json.js\";\n\ntest(\"Test JSON loader\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.json\"\n);\nconst loader = new JSONLoader(filePath);\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\n});\n\ntest(\"Test JSON  loader for complex json without keys\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/complex.json\"\n);\nconst loader = new JSONLoader(filePath);\nconst docs = await loader.load();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json.test.ts","loc":{"lines":{"from":1,"to":30}}}}],["473",{"pageContent":"expect(docs.length).toBe(10);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 1 },\npageContent: \"BD 2023 SUMMER\",\n})\n);\nexpect(docs[1]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 2 },\npageContent: \"LinkedIn Job\",\n})\n);\nexpect(docs[2]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 3 },\npageContent: \"IMPORTANT\",\n})\n);\n});\n\ntest(\"Test JSON loader for complex json with one key that points nothing\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/complex.json\"\n);\nconst loader = new JSONLoader(filePath, [\"/plop\"]);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(0);\n});\n\ntest(\"Test JSON loader for complex json with one key that exists\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/complex.json\"\n);\nconst loader = new JSONLoader(filePath, [\"/from\"]);\nconst docs = await loader.load();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json.test.ts","loc":{"lines":{"from":114,"to":152}}}}],["474",{"pageContent":"expect(docs.length).toBe(2);\nexpect(docs[1]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 2 },\npageContent: \"LinkedIn Job2\",\n})\n);\n});\n\ntest(\"Test JSON loader for complex json with two keys that exists\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/complex.json\"\n);\nconst loader = new JSONLoader(filePath, [\"/from\", \"/labels\"]);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(6);\nexpect(docs[3]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 4 },\npageContent: \"INBOX\",\n})\n);\n});\n\ntest(\"Test JSON loader for complex json with two existing keys on different level\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/complex.json\"\n);\nconst loader = new JSONLoader(filePath, [\"/from\", \"/surname\"]);\nconst docs = await loader.load();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json.test.ts","loc":{"lines":{"from":235,"to":267}}}}],["475",{"pageContent":"expect(docs.length).toBe(3);\nexpect(docs[2]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 3 },\npageContent: \"bob\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/json.test.ts","loc":{"lines":{"from":353,"to":360}}}}],["476",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport * as fs from \"node:fs/promises\";\nimport { test, expect } from \"@jest/globals\";\nimport { JSONLinesLoader } from \"../fs/json.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test JSONL loader from blob\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.jsonl\"\n);\nconst loader = new JSONLinesLoader(\nnew Blob([await fs.readFile(filePath)], { type: \"application/jsonl+json\" }),\n\"/html\"\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: \"blob\", blobType: \"application/jsonl+json\", line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/jsonl-blob.test.ts","loc":{"lines":{"from":1,"to":26}}}}],["477",{"pageContent":"test(\"Test JSONL loader from blob\", async () => {\nconst loader = new JSONLinesLoader(\nnew Blob(\n[\n`{\"html\": \"This is a sentence.\"}\n{\"html\": \"This is another sentence.\"}`,\n],\n{ type: \"application/jsonl+json\" }\n),\n\"/html\"\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(2);\nexpect(docs[0]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/jsonl+json\",\n\"line\": 1,\n\"source\": \"blob\",\n},\n\"pageContent\": \"This is a sentence.\",\n}\n`);\nexpect(docs[1]).toMatchInlineSnapshot(`\nDocument {\n\"metadata\": {\n\"blobType\": \"application/jsonl+json\",\n\"line\": 2,\n\"source\": \"blob\",\n},\n\"pageContent\": \"This is another sentence.\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/jsonl-blob.test.ts","loc":{"lines":{"from":64,"to":97}}}}],["478",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport { JSONLinesLoader } from \"../fs/json.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test JSON loader from file\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.jsonl\"\n);\nconst loader = new JSONLinesLoader(filePath, \"/html\");\nconst docs = await loader.load();\nexpect(docs.length).toBe(32);\nexpect(docs[0]).toEqual(\nnew Document({\nmetadata: { source: filePath, line: 1 },\npageContent:\n\"<i>Corruption discovered at the core of the Banking Clan!</i>\",\n})\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/jsonl.test.ts","loc":{"lines":{"from":1,"to":22}}}}],["479",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { NotionLoader } from \"../fs/notion.js\";\n\ntest(\"Test Notion Loader\", async () => {\nconst directoryPath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data\"\n);\nconst loader = new NotionLoader(directoryPath);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1);\nexpect(docs[0].pageContent).toContain(\"Testing the notion markdownloader\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/notion.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["480",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport * as fs from \"node:fs/promises\";\nimport { PDFLoader } from \"../fs/pdf.js\";\n\ntest(\"Test PDF loader from blob\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/1706.03762.pdf\"\n);\nconst loader = new PDFLoader(\nnew Blob([await fs.readFile(filePath)], {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/pdf-blob.test.ts","loc":{"lines":{"from":1,"to":13}}}}],["481",{"pageContent":": \"application/pdf\",\n})\n);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(15);\nexpect(docs[0].pageContent).toContain(\"Attention Is All You Need\");\nexpect(docs[0].metadata).toMatchInlineSnapshot(`\n{\n\"blobType\": \"application/pdf\",\n\"loc\": {\n\"pageNumber\": 1,\n},\n\"pdf\": {\n\"info\": {\n\"Author\": \"\",\n\"CreationDate\": \"D:20171207010315Z\",\n\"Creator\": \"LaTeX with hyperref package\",\n\"IsAcroFormPresent\": false,\n\"IsXFAPresent\": false,\n\"Keywords\": \"\",\n\"ModDate\": \"D:20171207010315Z\",\n\"PDFFormatVersion\": \"1.5\",\n\"Producer\": \"pdfTeX-1.40.17\",\n\"Subject\": \"\",\n\"Title\": \"\",\n\"Trapped\": {\n\"name\": \"False\",\n},\n},\n\"metadata\": null,\n\"totalPages\": 15,\n\"version\": \"1.10.100\",\n},\n\"source\": \"blob\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/pdf-blob.test.ts","loc":{"lines":{"from":51,"to":88}}}}],["482",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { PDFLoader } from \"../fs/pdf.js\";\n\ntest(\"Test PDF loader from file\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/1706.03762.pdf\"\n);\nconst loader = new PDFLoader(filePath);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(15);\nexpect(docs[0].pageContent).toContain(\"Attention Is All You Need\");\n});\n\ntest(\"Test PDF loader from file to single document\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/1706.03762.pdf\"\n);\nconst loader = new PDFLoader(filePath, { splitPages: false });\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1);\nexpect(docs[0].pageContent).toContain(\"Attention Is All You Need\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/pdf.test.ts","loc":{"lines":{"from":1,"to":28}}}}],["483",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { PlaywrightWebBaseLoader } from \"../web/playwright.js\";\n\ntest(\"Test playwright web scraper loader\", async () => {\nconst loader = new PlaywrightWebBaseLoader(\"https://www.google.com/\");\nconst result = await loader.load();\n\nexpect(result).toBeDefined();\nexpect(result.length).toBe(1);\n}, 20_000);\n\ntest(\"Test playwright web scraper loader with evaluate options\", async () => {\nlet nrTimesCalled = 0;\nconst loader = new PlaywrightWebBaseLoader(\"https://www.google.com/\", {\nlaunchOptions: {\nheadless: true,\n},\ngotoOptions: {\nwaitUntil: \"domcontentloaded\",\n},\nasync evaluate(page) {\nnrTimesCalled += 1;\nreturn page.content();\n},\n});\nconst result = await loader.load();\n\nexpect(nrTimesCalled).toBe(1);\nexpect(result).toBeDefined();\nexpect(result.length).toBe(1);\n}, 20_000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/playwright_web.int.test.ts","loc":{"lines":{"from":1,"to":31}}}}],["484",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { PuppeteerWebBaseLoader } from \"../web/puppeteer.js\";\n\ntest.skip(\"Test puppeteer web scraper loader\", async () => {\nconst loader = new PuppeteerWebBaseLoader(\"https://www.google.com/\");\nconst result = await loader.load();\n\nexpect(result).toBeDefined();\nexpect(result.length).toBe(1);\n}, 20_000);\n\ntest.skip(\"Test puppeteer web scraper loader with evaluate options\", async () => {\nlet nrTimesCalled = 0;\nconst loader = new PuppeteerWebBaseLoader(\"https://www.google.com/\", {\nlaunchOptions: {\nheadless: true,\nignoreDefaultArgs: [\"--disable-extensions\"],\n},\ngotoOptions: {\nwaitUntil: \"domcontentloaded\",\n},\nasync evaluate(page) {\nnrTimesCalled += 1;\nreturn page.evaluate(() => document.body.innerHTML);\n},\n});\nconst result = await loader.load();\n\nexpect(nrTimesCalled).toBe(1);\nexpect(result).toBeDefined();\nexpect(result.length).toBe(1);\n}, 20_000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/puppeteer.int.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["485",{"pageContent":"import { test, jest, expect } from \"@jest/globals\";\nimport S3Client from \"@aws-sdk/client-s3\";\nimport * as fs from \"node:fs\";\nimport * as path from \"node:path\";\nimport { Readable } from \"node:stream\";\nimport { S3Loader } from \"../web/s3.js\";\nimport { UnstructuredLoader } from \"../fs/unstructured.js\";\n\nconst fsMock = {\n...fs,\nmkdtempSync: jest.fn().mockReturnValue(\"tmp/s3fileloader-12345\"),\nmkdirSync: jest.fn().mockImplementation(() => {}),\nwriteFileSync: jest.fn().mockImplementation(() => {}),\n};\n\nconst UnstructuredLoaderMock = jest.fn().mockImplementation(() => ({\nload: jest.fn().mockImplementation(() => [\"fake document\"]),\n}));\n\njest.mock(\"@aws-sdk/client-s3\", () => ({\nS3Client: jest.fn().mockImplementation(() => ({\nsend: jest.fn().mockImplementation(() =>\nPromise.resolve({\nBody: new Readable({\nread() {\nthis.push(Buffer.from(\"Mock file content\"));\nthis.push(null);\n},\n}),\n})\n),\n})),\nGetObjectCommand: jest.fn(),\n}));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/s3.test.ts","loc":{"lines":{"from":1,"to":34}}}}],["486",{"pageContent":"test(\"Test S3 loader\", async () => {\nif (!S3Client) {\n// this is to avoid a linting error. S3Client is mocked above.\n}\n\nconst loader = new S3Loader({\nbucket: \"test-bucket-123\",\nkey: \"AccountingOverview.pdf\",\nunstructuredAPIURL: \"http://localhost:8000/general/v0/general\",\nfs: fsMock as typeof fs,\nUnstructuredLoader: UnstructuredLoaderMock as typeof UnstructuredLoader,\n});\n\nconst result = await loader.load();\n\nexpect(fsMock.mkdtempSync).toHaveBeenCalled();\nexpect(fsMock.mkdirSync).toHaveBeenCalled();\nexpect(fsMock.writeFileSync).toHaveBeenCalled();\nexpect(UnstructuredLoaderMock).toHaveBeenCalledWith(\n\"http://localhost:8000/general/v0/general\",\npath.join(\"tmp\", \"s3fileloader-12345\", \"AccountingOverview.pdf\")\n);\nexpect(result).toEqual([\"fake document\"]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/s3.test.ts","loc":{"lines":{"from":66,"to":89}}}}],["487",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport * as fs from \"node:fs/promises\";\nimport { test, expect } from \"@jest/globals\";\nimport { SRTLoader } from \"../fs/srt.js\";\n\ntest(\"Test SRT loader from blob\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt\"\n);\nconst loader = new SRTLoader(\nnew Blob([await fs.readFile(filePath)], { type: \"application/x-subrip\" })\n);\nconst docs = await loader.load();\nexpect(docs.length).toBe(1);\nexpect(docs[0].metadata).toMatchInlineSnapshot(`\n{\n\"blobType\": \"application/x-subrip\",\n\"source\": \"blob\",\n}\n`);\nexpect(docs[0].pageContent).toContain(\"Corruption discovered\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/srt-blob.test.ts","loc":{"lines":{"from":1,"to":24}}}}],["488",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport { SRTLoader } from \"../fs/srt.js\";\n\ntest(\"Test SRT loader from file\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt\"\n);\nconst loader = new SRTLoader(filePath);\nconst docs = await loader.load();\nexpect(docs.length).toBe(1);\nexpect(docs[0].metadata).toMatchInlineSnapshot(`\n{\n\"source\": \"${filePath}\",\n}\n`);\nexpect(docs[0].pageContent).toContain(\"Corruption discovered\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/srt.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["489",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { TextLoader } from \"../fs/text.js\";\n\ntest(\"Test Text loader from blob\", async () => {\nconst loader = new TextLoader(\nnew Blob([\"Hello, world!\"], { type: \"text/plain\" })\n);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1);\nexpect(docs[0].pageContent).toBe(\"Hello, world!\");\nexpect(docs[0].metadata).toMatchInlineSnapshot(`\n{\n\"blobType\": \"text/plain\",\n\"source\": \"blob\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/text-blob.test.ts","loc":{"lines":{"from":1,"to":18}}}}],["490",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { TextLoader } from \"../fs/text.js\";\n\ntest(\"Test Text loader from file\", async () => {\nconst loader = new TextLoader(\n\"../examples/src/document_loaders/example_data/example.txt\"\n);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(1);\nexpect(docs[0].pageContent).toMatchInlineSnapshot(`\n\"Foo\nBar\nBaz\n\n\"\n`);\nexpect(docs[0].metadata).toMatchInlineSnapshot(`\n{\n\"source\": \"../examples/src/document_loaders/example_data/example.txt\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/text.test.ts","loc":{"lines":{"from":1,"to":23}}}}],["491",{"pageContent":"import * as url from \"node:url\";\nimport * as path from \"node:path\";\nimport { test, expect } from \"@jest/globals\";\nimport {\nUnstructuredDirectoryLoader,\nUnstructuredLoader,\nUnknownHandling,\n} from \"../fs/unstructured.js\";\n\ntest(\"Test Unstructured base loader\", async () => {\nconst filePath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data/example.txt\"\n);\n\nconst loader = new UnstructuredLoader(\n\"https://api.unstructured.io/general/v0/general\",\nfilePath\n);\nconst docs = await loader.load();\n\nexpect(docs.length).toBe(3);\nfor (const doc of docs) {\nexpect(typeof doc.pageContent).toBe(\"string\");\n}\n});\n\ntest(\"Test Unstructured directory loader\", async () => {\nconst directoryPath = path.resolve(\npath.dirname(url.fileURLToPath(import.meta.url)),\n\"./example_data\"\n);\n\nconst loader = new UnstructuredDirectoryLoader(\n\"https://api.unstructured.io/general/v0/general\",\ndirectoryPath,\ntrue,\nUnknownHandling.Ignore\n);\nconst docs = await loader.load();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/unstructured.int.test.ts","loc":{"lines":{"from":1,"to":40}}}}],["492",{"pageContent":"expect(docs.length).toBe(619);\nexpect(typeof docs[0].pageContent).toBe(\"string\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/tests/unstructured.int.test.ts","loc":{"lines":{"from":47,"to":49}}}}],["493",{"pageContent":"import type { CheerioAPI, load as LoadT, SelectorType } from \"cheerio\";\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\nimport type { DocumentLoader } from \"../base.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../../util/async_caller.js\";\n\nexport interface WebBaseLoaderParams extends AsyncCallerParams {\n/**\n* The timeout in milliseconds for the fetch request. Defaults to 10s.\n*/\ntimeout?: number;\n\n/**\n* The selector to use to extract the text from the document. Defaults to\n* \"body\".\n*/\nselector?: SelectorType;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/cheerio.ts","loc":{"lines":{"from":1,"to":18}}}}],["494",{"pageContent":"class CheerioWebBaseLoader\nextends BaseDocumentLoader\nimplements DocumentLoader\n{\ntimeout: number;\n\ncaller: AsyncCaller;\n\nselector?: SelectorType;\n\nconstructor(public webPath: string, fields?: WebBaseLoaderParams) {\nsuper();\nconst { timeout, selector, ...rest } = fields ?? {};\nthis.timeout = timeout ?? 10000;\nthis.caller = new AsyncCaller(rest);\nthis.selector = selector ?? \"body\";\n}\n\nstatic async _scrape(\nurl: string,\ncaller: AsyncCaller,\ntimeout: number | undefined\n): Promise<CheerioAPI> {\nconst { load } = await CheerioWebBaseLoader.imports();\nconst response = await caller.call(fetch, url, {\nsignal: timeout ? AbortSignal.timeout(timeout) : undefined,\n});\nconst html = await response.text();\nreturn load(html);\n}\n\nasync scrape(): Promise<CheerioAPI> {\nreturn CheerioWebBaseLoader._scrape(\nthis.webPath,\nthis.caller,\nthis.timeout\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/cheerio.ts","loc":{"lines":{"from":80,"to":117}}}}],["495",{"pageContent":"async load(): Promise<Document[]> {\nconst $ = await this.scrape();\nconst text = $(this.selector).text();\nconst metadata = { source: this.webPath };\nreturn [new Document({ pageContent: text, metadata })];\n}\n\nstatic async imports(): Promise<{\nload: typeof LoadT;\n}> {\ntry {\nconst { load } = await import(\"cheerio\");\nreturn { load };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Please install cheerio as a dependency with, e.g. `yarn add cheerio`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/cheerio.ts","loc":{"lines":{"from":165,"to":185}}}}],["496",{"pageContent":"import { Document } from \"../../document.js\";\nimport { CheerioWebBaseLoader } from \"./cheerio.js\";\n\nexport class CollegeConfidentialLoader extends CheerioWebBaseLoader {\nconstructor(webPath: string) {\nsuper(webPath);\n}\n\npublic async load(): Promise<Document[]> {\nconst $ = await this.scrape();\nconst text = $(\"main[class='skin-handler']\").text();\nconst metadata = { source: this.webPath };\nreturn [new Document({ pageContent: text, metadata })];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/college_confidential.ts","loc":{"lines":{"from":1,"to":15}}}}],["497",{"pageContent":"import type { CheerioAPI } from \"cheerio\";\nimport { Document } from \"../../document.js\";\nimport { CheerioWebBaseLoader } from \"./cheerio.js\";\n\ninterface GitbookLoaderParams {\nshouldLoadAllPaths?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/gitbook.ts","loc":{"lines":{"from":1,"to":7}}}}],["498",{"pageContent":"class GitbookLoader extends CheerioWebBaseLoader {\nshouldLoadAllPaths = false;\n\nconstructor(public webPath: string, params: GitbookLoaderParams = {}) {\nsuper(webPath);\nthis.shouldLoadAllPaths =\nparams.shouldLoadAllPaths ?? this.shouldLoadAllPaths;\n}\n\npublic async load(): Promise<Document[]> {\nconst $ = await this.scrape();\n\nif (this.shouldLoadAllPaths === true) {\nreturn this.loadAllPaths($);\n}\nreturn this.loadPath($);\n}\n\nprivate loadPath($: CheerioAPI, url?: string): Document[] {\nconst pageContent = $(\"main *\")\n.contents()\n.toArray()\n.map((element) =>\nelement.type === \"text\" ? $(element).text().trim() : null\n)\n.filter((text) => text)\n.join(\"\\n\");\n\nconst title = $(\"main h1\").first().text().trim();\n\nreturn [\nnew Document({\npageContent,\nmetadata: { source: url ?? this.webPath, title },\n}),\n];\n}\n\nprivate async loadAllPaths($: CheerioAPI): Promise<Document[]> {\nconst relative_paths = $(\"nav a\")\n.toArray()\n.map((element) => $(element).attr(\"href\"))\n.filter((text) => text && text[0] === \"/\");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/gitbook.ts","loc":{"lines":{"from":64,"to":106}}}}],["499",{"pageContent":"const documents: Document[] = [];\nfor (const path of relative_paths) {\nconst url = this.webPath + path;\nconsole.log(`Fetching text from ${url}`);\nconst html = await GitbookLoader._scrape(url, this.caller, this.timeout);\ndocuments.push(...this.loadPath(html, url));\n}\nconsole.log(`Fetched ${documents.length} documents.`);\nreturn documents;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/gitbook.ts","loc":{"lines":{"from":132,"to":142}}}}],["500",{"pageContent":"import binaryExtensions from \"binary-extensions\";\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\nimport { UnknownHandling } from \"../fs/directory.js\";\nimport { extname } from \"../../util/extname.js\";\n\nconst extensions = new Set(binaryExtensions);\n\nfunction isBinaryPath(name: string) {\nreturn extensions.has(extname(name).slice(1).toLowerCase());\n}\n\ninterface GithubFile {\nname: string;\npath: string;\nsha: string;\nsize: number;\nurl: string;\nhtml_url: string;\ngit_url: string;\ndownload_url: string;\ntype: string;\n_links: {\nself: string;\ngit: string;\nhtml: string;\n};\n}\n\nexport interface GithubRepoLoaderParams {\nbranch?: string;\nrecursive?: boolean;\nunknown?: UnknownHandling;\naccessToken?: string;\nignoreFiles?: (string | RegExp)[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":1,"to":36}}}}],["501",{"pageContent":"class GithubRepoLoader\nextends BaseDocumentLoader\nimplements GithubRepoLoaderParams\n{\nprivate readonly owner: string;\n\nprivate readonly repo: string;\n\nprivate readonly initialPath: string;\n\nprivate headers: Record<string, string> = {};\n\npublic branch: string;\n\npublic recursive: boolean;\n\npublic unknown: UnknownHandling;\n\npublic accessToken?: string;\n\npublic ignoreFiles: (string | RegExp)[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":198,"to":218}}}}],["502",{"pageContent":"constructor(\ngithubUrl: string,\n{\naccessToken = typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.GITHUB_ACCESS_TOKEN\n: undefined,\nbranch = \"main\",\nrecursive = true,\nunknown = UnknownHandling.Warn,\nignoreFiles = [],\n}: GithubRepoLoaderParams = {}\n) {\nsuper();\nconst { owner, repo, path } = this.extractOwnerAndRepoAndPath(githubUrl);\nthis.owner = owner;\nthis.repo = repo;\nthis.initialPath = path;\nthis.branch = branch;\nthis.recursive = recursive;\nthis.unknown = unknown;\nthis.accessToken = accessToken;\nthis.ignoreFiles = ignoreFiles;\nif (this.accessToken) {\nthis.headers = {\nAuthorization: `Bearer ${this.accessToken}`,\n};\n}\n}\n\nprivate extractOwnerAndRepoAndPath(url: string): {\nowner: string;\nrepo: string;\npath: string;\n} {\nconst match = url.match(\n/https:\\/\\/github.com\\/([^/]+)\\/([^/]+)(\\/tree\\/[^/]+\\/(.+))?/i\n);\n\nif (!match) {\nthrow new Error(\"Invalid GitHub URL format.\");\n}\n\nreturn { owner: match[1], repo: match[2], path: match[4] || \"\" };\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":404,"to":448}}}}],["503",{"pageContent":"public async load(): Promise<Document[]> {\nconst documents: Document[] = [];\nawait this.processDirectory(this.initialPath, documents);\nreturn documents;\n}\n\nprivate shouldIgnore(path: string): boolean {\nreturn this.ignoreFiles.some((pattern) => {\nif (typeof pattern === \"string\") {\nreturn path === pattern;\n}\n\ntry {\nreturn pattern.test(path);\n} catch {\nthrow new Error(`Unknown ignore file pattern: ${pattern}`);\n}\n});\n}\n\nprivate async processDirectory(\npath: string,\ndocuments: Document[]\n): Promise<void> {\ntry {\nconst files = await this.fetchRepoFiles(path);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":600,"to":625}}}}],["504",{"pageContent":"for (const file of files) {\nif (file.type === \"dir\") {\nif (this.recursive) {\nawait this.processDirectory(file.path, documents);\n}\n} else {\ntry {\nif (!isBinaryPath(file.name) && !this.shouldIgnore(file.path)) {\nconst fileContent = await this.fetchFileContent(file);\nconst metadata = { source: file.path };\ndocuments.push(\nnew Document({ pageContent: fileContent, metadata })\n);\n}\n} catch (e) {\nthis.handleError(\n`Failed to fetch file content: ${file.path}, ${e}`\n);\n}\n}\n}\n} catch (error) {\nthis.handleError(`Failed to process directory: ${path}, ${error}`);\n}\n}\n\nprivate async fetchRepoFiles(path: string): Promise<GithubFile[]> {\nconst url = `https://api.github.com/repos/${this.owner}/${this.repo}/contents/${path}?ref=${this.branch}`;\nconst response = await fetch(url, { headers: this.headers });\nconst data = await response.json();\nif (!response.ok) {\nthrow new Error(\n`Unable to fetch repository files: ${response.status} ${JSON.stringify(\ndata\n)}`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":800,"to":836}}}}],["505",{"pageContent":"if (!Array.isArray(data)) {\nthrow new Error(\"Unable to fetch repository files.\");\n}\n\nreturn data as GithubFile[];\n}\n\nprivate async fetchFileContent(file: GithubFile): Promise<string> {\nconst response = await fetch(file.download_url, { headers: this.headers });\nreturn response.text();\n}\n\nprivate handleError(message: string): void {\nswitch (this.unknown) {\ncase UnknownHandling.Ignore:\nbreak;\ncase UnknownHandling.Warn:\nconsole.warn(message);\nbreak;\ncase UnknownHandling.Error:\nthrow new Error(message);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":990,"to":1010}}}}],["506",{"pageContent":":\nthrow new Error(`Unknown unknown handling: ${this.unknown}`);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/github.ts","loc":{"lines":{"from":1187,"to":1191}}}}],["507",{"pageContent":"import type { CheerioAPI } from \"cheerio\";\nimport { Document } from \"../../document.js\";\nimport { CheerioWebBaseLoader } from \"./cheerio.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/hn.ts","loc":{"lines":{"from":1,"to":3}}}}],["508",{"pageContent":"class HNLoader extends CheerioWebBaseLoader {\nconstructor(public webPath: string) {\nsuper(webPath);\n}\n\npublic async load(): Promise<Document[]> {\nconst $ = await this.scrape();\nif (this.webPath.includes(\"item\")) {\nreturn this.loadComments($);\n}\nreturn this.loadResults($);\n}\n\nprivate loadComments($: CheerioAPI): Document[] {\nconst comments = $(\"tr[class='athing comtr']\");\nconst title = $(\"tr[id='pagespace']\").attr(\"title\");\nconst documents: Document[] = [];\ncomments.each((_index, comment) => {\nconst text = $(comment).text().trim();\nconst metadata = { source: this.webPath, title };\ndocuments.push(new Document({ pageContent: text, metadata }));\n});\nreturn documents;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/hn.ts","loc":{"lines":{"from":47,"to":70}}}}],["509",{"pageContent":"private loadResults($: CheerioAPI): Document[] {\nconst items = $(\"tr[class='athing']\");\nconst documents: Document[] = [];\nitems.each((_index, item) => {\nconst ranking = $(item).find(\"span[class='rank']\").text();\nconst link = $(item).find(\"span[class='titleline'] a\").attr(\"href\");\nconst title = $(item).find(\"span[class='titleline']\").text().trim();\nconst metadata = {\nsource: this.webPath,\ntitle,\nlink,\nranking,\n};\ndocuments.push(new Document({ pageContent: title, metadata }));\n});\nreturn documents;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/hn.ts","loc":{"lines":{"from":95,"to":112}}}}],["510",{"pageContent":"import { Document } from \"../../document.js\";\nimport { CheerioWebBaseLoader } from \"./cheerio.js\";\n\nexport class IMSDBLoader extends CheerioWebBaseLoader {\nconstructor(public webPath: string) {\nsuper(webPath);\n}\n\npublic async load(): Promise<Document[]> {\nconst $ = await this.scrape();\nconst text = $(\"td[class='scrtext']\").text().trim();\nconst metadata = { source: this.webPath };\nreturn [new Document({ pageContent: text, metadata })];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/imsdb.ts","loc":{"lines":{"from":1,"to":15}}}}],["511",{"pageContent":"import type { LaunchOptions, Page, Browser } from \"playwright\";\n\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\nimport type { DocumentLoader } from \"../base.js\";\n\nexport { Page, Browser };\n\nexport type PlaywrightGotoOptions = {\nreferer?: string;\ntimeout?: number;\nwaitUntil?: \"load\" | \"domcontentloaded\" | \"networkidle\" | \"commit\";\n};\n\nexport type PlaywrightEvaluate = (\npage: Page,\nbrowser: Browser\n) => Promise<string>;\n\nexport type PlaywrightWebBaseLoaderOptions = {\nlaunchOptions?: LaunchOptions;\ngotoOptions?: PlaywrightGotoOptions;\nevaluate?: PlaywrightEvaluate;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/playwright.ts","loc":{"lines":{"from":1,"to":24}}}}],["512",{"pageContent":"class PlaywrightWebBaseLoader\nextends BaseDocumentLoader\nimplements DocumentLoader\n{\noptions: PlaywrightWebBaseLoaderOptions | undefined;\n\nconstructor(\npublic webPath: string,\noptions?: PlaywrightWebBaseLoaderOptions\n) {\nsuper();\nthis.options = options ?? undefined;\n}\n\nstatic async _scrape(\nurl: string,\noptions?: PlaywrightWebBaseLoaderOptions\n): Promise<string> {\nconst { chromium } = await PlaywrightWebBaseLoader.imports();\n\nconst browser = await chromium.launch({\nheadless: true,\n...options?.launchOptions,\n});\nconst page = await browser.newPage();\n\nawait page.goto(url, {\ntimeout: 180000,\nwaitUntil: \"domcontentloaded\",\n...options?.gotoOptions,\n});\nconst bodyHTML = options?.evaluate\n? await options?.evaluate(page, browser)\n: await page.content();\n\nawait browser.close();\n\nreturn bodyHTML;\n}\n\nasync scrape(): Promise<string> {\nreturn PlaywrightWebBaseLoader._scrape(this.webPath, this.options);\n}\n\nasync load(): Promise<Document[]> {\nconst text = await this.scrape();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/playwright.ts","loc":{"lines":{"from":92,"to":137}}}}],["513",{"pageContent":"const metadata = { source: this.webPath };\nreturn [new Document({ pageContent: text, metadata })];\n}\n\nstatic async imports(): Promise<{\nchromium: typeof import(\"playwright\").chromium;\n}> {\ntry {\nconst { chromium } = await import(\"playwright\");\n\nreturn { chromium };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Please install playwright as a dependency with, e.g. `yarn add playwright`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/playwright.ts","loc":{"lines":{"from":187,"to":205}}}}],["514",{"pageContent":"import type {\nlaunch,\nWaitForOptions,\nPage,\nBrowser,\nPuppeteerLaunchOptions,\n} from \"puppeteer\";\n\nimport { Document } from \"../../document.js\";\nimport { BaseDocumentLoader } from \"../base.js\";\nimport type { DocumentLoader } from \"../base.js\";\n\nexport { Page, Browser };\n\nexport type PuppeteerGotoOptions = WaitForOptions & {\nreferer?: string;\nreferrerPolicy?: string;\n};\n\nexport type PuppeteerEvaluate = (\npage: Page,\nbrowser: Browser\n) => Promise<string>;\n\nexport type PuppeteerWebBaseLoaderOptions = {\nlaunchOptions?: PuppeteerLaunchOptions;\ngotoOptions?: PuppeteerGotoOptions;\nevaluate?: PuppeteerEvaluate;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/puppeteer.ts","loc":{"lines":{"from":1,"to":29}}}}],["515",{"pageContent":"class PuppeteerWebBaseLoader\nextends BaseDocumentLoader\nimplements DocumentLoader\n{\noptions: PuppeteerWebBaseLoaderOptions | undefined;\n\nconstructor(public webPath: string, options?: PuppeteerWebBaseLoaderOptions) {\nsuper();\nthis.options = options ?? undefined;\n}\n\nstatic async _scrape(\nurl: string,\noptions?: PuppeteerWebBaseLoaderOptions\n): Promise<string> {\nconst { launch } = await PuppeteerWebBaseLoader.imports();\n\nconst browser = await launch({\nheadless: true,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/puppeteer.ts","loc":{"lines":{"from":97,"to":115}}}}],["516",{"pageContent":"Viewport: null,\nignoreDefaultArgs: [\"--disable-extensions\"],\n...options?.launchOptions,\n});\nconst page = await browser.newPage();\n\nawait page.goto(url, {\ntimeout: 180000,\nwaitUntil: \"domcontentloaded\",\n...options?.gotoOptions,\n});\nconst bodyHTML = options?.evaluate\n? await options?.evaluate(page, browser)\n: await page.evaluate(() => document.body.innerHTML);\n\nawait browser.close();\n\nreturn bodyHTML;\n}\n\nasync scrape(): Promise<string> {\nreturn PuppeteerWebBaseLoader._scrape(this.webPath, this.options);\n}\n\nasync load(): Promise<Document[]> {\nconst text = await this.scrape();\n\nconst metadata = { source: this.webPath };\nreturn [new Document({ pageContent: text, metadata })];\n}\n\nstatic async imports(): Promise<{\nlaunch: typeof launch;\n}> {\ntry {\n// eslint-disable-next-line import/no-extraneous-dependencies\nconst { launch } = await import(\"puppeteer\");\n\nreturn { launch };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Please install puppeteer as a dependency with, e.g. `yarn add puppeteer`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/puppeteer.ts","loc":{"lines":{"from":188,"to":234}}}}],["517",{"pageContent":"import * as fsDefault from \"node:fs\";\nimport * as path from \"node:path\";\nimport * as os from \"node:os\";\nimport { Readable } from \"node:stream\";\nimport { BaseDocumentLoader } from \"../base.js\";\nimport { UnstructuredLoader as UnstructuredLoaderDefault } from \"../fs/unstructured.js\";\n\nexport interface S3LoaderParams {\nbucket: string;\nkey: string;\nunstructuredAPIURL: string;\ns3Config?: S3Config;\n\nfs?: typeof fsDefault;\nUnstructuredLoader?: typeof UnstructuredLoaderDefault;\n}\n\ninterface S3Config {\nregion?: string;\naccessKeyId?: string;\nsecretAccessKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/s3.ts","loc":{"lines":{"from":1,"to":22}}}}],["518",{"pageContent":"class S3Loader extends BaseDocumentLoader {\nprivate bucket: string;\n\nprivate key: string;\n\nprivate unstructuredAPIURL: string;\n\nprivate s3Config: S3Config;\n\nprivate _fs: typeof fsDefault;\n\nprivate _UnstructuredLoader: typeof UnstructuredLoaderDefault;\n\nconstructor({\nbucket,\nkey,\nunstructuredAPIURL,\ns3Config = {},\nfs = fsDefault,\nUnstructuredLoader = UnstructuredLoaderDefault,\n}: S3LoaderParams) {\nsuper();\nthis.bucket = bucket;\nthis.key = key;\nthis.unstructuredAPIURL = unstructuredAPIURL;\nthis.s3Config = s3Config;\nthis._fs = fs;\nthis._UnstructuredLoader = UnstructuredLoader;\n}\n\npublic async load() {\nconst { S3Client, GetObjectCommand } = await S3LoaderImports();\n\nconst tempDir = this._fs.mkdtempSync(\npath.join(os.tmpdir(), \"s3fileloader-\")\n);\n\nconst filePath = path.join(tempDir, this.key);\n\ntry {\nconst s3Client = new S3Client(this.s3Config);\n\nconst getObjectCommand = new GetObjectCommand({\nBucket: this.bucket,\nKey: this.key,\n});\n\nconst response = await s3Client.send(getObjectCommand);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/s3.ts","loc":{"lines":{"from":124,"to":171}}}}],["519",{"pageContent":"const objectData = await new Promise<Buffer>((resolve, reject) => {\nconst chunks: Buffer[] = [];\n\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (response.Body instanceof Readable) {\nresponse.Body.on(\"data\", (chunk: Buffer) => chunks.push(chunk));\nresponse.Body.on(\"end\", () => resolve(Buffer.concat(chunks)));\nresponse.Body.on(\"error\", reject);\n} else {\nreject(new Error(\"Response body is not a readable stream.\"));\n}\n});\n\nthis._fs.mkdirSync(path.dirname(filePath), { recursive: true });\n\nthis._fs.writeFileSync(filePath, objectData);\n} catch {\nthrow new Error(\n`Failed to download file ${this.key} from S3 bucket ${this.bucket}.`\n);\n}\n\ntry {\nconst unstructuredLoader = new this._UnstructuredLoader(\nthis.unstructuredAPIURL,\nfilePath\n);\n\nconst docs = await unstructuredLoader.load();\n\nreturn docs;\n} catch {\nthrow new Error(\n`Failed to load file ${filePath} using unstructured loader.`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/s3.ts","loc":{"lines":{"from":251,"to":288}}}}],["520",{"pageContent":"S3LoaderImports() {\ntry {\nconst s3Module = await import(\"@aws-sdk/client-s3\");\n\nreturn s3Module as typeof s3Module;\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Failed to load @aws-sdk/client-s3'. Please install it eg. `yarn add @aws-sdk/client-s3`.\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/document_loaders/web/s3.ts","loc":{"lines":{"from":371,"to":382}}}}],["521",{"pageContent":"import { AsyncCaller, AsyncCallerParams } from \"../util/async_caller.js\";\n\nexport type EmbeddingsParams = AsyncCallerParams;\n\nexport abstract class Embeddings {\n/**\n* The async caller should be used by subclasses to make any async calls,\n* which will thus benefit from the concurrency and retry logic.\n*/\ncaller: AsyncCaller;\n\nconstructor(params: EmbeddingsParams) {\nthis.caller = new AsyncCaller(params ?? {});\n}\n\nabstract embedDocuments(documents: string[]): Promise<number[][]>;\n\nabstract embedQuery(document: string): Promise<number[]>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/base.ts","loc":{"lines":{"from":1,"to":19}}}}],["522",{"pageContent":"import { chunkArray } from \"../util/chunk.js\";\nimport { Embeddings, EmbeddingsParams } from \"./base.js\";\n\nexport interface CohereEmbeddingsParams extends EmbeddingsParams {\nmodelName: string;\n\n/**\n* The maximum number of documents to embed in a single request. This is\n* limited by the Cohere API to a maximum of 96.\n*/\nbatchSize?: number;\n}\n\n/**\n* A class for generating embeddings using the Cohere API.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/cohere.ts","loc":{"lines":{"from":1,"to":16}}}}],["523",{"pageContent":"class CohereEmbeddings\nextends Embeddings\nimplements CohereEmbeddingsParams\n{\nmodelName = \"small\";\n\nbatchSize = 48;\n\nprivate apiKey: string;\n\nprivate client: typeof import(\"cohere-ai\");\n\n/**\n* Constructor for the CohereEmbeddings class.\n* @param fields - An optional object with properties to configure the instance.\n*/\nconstructor(\nfields?: Partial<CohereEmbeddingsParams> & {\nverbose?: boolean;\napiKey?: string;\n}\n) {\nsuper(fields ?? {});\n\nconst apiKey =\nfields?.apiKey ||\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.COHERE_API_KEY\n: undefined);\n\nif (!apiKey) {\nthrow new Error(\"Cohere API key not found\");\n}\n\nthis.modelName = fields?.modelName ?? this.modelName;\nthis.batchSize = fields?.batchSize ?? this.batchSize;\nthis.apiKey = apiKey;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/cohere.ts","loc":{"lines":{"from":137,"to":175}}}}],["524",{"pageContent":"/**\n* Generates embeddings for an array of texts.\n* @param texts - An array of strings to generate embeddings for.\n* @returns A Promise that resolves to an array of embeddings.\n*/\nasync embedDocuments(texts: string[]): Promise<number[][]> {\nawait this.maybeInitClient();\n\nconst subPrompts = chunkArray(texts, this.batchSize);\n\nconst embeddings = [];\n\nfor (let i = 0; i < subPrompts.length; i += 1) {\nconst input = subPrompts[i];\nconst { body } = await this.embeddingWithRetry({\nmodel: this.modelName,\ntexts,\n});\nfor (let j = 0; j < input.length; j += 1) {\nembeddings.push(body.embeddings[j]);\n}\n}\n\nreturn embeddings;\n}\n\n/**\n* Generates an embedding for a single text.\n* @param text - A string to generate an embedding for.\n* @returns A Promise that resolves to an array of numbers representing the embedding.\n*/\nasync embedQuery(text: string): Promise<number[]> {\nawait this.maybeInitClient();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/cohere.ts","loc":{"lines":{"from":278,"to":310}}}}],["525",{"pageContent":"const { body } = await this.embeddingWithRetry({\nmodel: this.modelName,\ntexts: [text],\n});\nreturn body.embeddings[0];\n}\n\n/**\n* Generates embeddings with retry capabilities.\n* @param request - An object containing the request parameters for generating embeddings.\n* @returns A Promise that resolves to the API response.\n*/\nprivate async embeddingWithRetry(\nrequest: Parameters<typeof this.client.embed>[0]\n) {\nawait this.maybeInitClient();\n\nreturn this.caller.call(this.client.embed.bind(this.client), request);\n}\n\n/**\n* Initializes the Cohere client if it hasn't been initialized already.\n*/\nprivate async maybeInitClient() {\nif (!this.client) {\nconst { cohere } = await CohereEmbeddings.imports();\n\nthis.client = cohere;\nthis.client.init(this.apiKey);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/cohere.ts","loc":{"lines":{"from":409,"to":439}}}}],["526",{"pageContent":"/** @ignore */\nstatic async imports(): Promise<{\ncohere: typeof import(\"cohere-ai\");\n}> {\ntry {\nconst { default: cohere } = await import(\"cohere-ai\");\nreturn { cohere };\n} catch (e) {\nthrow new Error(\n\"Please install cohere-ai as a dependency with, e.g. `yarn add cohere-ai`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/cohere.ts","loc":{"lines":{"from":544,"to":557}}}}],["527",{"pageContent":"import { Embeddings, EmbeddingsParams } from \"./base.js\";\n\nexport class FakeEmbeddings extends Embeddings {\nconstructor(params?: EmbeddingsParams) {\nsuper(params ?? {});\n}\n\nembedDocuments(documents: string[]): Promise<number[][]> {\nreturn Promise.resolve(documents.map(() => [0.1, 0.2, 0.3, 0.4]));\n}\n\nembedQuery(_: string): Promise<number[]> {\nreturn Promise.resolve([0.1, 0.2, 0.3, 0.4]);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/fake.ts","loc":{"lines":{"from":1,"to":15}}}}],["528",{"pageContent":"import { HfInference } from \"@huggingface/inference\";\nimport { Embeddings, EmbeddingsParams } from \"./base.js\";\n\nexport interface HuggingFaceInferenceEmbeddingsParams extends EmbeddingsParams {\napiKey?: string;\nmodel?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/hf.ts","loc":{"lines":{"from":1,"to":7}}}}],["529",{"pageContent":"class HuggingFaceInferenceEmbeddings\nextends Embeddings\nimplements HuggingFaceInferenceEmbeddingsParams\n{\napiKey?: string;\n\nmodel: string;\n\nclient: HfInference;\n\nconstructor(fields?: HuggingFaceInferenceEmbeddingsParams) {\nsuper(fields ?? {});\n\nthis.model =\nfields?.model ?? \"sentence-transformers/distilbert-base-nli-mean-tokens\";\nthis.apiKey =\nfields?.apiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.HUGGINGFACEHUB_API_KEY\n: undefined);\nthis.client = new HfInference(this.apiKey);\n}\n\nasync _embed(texts: string[]): Promise<number[][]> {\n// replace newlines, which can negatively affect performance.\nconst clean = texts.map((text) => text.replace(/\\n/g, \" \"));\nreturn this.caller.call(() =>\nthis.client.featureExtraction({\nmodel: this.model,\ninputs: clean,\n})\n) as Promise<number[][]>;\n}\n\nembedQuery(document: string): Promise<number[]> {\nreturn this._embed([document]).then((embeddings) => embeddings[0]);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/hf.ts","loc":{"lines":{"from":52,"to":89}}}}],["530",{"pageContent":"embedDocuments(documents: string[]): Promise<number[][]> {\nreturn this._embed(documents);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/hf.ts","loc":{"lines":{"from":106,"to":109}}}}],["531",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/embeddings' is deprecated. Import from eg. 'langchain/embeddings/openai' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport { OpenAIEmbeddings } from \"./openai.js\";\nexport { CohereEmbeddings } from \"./cohere.js\";\nexport { Embeddings } from \"./base.js\";\nexport { FakeEmbeddings } from \"./fake.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/index.ts","loc":{"lines":{"from":1,"to":8}}}}],["532",{"pageContent":"import {\nConfiguration,\nOpenAIApi,\nCreateEmbeddingRequest,\nConfigurationParameters,\n} from \"openai\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { chunkArray } from \"../util/chunk.js\";\nimport { Embeddings, EmbeddingsParams } from \"./base.js\";\n\nexport interface OpenAIEmbeddingsParams extends EmbeddingsParams {\n/** Model name to use */\nmodelName: string;\n\n/**\n* Timeout to use when making requests to OpenAI.\n*/\ntimeout?: number;\n\n/**\n* The maximum number of documents to embed in a single request. This is\n* limited by the OpenAI API to a maximum of 2048.\n*/\nbatchSize?: number;\n\n/**\n* Whether to strip new lines from the input text. This is recommended by\n* OpenAI, but may not be suitable for all use cases.\n*/\nstripNewLines?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/openai.ts","loc":{"lines":{"from":1,"to":31}}}}],["533",{"pageContent":"class OpenAIEmbeddings\nextends Embeddings\nimplements OpenAIEmbeddingsParams\n{\nmodelName = \"text-embedding-ada-002\";\n\nbatchSize = 512;\n\nstripNewLines = true;\n\ntimeout?: number;\n\nprivate client: OpenAIApi;\n\nprivate clientConfig: ConfigurationParameters;\n\nconstructor(\nfields?: Partial<OpenAIEmbeddingsParams> & {\nverbose?: boolean;\nopenAIApiKey?: string;\n},\nconfiguration?: ConfigurationParameters\n) {\nsuper(fields ?? {});\n\nconst apiKey =\nfields?.openAIApiKey ??\n// eslint-disable-next-line no-process-env\n(typeof process !== \"undefined\" ? process.env.OPENAI_API_KEY : undefined);\nif (!apiKey) {\nthrow new Error(\"OpenAI API key not found\");\n}\n\nthis.modelName = fields?.modelName ?? this.modelName;\nthis.batchSize = fields?.batchSize ?? this.batchSize;\nthis.stripNewLines = fields?.stripNewLines ?? this.stripNewLines;\nthis.timeout = fields?.timeout;\n\nthis.clientConfig = {\napiKey,\n...configuration,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/openai.ts","loc":{"lines":{"from":127,"to":169}}}}],["534",{"pageContent":"async embedDocuments(texts: string[]): Promise<number[][]> {\nconst subPrompts = chunkArray(\nthis.stripNewLines ? texts.map((t) => t.replaceAll(\"\\n\", \" \")) : texts,\nthis.batchSize\n);\n\nconst embeddings: number[][] = [];\n\nfor (let i = 0; i < subPrompts.length; i += 1) {\nconst input = subPrompts[i];\nconst { data } = await this.embeddingWithRetry({\nmodel: this.modelName,\ninput,\n});\nfor (let j = 0; j < input.length; j += 1) {\nembeddings.push(data.data[j].embedding);\n}\n}\n\nreturn embeddings;\n}\n\nasync embedQuery(text: string): Promise<number[]> {\nconst { data } = await this.embeddingWithRetry({\nmodel: this.modelName,\ninput: this.stripNewLines ? text.replaceAll(\"\\n\", \" \") : text,\n});\nreturn data.data[0].embedding;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/openai.ts","loc":{"lines":{"from":257,"to":285}}}}],["535",{"pageContent":"private async embeddingWithRetry(request: CreateEmbeddingRequest) {\nif (!this.client) {\nconst clientConfig = new Configuration({\n...this.clientConfig,\nbaseOptions: {\ntimeout: this.timeout,\nadapter: fetchAdapter,\n...this.clientConfig.baseOptions,\n},\n});\nthis.client = new OpenAIApi(clientConfig);\n}\nreturn this.caller.call(\nthis.client.createEmbedding.bind(this.client),\nrequest\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/openai.ts","loc":{"lines":{"from":382,"to":399}}}}],["536",{"pageContent":"import { load } from \"@tensorflow-models/universal-sentence-encoder\";\nimport * as tf from \"@tensorflow/tfjs-core\";\n\nimport { Embeddings, EmbeddingsParams } from \"./base.js\";\n\nexport interface TensorFlowEmbeddingsParams extends EmbeddingsParams {}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tensorflow.ts","loc":{"lines":{"from":1,"to":6}}}}],["537",{"pageContent":"class TensorFlowEmbeddings extends Embeddings {\nconstructor(fields?: TensorFlowEmbeddingsParams) {\nsuper(fields ?? {});\n\ntry {\ntf.backend();\n} catch (e) {\nthrow new Error(\"No TensorFlow backend found, see instructions at ...\");\n}\n}\n\n_cached: ReturnType<typeof load>;\n\nprivate async load() {\nif (this._cached === undefined) {\nthis._cached = load();\n}\nreturn this._cached;\n}\n\nprivate _embed(texts: string[]) {\nreturn this.caller.call(async () => {\nconst model = await this.load();\nreturn model.embed(texts);\n});\n}\n\nembedQuery(document: string): Promise<number[]> {\nreturn this._embed([document])\n.then((embeddings) => embeddings.array())\n.then((embeddings) => embeddings[0]);\n}\n\nembedDocuments(documents: string[]): Promise<number[][]> {\nreturn this._embed(documents).then((embeddings) => embeddings.array());\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tensorflow.ts","loc":{"lines":{"from":44,"to":80}}}}],["538",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { CohereEmbeddings } from \"../cohere.js\";\n\ntest(\"Test CohereEmbeddings.embedQuery\", async () => {\nconst embeddings = new CohereEmbeddings();\nconst res = await embeddings.embedQuery(\"Hello world\");\nexpect(typeof res[0]).toBe(\"number\");\n});\n\ntest(\"Test CohereEmbeddings.embedDocuments\", async () => {\nconst embeddings = new CohereEmbeddings();\nconst res = await embeddings.embedDocuments([\"Hello world\", \"Bye bye\"]);\nexpect(res).toHaveLength(2);\nexpect(typeof res[0][0]).toBe(\"number\");\nexpect(typeof res[1][0]).toBe(\"number\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tests/cohere.int.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["539",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { HuggingFaceInferenceEmbeddings } from \"../hf.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"HuggingFaceInferenceEmbeddings\", async () => {\nconst embeddings = new HuggingFaceInferenceEmbeddings();\n\nconst documents = [\n\"Hello world!\",\n\"Hello bad world!\",\n\"Hello nice world!\",\n\"Hello good world!\",\n\"1 + 1 = 2\",\n\"1 + 1 = 3\",\n];\n\nconst queryEmbedding = await embeddings.embedQuery(documents[0]);\nexpect(queryEmbedding).toHaveLength(768);\nexpect(typeof queryEmbedding[0]).toBe(\"number\");\n\nconst store = new MemoryVectorStore(embeddings);\n\nawait store.addDocuments(\ndocuments.map((pageContent) => new Document({ pageContent }))\n);\n\nexpect(await store.similaritySearch(documents[4], 2)).toMatchInlineSnapshot(`\n[\nDocument {\n\"metadata\": {},\n\"pageContent\": \"1 + 1 = 2\",\n},\nDocument {\n\"metadata\": {},\n\"pageContent\": \"1 + 1 = 3\",\n},\n]\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tests/hf.int.test.ts","loc":{"lines":{"from":1,"to":40}}}}],["540",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { OpenAIEmbeddings } from \"../openai.js\";\n\ntest(\"Test OpenAIEmbeddings.embedQuery\", async () => {\nconst embeddings = new OpenAIEmbeddings();\nconst res = await embeddings.embedQuery(\"Hello world\");\nexpect(typeof res[0]).toBe(\"number\");\n});\n\ntest(\"Test OpenAIEmbeddings.embedDocuments\", async () => {\nconst embeddings = new OpenAIEmbeddings();\nconst res = await embeddings.embedDocuments([\"Hello world\", \"Bye bye\"]);\nexpect(res).toHaveLength(2);\nexpect(typeof res[0][0]).toBe(\"number\");\nexpect(typeof res[1][0]).toBe(\"number\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tests/openai.int.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["541",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport \"@tensorflow/tfjs-backend-cpu\";\nimport { TensorFlowEmbeddings } from \"../tensorflow.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"TensorflowEmbeddings\", async () => {\nconst embeddings = new TensorFlowEmbeddings();\n\nconst documents = [\n\"Hello world!\",\n\"Hello bad world!\",\n\"Hello nice world!\",\n\"Hello good world!\",\n\"1 + 1 = 2\",\n\"1 + 1 = 3\",\n];\n\nconst queryEmbedding = await embeddings.embedQuery(documents[0]);\nexpect(queryEmbedding).toHaveLength(512);\nexpect(typeof queryEmbedding[0]).toBe(\"number\");\n\nconst store = new MemoryVectorStore(embeddings);\n\nawait store.addDocuments(\ndocuments.map((pageContent) => new Document({ pageContent }))\n);\n\nexpect(await store.similaritySearch(documents[4], 2)).toMatchInlineSnapshot(`\n[\nDocument {\n\"metadata\": {},\n\"pageContent\": \"1 + 1 = 2\",\n},\nDocument {\n\"metadata\": {},\n\"pageContent\": \"1 + 1 = 3\",\n},\n]\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/embeddings/tests/tensorflow.int.test.ts","loc":{"lines":{"from":1,"to":41}}}}],["542",{"pageContent":"import { LLMChain } from \"../../chains/llm_chain.js\";\nimport { BaseChatModel } from \"../../chat_models/base.js\";\nimport { VectorStoreRetriever } from \"../../vectorstores/base.js\";\nimport { Tool } from \"../../tools/base.js\";\n\nimport { AutoGPTOutputParser } from \"./output_parser.js\";\nimport { AutoGPTPrompt } from \"./prompt.js\";\nimport {\nAIChatMessage,\nBaseChatMessage,\nHumanChatMessage,\nSystemChatMessage,\n} from \"../../schema/index.js\";\n// import { HumanInputRun } from \"./tools/human/tool\"; // TODO\nimport { ObjectTool, FINISH_NAME } from \"./schema.js\";\nimport { TokenTextSplitter } from \"../../text_splitter.js\";\nimport {\ngetEmbeddingContextSize,\ngetModelContextSize,\n} from \"../../base_language/count_tokens.js\";\n\nexport interface AutoGPTInput {\naiName: string;\naiRole: string;\nmemory: VectorStoreRetriever;\nhumanInTheLoop?: boolean;\noutputParser?: AutoGPTOutputParser;\nmaxIterations?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":1,"to":29}}}}],["543",{"pageContent":"class AutoGPT {\naiName: string;\n\nmemory: VectorStoreRetriever;\n\nfullMessageHistory: BaseChatMessage[];\n\nnextActionCount: number;\n\nchain: LLMChain;\n\noutputParser: AutoGPTOutputParser;\n\ntools: ObjectTool[];\n\nfeedbackTool?: Tool;\n\nmaxIterations: number;\n\n// Currently not generic enough to support any text splitter.\ntextSplitter: TokenTextSplitter;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":181,"to":201}}}}],["544",{"pageContent":"constructor({\naiName,\nmemory,\nchain,\noutputParser,\ntools,\nfeedbackTool,\nmaxIterations,\n}: Omit<Required<AutoGPTInput>, \"aiRole\" | \"humanInTheLoop\"> & {\nchain: LLMChain;\ntools: ObjectTool[];\nfeedbackTool?: Tool;\n}) {\nthis.aiName = aiName;\nthis.memory = memory;\nthis.fullMessageHistory = [];\nthis.nextActionCount = 0;\nthis.chain = chain;\nthis.outputParser = outputParser;\nthis.tools = tools;\nthis.feedbackTool = feedbackTool;\nthis.maxIterations = maxIterations;\nconst chunkSize = getEmbeddingContextSize(\n\"modelName\" in memory.vectorStore.embeddings\n? (memory.vectorStore.embeddings.modelName as string)\n: undefined\n);\nthis.textSplitter = new TokenTextSplitter({\nchunkSize,\nchunkOverlap: Math.round(chunkSize / 10),\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":372,"to":403}}}}],["545",{"pageContent":"static fromLLMAndTools(\nllm: BaseChatModel,\ntools: ObjectTool[],\n{\naiName,\naiRole,\nmemory,\nmaxIterations = 100,\n// humanInTheLoop = false,\noutputParser = new AutoGPTOutputParser(),\n}: AutoGPTInput\n): AutoGPT {\nconst prompt = new AutoGPTPrompt({\naiName,\naiRole,\ntools,\ntokenCounter: llm.getNumTokens.bind(llm),\nsendTokenLimit: getModelContextSize(\n\"modelName\" in llm ? (llm.modelName as string) : \"gpt2\"\n),\n});\n// const feedbackTool = humanInTheLoop ? new HumanInputRun() : null;\nconst chain = new LLMChain({ llm, prompt });\nreturn new AutoGPT({\naiName,\nmemory,\nchain,\noutputParser,\ntools,\n// feedbackTool,\nmaxIterations,\n});\n}\n\nasync run(goals: string[]): Promise<string | undefined> {\nconst user_input =\n\"Determine which next command to use, and respond using the format specified above:\";\nlet loopCount = 0;\nwhile (loopCount < this.maxIterations) {\nloopCount += 1;\n\nconst { text: assistantReply } = await this.chain.call({\ngoals,\nuser_input,\nmemory: this.memory,\nmessages: this.fullMessageHistory,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":563,"to":609}}}}],["546",{"pageContent":"// Print the assistant reply\nconsole.log(assistantReply);\nthis.fullMessageHistory.push(new HumanChatMessage(user_input));\nthis.fullMessageHistory.push(new AIChatMessage(assistantReply));\n\nconst action = await this.outputParser.parse(assistantReply);\nconst tools = this.tools.reduce(\n(acc, tool) => ({ ...acc, [tool.name]: tool }),\n{} as { [key: string]: ObjectTool }\n);\nif (action.name === FINISH_NAME) {\nreturn action.args.response;\n}\nlet result: string;\nif (action.name in tools) {\nconst tool = tools[action.name];\nlet observation;\ntry {\nobservation = await tool.call(action.args);\n} catch (e) {\nobservation = `Error in args: ${e}`;\n}\nresult = `Command ${tool.name} returned: ${observation}`;\n} else if (action.name === \"ERROR\") {\nresult = `Error: ${action.args}. `;\n} else {\nresult = `Unknown command '${action.name}'. Please refer to the 'COMMANDS' list for available commands and only respond in the specified JSON format.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":753,"to":780}}}}],["547",{"pageContent":"let memoryToAdd = `Assistant Reply: ${assistantReply}\\nResult: ${result} `;\nif (this.feedbackTool) {\nconst feedback = `\\n${await this.feedbackTool.call(\"Input: \")}`;\nif (feedback === \"q\" || feedback === \"stop\") {\nconsole.log(\"EXITING\");\nreturn \"EXITING\";\n}\nmemoryToAdd += feedback;\n}\n\nconst documents = await this.textSplitter.createDocuments([memoryToAdd]);\nawait this.memory.addDocuments(documents);\nthis.fullMessageHistory.push(new SystemChatMessage(result));\n}\n\nreturn undefined;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/agent.ts","loc":{"lines":{"from":929,"to":946}}}}],["548",{"pageContent":"export { AutoGPTPrompt, AutoGPTPromptInput } from \"./prompt.js\";\n\nexport { AutoGPTOutputParser, preprocessJsonInput } from \"./output_parser.js\";\n\nexport { AutoGPT, AutoGPTInput } from \"./agent.js\";\n\nexport { AutoGPTAction } from \"./schema.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/index.ts","loc":{"lines":{"from":1,"to":7}}}}],["549",{"pageContent":"import { BaseOutputParser } from \"../../schema/output_parser.js\";\nimport { AutoGPTAction } from \"./schema.js\";\n\nexport function preprocessJsonInput(inputStr: string): string {\n// Replace single backslashes with double backslashes,\n// while leaving already escaped ones intact\nconst correctedStr = inputStr.replace(\n/(?<!\\\\)\\\\(?![\"\\\\/bfnrt]|u[0-9a-fA-F]{4})/g,\n\"\\\\\\\\\"\n);\nreturn correctedStr;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/output_parser.ts","loc":{"lines":{"from":1,"to":12}}}}],["550",{"pageContent":"class AutoGPTOutputParser extends BaseOutputParser<AutoGPTAction> {\ngetFormatInstructions(): string {\nthrow new Error(\"Method not implemented.\");\n}\n\nasync parse(text: string): Promise<AutoGPTAction> {\nlet parsed: {\ncommand: {\nname: string;\nargs: Record<string, unknown>;\n};\n};\ntry {\nparsed = JSON.parse(text);\n} catch (error) {\nconst preprocessedText = preprocessJsonInput(text);\ntry {\nparsed = JSON.parse(preprocessedText);\n} catch (error) {\nreturn {\nname: \"ERROR\",\nargs: { error: `Could not parse invalid json: ${text}` },\n};\n}\n}\ntry {\nreturn {\nname: parsed.command.name,\nargs: parsed.command.args,\n};\n} catch (error) {\nreturn {\nname: \"ERROR\",\nargs: { error: `Incomplete command args: ${parsed}` },\n};\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/output_parser.ts","loc":{"lines":{"from":52,"to":89}}}}],["551",{"pageContent":"import { BaseChatPromptTemplate } from \"../../prompts/chat.js\";\nimport {\nBaseChatMessage,\nHumanChatMessage,\nPartialValues,\nSystemChatMessage,\n} from \"../../schema/index.js\";\nimport { VectorStoreRetriever } from \"../../vectorstores/base.js\";\nimport { ObjectTool } from \"./schema.js\";\nimport { getPrompt } from \"./prompt_generator.js\";\nimport { BasePromptTemplate } from \"../../index.js\";\nimport { SerializedBasePromptTemplate } from \"../../prompts/serde.js\";\n\nexport interface AutoGPTPromptInput {\naiName: string;\naiRole: string;\ntools: ObjectTool[];\ntokenCounter: (text: string) => Promise<number>;\nsendTokenLimit?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt.ts","loc":{"lines":{"from":1,"to":20}}}}],["552",{"pageContent":"class AutoGPTPrompt\nextends BaseChatPromptTemplate\nimplements AutoGPTPromptInput\n{\naiName: string;\n\naiRole: string;\n\ntools: ObjectTool[];\n\ntokenCounter: (text: string) => Promise<number>;\n\nsendTokenLimit: number;\n\nconstructor(fields: AutoGPTPromptInput) {\nsuper({ inputVariables: [\"goals\", \"memory\", \"messages\", \"user_input\"] });\nthis.aiName = fields.aiName;\nthis.aiRole = fields.aiRole;\nthis.tools = fields.tools;\nthis.tokenCounter = fields.tokenCounter;\nthis.sendTokenLimit = fields.sendTokenLimit || 4196;\n}\n\n_getPromptType() {\nreturn \"autogpt\" as const;\n}\n\nconstructFullPrompt(goals: string[]): string {\nconst promptStart = `Your decisions must always be made independently \nwithout seeking user assistance. Play to your strengths \nas an LLM and pursue simple strategies with no legal complications. \nIf you have completed all your tasks, \nmake sure to use the \"finish\" command.`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt.ts","loc":{"lines":{"from":135,"to":167}}}}],["553",{"pageContent":"let fullPrompt = `You are ${this.aiName}, ${this.aiRole}\\n${promptStart}\\n\\nGOALS:\\n\\n`;\ngoals.forEach((goal, index) => {\nfullPrompt += `${index + 1}. ${goal}\\n`;\n});\n\nfullPrompt += `\\n\\n${getPrompt(this.tools)}`;\nreturn fullPrompt;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt.ts","loc":{"lines":{"from":265,"to":272}}}}],["554",{"pageContent":"async formatMessages({\ngoals,\nmemory,\nmessages: previousMessages,\nuser_input,\n}: {\ngoals: string[];\nmemory: VectorStoreRetriever;\nmessages: BaseChatMessage[];\nuser_input: string;\n}) {\nconst basePrompt = new SystemChatMessage(this.constructFullPrompt(goals));\nconst timePrompt = new SystemChatMessage(\n`The current time and date is ${new Date().toLocaleString()}`\n);\nconst usedTokens =\n(await this.tokenCounter(basePrompt.text)) +\n(await this.tokenCounter(timePrompt.text));\nconst relevantDocs = await memory.getRelevantDocuments(\nJSON.stringify(previousMessages.slice(-10))\n);\nconst relevantMemory = relevantDocs.map((d) => d.pageContent);\nlet relevantMemoryTokens = await relevantMemory.reduce(\nasync (acc, doc) => (await acc) + (await this.tokenCounter(doc)),\nPromise.resolve(0)\n);\n\nwhile (usedTokens + relevantMemoryTokens > 2500) {\nrelevantMemory.pop();\nrelevantMemoryTokens = await relevantMemory.reduce(\nasync (acc, doc) => (await acc) + (await this.tokenCounter(doc)),\nPromise.resolve(0)\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt.ts","loc":{"lines":{"from":398,"to":431}}}}],["555",{"pageContent":"const contentFormat = `This reminds you of these events from your past:\\n${relevantMemory.join(\n\"\\n\"\n)}\\n\\n`;\nconst memoryMessage = new SystemChatMessage(contentFormat);\nconst usedTokensWithMemory =\n(await usedTokens) + (await this.tokenCounter(memoryMessage.text));\nconst historicalMessages: BaseChatMessage[] = [];\n\nfor (const message of previousMessages.slice(-10).reverse()) {\nconst messageTokens = await this.tokenCounter(message.text);\nif (usedTokensWithMemory + messageTokens > this.sendTokenLimit - 1000) {\nbreak;\n}\nhistoricalMessages.unshift(message);\n}\n\nconst inputMessage = new HumanChatMessage(user_input);\nconst messages: BaseChatMessage[] = [\nbasePrompt,\ntimePrompt,\nmemoryMessage,\n...historicalMessages,\ninputMessage,\n];\nreturn messages;\n}\n\nasync partial(_values: PartialValues): Promise<BasePromptTemplate> {\nthrow new Error(\"Method not implemented.\");\n}\n\nserialize(): SerializedBasePromptTemplate {\nthrow new Error(\"Method not implemented.\");\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt.ts","loc":{"lines":{"from":528,"to":562}}}}],["556",{"pageContent":"import { zodToJsonSchema } from \"zod-to-json-schema\";\nimport { JsonSchema7ObjectType } from \"zod-to-json-schema/src/parsers/object.js\";\n\nimport { ObjectTool, FINISH_NAME } from \"./schema.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":1,"to":4}}}}],["557",{"pageContent":"class PromptGenerator {\nconstraints: string[];\n\ncommands: ObjectTool[];\n\nresources: string[];\n\nperformance_evaluation: string[];\n\nresponse_format: object;\n\nconstructor() {\nthis.constraints = [];\nthis.commands = [];\nthis.resources = [];\nthis.performance_evaluation = [];\nthis.response_format = {\nthoughts: {\ntext: \"thought\",\nreasoning: \"reasoning\",\nplan: \"- short bulleted\\n- list that conveys\\n- long-term plan\",\ncriticism: \"constructive self-criticism\",\nspeak: \"thoughts summary to say to user\",\n},\ncommand: { name: \"command name\", args: { \"arg name\": \"value\" } },\n};\n}\n\nadd_constraint(constraint: string): void {\nthis.constraints.push(constraint);\n}\n\nadd_tool(tool: ObjectTool): void {\nthis.commands.push(tool);\n}\n\n_generate_command_string(tool: ObjectTool): string {\nlet output = `\"${tool.name}\": ${tool.description}`;\noutput += `, args json schema: ${JSON.stringify(\n(zodToJsonSchema(tool.schema) as JsonSchema7ObjectType).properties\n)}`;\nreturn output;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":150,"to":192}}}}],["558",{"pageContent":"add_resource(resource: string): void {\nthis.resources.push(resource);\n}\n\nadd_performance_evaluation(evaluation: string): void {\nthis.performance_evaluation.push(evaluation);\n}\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_generate_numbered_list(items: any[], item_type = \"list\"): string {\nif (item_type === \"command\") {\nconst command_strings = items.map(\n(item, i) => `${i + 1}. ${this._generate_command_string(item)}`\n);\nconst finish_description =\n\"use this to signal that you have finished all your objectives\";\nconst finish_args =\n'\"response\": \"final response to let people know you have finished your objectives\"';\nconst finish_string = `${\nitems.length + 1\n}. ${FINISH_NAME}: ${finish_description}, args: ${finish_args}`;\nreturn command_strings.concat([finish_string]).join(\"\\n\");\n}\n\nreturn items.map((item, i) => `${i + 1}. ${item}`).join(\"\\n\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":304,"to":329}}}}],["559",{"pageContent":"generate_prompt_string(): string {\nconst formatted_response_format = JSON.stringify(\nthis.response_format,\nnull,\n4\n);\nconst prompt_string =\n`Constraints:\\n${this._generate_numbered_list(this.constraints)}\\n\\n` +\n`Commands:\\n${this._generate_numbered_list(\nthis.commands,\n\"command\"\n)}\\n\\n` +\n`Resources:\\n${this._generate_numbered_list(this.resources)}\\n\\n` +\n`Performance Evaluation:\\n${this._generate_numbered_list(\nthis.performance_evaluation\n)}\\n\\n` +\n`You should only respond in JSON format as described below ` +\n`\\nResponse Format: \\n${formatted_response_format} ` +\n`\\nEnsure the response can be parsed by Python json.loads`;\n\nreturn prompt_string;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":445,"to":467}}}}],["560",{"pageContent":"function getPrompt(tools: ObjectTool[]): string {\nconst prompt_generator = new PromptGenerator();\n\nprompt_generator.add_constraint(\n\"~4000 word limit for short term memory. \" +\n\"Your short term memory is short, \" +\n\"so immediately save important information to files.\"\n);\nprompt_generator.add_constraint(\n\"If you are unsure how you previously did something \" +\n\"or want to recall past events, \" +\n\"thinking about similar events will help you remember.\"\n);\nprompt_generator.add_constraint(\"No user assistance\");\nprompt_generator.add_constraint(\n'Exclusively use the commands listed in double quotes e.g. \"command name\"'\n);\n\nfor (const tool of tools) {\nprompt_generator.add_tool(tool);\n}\n\nprompt_generator.add_resource(\n\"Internet access for searches and information gathering.\"\n);\nprompt_generator.add_resource(\"Long Term memory management.\");\nprompt_generator.add_resource(\n\"GPT-3.5 powered Agents for delegation of simple tasks.\"\n);\nprompt_generator.add_resource(\"File output.\");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":591,"to":620}}}}],["561",{"pageContent":"prompt_generator.add_performance_evaluation(\n\"Continuously review and analyze your actions \" +\n\"to ensure you are performing to the best of your abilities.\"\n);\nprompt_generator.add_performance_evaluation(\n\"Constructively self-criticize your big-picture behavior constantly.\"\n);\nprompt_generator.add_performance_evaluation(\n\"Reflect on past decisions and strategies to refine your approach.\"\n);\nprompt_generator.add_performance_evaluation(\n\"Every command has a cost, so be smart and efficient. \" +\n\"Aim to complete tasks in the least number of steps.\"\n);\n\nconst prompt_string = prompt_generator.generate_prompt_string();\n\nreturn prompt_string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/prompt_generator.ts","loc":{"lines":{"from":732,"to":750}}}}],["562",{"pageContent":"import { StructuredTool } from \"../../tools/base.js\";\n\nexport type ObjectTool = StructuredTool;\n\nexport const FINISH_NAME = \"finish\";\n\nexport interface AutoGPTAction {\nname: string;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nargs: Record<string, any>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/schema.ts","loc":{"lines":{"from":1,"to":11}}}}],["563",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { z } from \"zod\";\n\nimport { getPrompt } from \"../prompt_generator.js\";\nimport { StructuredTool } from \"../../../tools/base.js\";\nimport { Calculator } from \"../../../tools/calculator.js\";\nimport { ReadFileTool, WriteFileTool } from \"../../../tools/fs.js\";\nimport { InMemoryFileStore } from \"../../../stores/file/in_memory.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/tests/prompt_generator.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["564",{"pageContent":"FakeBrowserTool extends StructuredTool {\nschema = z.object({\nurl: z.string(),\nquery: z.string().optional(),\n});\n\nname = \"fake_browser_tool\";\n\ndescription =\n\"useful for when you need to find something on the web or summarize a webpage.\";\n\nasync _call({\nurl: _url,\nquery: _query,\n}: z.infer<this[\"schema\"]>): Promise<string> {\nreturn \"fake_browser_tool\";\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/tests/prompt_generator.test.ts","loc":{"lines":{"from":83,"to":100}}}}],["565",{"pageContent":"test(\"prompt with several tools\", () => {\nconst store = new InMemoryFileStore();\nconst tools = [\nnew FakeBrowserTool(),\nnew Calculator(),\nnew ReadFileTool({ store }),\nnew WriteFileTool({ store }),\n];\nconst prompt = getPrompt(tools);\nexpect(prompt).toMatchInlineSnapshot(`\n\"Constraints:\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n3. No user assistance\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/tests/prompt_generator.test.ts","loc":{"lines":{"from":175,"to":189}}}}],["566",{"pageContent":"Commands:\n1. \"fake_browser_tool\": useful for when you need to find something on the web or summarize a webpage., args json schema: {\"url\":{\"type\":\"string\"},\"query\":{\"type\":\"string\"}}\n2. \"calculator\": Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator., args json schema: {\"input\":{\"type\":\"string\"}}\n3. \"read_file\": Read file from disk, args json schema: {\"file_path\":{\"type\":\"string\",\"description\":\"name of file\"}}\n4. \"write_file\": Write file from disk, args json schema: {\"file_path\":{\"type\":\"string\",\"description\":\"name of file\"},\"text\":{\"type\":\"string\",\"description\":\"text to write to file\"}}\n5. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/tests/prompt_generator.test.ts","loc":{"lines":{"from":251,"to":256}}}}],["567",{"pageContent":"Resources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n\nYou should only respond in JSON format as described below \nResponse Format: \n{\n\"thoughts\": {\n\"text\": \"thought\",\n\"reasoning\": \"reasoning\",\n\"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\n\"criticism\": \"constructive self-criticism\",\n\"speak\": \"thoughts summary to say to user\"\n},\n\"command\": {\n\"name\": \"command name\",\n\"args\": {\n\"arg name\": \"value\"\n}\n}\n} \nEnsure the response can be parsed by Python json.loads\"\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/autogpt/tests/prompt_generator.test.ts","loc":{"lines":{"from":308,"to":339}}}}],["568",{"pageContent":"import { BaseLanguageModel } from \"../../base_language/index.js\";\nimport { CallbackManagerForChainRun } from \"../../callbacks/manager.js\";\nimport { BaseChain, ChainInputs } from \"../../chains/base.js\";\nimport { SerializedBaseChain } from \"../../chains/serde.js\";\nimport { Document } from \"../../document.js\";\nimport { ChainValues } from \"../../schema/index.js\";\nimport { Optional } from \"../../types/type-utils.js\";\nimport { VectorStore } from \"../../vectorstores/base.js\";\nimport { TaskCreationChain } from \"./task_creation.js\";\nimport { TaskExecutionChain } from \"./task_execution.js\";\nimport { TaskPrioritizationChain } from \"./task_prioritization.js\";\n\nexport interface Task {\ntaskID: string;\ntaskName: string;\n}\n\nexport interface BabyAGIInputs\nextends Omit<ChainInputs, \"memory\" | \"callbackManager\"> {\ncreationChain: BaseChain;\nprioritizationChain: BaseChain;\nexecutionChain: BaseChain;\nvectorstore: VectorStore;\nmaxIterations?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":1,"to":25}}}}],["569",{"pageContent":"class BabyAGI extends BaseChain implements BabyAGIInputs {\ntaskList: Task[];\n\ncreationChain: BaseChain;\n\nprioritizationChain: BaseChain;\n\nexecutionChain: BaseChain;\n\ntaskIDCounter: number;\n\nvectorstore: VectorStore;\n\nmaxIterations: number;\n\nconstructor({\ncreationChain,\nprioritizationChain,\nexecutionChain,\nvectorstore,\nmaxIterations = 100,\nverbose,\ncallbacks,\n}: BabyAGIInputs) {\nsuper(undefined, verbose, callbacks);\nthis.taskList = [];\nthis.creationChain = creationChain;\nthis.prioritizationChain = prioritizationChain;\nthis.executionChain = executionChain;\nthis.taskIDCounter = 1;\nthis.vectorstore = vectorstore;\nthis.maxIterations = maxIterations;\n}\n\n_chainType() {\nreturn \"BabyAGI\" as const;\n}\n\nget inputKeys() {\nreturn [\"objective\", \"firstTask\"];\n}\n\nget outputKeys() {\nreturn [];\n}\n\nasync addTask(task: Task) {\nthis.taskList.push(task);\n}\n\nprintTaskList() {\nconsole.log(\"\\x1b[95m\\x1b[1m\\n*****TASK LIST*****\\n\\x1b[0m\\x1b[0m\");\nfor (const t of this.taskList) {\nconsole.log(`${t.taskID}: ${t.taskName}`);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":264,"to":319}}}}],["570",{"pageContent":"printNextTask(task: Task) {\nconsole.log(\"\\x1b[92m\\x1b[1m\\n*****NEXT TASK*****\\n\\x1b[0m\\x1b[0m\");\nconsole.log(`${task.taskID}: ${task.taskName}`);\n}\n\nprintTaskResult(result: string) {\nconsole.log(\"\\x1b[93m\\x1b[1m\\n*****TASK RESULT*****\\n\\x1b[0m\\x1b[0m\");\nconsole.log(result.trim());\n}\n\nasync getNextTasks(\nresult: string,\ntask_description: string,\nobjective: string,\nrunManager?: CallbackManagerForChainRun\n): Promise<Optional<Task, \"taskID\">[]> {\nconst taskNames = this.taskList.map((t) => t.taskName);\nconst incomplete_tasks = taskNames.join(\", \");\nconst { [this.creationChain.outputKeys[0]]: text } =\nawait this.creationChain.call(\n{\nresult,\ntask_description,\nincomplete_tasks,\nobjective,\n},\nrunManager?.getChild()\n);\nconst newTasks = (text as string).split(\"\\n\");\nreturn newTasks\n.filter((taskName) => taskName.trim())\n.map((taskName) => ({ taskName }));\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":555,"to":587}}}}],["571",{"pageContent":"async prioritizeTasks(\nthisTaskID: number,\nobjective: string,\nrunManager?: CallbackManagerForChainRun\n) {\nconst taskNames = this.taskList.map((t) => t.taskName);\nconst nextTaskID = thisTaskID + 1;\nconst { [this.prioritizationChain.outputKeys[0]]: text } =\nawait this.prioritizationChain.call(\n{\ntask_names: taskNames.join(\", \"),\nnext_task_id: String(nextTaskID),\nobjective,\n},\nrunManager?.getChild()\n);\nconst newTasks = (text as string).trim().split(\"\\n\");\nconst prioritizedTaskList = [];\nfor (const taskString of newTasks) {\nconst taskParts = taskString.trim().split(\".\", 2);\nif (taskParts.length === 2) {\nconst taskID = taskParts[0].trim();\nconst taskName = taskParts[1].trim();\nprioritizedTaskList.push({ taskID, taskName });\n}\n}\nreturn prioritizedTaskList;\n}\n\nasync getTopTasks(query: string, k = 5) {\nconst results = await this.vectorstore.similaritySearch(query, k);\nif (!results) {\nreturn [];\n}\nreturn results.map((item) => String(item.metadata.task));\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":829,"to":864}}}}],["572",{"pageContent":"async executeTask(\nobjective: string,\ntask: string,\nrunManager?: CallbackManagerForChainRun\n) {\nconst context = await this.getTopTasks(objective);\nconst { [this.executionChain.outputKeys[0]]: text } =\nawait this.executionChain.call(\n{\nobjective,\ncontext: context.join(\"\\n\"),\ntask,\n},\nrunManager?.getChild()\n);\nreturn text as string;\n}\n\nasync _call(\n{ objective, firstTask = \"Make a todo list\" }: ChainValues,\nrunManager?: CallbackManagerForChainRun\n) {\nthis.taskList = [];\nthis.taskIDCounter = 1;\nawait this.addTask({ taskID: \"1\", taskName: firstTask });\n\nlet numIters = 0;\nwhile (numIters < this.maxIterations && this.taskList.length > 0) {\nthis.printTaskList();\n\n// eslint-disable-next-line @typescript-eslint/no-non-null-assertion\nconst task = this.taskList.shift()!;\nthis.printNextTask(task);\n\nconst result = await this.executeTask(\nobjective,\ntask.taskName,\nrunManager\n);\nconst thisTaskID = parseInt(task.taskID, 10);\nthis.printTaskResult(result);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":1101,"to":1141}}}}],["573",{"pageContent":"await this.vectorstore.addDocuments([\nnew Document({\npageContent: result,\nmetadata: { task: task.taskName },\n}),\n]);\n\nconst newTasks = await this.getNextTasks(\nresult,\ntask.taskName,\nobjective,\nrunManager\n);\nfor (const newTask of newTasks) {\nthis.taskIDCounter += 1;\nnewTask.taskID = this.taskIDCounter.toFixed();\nawait this.addTask(newTask as Task);\n}\nthis.taskList = await this.prioritizeTasks(\nthisTaskID,\nobjective,\nrunManager\n);\n\nnumIters += 1;\n}\nreturn {};\n}\n\nserialize(): SerializedBaseChain {\nthrow new Error(\"Method not implemented.\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":1380,"to":1411}}}}],["574",{"pageContent":"static fromLLM({\nllm,\nvectorstore,\nexecutionChain,\nverbose,\ncallbacks,\n...rest\n}: Optional<\nBabyAGIInputs,\n\"executionChain\" | \"creationChain\" | \"prioritizationChain\"\n> & { llm: BaseLanguageModel }) {\nconst creationChain = TaskCreationChain.fromLLM({\nllm,\nverbose,\ncallbacks,\n});\nconst prioritizationChain = TaskPrioritizationChain.fromLLM({\nllm,\nverbose,\ncallbacks,\n});\nreturn new BabyAGI({\ncreationChain,\nprioritizationChain,\nexecutionChain:\nexecutionChain ||\nTaskExecutionChain.fromLLM({ llm, verbose, callbacks }),\nvectorstore,\nverbose,\ncallbacks,\n...rest,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/agent.ts","loc":{"lines":{"from":1664,"to":1697}}}}],["575",{"pageContent":"export { TaskCreationChain } from \"./task_creation.js\";\nexport { TaskExecutionChain } from \"./task_execution.js\";\nexport { TaskPrioritizationChain } from \"./task_prioritization.js\";\nexport { BabyAGI, Task, BabyAGIInputs } from \"./agent.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/index.ts","loc":{"lines":{"from":1,"to":4}}}}],["576",{"pageContent":"import { LLMChain, LLMChainInput } from \"../../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\n\n/** Chain to generate tasks. */\nexport class TaskCreationChain extends LLMChain {\nstatic fromLLM(fields: Omit<LLMChainInput, \"prompt\">): LLMChain {\nconst taskCreationTemplate =\n`You are an task creation AI that uses the result of an execution agent` +\n` to create new tasks with the following objective: {objective},` +\n` The last completed task has the result: {result}.` +\n` This result was based on this task description: {task_description}.` +\n` These are incomplete tasks: {incomplete_tasks}.` +\n` Based on the result, create new tasks to be completed` +\n` by the AI system that do not overlap with incomplete tasks.` +\n` Return the tasks as an array.`;\nconst prompt = new PromptTemplate({\ntemplate: taskCreationTemplate,\ninputVariables: [\n\"result\",\n\"task_description\",\n\"incomplete_tasks\",\n\"objective\",\n],\n});\nreturn new TaskCreationChain({ prompt, ...fields });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/task_creation.ts","loc":{"lines":{"from":1,"to":27}}}}],["577",{"pageContent":"import { LLMChain, LLMChainInput } from \"../../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\n\n/** Chain to execute tasks. */\nexport class TaskExecutionChain extends LLMChain {\nstatic fromLLM(fields: Omit<LLMChainInput, \"prompt\">): LLMChain {\nconst executionTemplate =\n`You are an AI who performs one task based on the following objective: ` +\n`{objective}.` +\n`Take into account these previously completed tasks: {context}.` +\n` Your task: {task}. Response:`;\nconst prompt = new PromptTemplate({\ntemplate: executionTemplate,\ninputVariables: [\"objective\", \"context\", \"task\"],\n});\nreturn new TaskExecutionChain({ prompt, ...fields });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/task_execution.ts","loc":{"lines":{"from":1,"to":18}}}}],["578",{"pageContent":"import { LLMChain, LLMChainInput } from \"../../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\n\n/** Chain to prioritize tasks. */\nexport class TaskPrioritizationChain extends LLMChain {\nstatic fromLLM(fields: Omit<LLMChainInput, \"prompt\">): LLMChain {\nconst taskPrioritizationTemplate =\n`You are a task prioritization AI tasked with cleaning the formatting of ` +\n`and reprioritizing the following tasks: {task_names}.` +\n` Consider the ultimate objective of your team: {objective}.` +\n` Do not remove any tasks. Return the result as a numbered list, like:` +\n` #. First task` +\n` #. Second task` +\n` Start the task list with number {next_task_id}.`;\nconst prompt = new PromptTemplate({\ntemplate: taskPrioritizationTemplate,\ninputVariables: [\"task_names\", \"next_task_id\", \"objective\"],\n});\nreturn new TaskPrioritizationChain({ prompt, ...fields });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/experimental/babyagi/task_prioritization.ts","loc":{"lines":{"from":1,"to":21}}}}],["579",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain' is deprecated. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport {\nPromptTemplate,\nBasePromptTemplate,\nFewShotPromptTemplate,\n} from \"./prompts/index.js\";\nexport { LLMChain } from \"./chains/llm_chain.js\";\nexport { OpenAI } from \"./llms/openai.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/index.ts","loc":{"lines":{"from":1,"to":11}}}}],["580",{"pageContent":"import { InMemoryCache } from \"../cache/index.js\";\nimport {\nBaseCache,\nBasePromptValue,\nGeneration,\nLLMResult,\nRUN_KEY,\n} from \"../schema/index.js\";\nimport {\nBaseLanguageModel,\nBaseLanguageModelCallOptions,\nBaseLanguageModelParams,\n} from \"../base_language/index.js\";\nimport {\nCallbackManager,\nCallbackManagerForLLMRun,\nCallbacks,\n} from \"../callbacks/manager.js\";\n\nexport type SerializedLLM = {\n_model: string;\n_type: string;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\nexport interface BaseLLMParams extends BaseLanguageModelParams {\n/**\n* @deprecated Use `maxConcurrency` instead\n*/\nconcurrency?: number;\ncache?: BaseCache | boolean;\n}\n\nexport interface BaseLLMCallOptions extends BaseLanguageModelCallOptions {}\n\n/**\n* LLM Wrapper. Provides an {@link call} (an {@link generate}) function that takes in a prompt (or prompts) and returns a string.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":1,"to":38}}}}],["581",{"pageContent":"abstract class BaseLLM extends BaseLanguageModel {\ndeclare CallOptions: BaseLanguageModelCallOptions;\n\ncache?: BaseCache;\n\nconstructor({ cache, concurrency, ...rest }: BaseLLMParams) {\nsuper(concurrency ? { maxConcurrency: concurrency, ...rest } : rest);\nif (typeof cache === \"object\") {\nthis.cache = cache;\n} else if (cache) {\nthis.cache = InMemoryCache.global();\n} else {\nthis.cache = undefined;\n}\n}\n\nasync generatePrompt(\npromptValues: BasePromptValue[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult> {\nconst prompts: string[] = promptValues.map((promptValue) =>\npromptValue.toString()\n);\nreturn this.generate(prompts, stop, callbacks);\n}\n\n/**\n* Run the LLM on the given prompts and input.\n*/\nabstract _generate(\nprompts: string[],\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<LLMResult>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":249,"to":283}}}}],["582",{"pageContent":"/** @ignore */\nasync _generateUncached(\nprompts: string[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult> {\nconst callbackManager_ = await CallbackManager.configure(\ncallbacks,\nthis.callbacks,\n{ verbose: this.verbose }\n);\nconst runManager = await callbackManager_?.handleLLMStart(\n{ name: this._llmType() },\nprompts\n);\nlet output;\ntry {\noutput = await this._generate(prompts, stop, runManager);\n} catch (err) {\nawait runManager?.handleLLMError(err);\nthrow err;\n}\n\nawait runManager?.handleLLMEnd(output);\n// This defines RUN_KEY as a non-enumerable property on the output object\n// so that it is not serialized when the output is stringified, and so that\n// it isnt included when listing the keys of the output object.\nObject.defineProperty(output, RUN_KEY, {\nvalue: runManager ? { runId: runManager?.runId } : undefined,\nconfigurable: true,\n});\nreturn output;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":494,"to":526}}}}],["583",{"pageContent":"/**\n* Run the LLM on the given propmts an input, handling caching.\n*/\nasync generate(\nprompts: string[],\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n): Promise<LLMResult> {\nif (!Array.isArray(prompts)) {\nthrow new Error(\"Argument 'prompts' is expected to be a string[]\");\n}\n\nif (!this.cache) {\nreturn this._generateUncached(prompts, stop, callbacks);\n}\n\nconst { cache } = this;\nconst params = this.serialize();\nparams.stop = stop;\n\nconst llmStringKey = `${Object.entries(params).sort()}`;\nconst missingPromptIndices: number[] = [];\nconst generations = await Promise.all(\nprompts.map(async (prompt, index) => {\nconst result = await cache.lookup(prompt, llmStringKey);\nif (!result) {\nmissingPromptIndices.push(index);\n}\nreturn result;\n})\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":737,"to":767}}}}],["584",{"pageContent":"let llmOutput = {};\nif (missingPromptIndices.length > 0) {\nconst results = await this._generateUncached(\nmissingPromptIndices.map((i) => prompts[i]),\nstop,\ncallbacks\n);\nawait Promise.all(\nresults.generations.map(async (generation, index) => {\nconst promptIndex = missingPromptIndices[index];\ngenerations[promptIndex] = generation;\nreturn cache.update(prompts[promptIndex], llmStringKey, generation);\n})\n);\nllmOutput = results.llmOutput ?? {};\n}\n\nreturn { generations, llmOutput } as LLMResult;\n}\n\n/**\n* Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.\n*/\nasync call(\nprompt: string,\nstop?: string[] | this[\"CallOptions\"],\ncallbacks?: Callbacks\n) {\nconst { generations } = await this.generate([prompt], stop, callbacks);\nreturn generations[0][0].text;\n}\n\n/**\n* Get the identifying parameters of the LLM.\n*/\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_identifyingParams(): Record<string, any> {\nreturn {};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":981,"to":1019}}}}],["585",{"pageContent":"/**\n* Return the string type key uniquely identifying this class of LLM.\n*/\nabstract _llmType(): string;\n\n/**\n* Return a json-like object representing this LLM.\n*/\nserialize(): SerializedLLM {\nreturn {\n...this._identifyingParams(),\n_type: this._llmType(),\n_model: this._modelType(),\n};\n}\n\n_modelType(): string {\nreturn \"base_llm\" as const;\n}\n\n/**\n* Load an LLM from a json-like object describing it.\n*/\nstatic async deserialize(data: SerializedLLM): Promise<BaseLLM> {\nconst { _type, _model, ...rest } = data;\nif (_model && _model !== \"base_llm\") {\nthrow new Error(`Cannot load LLM with model ${_model}`);\n}\nconst Cls = {\nopenai: (await import(\"./openai.js\")).OpenAI,\n}[_type];\nif (Cls === undefined) {\nthrow new Error(`Cannot load  LLM with type ${_type}`);\n}\nreturn new Cls(rest);\n}\n}\n\n/**\n* LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n*\n* Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n*\n* @augments BaseLLM\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":1228,"to":1272}}}}],["586",{"pageContent":"abstract class LLM extends BaseLLM {\n/**\n* Run the LLM on the given prompt and input.\n*/\nabstract _call(\nprompt: string,\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<string>;\n\nasync _generate(\nprompts: string[],\nstop?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<LLMResult> {\nconst generations: Generation[][] = [];\nfor (let i = 0; i < prompts.length; i += 1) {\nconst text = await this._call(prompts[i], stop, runManager);\ngenerations.push([{ text }]);\n}\nreturn { generations };\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/base.ts","loc":{"lines":{"from":1481,"to":1503}}}}],["587",{"pageContent":"import { LLM, BaseLLMParams } from \"./base.js\";\n\nexport interface CohereInput extends BaseLLMParams {\n/** Sampling temperature to use */\ntemperature?: number;\n\n/**\n* Maximum number of tokens to generate in the completion.\n*/\nmaxTokens?: number;\n\n/** Model to use */\nmodel?: string;\n\napiKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/cohere.ts","loc":{"lines":{"from":1,"to":16}}}}],["588",{"pageContent":"class Cohere extends LLM implements CohereInput {\ntemperature = 0;\n\nmaxTokens = 250;\n\nmodel: string;\n\napiKey: string;\n\nconstructor(fields?: CohereInput) {\nsuper(fields ?? {});\n\nconst apiKey =\nfields?.apiKey ?? typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.COHERE_API_KEY\n: undefined;\n\nif (!apiKey) {\nthrow new Error(\n\"Please set the COHERE_API_KEY environment variable or pass it to the constructor as the apiKey field.\"\n);\n}\n\nthis.apiKey = apiKey;\nthis.maxTokens = fields?.maxTokens ?? this.maxTokens;\nthis.temperature = fields?.temperature ?? this.temperature;\nthis.model = fields?.model ?? this.model;\n}\n\n_llmType() {\nreturn \"cohere\";\n}\n\n/** @ignore */\nasync _call(prompt: string, _stop?: string[]): Promise<string> {\nconst { cohere } = await Cohere.imports();\n\ncohere.init(this.apiKey);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/cohere.ts","loc":{"lines":{"from":91,"to":129}}}}],["589",{"pageContent":"// Hit the `generate` endpoint on the `large` model\nconst generateResponse = await this.caller.call(\ncohere.generate.bind(cohere),\n{\nprompt,\nmodel: this.model,\nmax_tokens: this.maxTokens,\ntemperature: this.temperature,\n}\n);\ntry {\nreturn generateResponse.body.generations[0].text;\n} catch {\nconsole.log(generateResponse);\nthrow new Error(\"Could not parse response.\");\n}\n}\n\n/** @ignore */\nstatic async imports(): Promise<{\ncohere: typeof import(\"cohere-ai\");\n}> {\ntry {\nconst { default: cohere } = await import(\"cohere-ai\");\nreturn { cohere };\n} catch (e) {\nthrow new Error(\n\"Please install cohere-ai as a dependency with, e.g. `yarn add cohere-ai`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/cohere.ts","loc":{"lines":{"from":180,"to":211}}}}],["590",{"pageContent":"import { LLM, BaseLLMParams } from \"./base.js\";\n\nexport interface HFInput {\n/** Model to use */\nmodel: string;\n\n/** Sampling temperature to use */\ntemperature?: number;\n\n/**\n* Maximum number of tokens to generate in the completion.\n*/\nmaxTokens?: number;\n\n/** Total probability mass of tokens to consider at each step */\ntopP?: number;\n\n/** Integer to define the top tokens considered within the sample operation to create new text. */\ntopK?: number;\n\n/** Penalizes repeated tokens according to frequency */\nfrequencyPenalty?: number;\n\n/** API key to use. */\napiKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/hf.ts","loc":{"lines":{"from":1,"to":26}}}}],["591",{"pageContent":"class HuggingFaceInference extends LLM implements HFInput {\nmodel = \"gpt2\";\n\ntemperature: number | undefined = undefined;\n\nmaxTokens: number | undefined = undefined;\n\ntopP: number | undefined = undefined;\n\ntopK: number | undefined = undefined;\n\nfrequencyPenalty: number | undefined = undefined;\n\napiKey: string | undefined = undefined;\n\nconstructor(fields?: Partial<HFInput> & BaseLLMParams) {\nsuper(fields ?? {});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/hf.ts","loc":{"lines":{"from":104,"to":120}}}}],["592",{"pageContent":"this.model = fields?.model ?? this.model;\nthis.temperature = fields?.temperature ?? this.temperature;\nthis.maxTokens = fields?.maxTokens ?? this.maxTokens;\nthis.topP = fields?.topP ?? this.topP;\nthis.topK = fields?.topK ?? this.topK;\nthis.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\nthis.apiKey =\nfields?.apiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.HUGGINGFACEHUB_API_KEY\n: undefined);\nif (!this.apiKey) {\nthrow new Error(\n\"Please set an API key for HuggingFace Hub in the environment variable HUGGINGFACEHUB_API_KEY or in the apiKey field of the HuggingFaceInference constructor.\"\n);\n}\n}\n\n_llmType() {\nreturn \"huggingface_hub\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/hf.ts","loc":{"lines":{"from":204,"to":225}}}}],["593",{"pageContent":"/** @ignore */\nasync _call(prompt: string, _stop?: string[]): Promise<string> {\nconst { HfInference } = await HuggingFaceInference.imports();\nconst hf = new HfInference(this.apiKey);\nconst res = await this.caller.call(hf.textGeneration.bind(hf), {\nmodel: this.model,\nparameters: {\n// make it behave similar to openai, returning only the generated text\nreturn_full_text: false,\ntemperature: this.temperature,\nmax_new_tokens: this.maxTokens,\ntop_p: this.topP,\ntop_k: this.topK,\nrepetition_penalty: this.frequencyPenalty,\n},\ninputs: prompt,\n});\nreturn res.generated_text;\n}\n\n/** @ignore */\nstatic async imports(): Promise<{\nHfInference: typeof import(\"@huggingface/inference\").HfInference;\n}> {\ntry {\nconst { HfInference } = await import(\"@huggingface/inference\");\nreturn { HfInference };\n} catch (e) {\nthrow new Error(\n\"Please install huggingface as a dependency with, e.g. `yarn add @huggingface/inference`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/hf.ts","loc":{"lines":{"from":296,"to":329}}}}],["594",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/llms' is deprecated. Import from eg. 'langchain/llms/openai' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport { BaseLLM, BaseLLMParams, LLM, SerializedLLM } from \"./base.js\";\nexport { OpenAI, PromptLayerOpenAI } from \"./openai.js\";\nexport { OpenAIChat } from \"./openai-chat.js\";\nexport { Cohere } from \"./cohere.js\";\nexport { HuggingFaceInference } from \"./hf.js\";\nexport { Replicate } from \"./replicate.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/index.ts","loc":{"lines":{"from":1,"to":10}}}}],["595",{"pageContent":"import { FileLoader, loadFromFile } from \"../util/load.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { parseFileConfig } from \"../util/parse.js\";\n\n/**\n* Load an LLM from a local file.\n*\n* @example\n* ```ts\n* import { loadLLM } from \"langchain/llms/load\";\n* const model = await loadLLM(\"/path/to/llm.json\");\n* ```\n*/\nconst loader: FileLoader<BaseLanguageModel> = (file: string, path: string) =>\nBaseLanguageModel.deserialize(parseFileConfig(file, path));\n\nexport const loadLLM = (uri: string): Promise<BaseLanguageModel> =>\nloadFromFile(uri, loader);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/load.ts","loc":{"lines":{"from":1,"to":18}}}}],["596",{"pageContent":"import {\nConfiguration,\nOpenAIApi,\nChatCompletionRequestMessage,\nCreateChatCompletionRequest,\nConfigurationParameters,\nChatCompletionResponseMessageRoleEnum,\nCreateChatCompletionResponse,\n} from \"openai\";\nimport type { AxiosRequestConfig } from \"axios\";\nimport type { StreamingAxiosConfiguration } from \"../util/axios-types.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { BaseLLMCallOptions, BaseLLMParams, LLM } from \"./base.js\";\nimport { CallbackManagerForLLMRun } from \"../callbacks/manager.js\";\n\n/**\n* Input to OpenAI class.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":1,"to":18}}}}],["597",{"pageContent":"interface OpenAIChatInput {\n/** Sampling temperature to use, between 0 and 2, defaults to 1 */\ntemperature: number;\n\n/** Total probability mass of tokens to consider at each step, between 0 and 1, defaults to 1 */\ntopP: number;\n\n/** Penalizes repeated tokens according to frequency */\nfrequencyPenalty: number;\n\n/** Penalizes repeated tokens */\npresencePenalty: number;\n\n/** Number of chat completions to generate for each prompt */\nn: number;\n\n/** Dictionary used to adjust the probability of specific tokens being generated */\nlogitBias?: Record<string, number>;\n\n/** Whether to stream the results or not */\nstreaming: boolean;\n\n/** Model name to use */\nmodelName: string;\n\n/** ChatGPT messages to pass as a prefix to the prompt */\nprefixMessages?: ChatCompletionRequestMessage[];\n\n/** Holds any additional parameters that are valid to pass to {@link\n* https://platform.openai.com/docs/api-reference/completions/create |\n* `openai.create`} that are not explicitly specified on this class.\n*/\nmodelKwargs?: Kwargs;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":356,"to":388}}}}],["598",{"pageContent":"/** List of stop words to use when generating */\nstop?: string[];\n\n/**\n* Timeout to use when making requests to OpenAI.\n*/\ntimeout?: number;\n\n/**\n* Maximum number of tokens to generate in the completion.  If not specified,\n* defaults to the maximum number of tokens allowed by the model.\n*/\nmaxTokens?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":711,"to":724}}}}],["599",{"pageContent":"interface OpenAIChatCallOptions extends BaseLLMCallOptions {\n/**\n* List of stop words to use when generating\n*/\nstop?: string[];\n\n/**\n* Additional options to pass to the underlying axios request.\n*/\noptions?: AxiosRequestConfig;\n}\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ntype Kwargs = Record<string, any>;\n\n/**\n* Wrapper around OpenAI large language models that use the Chat endpoint.\n*\n* To use you should have the `openai` package installed, with the\n* `OPENAI_API_KEY` environment variable set.\n*\n* @remarks\n* Any parameters that are valid to be passed to {@link\n* https://platform.openai.com/docs/api-reference/chat/create |\n* `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n* if not explicitly available on this class.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":1068,"to":1094}}}}],["600",{"pageContent":"class OpenAIChat extends LLM implements OpenAIChatInput {\ndeclare CallOptions: OpenAIChatCallOptions;\n\ntemperature = 1;\n\ntopP = 1;\n\nfrequencyPenalty = 0;\n\npresencePenalty = 0;\n\nn = 1;\n\nlogitBias?: Record<string, number>;\n\nmaxTokens?: number;\n\nmodelName = \"gpt-3.5-turbo\";\n\nprefixMessages?: ChatCompletionRequestMessage[];\n\nmodelKwargs?: Kwargs;\n\ntimeout?: number;\n\nstop?: string[];\n\nstreaming = false;\n\nprivate client: OpenAIApi;\n\nprivate clientConfig: ConfigurationParameters;\n\nconstructor(\nfields?: Partial<OpenAIChatInput> &\nBaseLLMParams & {\nopenAIApiKey?: string;\n},\nconfiguration?: ConfigurationParameters\n) {\nsuper(fields ?? {});\n\nconst apiKey =\nfields?.openAIApiKey ??\n// eslint-disable-next-line no-process-env\n(typeof process !== \"undefined\" ? process.env.OPENAI_API_KEY : undefined);\nif (!apiKey) {\nthrow new Error(\"OpenAI API key not found\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":1426,"to":1474}}}}],["601",{"pageContent":"this.modelName = fields?.modelName ?? this.modelName;\nthis.prefixMessages = fields?.prefixMessages ?? this.prefixMessages;\nthis.modelKwargs = fields?.modelKwargs ?? {};\nthis.timeout = fields?.timeout;\n\nthis.temperature = fields?.temperature ?? this.temperature;\nthis.topP = fields?.topP ?? this.topP;\nthis.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\nthis.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\nthis.n = fields?.n ?? this.n;\nthis.logitBias = fields?.logitBias;\nthis.maxTokens = fields?.maxTokens;\nthis.stop = fields?.stop;\n\nthis.streaming = fields?.streaming ?? false;\n\nif (this.streaming && this.n > 1) {\nthrow new Error(\"Cannot stream results when n > 1\");\n}\n\nthis.clientConfig = {\napiKey,\n...configuration,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":1803,"to":1827}}}}],["602",{"pageContent":"/**\n* Get the parameters used to invoke the model\n*/\ninvocationParams(): Omit<CreateChatCompletionRequest, \"messages\"> & Kwargs {\nreturn {\nmodel: this.modelName,\ntemperature: this.temperature,\ntop_p: this.topP,\nfrequency_penalty: this.frequencyPenalty,\npresence_penalty: this.presencePenalty,\nn: this.n,\nlogit_bias: this.logitBias,\nmax_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\nstop: this.stop,\nstream: this.streaming,\n...this.modelKwargs,\n};\n}\n\n/** @ignore */\n_identifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n...this.clientConfig,\n};\n}\n\n/**\n* Get the identifying parameters for the model\n*/\nidentifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n...this.clientConfig,\n};\n}\n\nprivate formatMessages(prompt: string): ChatCompletionRequestMessage[] {\nconst message: ChatCompletionRequestMessage = {\nrole: \"user\",\ncontent: prompt,\n};\nreturn this.prefixMessages ? [...this.prefixMessages, message] : [message];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":2159,"to":2204}}}}],["603",{"pageContent":"/** @ignore */\nasync _call(\nprompt: string,\nstopOrOptions?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<string> {\nconst stop = Array.isArray(stopOrOptions)\n? stopOrOptions\n: stopOrOptions?.stop;\nconst options = Array.isArray(stopOrOptions)\n? {}\n: stopOrOptions?.options ?? {};\n\nif (this.stop && stop) {\nthrow new Error(\"Stop found in input and default params\");\n}\n\nconst params = this.invocationParams();\nparams.stop = stop ?? params.stop;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":2527,"to":2545}}}}],["604",{"pageContent":"const data = params.stream\n? await new Promise<CreateChatCompletionResponse>((resolve, reject) => {\nlet response: CreateChatCompletionResponse;\nlet rejected = false;\nthis.completionWithRetry(\n{\n...params,\nmessages: this.formatMessages(prompt),\n},\n{\n...options,\nresponseType: \"stream\",\nonmessage: (event) => {\nif (event.data?.trim?.() === \"[DONE]\") {\nresolve(response);\n} else {\nconst message = JSON.parse(event.data) as {\nid: string;\nobject: string;\ncreated: number;\nmodel: string;\nchoices: Array<{\nindex: number;\nfinish_reason: string | null;\ndelta: { content?: string; role?: string };\n}>;\n};\n\n// on the first message set the response properties\nif (!response) {\nresponse = {\nid: message.id,\nobject: message.object,\ncreated: message.created,\nmodel: message.model,\nchoices: [],\n};\n}\n\n// on all messages, update choice\nconst part = message.choices[0];\nif (part != null) {\nlet choice = response.choices.find(\n(c) => c.index === part.index\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":2886,"to":2930}}}}],["605",{"pageContent":"if (!choice) {\nchoice = {\nindex: part.index,\nfinish_reason: part.finish_reason ?? undefined,\n};\nresponse.choices.push(choice);\n}\n\nif (!choice.message) {\nchoice.message = {\nrole: part.delta\n?.role as ChatCompletionResponseMessageRoleEnum,\ncontent: part.delta?.content ?? \"\",\n};\n}\n\nchoice.message.content += part.delta?.content ?? \"\";\n// eslint-disable-next-line no-void\nvoid runManager?.handleLLMNewToken(\npart.delta?.content ?? \"\"\n);\n}\n}\n},\n}\n).catch((error) => {\nif (!rejected) {\nrejected = true;\nreject(error);\n}\n});\n})\n: await this.completionWithRetry(\n{\n...params,\nmessages: this.formatMessages(prompt),\n},\noptions\n);\n\nreturn data.choices[0].message?.content ?? \"\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":3255,"to":3296}}}}],["606",{"pageContent":"/** @ignore */\nasync completionWithRetry(\nrequest: CreateChatCompletionRequest,\noptions?: StreamingAxiosConfiguration\n) {\nif (!this.client) {\nconst clientConfig = new Configuration({\n...this.clientConfig,\nbaseOptions: {\ntimeout: this.timeout,\nadapter: fetchAdapter,\n...this.clientConfig.baseOptions,\n},\n});\nthis.client = new OpenAIApi(clientConfig);\n}\nreturn this.caller\n.call(\nthis.client.createChatCompletion.bind(this.client),\nrequest,\noptions\n)\n.then((res) => res.data);\n}\n\n_llmType() {\nreturn \"openai\";\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai-chat.ts","loc":{"lines":{"from":3631,"to":3659}}}}],["607",{"pageContent":"import { TiktokenModel } from \"@dqbd/tiktoken\";\nimport {\nConfiguration,\nConfigurationParameters,\nCreateCompletionRequest,\nCreateCompletionResponse,\nCreateCompletionResponseChoicesInner,\nOpenAIApi,\n} from \"openai\";\nimport type { AxiosRequestConfig } from \"axios\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport type { StreamingAxiosConfiguration } from \"../util/axios-types.js\";\nimport { chunkArray } from \"../util/chunk.js\";\nimport { BaseLLM, BaseLLMCallOptions, BaseLLMParams } from \"./base.js\";\nimport { calculateMaxTokens } from \"../base_language/count_tokens.js\";\nimport { OpenAIChat } from \"./openai-chat.js\";\nimport { LLMResult } from \"../schema/index.js\";\nimport { CallbackManagerForLLMRun } from \"../callbacks/manager.js\";\n\n/**\n* Input to OpenAI class.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":1,"to":22}}}}],["608",{"pageContent":"interface OpenAIInput {\n/** Sampling temperature to use */\ntemperature: number;\n\n/**\n* Maximum number of tokens to generate in the completion. -1 returns as many\n* tokens as possible given the prompt and the model's maximum context size.\n*/\nmaxTokens: number;\n\n/** Total probability mass of tokens to consider at each step */\ntopP: number;\n\n/** Penalizes repeated tokens according to frequency */\nfrequencyPenalty: number;\n\n/** Penalizes repeated tokens */\npresencePenalty: number;\n\n/** Number of completions to generate for each prompt */\nn: number;\n\n/** Generates `bestOf` completions server side and returns the \"best\" */\nbestOf: number;\n\n/** Dictionary used to adjust the probability of specific tokens being generated */\nlogitBias?: Record<string, number>;\n\n/** Whether to stream the results or not. Enabling disables tokenUsage reporting */\nstreaming: boolean;\n\n/** Model name to use */\nmodelName: string;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":482,"to":514}}}}],["609",{"pageContent":"/** Holds any additional parameters that are valid to pass to {@link\n* https://platform.openai.com/docs/api-reference/completions/create |\n* `openai.createCompletion`} that are not explicitly specified on this class.\n*/\nmodelKwargs?: Kwargs;\n\n/** Batch size to use when passing multiple documents to generate */\nbatchSize: number;\n\n/** List of stop words to use when generating */\nstop?: string[];\n\n/**\n* Timeout to use when making requests to OpenAI.\n*/\ntimeout?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":967,"to":983}}}}],["610",{"pageContent":"interface OpenAICallOptions extends BaseLLMCallOptions {\n/**\n* List of stop words to use when generating\n*/\nstop?: string[];\n\n/**\n* Additional options to pass to the underlying axios request.\n*/\noptions?: AxiosRequestConfig;\n}\n\ninterface TokenUsage {\ncompletionTokens?: number;\npromptTokens?: number;\ntotalTokens?: number;\n}\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ntype Kwargs = Record<string, any>;\n\n/**\n* Wrapper around OpenAI large language models.\n*\n* To use you should have the `openai` package installed, with the\n* `OPENAI_API_KEY` environment variable set.\n*\n* @remarks\n* Any parameters that are valid to be passed to {@link\n* https://platform.openai.com/docs/api-reference/completions/create |\n* `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n* if not explicitly available on this class.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":1450,"to":1482}}}}],["611",{"pageContent":"class OpenAI extends BaseLLM implements OpenAIInput {\ndeclare CallOptions: OpenAICallOptions;\n\ntemperature = 0.7;\n\nmaxTokens = 256;\n\ntopP = 1;\n\nfrequencyPenalty = 0;\n\npresencePenalty = 0;\n\nn = 1;\n\nbestOf = 1;\n\nlogitBias?: Record<string, number>;\n\nmodelName = \"text-davinci-003\";\n\nmodelKwargs?: Kwargs;\n\nbatchSize = 20;\n\ntimeout?: number;\n\nstop?: string[];\n\nstreaming = false;\n\nprivate client: OpenAIApi;\n\nprivate clientConfig: ConfigurationParameters;\n\nconstructor(\nfields?: Partial<OpenAIInput> &\nBaseLLMParams & {\nopenAIApiKey?: string;\n},\nconfiguration?: ConfigurationParameters\n) {\nif (\nfields?.modelName?.startsWith(\"gpt-3.5-turbo\") ||\nfields?.modelName?.startsWith(\"gpt-4\")\n) {\n// eslint-disable-next-line no-constructor-return, @typescript-eslint/no-explicit-any\nreturn new OpenAIChat(fields, configuration) as any as OpenAI;\n}\nsuper(fields ?? {});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":1939,"to":1988}}}}],["612",{"pageContent":"const apiKey =\nfields?.openAIApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.OPENAI_API_KEY\n: undefined);\nif (!apiKey) {\nthrow new Error(\"OpenAI API key not found\");\n}\n\nthis.modelName = fields?.modelName ?? this.modelName;\nthis.modelKwargs = fields?.modelKwargs ?? {};\nthis.batchSize = fields?.batchSize ?? this.batchSize;\nthis.timeout = fields?.timeout;\n\nthis.temperature = fields?.temperature ?? this.temperature;\nthis.maxTokens = fields?.maxTokens ?? this.maxTokens;\nthis.topP = fields?.topP ?? this.topP;\nthis.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\nthis.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\nthis.n = fields?.n ?? this.n;\nthis.bestOf = fields?.bestOf ?? this.bestOf;\nthis.logitBias = fields?.logitBias;\nthis.stop = fields?.stop;\n\nthis.streaming = fields?.streaming ?? false;\n\nif (this.streaming && this.n > 1) {\nthrow new Error(\"Cannot stream results when n > 1\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":2445,"to":2474}}}}],["613",{"pageContent":"if (this.streaming && this.bestOf > 1) {\nthrow new Error(\"Cannot stream results when bestOf > 1\");\n}\n\nthis.clientConfig = {\napiKey,\n...configuration,\n};\n}\n\n/**\n* Get the parameters used to invoke the model\n*/\ninvocationParams(): CreateCompletionRequest & Kwargs {\nreturn {\nmodel: this.modelName,\ntemperature: this.temperature,\nmax_tokens: this.maxTokens,\ntop_p: this.topP,\nfrequency_penalty: this.frequencyPenalty,\npresence_penalty: this.presencePenalty,\nn: this.n,\nbest_of: this.bestOf,\nlogit_bias: this.logitBias,\nstop: this.stop,\nstream: this.streaming,\n...this.modelKwargs,\n};\n}\n\n_identifyingParams() {\nreturn {\nmodel_name: this.modelName,\n...this.invocationParams(),\n...this.clientConfig,\n};\n}\n\n/**\n* Get the identifying parameters for the model\n*/\nidentifyingParams() {\nreturn this._identifyingParams();\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":2926,"to":2969}}}}],["614",{"pageContent":"/**\n* Call out to OpenAI's endpoint with k unique prompts\n*\n* @param prompts - The prompts to pass into the model.\n* @param [stop] - Optional list of stop words to use when generating.\n* @param [runManager] - Optional callback manager to use when generating.\n*\n* @returns The full LLM output.\n*\n* @example\n* ```ts\n* import { OpenAI } from \"langchain/llms/openai\";\n* const openai = new OpenAI();\n* const response = await openai.generate([\"Tell me a joke.\"]);\n* ```\n*/\nasync _generate(\nprompts: string[],\nstopOrOptions?: string[] | this[\"CallOptions\"],\nrunManager?: CallbackManagerForLLMRun\n): Promise<LLMResult> {\nconst stop = Array.isArray(stopOrOptions)\n? stopOrOptions\n: stopOrOptions?.stop;\nconst options = Array.isArray(stopOrOptions)\n? {}\n: stopOrOptions?.options ?? {};\nconst subPrompts = chunkArray(prompts, this.batchSize);\nconst choices: CreateCompletionResponseChoicesInner[] = [];\nconst tokenUsage: TokenUsage = {};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":3427,"to":3456}}}}],["615",{"pageContent":"if (this.stop && stop) {\nthrow new Error(\"Stop found in input and default params\");\n}\n\nconst params = this.invocationParams();\nparams.stop = stop ?? params.stop;\n\nif (params.max_tokens === -1) {\nif (prompts.length !== 1) {\nthrow new Error(\n\"max_tokens set to -1 not supported for multiple inputs\"\n);\n}\nparams.max_tokens = await calculateMaxTokens({\nprompt: prompts[0],\n// Cast here to allow for other models that may not fit the union\nmodelName: this.modelName as TiktokenModel,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":3909,"to":3927}}}}],["616",{"pageContent":"for (let i = 0; i < subPrompts.length; i += 1) {\nconst data = params.stream\n? await new Promise<CreateCompletionResponse>((resolve, reject) => {\nconst choice: CreateCompletionResponseChoicesInner = {};\nlet response: Omit<CreateCompletionResponse, \"choices\">;\nlet rejected = false;\nthis.completionWithRetry(\n{\n...params,\nprompt: subPrompts[i],\n},\n{\n...options,\nresponseType: \"stream\",\nonmessage: (event) => {\nif (event.data?.trim?.() === \"[DONE]\") {\nresolve({\n...response,\nchoices: [choice],\n});\n} else {\nconst message = JSON.parse(event.data) as Omit<\nCreateCompletionResponse,\n\"usage\"\n>;\n\n// on the first message set the response properties\nif (!response) {\nresponse = {\nid: message.id,\nobject: message.object,\ncreated: message.created,\nmodel: message.model,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":4394,"to":4428}}}}],["617",{"pageContent":"// on all messages, update choice\nconst part = message.choices[0];\nif (part != null) {\nchoice.text = (choice.text ?? \"\") + (part.text ?? \"\");\nchoice.finish_reason = part.finish_reason;\nchoice.logprobs = part.logprobs;\n// eslint-disable-next-line no-void\nvoid runManager?.handleLLMNewToken(part.text ?? \"\");\n}\n}\n},\n}\n).catch((error) => {\nif (!rejected) {\nrejected = true;\nreject(error);\n}\n});\n})\n: await this.completionWithRetry(\n{\n...params,\nprompt: subPrompts[i],\n},\noptions\n);\n\nchoices.push(...data.choices);\n\nconst {\ncompletion_tokens: completionTokens,\nprompt_tokens: promptTokens,\ntotal_tokens: totalTokens,\n} = data.usage ?? {};\n\nif (completionTokens) {\ntokenUsage.completionTokens =\n(tokenUsage.completionTokens ?? 0) + completionTokens;\n}\n\nif (promptTokens) {\ntokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n}\n\nif (totalTokens) {\ntokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":4889,"to":4936}}}}],["618",{"pageContent":"const generations = chunkArray(choices, this.n).map((promptChoices) =>\npromptChoices.map((choice) => ({\ntext: choice.text ?? \"\",\ngenerationInfo: {\nfinishReason: choice.finish_reason,\nlogprobs: choice.logprobs,\n},\n}))\n);\nreturn {\ngenerations,\nllmOutput: { tokenUsage },\n};\n}\n\n/** @ignore */\nasync completionWithRetry(\nrequest: CreateCompletionRequest,\noptions?: StreamingAxiosConfiguration\n) {\nif (!this.client) {\nconst clientConfig = new Configuration({\n...this.clientConfig,\nbaseOptions: {\ntimeout: this.timeout,\nadapter: fetchAdapter,\n...this.clientConfig.baseOptions,\n},\n});\nthis.client = new OpenAIApi(clientConfig);\n}\nreturn this.caller\n.call(this.client.createCompletion.bind(this.client), request, options)\n.then((res) => res.data);\n}\n\n_llmType() {\nreturn \"openai\";\n}\n}\n\n/**\n* PromptLayer wrapper to OpenAI\n* @augments OpenAI\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":5389,"to":5433}}}}],["619",{"pageContent":"class PromptLayerOpenAI extends OpenAI {\npromptLayerApiKey?: string;\n\nplTags?: string[];\n\nconstructor(\nfields?: ConstructorParameters<typeof OpenAI>[0] & {\npromptLayerApiKey?: string;\nplTags?: string[];\n}\n) {\nsuper(fields);\n\nthis.plTags = fields?.plTags ?? [];\nthis.promptLayerApiKey =\nfields?.promptLayerApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.PROMPTLAYER_API_KEY\n: undefined);\n\nif (!this.promptLayerApiKey) {\nthrow new Error(\"Missing PromptLayer API key\");\n}\n}\n\nasync completionWithRetry(\nrequest: CreateCompletionRequest,\noptions?: StreamingAxiosConfiguration\n) {\nif (request.stream) {\nreturn super.completionWithRetry(request, options);\n}\n\nconst requestStartTime = Date.now();\nconst response = await super.completionWithRetry(request);\nconst requestEndTime = Date.now();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":5890,"to":5926}}}}],["620",{"pageContent":"// https://github.com/MagnivOrg/promptlayer-js-helper\nawait this.caller.call(fetch, \"https://api.promptlayer.com/track-request\", {\nmethod: \"POST\",\nheaders: {\n\"Content-Type\": \"application/json\",\nAccept: \"application/json\",\n},\nbody: JSON.stringify({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":6383,"to":6390}}}}],["621",{"pageContent":"_name: \"openai.Completion.create\",\nargs: [],\nkwargs: { engine: request.model, prompt: request.prompt },\ntags: this.plTags ?? [],\nrequest_response: response,\nrequest_start_time: Math.floor(requestStartTime / 1000),\nrequest_end_time: Math.floor(requestEndTime / 1000),\napi_key: this.promptLayerApiKey,\n}),\n});\n\nreturn response;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":6861,"to":6874}}}}],["622",{"pageContent":"{\nOpenAIChat,\nOpenAIChatInput,\nOpenAIChatCallOptions,\n} from \"./openai-chat.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/openai.ts","loc":{"lines":{"from":7344,"to":7348}}}}],["623",{"pageContent":"import { LLM, BaseLLMParams } from \"./base.js\";\n\nexport interface ReplicateInput {\n// owner/model_name:version\nmodel: `${string}/${string}:${string}`;\n\ninput?: {\n// different models accept different inputs\n[key: string]: string | number | boolean;\n};\n\napiKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/replicate.ts","loc":{"lines":{"from":1,"to":13}}}}],["624",{"pageContent":"class Replicate extends LLM implements ReplicateInput {\nmodel: ReplicateInput[\"model\"];\n\ninput: ReplicateInput[\"input\"];\n\napiKey: string;\n\nconstructor(fields: ReplicateInput & BaseLLMParams) {\nsuper(fields);\n\nconst apiKey =\nfields?.apiKey ??\n// eslint-disable-next-line no-process-env\n(typeof process !== \"undefined\" && process.env?.REPLICATE_API_KEY);\n\nif (!apiKey) {\nthrow new Error(\"Please set the REPLICATE_API_KEY environment variable\");\n}\n\nthis.apiKey = apiKey;\nthis.model = fields.model;\nthis.input = fields.input ?? {};\n}\n\n_llmType() {\nreturn \"replicate\";\n}\n\n/** @ignore */\nasync _call(prompt: string, _stop?: string[]): Promise<string> {\nconst imports = await Replicate.imports();\n\nconst replicate = new imports.Replicate({\nuserAgent: \"langchain\",\nauth: this.apiKey,\n});\n\nconst output = await this.caller.call(() =>\nreplicate.run(this.model, {\nwait: true,\ninput: {\n...this.input,\nprompt,\n},\n})\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/replicate.ts","loc":{"lines":{"from":81,"to":126}}}}],["625",{"pageContent":"// Note this is a little odd, but the output format is not consistent\n// across models, so it makes some amount of sense.\nreturn String(output);\n}\n\n/** @ignore */\nstatic async imports(): Promise<{\nReplicate: typeof import(\"replicate\").default;\n}> {\ntry {\nconst { default: Replicate } = await import(\"replicate\");\nreturn { Replicate };\n} catch (e) {\nthrow new Error(\n\"Please install replicate as a dependency with, e.g. `yarn add replicate`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/replicate.ts","loc":{"lines":{"from":167,"to":185}}}}],["626",{"pageContent":"import { test } from \"@jest/globals\";\nimport { Cohere } from \"../cohere.js\";\n\ntest(\"Test Cohere\", async () => {\nconst model = new Cohere({ maxTokens: 20 });\nconst res = await model.call(\"1 + 1 =\");\nconsole.log(res);\n}, 50000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/cohere.int.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["627",{"pageContent":"import { test } from \"@jest/globals\";\nimport { HuggingFaceInference } from \"../hf.js\";\n\ntest(\"Test HuggingFace\", async () => {\nconst model = new HuggingFaceInference({ temperature: 0.1, topP: 0.5 });\nconst res = await model.call(\"1 + 1 =\");\nconsole.log(res);\n}, 50000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/huggingface_hub.int.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["628",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { OpenAIChat } from \"../openai-chat.js\";\nimport { CallbackManager } from \"../../callbacks/index.js\";\n\ntest(\"Test OpenAI\", async () => {\nconst model = new OpenAIChat({ modelName: \"gpt-3.5-turbo\", maxTokens: 10 });\nconst res = await model.call(\"Print hello world\");\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with prefix messages\", async () => {\nconst model = new OpenAIChat({\nprefixMessages: [\n{ role: \"user\", content: \"My name is John\" },\n{ role: \"assistant\", content: \"Hi there\" },\n],\nmaxTokens: 10,\n});\nconst res = await model.call(\"What is my name\");\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI in streaming mode\", async () => {\nlet nrNewTokens = 0;\nlet streamedCompletion = \"\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai-chat.int.test.ts","loc":{"lines":{"from":1,"to":25}}}}],["629",{"pageContent":"const model = new OpenAIChat({\nmaxTokens: 10,\nmodelName: \"gpt-3.5-turbo\",\nstreaming: true,\ncallbackManager: CallbackManager.fromHandlers({\nasync handleLLMNewToken(token: string) {\nnrNewTokens += 1;\nstreamedCompletion += token;\n},\n}),\n});\nconst res = await model.call(\"Print hello world\");\nconsole.log({ res });\n\nexpect(nrNewTokens > 0).toBe(true);\nexpect(res).toBe(streamedCompletion);\n}, 30000);\n\ntest(\"Test OpenAI with stop\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nconst res = await model.call(\"Print hello world\", [\"world\"]);\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with stop in object\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nconst res = await model.call(\"Print hello world\", { stop: [\"world\"] });\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with timeout in call options\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nawait expect(() =>\nmodel.call(\"Print hello world\", {\noptions: { timeout: 10 },\n})\n).rejects.toThrow();\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai-chat.int.test.ts","loc":{"lines":{"from":102,"to":139}}}}],["630",{"pageContent":"test(\"Test OpenAI with timeout in call options and node adapter\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nawait expect(() =>\nmodel.call(\"Print hello world\", {\noptions: { timeout: 10, adapter: undefined },\n})\n).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with signal in call options\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call(\"Print hello world\", {\noptions: { signal: controller.signal },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with signal in call options and node adapter\", async () => {\nconst model = new OpenAIChat({ maxTokens: 5 });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call(\"Print hello world\", {\noptions: { signal: controller.signal, adapter: undefined },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai-chat.int.test.ts","loc":{"lines":{"from":207,"to":242}}}}],["631",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { LLMResult } from \"../../schema/index.js\";\nimport { OpenAIChat } from \"../openai-chat.js\";\nimport { OpenAI } from \"../openai.js\";\nimport { StringPromptValue } from \"../../prompts/index.js\";\nimport { CallbackManager } from \"../../callbacks/index.js\";\n\ntest(\"Test OpenAI\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst res = await model.call(\"Print hello world\");\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with stop\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst res = await model.call(\"Print hello world\", [\"world\"]);\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with stop in object\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst res = await model.call(\"Print hello world\", { stop: [\"world\"] });\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai.int.test.ts","loc":{"lines":{"from":1,"to":24}}}}],["632",{"pageContent":"test(\"Test OpenAI with timeout in call options\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nawait expect(() =>\nmodel.call(\"Print hello world\", {\noptions: { timeout: 10 },\n})\n).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with timeout in call options and node adapter\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nawait expect(() =>\nmodel.call(\"Print hello world\", {\noptions: { timeout: 10, adapter: undefined },\n})\n).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with signal in call options\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call(\"Print hello world\", {\noptions: { signal: controller.signal },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai.int.test.ts","loc":{"lines":{"from":157,"to":187}}}}],["633",{"pageContent":"test(\"Test OpenAI with signal in call options and node adapter\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst controller = new AbortController();\nawait expect(() => {\nconst ret = model.call(\"Print hello world\", {\noptions: { signal: controller.signal, adapter: undefined },\n});\n\ncontroller.abort();\n\nreturn ret;\n}).rejects.toThrow();\n}, 5000);\n\ntest(\"Test OpenAI with concurrency == 1\", async () => {\nconst model = new OpenAI({\nmaxTokens: 5,\nmodelName: \"text-ada-001\",\nmaxConcurrency: 1,\n});\nconst res = await Promise.all([\nmodel.call(\"Print hello world\"),\nmodel.call(\"Print hello world\"),\n]);\nconsole.log({ res });\n});\n\ntest(\"Test OpenAI with maxTokens -1\", async () => {\nconst model = new OpenAI({ maxTokens: -1, modelName: \"text-ada-001\" });\nconst res = await model.call(\"Print hello world\", [\"world\"]);\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai.int.test.ts","loc":{"lines":{"from":321,"to":352}}}}],["634",{"pageContent":"test(\"Test OpenAI with chat model returns OpenAIChat\", async () => {\nconst model = new OpenAI({ modelName: \"gpt-3.5-turbo\" });\nexpect(model).toBeInstanceOf(OpenAIChat);\nconst res = await model.call(\"Print hello world\");\nconsole.log({ res });\nexpect(typeof res).toBe(\"string\");\n});\n\ntest(\"Test ChatOpenAI tokenUsage\", async () => {\nlet tokenUsage = {\ncompletionTokens: 0,\npromptTokens: 0,\ntotalTokens: 0,\n};\n\nconst model = new OpenAI({\nmaxTokens: 5,\nmodelName: \"text-ada-001\",\ncallbackManager: CallbackManager.fromHandlers({\nasync handleLLMEnd(output: LLMResult) {\ntokenUsage = output.llmOutput?.tokenUsage;\n},\n}),\n});\nconst res = await model.call(\"Hello\");\nconsole.log({ res });\n\nexpect(tokenUsage.promptTokens).toBe(1);\n});\n\ntest(\"Test OpenAI in streaming mode\", async () => {\nlet nrNewTokens = 0;\nlet streamedCompletion = \"\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai.int.test.ts","loc":{"lines":{"from":486,"to":518}}}}],["635",{"pageContent":"const model = new OpenAI({\nmaxTokens: 5,\nmodelName: \"text-ada-001\",\nstreaming: true,\ncallbacks: CallbackManager.fromHandlers({\nasync handleLLMNewToken(token: string) {\nnrNewTokens += 1;\nstreamedCompletion += token;\n},\n}),\n});\nconst res = await model.call(\"Print hello world\");\nconsole.log({ res });\n\nexpect(nrNewTokens > 0).toBe(true);\nexpect(res).toBe(streamedCompletion);\n});\n\ntest(\"Test OpenAI prompt value\", async () => {\nconst model = new OpenAI({ maxTokens: 5, modelName: \"text-ada-001\" });\nconst res = await model.generatePrompt([\nnew StringPromptValue(\"Print hello world\"),\n]);\nexpect(res.generations.length).toBe(1);\nfor (const generation of res.generations) {\nexpect(generation.length).toBe(1);\nfor (const g of generation) {\nconsole.log(g.text);\n}\n}\nconsole.log({ res });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/openai.int.test.ts","loc":{"lines":{"from":653,"to":684}}}}],["636",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { Replicate } from \"../replicate.js\";\n\n// Test skipped because Replicate appears to be timing out often when called\ntest.skip(\"Test Replicate\", async () => {\nconst model = new Replicate({\nmodel:\n\"daanelson/flan-t5:04e422a9b85baed86a4f24981d7f9953e20c5fd82f6103b74ebc431588e1cec8\",\ninput: {\nmax_length: 10,\n},\n});\n\nconst res = await model.call(\"Hello, my name is \");\n\nexpect(typeof res).toBe(\"string\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/llms/tests/replicate.int.test.ts","loc":{"lines":{"from":1,"to":17}}}}],["637",{"pageContent":"import { BaseChatMessage, ChatMessage } from \"../schema/index.js\";\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type InputValues = Record<string, any>;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type OutputValues = Record<string, any>;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type MemoryVariables = Record<string, any>;\n\nexport abstract class BaseMemory {\nabstract get memoryKeys(): string[];\n\nabstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n\nabstract saveContext(\ninputValues: InputValues,\noutputValues: OutputValues\n): Promise<void>;\n}\n\n/**\n* This function is used by memory classes to select the input value\n* to use for the memory. If there is only one input value, it is used.\n* If there are multiple input values, the inputKey must be specified.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/base.ts","loc":{"lines":{"from":1,"to":24}}}}],["638",{"pageContent":"const getInputValue = (inputValues: InputValues, inputKey?: string) => {\nif (inputKey !== undefined) {\nreturn inputValues[inputKey];\n}\nconst keys = Object.keys(inputValues);\nif (keys.length === 1) {\nreturn inputValues[keys[0]];\n}\nthrow new Error(\n`input values have multiple keys, memory only supported when one key currently: ${keys}`\n);\n};\n\n/**\n* This function is used by memory classes to get a string representation\n* of the chat message history, based on the message content and role.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/base.ts","loc":{"lines":{"from":65,"to":81}}}}],["639",{"pageContent":"function getBufferString(\nmessages: BaseChatMessage[],\nhumanPrefix = \"Human\",\naiPrefix = \"AI\"\n): string {\nconst string_messages: string[] = [];\nfor (const m of messages) {\nlet role: string;\nif (m._getType() === \"human\") {\nrole = humanPrefix;\n} else if (m._getType() === \"ai\") {\nrole = aiPrefix;\n} else if (m._getType() === \"system\") {\nrole = \"System\";\n} else if (m._getType() === \"generic\") {\nrole = (m as ChatMessage).role;\n} else {\nthrow new Error(`Got unsupported message type: ${m}`);\n}\nstring_messages.push(`${role}: ${m.text}`);\n}\nreturn string_messages.join(\"\\n\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/base.ts","loc":{"lines":{"from":133,"to":155}}}}],["640",{"pageContent":"import { InputValues, MemoryVariables, getBufferString } from \"./base.js\";\nimport { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\n\nexport interface BufferMemoryInput extends BaseChatMemoryInput {\nhumanPrefix?: string;\naiPrefix?: string;\nmemoryKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/buffer_memory.ts","loc":{"lines":{"from":1,"to":8}}}}],["641",{"pageContent":"class BufferMemory extends BaseChatMemory implements BufferMemoryInput {\nhumanPrefix = \"Human\";\n\naiPrefix = \"AI\";\n\nmemoryKey = \"history\";\n\nconstructor(fields?: BufferMemoryInput) {\nsuper({\nchatHistory: fields?.chatHistory,\nreturnMessages: fields?.returnMessages ?? false,\ninputKey: fields?.inputKey,\noutputKey: fields?.outputKey,\n});\nthis.humanPrefix = fields?.humanPrefix ?? this.humanPrefix;\nthis.aiPrefix = fields?.aiPrefix ?? this.aiPrefix;\nthis.memoryKey = fields?.memoryKey ?? this.memoryKey;\n}\n\nget memoryKeys() {\nreturn [this.memoryKey];\n}\n\nasync loadMemoryVariables(_values: InputValues): Promise<MemoryVariables> {\nconst messages = await this.chatHistory.getMessages();\nif (this.returnMessages) {\nconst result = {\n[this.memoryKey]: messages,\n};\nreturn result;\n}\nconst result = {\n[this.memoryKey]: getBufferString(messages),\n};\nreturn result;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/buffer_memory.ts","loc":{"lines":{"from":47,"to":83}}}}],["642",{"pageContent":"import { InputValues, MemoryVariables, getBufferString } from \"./base.js\";\n\nimport { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\n\nexport interface BufferWindowMemoryInput extends BaseChatMemoryInput {\nhumanPrefix?: string;\naiPrefix?: string;\nmemoryKey?: string;\nk?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/buffer_window_memory.ts","loc":{"lines":{"from":1,"to":10}}}}],["643",{"pageContent":"class BufferWindowMemory\nextends BaseChatMemory\nimplements BufferWindowMemoryInput\n{\nhumanPrefix = \"Human\";\n\naiPrefix = \"AI\";\n\nmemoryKey = \"history\";\n\nk = 5;\n\nconstructor(fields?: BufferWindowMemoryInput) {\nsuper({\nreturnMessages: fields?.returnMessages ?? false,\nchatHistory: fields?.chatHistory,\ninputKey: fields?.inputKey,\noutputKey: fields?.outputKey,\n});\nthis.humanPrefix = fields?.humanPrefix ?? this.humanPrefix;\nthis.aiPrefix = fields?.aiPrefix ?? this.aiPrefix;\nthis.memoryKey = fields?.memoryKey ?? this.memoryKey;\nthis.k = fields?.k ?? this.k;\n}\n\nget memoryKeys() {\nreturn [this.memoryKey];\n}\n\nasync loadMemoryVariables(_values: InputValues): Promise<MemoryVariables> {\nconst messages = await this.chatHistory.getMessages();\nif (this.returnMessages) {\nconst result = {\n[this.memoryKey]: messages.slice(-this.k * 2),\n};\nreturn result;\n}\nconst result = {\n[this.memoryKey]: getBufferString(messages.slice(-this.k * 2)),\n};\nreturn result;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/buffer_window_memory.ts","loc":{"lines":{"from":55,"to":97}}}}],["644",{"pageContent":"import { BaseChatMessageHistory } from \"../schema/index.js\";\nimport {\nBaseMemory,\nInputValues,\nOutputValues,\ngetInputValue,\n} from \"./base.js\";\nimport { ChatMessageHistory } from \"../stores/message/in_memory.js\";\n\nexport interface BaseChatMemoryInput {\nchatHistory?: BaseChatMessageHistory;\nreturnMessages?: boolean;\ninputKey?: string;\noutputKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/chat_memory.ts","loc":{"lines":{"from":1,"to":15}}}}],["645",{"pageContent":"abstract class BaseChatMemory extends BaseMemory {\nchatHistory: BaseChatMessageHistory;\n\nreturnMessages = false;\n\ninputKey?: string;\n\noutputKey?: string;\n\nconstructor(fields?: BaseChatMemoryInput) {\nsuper();\nthis.chatHistory = fields?.chatHistory ?? new ChatMessageHistory();\nthis.returnMessages = fields?.returnMessages ?? this.returnMessages;\nthis.inputKey = fields?.inputKey ?? this.inputKey;\nthis.outputKey = fields?.outputKey ?? this.outputKey;\n}\n\nasync saveContext(\ninputValues: InputValues,\noutputValues: OutputValues\n): Promise<void> {\n// this is purposefully done in sequence so they're saved in order\nawait this.chatHistory.addUserMessage(\ngetInputValue(inputValues, this.inputKey)\n);\nawait this.chatHistory.addAIChatMessage(\ngetInputValue(outputValues, this.outputKey)\n);\n}\n\nasync clear(): Promise<void> {\nawait this.chatHistory.clear();\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/chat_memory.ts","loc":{"lines":{"from":51,"to":84}}}}],["646",{"pageContent":"export { BufferMemory, BufferMemoryInput } from \"./buffer_memory.js\";\nexport { BaseMemory, getInputValue, getBufferString } from \"./base.js\";\nexport {\nConversationSummaryMemory,\nConversationSummaryMemoryInput,\n} from \"./summary.js\";\nexport {\nBufferWindowMemory,\nBufferWindowMemoryInput,\n} from \"./buffer_window_memory.js\";\nexport { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\nexport { ChatMessageHistory } from \"../stores/message/in_memory.js\";\nexport { MotorheadMemory, MotorheadMemoryInput } from \"./motorhead_memory.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/index.ts","loc":{"lines":{"from":1,"to":13}}}}],["647",{"pageContent":"import { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\nimport {\nInputValues,\nOutputValues,\nMemoryVariables,\ngetBufferString,\ngetInputValue,\n} from \"./base.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../util/async_caller.js\";\n\nexport interface MotorheadMemoryMessage {\nrole: string;\ncontent: string;\n}\n\n/**\n* @interface\n*/\nexport type MotorheadMemoryInput = BaseChatMemoryInput &\nAsyncCallerParams & {\nsessionId: string;\nmotorheadURL?: string;\nmemoryKey?: string;\ntimeout?: number;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/motorhead_memory.ts","loc":{"lines":{"from":1,"to":25}}}}],["648",{"pageContent":"class MotorheadMemory extends BaseChatMemory {\nmotorheadURL = \"localhost:8080\";\n\ntimeout = 3000;\n\nmemoryKey = \"history\";\n\nsessionId: string;\n\ncontext?: string;\n\ncaller: AsyncCaller;\n\nconstructor(fields: MotorheadMemoryInput) {\nconst {\nsessionId,\nmotorheadURL,\nmemoryKey,\ntimeout,\nreturnMessages,\ninputKey,\noutputKey,\nchatHistory,\n...rest\n} = fields;\nsuper({ returnMessages, inputKey, outputKey, chatHistory });\n\nthis.caller = new AsyncCaller(rest);\nthis.sessionId = sessionId;\nthis.motorheadURL = motorheadURL ?? this.motorheadURL;\nthis.memoryKey = memoryKey ?? this.memoryKey;\nthis.timeout = timeout ?? this.timeout;\n}\n\nget memoryKeys() {\nreturn [this.memoryKey];\n}\n\nasync init(): Promise<void> {\nconst res = await this.caller.call(\nfetch,\n`${this.motorheadURL}/sessions/${this.sessionId}/memory`,\n{\nsignal: this.timeout ? AbortSignal.timeout(this.timeout) : undefined,\nheaders: {\n\"Content-Type\": \"application/json\",\n},\n}\n);\n\nconst { messages = [], context = \"NONE\" } = await res.json();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/motorhead_memory.ts","loc":{"lines":{"from":137,"to":187}}}}],["649",{"pageContent":"await Promise.all(\nmessages.map(async (message: MotorheadMemoryMessage) => {\nif (message.role === \"AI\") {\nawait this.chatHistory.addAIChatMessage(message.content);\n} else {\nawait this.chatHistory.addUserMessage(message.content);\n}\n})\n);\n\nif (context && context !== \"NONE\") {\nthis.context = context;\n}\n}\n\nasync loadMemoryVariables(_values: InputValues): Promise<MemoryVariables> {\nconst messages = await this.chatHistory.getMessages();\nif (this.returnMessages) {\nconst result = {\n[this.memoryKey]: messages,\n};\nreturn result;\n}\nconst result = {\n[this.memoryKey]: getBufferString(messages),\n};\nreturn result;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/motorhead_memory.ts","loc":{"lines":{"from":270,"to":297}}}}],["650",{"pageContent":"async saveContext(\ninputValues: InputValues,\noutputValues: OutputValues\n): Promise<void> {\nconst input = getInputValue(inputValues, this.inputKey);\nconst output = getInputValue(outputValues, this.outputKey);\nawait Promise.all([\nthis.caller.call(\nfetch,\n`${this.motorheadURL}/sessions/${this.sessionId}/memory`,\n{\nsignal: this.timeout ? AbortSignal.timeout(this.timeout) : undefined,\nmethod: \"POST\",\nbody: JSON.stringify({\nmessages: [\n{ role: \"Human\", content: `${input}` },\n{ role: \"AI\", content: `${output}` },\n],\n}),\nheaders: {\n\"Content-Type\": \"application/json\",\n},\n}\n),\nsuper.saveContext(inputValues, outputValues),\n]);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/motorhead_memory.ts","loc":{"lines":{"from":404,"to":431}}}}],["651",{"pageContent":"import { PromptTemplate } from \"../prompts/prompt.js\";\n\nconst _DEFAULT_SUMMARIZER_TEMPLATE = `Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n\nEXAMPLE\nCurrent summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n\nNew lines of conversation:\nHuman: Why do you think artificial intelligence is a force for good?\nAI: Because artificial intelligence will help humans reach their full potential.\n\nNew summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\nEND OF EXAMPLE\n\nCurrent summary:\n{summary}\n\nNew lines of conversation:\n{new_lines}\n\nNew summary:`;\n\n// eslint-disable-next-line spaced-comment","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/prompt.ts","loc":{"lines":{"from":1,"to":25}}}}],["652",{"pageContent":"const SUMMARY_PROMPT = /*#__PURE__*/ new PromptTemplate({\ninputVariables: [\"summary\", \"new_lines\"],\ntemplate: _DEFAULT_SUMMARIZER_TEMPLATE,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/prompt.ts","loc":{"lines":{"from":29,"to":32}}}}],["653",{"pageContent":"import { BaseLanguageModel } from \"../base_language/index.js\";\nimport { LLMChain } from \"../chains/llm_chain.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { BaseChatMessage, SystemChatMessage } from \"../schema/index.js\";\nimport {\ngetBufferString,\nInputValues,\nMemoryVariables,\nOutputValues,\n} from \"./base.js\";\nimport { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\nimport { SUMMARY_PROMPT } from \"./prompt.js\";\n\nexport interface ConversationSummaryMemoryInput extends BaseChatMemoryInput {\nllm: BaseLanguageModel;\nmemoryKey?: string;\nhumanPrefix?: string;\naiPrefix?: string;\nprompt?: BasePromptTemplate;\nsummaryChatMessageClass?: new (content: string) => BaseChatMessage;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/summary.ts","loc":{"lines":{"from":1,"to":21}}}}],["654",{"pageContent":"class ConversationSummaryMemory extends BaseChatMemory {\nbuffer = \"\";\n\nmemoryKey = \"history\";\n\nhumanPrefix = \"Human\";\n\naiPrefix = \"AI\";\n\nllm: BaseLanguageModel;\n\nprompt: BasePromptTemplate = SUMMARY_PROMPT;\n\nsummaryChatMessageClass: new (content: string) => BaseChatMessage =\nSystemChatMessage;\n\nconstructor(fields: ConversationSummaryMemoryInput) {\nconst {\nreturnMessages,\ninputKey,\noutputKey,\nchatHistory,\nhumanPrefix,\naiPrefix,\nllm,\nprompt,\nsummaryChatMessageClass,\n} = fields;\n\nsuper({ returnMessages, inputKey, outputKey, chatHistory });\n\nthis.memoryKey = fields?.memoryKey ?? this.memoryKey;\nthis.humanPrefix = humanPrefix ?? this.humanPrefix;\nthis.aiPrefix = aiPrefix ?? this.aiPrefix;\nthis.llm = llm;\nthis.prompt = prompt ?? this.prompt;\nthis.summaryChatMessageClass =\nsummaryChatMessageClass ?? this.summaryChatMessageClass;\n}\n\nget memoryKeys() {\nreturn [this.memoryKey];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/summary.ts","loc":{"lines":{"from":104,"to":146}}}}],["655",{"pageContent":"async predictNewSummary(\nmessages: BaseChatMessage[],\nexistingSummary: string\n): Promise<string> {\nconst newLines = getBufferString(messages, this.humanPrefix, this.aiPrefix);\nconst chain = new LLMChain({ llm: this.llm, prompt: this.prompt });\nreturn await chain.predict({\nsummary: existingSummary,\nnew_lines: newLines,\n});\n}\n\nasync loadMemoryVariables(_: InputValues): Promise<MemoryVariables> {\nif (this.returnMessages) {\nconst result = {\n[this.memoryKey]: [new this.summaryChatMessageClass(this.buffer)],\n};\nreturn result;\n}\nconst result = { [this.memoryKey]: this.buffer };\nreturn result;\n}\n\nasync saveContext(\ninputValues: InputValues,\noutputValues: OutputValues\n): Promise<void> {\nawait super.saveContext(inputValues, outputValues);\nconst messages = await this.chatHistory.getMessages();\nthis.buffer = await this.predictNewSummary(messages.slice(-2), this.buffer);\n}\n\nasync clear() {\nawait super.clear();\nthis.buffer = \"\";\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/summary.ts","loc":{"lines":{"from":219,"to":255}}}}],["656",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { BufferMemory } from \"../buffer_memory.js\";\nimport { ChatMessageHistory } from \"../../stores/message/in_memory.js\";\nimport { HumanChatMessage, AIChatMessage } from \"../../schema/index.js\";\n\ntest(\"Test buffer memory\", async () => {\nconst memory = new BufferMemory();\nconst result1 = await memory.loadMemoryVariables({});\nexpect(result1).toStrictEqual({ history: \"\" });\n\nawait memory.saveContext({ foo: \"bar\" }, { bar: \"foo\" });\nconst expectedString = \"Human: bar\\nAI: foo\";\nconst result2 = await memory.loadMemoryVariables({});\nexpect(result2).toStrictEqual({ history: expectedString });\n});\n\ntest(\"Test buffer memory return messages\", async () => {\nconst memory = new BufferMemory({ returnMessages: true });\nconst result1 = await memory.loadMemoryVariables({});\nexpect(result1).toStrictEqual({ history: [] });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/buffer_memory.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["657",{"pageContent":"await memory.saveContext({ foo: \"bar\" }, { bar: \"foo\" });\nconst expectedResult = [\nnew HumanChatMessage(\"bar\"),\nnew AIChatMessage(\"foo\"),\n];\nconst result2 = await memory.loadMemoryVariables({});\nexpect(result2).toStrictEqual({ history: expectedResult });\n});\n\ntest(\"Test buffer memory with pre-loaded history\", async () => {\nconst pastMessages = [\nnew HumanChatMessage(\"My name's Jonas\"),\nnew AIChatMessage(\"Nice to meet you, Jonas!\"),\n];\nconst memory = new BufferMemory({\nreturnMessages: true,\nchatHistory: new ChatMessageHistory(pastMessages),\n});\nconst result = await memory.loadMemoryVariables({});\nexpect(result).toStrictEqual({ history: pastMessages });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/buffer_memory.test.ts","loc":{"lines":{"from":42,"to":62}}}}],["658",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { BufferWindowMemory } from \"../buffer_window_memory.js\";\nimport { ChatMessageHistory } from \"../../stores/message/in_memory.js\";\nimport { HumanChatMessage, AIChatMessage } from \"../../schema/index.js\";\n\ntest(\"Test buffer window memory\", async () => {\nconst memory = new BufferWindowMemory({ k: 1 });\nconst result1 = await memory.loadMemoryVariables({});\nexpect(result1).toStrictEqual({ history: \"\" });\n\nawait memory.saveContext({ foo: \"bar\" }, { bar: \"foo\" });\nconst expectedString = \"Human: bar\\nAI: foo\";\nconst result2 = await memory.loadMemoryVariables({});\nexpect(result2).toStrictEqual({ history: expectedString });\n\nawait memory.saveContext({ foo: \"bar1\" }, { bar: \"foo\" });\nconst expectedString3 = \"Human: bar1\\nAI: foo\";\nconst result3 = await memory.loadMemoryVariables({});\nexpect(result3).toStrictEqual({ history: expectedString3 });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/buffer_window_memory.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["659",{"pageContent":"test(\"Test buffer window memory return messages\", async () => {\nconst memory = new BufferWindowMemory({ k: 1, returnMessages: true });\nconst result1 = await memory.loadMemoryVariables({});\nexpect(result1).toStrictEqual({ history: [] });\n\nawait memory.saveContext({ foo: \"bar\" }, { bar: \"foo\" });\nconst expectedResult = [\nnew HumanChatMessage(\"bar\"),\nnew AIChatMessage(\"foo\"),\n];\nconst result2 = await memory.loadMemoryVariables({});\nexpect(result2).toStrictEqual({ history: expectedResult });\n\nawait memory.saveContext({ foo: \"bar1\" }, { bar: \"foo\" });\nconst expectedResult2 = [\nnew HumanChatMessage(\"bar1\"),\nnew AIChatMessage(\"foo\"),\n];\nconst result3 = await memory.loadMemoryVariables({});\nexpect(result3).toStrictEqual({ history: expectedResult2 });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/buffer_window_memory.test.ts","loc":{"lines":{"from":56,"to":76}}}}],["660",{"pageContent":"test(\"Test buffer window memory with pre-loaded history\", async () => {\nconst pastMessages = [\nnew HumanChatMessage(\"My name's Jonas\"),\nnew AIChatMessage(\"Nice to meet you, Jonas!\"),\n];\nconst memory = new BufferWindowMemory({\nreturnMessages: true,\nchatHistory: new ChatMessageHistory(pastMessages),\n});\nconst result = await memory.loadMemoryVariables({});\nexpect(result).toStrictEqual({ history: pastMessages });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/buffer_window_memory.test.ts","loc":{"lines":{"from":114,"to":125}}}}],["661",{"pageContent":"import { test, expect, jest } from \"@jest/globals\";\nimport { MotorheadMemory } from \"../motorhead_memory.js\";\nimport { HumanChatMessage, AIChatMessage } from \"../../schema/index.js\";\n\ntest(\"Test motrhead memory\", async () => {\nglobal.fetch = jest.fn(() =>\nPromise.resolve({\njson: () =>\nPromise.resolve({\nmessages: [\n{ role: \"AI\", content: \"Ozzy Osbourne\" },\n{ role: \"Human\", content: \"Who is the best vocalist?\" },\n],\n}),\n} as Response)\n);\n\nconst memory = new MotorheadMemory({ sessionId: \"1\" });\nconst result1 = await memory.loadMemoryVariables({});\nexpect(result1).toStrictEqual({ history: \"\" });\n\nawait memory.saveContext(\n{ input: \"Who is the best vocalist?\" },\n{ response: \"Ozzy Osbourne\" }\n);\nconst expectedString = \"Human: Who is the best vocalist?\\nAI: Ozzy Osbourne\";\nconst result2 = await memory.loadMemoryVariables({});\nexpect(result2).toStrictEqual({ history: expectedString });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/motorhead_memory.test.ts","loc":{"lines":{"from":1,"to":29}}}}],["662",{"pageContent":"test(\"Test motrhead memory with pre-loaded history\", async () => {\nconst pastMessages = [\nnew AIChatMessage(\"Nice to meet you, Ozzy!\"),\nnew HumanChatMessage(\"My name is Ozzy\"),\n];\n\nglobal.fetch = jest.fn(() =>\nPromise.resolve({\njson: () =>\nPromise.resolve({\nmessages: [\n{ role: \"AI\", content: \"Nice to meet you, Ozzy!\" },\n{ role: \"Human\", content: \"My name is Ozzy\" },\n],\n}),\n} as Response)\n);\nconst memory = new MotorheadMemory({\nreturnMessages: true,\nsessionId: \"2\",\n});\nawait memory.init();\nconst result = await memory.loadMemoryVariables({});\nexpect(result).toStrictEqual({ history: pastMessages });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/motorhead_memory.test.ts","loc":{"lines":{"from":57,"to":81}}}}],["663",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { ConversationSummaryMemory } from \"../summary.js\";\nimport { OpenAIChat } from \"../../llms/openai-chat.js\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\nimport { SystemChatMessage } from \"../../schema/index.js\";\n\ntest(\"Test summary memory\", async () => {\nconst memory = new ConversationSummaryMemory({\nllm: new OpenAIChat({ modelName: \"gpt-3.5-turbo\", temperature: 0 }),\n});\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: \"\",\n});\n\nawait memory.saveContext(\n{ input: \"How's it going?\" },\n{ response: \"Hello! I'm doing fine. and you?\" }\n);\nconst result2 = await memory.loadMemoryVariables({});\nconsole.log(\"result2\", result2);\n\nawait memory.clear();\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: \"\",\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/summary.int.test.ts","loc":{"lines":{"from":1,"to":26}}}}],["664",{"pageContent":"test(\"Test summary memory with chat model\", async () => {\nconst memory = new ConversationSummaryMemory({\nllm: new ChatOpenAI({ temperature: 0 }),\n});\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: \"\",\n});\n\nawait memory.saveContext(\n{ input: \"How's it going?\" },\n{ response: \"Hello! I'm doing fine. and you?\" }\n);\nconst result2 = await memory.loadMemoryVariables({});\nconsole.log(\"result2\", result2);\n\nawait memory.clear();\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: \"\",\n});\n});\n\ntest(\"Test summary memory return messages\", async () => {\nconst memory = new ConversationSummaryMemory({\nllm: new OpenAIChat({ modelName: \"gpt-3.5-turbo\", temperature: 0 }),\nreturnMessages: true,\n});\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: [new SystemChatMessage(\"\")],\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/summary.int.test.ts","loc":{"lines":{"from":72,"to":100}}}}],["665",{"pageContent":"await memory.saveContext(\n{ input: \"How's it going?\" },\n{ response: \"Hello! I'm doing fine. and you?\" }\n);\nconst result2 = await memory.loadMemoryVariables({});\nconsole.log(\"result2\", result2);\n\nawait memory.clear();\nexpect(await memory.loadMemoryVariables({})).toEqual({\nhistory: [new SystemChatMessage(\"\")],\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/memory/tests/summary.int.test.ts","loc":{"lines":{"from":146,"to":157}}}}],["666",{"pageContent":"import { Callbacks } from \"../callbacks/manager.js\";\nimport { BaseOutputParser } from \"../schema/output_parser.js\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type CombinedOutput = Record<string, any>;\n\n/**\n* Class to combine multiple output parsers\n* @augments BaseOutputParser\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/combining.ts","loc":{"lines":{"from":1,"to":10}}}}],["667",{"pageContent":"class CombiningOutputParser extends BaseOutputParser {\nparsers: BaseOutputParser[];\n\nconstructor(...parsers: BaseOutputParser[]) {\nsuper();\nthis.parsers = parsers;\n}\n\nasync parse(input: string, callbacks?: Callbacks): Promise<CombinedOutput> {\nconst inputs = input\n.trim()\n.split(/Output \\d+:/)\n.slice(1);\nconst ret: CombinedOutput = {};\nfor (const [i, p] of this.parsers.entries()) {\nlet parsed;\ntry {\nconst extracted = inputs[i].includes(\"```\")\n? inputs[i].trim().split(/```/)[1]\n: inputs[i].trim();\nparsed = await p.parse(extracted, callbacks);\n} catch (e) {\nparsed = await p.parse(input.trim(), callbacks);\n}\nObject.assign(ret, parsed);\n}\nreturn ret;\n}\n\ngetFormatInstructions(): string {\nreturn `${[\n`Return the following ${this.parsers.length} outputs, each formatted as described below:`,\n...this.parsers.map(\n(p, i) => `Output ${i + 1}:\\n${p.getFormatInstructions().trim()}`\n),\n].join(\"\\n\\n\")}\\n`;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/combining.ts","loc":{"lines":{"from":49,"to":86}}}}],["668",{"pageContent":"import {\nBaseOutputParser,\nOutputParserException,\n} from \"../schema/output_parser.js\";\nimport { BasePromptTemplate } from \"../prompts/base.js\";\nimport { LLMChain } from \"../chains/llm_chain.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { Callbacks } from \"../callbacks/manager.js\";\nimport { NAIVE_FIX_PROMPT } from \"./prompts.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/fix.ts","loc":{"lines":{"from":1,"to":9}}}}],["669",{"pageContent":"class OutputFixingParser<T> extends BaseOutputParser<T> {\nparser: BaseOutputParser<T>;\n\nretryChain: LLMChain;\n\nstatic fromLLM<T>(\nllm: BaseLanguageModel,\nparser: BaseOutputParser<T>,\nfields?: {\nprompt?: BasePromptTemplate;\n}\n) {\nconst prompt = fields?.prompt ?? NAIVE_FIX_PROMPT;\nconst chain = new LLMChain({ llm, prompt });\nreturn new OutputFixingParser<T>({ parser, retryChain: chain });\n}\n\nconstructor({\nparser,\nretryChain,\n}: {\nparser: BaseOutputParser<T>;\nretryChain: LLMChain;\n}) {\nsuper();\nthis.parser = parser;\nthis.retryChain = retryChain;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/fix.ts","loc":{"lines":{"from":64,"to":91}}}}],["670",{"pageContent":"async parse(completion: string, callbacks?: Callbacks) {\ntry {\nreturn await this.parser.parse(completion, callbacks);\n} catch (e) {\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (e instanceof OutputParserException) {\nconst result = await this.retryChain.call(\n{\ninstructions: this.parser.getFormatInstructions(),\ncompletion,\nerror: e,\n},\ncallbacks\n);\nconst newCompletion: string = result[this.retryChain.outputKey];\nreturn this.parser.parse(newCompletion);\n}\nthrow e;\n}\n}\n\ngetFormatInstructions() {\nreturn this.parser.getFormatInstructions();\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/fix.ts","loc":{"lines":{"from":137,"to":161}}}}],["671",{"pageContent":"export { ListOutputParser, CommaSeparatedListOutputParser } from \"./list.js\";\nexport { RegexParser } from \"./regex.js\";\nexport { StructuredOutputParser } from \"./structured.js\";\nexport { OutputFixingParser } from \"./fix.js\";\nexport { CombiningOutputParser } from \"./combining.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/index.ts","loc":{"lines":{"from":1,"to":5}}}}],["672",{"pageContent":"import {\nBaseOutputParser,\nOutputParserException,\n} from \"../schema/output_parser.js\";\n\n/**\n* Class to parse the output of an LLM call to a list.\n* @augments BaseOutputParser\n*/\nexport abstract class ListOutputParser extends BaseOutputParser<string[]> {}\n\n/**\n* Class to parse the output of an LLM call as a comma-separated list.\n* @augments ListOutputParser\n*/\nexport class CommaSeparatedListOutputParser extends ListOutputParser {\nasync parse(text: string): Promise<string[]> {\ntry {\nreturn text\n.trim()\n.split(\",\")\n.map((s) => s.trim());\n} catch (e) {\nthrow new OutputParserException(`Could not parse output: ${text}`);\n}\n}\n\ngetFormatInstructions(): string {\nreturn `Your response should be a list of comma separated values, eg: \\`foo, bar, baz\\``;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/list.ts","loc":{"lines":{"from":1,"to":31}}}}],["673",{"pageContent":"import { PromptTemplate } from \"../prompts/prompt.js\";\n\nexport const NAIVE_FIX_TEMPLATE = `Instructions:\n--------------\n{instructions}\n--------------\nCompletion:\n--------------\n{completion}\n--------------\n\nAbove, the Completion did not satisfy the constraints given in the Instructions.\nError:\n--------------\n{error}\n--------------\n\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:`;\n\nexport const NAIVE_FIX_PROMPT =\n/* #__PURE__ */ PromptTemplate.fromTemplate(NAIVE_FIX_TEMPLATE);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/prompts.ts","loc":{"lines":{"from":1,"to":21}}}}],["674",{"pageContent":"import {\nBaseOutputParser,\nOutputParserException,\n} from \"../schema/output_parser.js\";\n\n/**\n* Class to parse the output of an LLM call into a dictionary.\n* @augments BaseOutputParser\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/regex.ts","loc":{"lines":{"from":1,"to":9}}}}],["675",{"pageContent":"class RegexParser extends BaseOutputParser<Record<string, string>> {\nregex: string | RegExp;\n\noutputKeys: string[];\n\ndefaultOutputKey?: string;\n\nconstructor(\nregex: string | RegExp,\noutputKeys: string[],","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/regex.ts","loc":{"lines":{"from":55,"to":64}}}}],["676",{"pageContent":"OutputKey?: string\n) {\nsuper();\nthis.regex = typeof regex === \"string\" ? new RegExp(regex) : regex;\nthis.outputKeys = outputKeys;\nthis.defaultOutputKey = defaultOutputKey;\n}\n\n_type() {\nreturn \"regex_parser\";\n}\n\nasync parse(text: string): Promise<Record<string, string>> {\nconst match = text.match(this.regex);\nif (match) {\nreturn this.outputKeys.reduce((acc, key, index) => {\nacc[key] = match[index + 1];\nreturn acc;\n}, {} as Record<string, string>);\n}\n\nif (this.defaultOutputKey === undefined) {\nthrow new OutputParserException(`Could not parse output: ${text}`);\n}\n\nreturn this.outputKeys.reduce((acc, key) => {\nacc[key] = key === this.defaultOutputKey ? text : \"\";\nreturn acc;\n}, {} as Record<string, string>);\n}\n\ngetFormatInstructions(): string {\nreturn `Your response should match the following regex: ${this.regex}`;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/regex.ts","loc":{"lines":{"from":108,"to":142}}}}],["677",{"pageContent":"import { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\nimport {\nBaseOutputParser,\nOutputParserException,\n} from \"../schema/output_parser.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/structured.ts","loc":{"lines":{"from":1,"to":6}}}}],["678",{"pageContent":"class StructuredOutputParser<\nT extends z.ZodTypeAny\n> extends BaseOutputParser<z.infer<T>> {\nconstructor(public schema: T) {\nsuper();\n}\n\nstatic fromZodSchema<T extends z.ZodTypeAny>(schema: T) {\nreturn new this(schema);\n}\n\nstatic fromNamesAndDescriptions<S extends { [key: string]: string }>(\nschemas: S\n) {\nconst zodSchema = z.object(\nObject.fromEntries(\nObject.entries(schemas).map(\n([name, description]) =>\n[name, z.string().describe(description)] as const\n)\n)\n);\n\nreturn new this(zodSchema);\n}\n\ngetFormatInstructions(): string {\nreturn `The output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/structured.ts","loc":{"lines":{"from":59,"to":89}}}}],["679",{"pageContent":"Here is the output schema:\n\\`\\`\\`\n${JSON.stringify(zodToJsonSchema(this.schema))}\n\\`\\`\\`\n`;\n}\n\nasync parse(text: string): Promise<z.infer<T>> {\ntry {\nconst json = text.includes(\"```\")\n? text.trim().split(/```(?:json)?/)[1]\n: text.trim();\nreturn this.schema.parseAsync(JSON.parse(json));\n} catch (e) {\nthrow new OutputParserException(\n`Failed to parse. Text: \"${text}\". Error: ${e}`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/structured.ts","loc":{"lines":{"from":111,"to":130}}}}],["680",{"pageContent":"import { expect, test } from \"@jest/globals\";\n\nimport { CombiningOutputParser } from \"../combining.js\";\nimport { StructuredOutputParser } from \"../structured.js\";\nimport { RegexParser } from \"../regex.js\";\n\ntest(\"CombiningOutputParser\", async () => {\nconst parser = new CombiningOutputParser(\nStructuredOutputParser.fromNamesAndDescriptions({\nurl: \"A link to the resource\",\n}),\nnew RegexParser(\n/Confidence: (A|B|C), Explanation: (.*)/,\n[\"confidence\", \"explanation\"],\n\"noConfidence\"\n)\n);\n\nexpect(parser.getFormatInstructions()).toMatchInlineSnapshot(`\n\"Return the following 2 outputs, each formatted as described below:\n\nOutput 1:\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/combining.test.ts","loc":{"lines":{"from":1,"to":23}}}}],["681",{"pageContent":"As an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nHere is the output schema:\n\\`\\`\\`\n{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\",\"description\":\"A link to the resource\"}},\"required\":[\"url\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n\\`\\`\\`\n\nOutput 2:\nYour response should match the following regex: /Confidence: (A|B|C), Explanation: (.*)/\n\"\n`);\n\nexpect(\nawait parser.parse(\n`Output 0:\n{\"url\": \"https://en.wikipedia.org/wiki/Paris\"}\n\nOutput 1:\nConfidence: A, Explanation: Because it is the capital of France.`\n)\n).toMatchInlineSnapshot(`\n{\n\"confidence\": \"A\",\n\"explanation\": \"Because it is the capital of France.\",\n\"url\": \"https://en.wikipedia.org/wiki/Paris\",\n}\n`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/combining.test.ts","loc":{"lines":{"from":65,"to":92}}}}],["682",{"pageContent":"expect(\nawait parser.parse(\n'```\\n{\"url\": \"https://en.wikipedia.org/wiki/Paris\"}\\n```\\nConfidence: A, Explanation: Because it is the capital of France.'\n)\n).toMatchInlineSnapshot(`\n{\n\"confidence\": \"A\",\n\"explanation\": \"Because it is the capital of France.\",\n\"url\": \"https://en.wikipedia.org/wiki/Paris\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/combining.test.ts","loc":{"lines":{"from":131,"to":142}}}}],["683",{"pageContent":"import { test, expect } from \"@jest/globals\";\n\nimport { CommaSeparatedListOutputParser } from \"../list.js\";\n\ntest(\"CommaSeparatedListOutputParser\", async () => {\nconst parser = new CommaSeparatedListOutputParser();\n\nexpect(await parser.parse(\"hello, bye\")).toEqual([\"hello\", \"bye\"]);\n\nexpect(await parser.parse(\"hello,bye\")).toEqual([\"hello\", \"bye\"]);\n\nexpect(await parser.parse(\"hello\")).toEqual([\"hello\"]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/list.test.ts","loc":{"lines":{"from":1,"to":13}}}}],["684",{"pageContent":"import { z } from \"zod\";\n\nimport { expect, test } from \"@jest/globals\";\n\nimport { StructuredOutputParser } from \"../structured.js\";\n\ntest(\"StructuredOutputParser.fromNamesAndDescriptions\", async () => {\nconst parser = StructuredOutputParser.fromNamesAndDescriptions({\nurl: \"A link to the resource\",\n});\n\nexpect(await parser.parse('```\\n{\"url\": \"value\"}```')).toEqual({\nurl: \"value\",\n});\n\nexpect(parser.getFormatInstructions()).toMatchInlineSnapshot(`\n\"The output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["685",{"pageContent":"Here is the output schema:\n\\`\\`\\`\n{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\",\"description\":\"A link to the resource\"}},\"required\":[\"url\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n\\`\\`\\`\n\"\n`);\n});\n\nenum StateProvinceEnum {\nAlabama = \"AL\",\nAlaska = \"AK\",\nArizona = \"AZ\",\n}\n\ntest(\"StructuredOutputParser.fromZodSchema\", async () => {\nconst parser = StructuredOutputParser.fromZodSchema(\nz.object({ url: z.string().describe(\"A link to the resource\") })\n);\n\nexpect(await parser.parse('```\\n{\"url\": \"value\"}```')).toEqual({\nurl: \"value\",\n});\n\nexpect(parser.getFormatInstructions()).toMatchInlineSnapshot(`\n\"The output should be formatted as a JSON instance that conforms to the JSON schema below.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":169,"to":193}}}}],["686",{"pageContent":"As an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nHere is the output schema:\n\\`\\`\\`\n{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\",\"description\":\"A link to the resource\"}},\"required\":[\"url\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n\\`\\`\\`\n\"\n`);\n});\n\ntest(\"StructuredOutputParser.fromZodSchema\", async () => {\nconst parser = StructuredOutputParser.fromZodSchema(\nz.object({\nanswer: z.string().describe(\"answer to the user's question\"),\nsources: z\n.array(z.string())\n.describe(\"sources used to answer the question, should be websites.\"),\n})\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":343,"to":362}}}}],["687",{"pageContent":"expect(\nawait parser.parse(\n'```\\n{\"answer\": \"value\", \"sources\": [\"this-source\"]}```'\n)\n).toEqual({\nanswer: \"value\",\nsources: [\"this-source\"],\n});\n\nexpect(\nawait parser.parse(\n'```json\\n{\"answer\": \"value\", \"sources\": [\"this-source\"]}```'\n)\n).toEqual({\nanswer: \"value\",\nsources: [\"this-source\"],\n});\n\nexpect(\nawait parser.parse(\n'some other stuff```json\\n{\"answer\": \"value\", \"sources\": [\"this-source\"]}```some other stuff at the end'\n)\n).toEqual({\nanswer: \"value\",\nsources: [\"this-source\"],\n});\n\nexpect(parser.getFormatInstructions()).toMatchInlineSnapshot(`\n\"The output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":511,"to":542}}}}],["688",{"pageContent":"Here is the output schema:\n\\`\\`\\`\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer to the user's question\"},\"sources\":{\"type\":\"array\",\"items\":{\"type\":\"string\"},\"description\":\"sources used to answer the question, should be websites.\"}},\"required\":[\"answer\",\"sources\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n\\`\\`\\`\n\"\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":620,"to":626}}}}],["689",{"pageContent":"test(\"StructuredOutputParser.fromZodSchema\", async () => {\nconst parser = StructuredOutputParser.fromZodSchema(\nz\n.object({\nurl: z.string().describe(\"A link to the resource\"),\ntitle: z.string().describe(\"A title for the resource\"),\nyear: z.number().describe(\"The year the resource was created\"),\ncreatedAt: z\n.string()\n.datetime()\n.describe(\"The date and time the resource was created\"),\ncreatedAtDate: z.coerce\n.date()\n.describe(\"The date the resource was created\")\n.optional(),\nauthors: z.array(\nz.object({\nname: z.string().describe(\"The name of the author\"),\nemail: z.string().describe(\"The email of the author\"),","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":687,"to":705}}}}],["690",{"pageContent":": z.enum([\"author\", \"editor\"]).optional(),\naddress: z\n.string()\n.optional()\n.describe(\"The address of the author\"),\nstateProvince: z\n.nativeEnum(StateProvinceEnum)\n.optional()\n.describe(\"The state or province of the author\"),\n})\n),\n})\n.describe(\"Only One object\")\n);\n\nexpect(\nawait parser.parse(\n'```\\n{\"url\": \"value\", \"title\": \"value\", \"year\": 2011, \"createdAt\": \"2023-03-29T16:07:09.600Z\", \"createdAtDate\": \"2023-03-29\", \"authors\": [{\"name\": \"value\", \"email\": \"value\", \"stateProvince\": \"AZ\"}]}```'\n)\n).toEqual({\nurl: \"value\",\ntitle: \"value\",\nyear: 2011,\ncreatedAt: \"2023-03-29T16:07:09.600Z\",\ncreatedAtDate: new Date(\"2023-03-29T00:00:00.000Z\"),\nauthors: [{ name: \"value\", email: \"value\", stateProvince: \"AZ\" }],\n});\n\nexpect(parser.getFormatInstructions()).toMatchInlineSnapshot(`\n\"The output should be formatted as a JSON instance that conforms to the JSON schema below.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":855,"to":884}}}}],["691",{"pageContent":"As an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":884,"to":885}}}}],["692",{"pageContent":"Here is the output schema:\n\\`\\`\\`","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":887,"to":888}}}}],["693",{"pageContent":"{\"type\":\"object\",\"properties\":{\"url\":{\"type\":\"string\",\"description\":\"A link to the resource\"},\"title\":{\"type\":\"string\",\"description\":\"A title for the resource\"},\"year\":{\"type\":\"number\",\"description\":\"The year the resource was created\"},\"createdAt\":{\"type\":\"string\",\"format\":\"date-time\",\"description\":\"The date and time the resource was created\"},\"createdAtDate\":{\"type\":\"string\",\"format\":\"date-time\",\"description\":\"The date the resource was created\"},\"authors\":{\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\",\"description\":\"The name of the author\"},\"email\":{\"type\":\"string\",\"description\":\"The email of the author\"},\"type\":{\"type\":\"string\",\"enum\":[\"author\",\"editor\"]},\"address\":{\"type\":\"string\",\"description\":\"The address of the author\"},\"stateProvince\":{\"type\":\"string\",\"enum\":[\"AL\",\"AK\",\"AZ\"],\"description\":\"The state or province of the","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":1030,"to":1030}}}}],["694",{"pageContent":"author\"}},\"required\":[\"name\",\"email\"],\"additionalProperties\":false}}},\"required\":[\"url\",\"title\",\"year\",\"createdAt\",\"authors\"],\"additionalProperties\":false,\"description\":\"Only One object\",\"$schema\":\"http://json-schema.org/draft-07/schema#\"}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":1030,"to":1030}}}}],["695",{"pageContent":"\\`\\`\\`\n\"\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/output_parsers/tests/structured.test.ts","loc":{"lines":{"from":1030,"to":1033}}}}],["696",{"pageContent":"import {\nBasePromptValue,\nExample,\nHumanChatMessage,\nInputValues,\nPartialValues,\n} from \"../schema/index.js\";\nimport { BaseOutputParser } from \"../schema/output_parser.js\";\nimport { SerializedBasePromptTemplate } from \"./serde.js\";\n\nexport class StringPromptValue extends BasePromptValue {\nvalue: string;\n\nconstructor(value: string) {\nsuper();\nthis.value = value;\n}\n\ntoString() {\nreturn this.value;\n}\n\ntoChatMessages() {\nreturn [new HumanChatMessage(this.value)];\n}\n}\n\n/**\n* Input common to all prompt templates.\n*/\nexport interface BasePromptTemplateInput {\n/**\n* A list of variable names the prompt template expects\n*/\ninputVariables: string[];\n\n/**\n* How to parse the output of calling an LLM on this formatted prompt\n*/\noutputParser?: BaseOutputParser;\n\n/** Partial variables */\npartialVariables?: PartialValues;\n}\n\n/**\n* Base class for prompt templates. Exposes a format method that returns a\n* string prompt given a set of input values.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":1,"to":49}}}}],["697",{"pageContent":"abstract class BasePromptTemplate implements BasePromptTemplateInput {\ninputVariables: string[];\n\noutputParser?: BaseOutputParser;\n\npartialVariables?: InputValues;\n\nconstructor(input: BasePromptTemplateInput) {\nconst { inputVariables } = input;\nif (inputVariables.includes(\"stop\")) {\nthrow new Error(\n\"Cannot have an input variable named 'stop', as it is used internally, please rename.\"\n);\n}\nObject.assign(this, input);\n}\n\nabstract partial(values: PartialValues): Promise<BasePromptTemplate>;\n\nasync mergePartialAndUserVariables(\nuserVariables: InputValues\n): Promise<InputValues> {\nconst partialVariables = this.partialVariables ?? {};\nconst partialValues: InputValues = {};\n\nfor (const [key, value] of Object.entries(partialVariables)) {\nif (typeof value === \"string\") {\npartialValues[key] = value;\n} else {\npartialValues[key] = await value();\n}\n}\n\nconst allKwargs = { ...partialValues, ...userVariables };\nreturn allKwargs;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":167,"to":202}}}}],["698",{"pageContent":"/**\n* Format the prompt given the input values.\n*\n* @param values - A dictionary of arguments to be passed to the prompt template.\n* @returns A formatted prompt string.\n*\n* @example\n* ```ts\n* prompt.format({ foo: \"bar\" });\n* ```\n*/\nabstract format(values: InputValues): Promise<string>;\n\n/**\n* Format the prompt given the input values and return a formatted prompt value.\n* @param values\n* @returns A formatted PromptValue.\n*/\nabstract formatPromptValue(values: InputValues): Promise<BasePromptValue>;\n\n/**\n* Return the string type key uniquely identifying this class of prompt template.\n*/\nabstract _getPromptType(): string;\n\n/**\n* Return a json-like object representing this prompt template.\n*/\nabstract serialize(): SerializedBasePromptTemplate;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":320,"to":348}}}}],["699",{"pageContent":"/**\n* Load a prompt template from a json-like object describing it.\n*\n* @remarks\n* Deserializing needs to be async because templates (e.g. {@link FewShotPromptTemplate}) can\n* reference remote resources that we read asynchronously with a web\n* request.\n*/\nstatic async deserialize(\ndata: SerializedBasePromptTemplate\n): Promise<BasePromptTemplate> {\nswitch (data._type) {\ncase \"prompt\": {\nconst { PromptTemplate } = await import(\"./prompt.js\");\nreturn PromptTemplate.deserialize(data);\n}\ncase undefined: {\nconst { PromptTemplate } = await import(\"./prompt.js\");\nreturn PromptTemplate.deserialize({ ...data, _type: \"prompt\" });\n}\ncase \"few_shot\": {\nconst { FewShotPromptTemplate } = await import(\"./few_shot.js\");\nreturn FewShotPromptTemplate.deserialize(data);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":475,"to":498}}}}],["700",{"pageContent":":\nthrow new Error(\n`Invalid prompt type in config: ${\n(data as SerializedBasePromptTemplate)._type\n}`\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":625,"to":633}}}}],["701",{"pageContent":"abstract class BaseStringPromptTemplate extends BasePromptTemplate {\nasync formatPromptValue(values: InputValues): Promise<BasePromptValue> {\nconst formattedPrompt = await this.format(values);\nreturn new StringPromptValue(formattedPrompt);\n}\n}\n\n/**\n* Base class for example selectors.\n*/\nexport abstract class BaseExampleSelector {\nabstract addExample(example: Example): Promise<void | string>;\n\nabstract selectExamples(input_variables: Example): Promise<Example[]>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/base.ts","loc":{"lines":{"from":791,"to":805}}}}],["702",{"pageContent":"import {\nAIChatMessage,\nBaseChatMessage,\nBasePromptValue,\nChatMessage,\nHumanChatMessage,\nInputValues,\nPartialValues,\nSystemChatMessage,\n} from \"../schema/index.js\";\nimport {\nBasePromptTemplate,\nBasePromptTemplateInput,\nBaseStringPromptTemplate,\n} from \"./base.js\";\nimport { PromptTemplate } from \"./prompt.js\";\nimport {\nSerializedChatPromptTemplate,\nSerializedMessagePromptTemplate,\n} from \"./serde.js\";\n\nexport abstract class BaseMessagePromptTemplate {\nabstract inputVariables: string[];\n\nabstract formatMessages(values: InputValues): Promise<BaseChatMessage[]>;\n\nserialize(): SerializedMessagePromptTemplate {\nreturn {\n_type: this.constructor.name,\n...JSON.parse(JSON.stringify(this)),\n};\n}\n}\n\nexport class ChatPromptValue extends BasePromptValue {\nmessages: BaseChatMessage[];\n\nconstructor(messages: BaseChatMessage[]) {\nsuper();\nthis.messages = messages;\n}\n\ntoString() {\nreturn JSON.stringify(this.messages);\n}\n\ntoChatMessages() {\nreturn this.messages;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":1,"to":50}}}}],["703",{"pageContent":"class MessagesPlaceholder extends BaseMessagePromptTemplate {\nvariableName: string;\n\nconstructor(variableName: string) {\nsuper();\nthis.variableName = variableName;\n}\n\nget inputVariables() {\nreturn [this.variableName];\n}\n\nformatMessages(values: InputValues): Promise<BaseChatMessage[]> {\nreturn Promise.resolve(values[this.variableName] as BaseChatMessage[]);\n}\n}\n\nexport abstract class BaseMessageStringPromptTemplate extends BaseMessagePromptTemplate {\nprompt: BaseStringPromptTemplate;\n\nprotected constructor(prompt: BaseStringPromptTemplate) {\nsuper();\nthis.prompt = prompt;\n}\n\nget inputVariables() {\nreturn this.prompt.inputVariables;\n}\n\nabstract format(values: InputValues): Promise<BaseChatMessage>;\n\nasync formatMessages(values: InputValues): Promise<BaseChatMessage[]> {\nreturn [await this.format(values)];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":326,"to":360}}}}],["704",{"pageContent":"abstract class BaseChatPromptTemplate extends BasePromptTemplate {\nconstructor(input: BasePromptTemplateInput) {\nsuper(input);\n}\n\nabstract formatMessages(values: InputValues): Promise<BaseChatMessage[]>;\n\nasync format(values: InputValues): Promise<string> {\nreturn (await this.formatPromptValue(values)).toString();\n}\n\nasync formatPromptValue(values: InputValues): Promise<BasePromptValue> {\nconst resultMessages = await this.formatMessages(values);\nreturn new ChatPromptValue(resultMessages);\n}\n}\n\nexport class ChatMessagePromptTemplate extends BaseMessageStringPromptTemplate {\nrole: string;\n\nasync format(values: InputValues): Promise<BaseChatMessage> {\nreturn new ChatMessage(await this.prompt.format(values), this.role);\n}\n\nconstructor(prompt: BaseStringPromptTemplate, role: string) {\nsuper(prompt);\nthis.role = role;\n}\n\nstatic fromTemplate(template: string, role: string) {\nreturn new this(PromptTemplate.fromTemplate(template), role);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":644,"to":676}}}}],["705",{"pageContent":"class HumanMessagePromptTemplate extends BaseMessageStringPromptTemplate {\nasync format(values: InputValues): Promise<BaseChatMessage> {\nreturn new HumanChatMessage(await this.prompt.format(values));\n}\n\nconstructor(prompt: BaseStringPromptTemplate) {\nsuper(prompt);\n}\n\nstatic fromTemplate(template: string) {\nreturn new this(PromptTemplate.fromTemplate(template));\n}\n}\n\nexport class AIMessagePromptTemplate extends BaseMessageStringPromptTemplate {\nasync format(values: InputValues): Promise<BaseChatMessage> {\nreturn new AIChatMessage(await this.prompt.format(values));\n}\n\nconstructor(prompt: BaseStringPromptTemplate) {\nsuper(prompt);\n}\n\nstatic fromTemplate(template: string) {\nreturn new this(PromptTemplate.fromTemplate(template));\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":953,"to":979}}}}],["706",{"pageContent":"class SystemMessagePromptTemplate extends BaseMessageStringPromptTemplate {\nasync format(values: InputValues): Promise<BaseChatMessage> {\nreturn new SystemChatMessage(await this.prompt.format(values));\n}\n\nconstructor(prompt: BaseStringPromptTemplate) {\nsuper(prompt);\n}\n\nstatic fromTemplate(template: string) {\nreturn new this(PromptTemplate.fromTemplate(template));\n}\n}\n\nexport interface ChatPromptTemplateInput extends BasePromptTemplateInput {\n/**\n* The prompt messages\n*/\npromptMessages: BaseMessagePromptTemplate[];\n\n/**\n* Whether to try validating the template on initialization\n*\n* @defaultValue `true`\n*/\nvalidateTemplate?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":1268,"to":1294}}}}],["707",{"pageContent":"class ChatPromptTemplate\nextends BaseChatPromptTemplate\nimplements ChatPromptTemplateInput\n{\npromptMessages: BaseMessagePromptTemplate[];\n\nvalidateTemplate = true;\n\nconstructor(input: ChatPromptTemplateInput) {\nsuper(input);\nObject.assign(this, input);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":1587,"to":1597}}}}],["708",{"pageContent":"if (this.validateTemplate) {\nconst inputVariablesMessages = new Set<string>();\nfor (const promptMessage of this.promptMessages) {\nfor (const inputVariable of promptMessage.inputVariables) {\ninputVariablesMessages.add(inputVariable);\n}\n}\nconst inputVariablesInstance = new Set(\nthis.partialVariables\n? this.inputVariables.concat(Object.keys(this.partialVariables))\n: this.inputVariables\n);\nconst difference = new Set(\n[...inputVariablesInstance].filter(\n(x) => !inputVariablesMessages.has(x)\n)\n);\nif (difference.size > 0) {\nthrow new Error(\n`Input variables \\`${[\n...difference,\n]}\\` are not used in any of the prompt messages.`\n);\n}\nconst otherDifference = new Set(\n[...inputVariablesMessages].filter(\n(x) => !inputVariablesInstance.has(x)\n)\n);\nif (otherDifference.size > 0) {\nthrow new Error(\n`Input variables \\`${[\n...otherDifference,\n]}\\` are used in prompt messages but not in the prompt template.`\n);\n}\n}\n}\n\n_getPromptType(): \"chat\" {\nreturn \"chat\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":1903,"to":1944}}}}],["709",{"pageContent":"async formatMessages(values: InputValues): Promise<BaseChatMessage[]> {\nconst allValues = await this.mergePartialAndUserVariables(values);\n\nlet resultMessages: BaseChatMessage[] = [];\n\nfor (const promptMessage of this.promptMessages) {\nconst inputValues = promptMessage.inputVariables.reduce(\n(acc, inputVariable) => {\nif (!(inputVariable in allValues)) {\nthrow new Error(\n`Missing value for input variable \\`${inputVariable}\\``\n);\n}\nacc[inputVariable] = allValues[inputVariable];\nreturn acc;\n},\n{} as InputValues\n);\nconst message = await promptMessage.formatMessages(inputValues);\nresultMessages = resultMessages.concat(message);\n}\nreturn resultMessages;\n}\n\nserialize(): SerializedChatPromptTemplate {\nif (this.outputParser !== undefined) {\nthrow new Error(\n\"ChatPromptTemplate cannot be serialized if outputParser is set\"\n);\n}\nreturn {\ninput_variables: this.inputVariables,\nprompt_messages: this.promptMessages.map((m) => m.serialize()),\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":2220,"to":2254}}}}],["710",{"pageContent":"async partial(values: PartialValues): Promise<ChatPromptTemplate> {\n// This is implemented in a way it doesn't require making\n// BaseMessagePromptTemplate aware of .partial()\nconst promptDict: ChatPromptTemplateInput = { ...this };\npromptDict.inputVariables = this.inputVariables.filter(\n(iv) => !(iv in values)\n);\npromptDict.partialVariables = {\n...(this.partialVariables ?? {}),\n...values,\n};\nreturn new ChatPromptTemplate(promptDict);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":2531,"to":2543}}}}],["711",{"pageContent":"static fromPromptMessages(\npromptMessages: (BaseMessagePromptTemplate | ChatPromptTemplate)[]\n): ChatPromptTemplate {\nconst flattenedMessages = promptMessages.reduce(\n(acc, promptMessage) =>\nacc.concat(\n// eslint-disable-next-line no-instanceof/no-instanceof\npromptMessage instanceof ChatPromptTemplate\n? promptMessage.promptMessages\n: [promptMessage]\n),\n[] as BaseMessagePromptTemplate[]\n);\nconst flattenedPartialVariables = promptMessages.reduce(\n(acc, promptMessage) =>\n// eslint-disable-next-line no-instanceof/no-instanceof\npromptMessage instanceof ChatPromptTemplate\n? Object.assign(acc, promptMessage.partialVariables)\n: acc,\nObject.create(null) as PartialValues\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":2841,"to":2861}}}}],["712",{"pageContent":"const inputVariables = new Set<string>();\nfor (const promptMessage of flattenedMessages) {\nfor (const inputVariable of promptMessage.inputVariables) {\nif (inputVariable in flattenedPartialVariables) {\ncontinue;\n}\ninputVariables.add(inputVariable);\n}\n}\nreturn new ChatPromptTemplate({\ninputVariables: [...inputVariables],\npromptMessages: flattenedMessages,\npartialVariables: flattenedPartialVariables,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/chat.ts","loc":{"lines":{"from":3152,"to":3167}}}}],["713",{"pageContent":"import {\nBaseStringPromptTemplate,\nBasePromptTemplateInput,\nBaseExampleSelector,\n} from \"./base.js\";\nimport {\nTemplateFormat,\ncheckValidTemplate,\nrenderTemplate,\n} from \"./template.js\";\nimport { PromptTemplate } from \"./prompt.js\";\nimport { SerializedFewShotTemplate } from \"./serde.js\";\nimport { Example, InputValues, PartialValues } from \"../schema/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":1,"to":13}}}}],["714",{"pageContent":"interface FewShotPromptTemplateInput extends BasePromptTemplateInput {\n/**\n* Examples to format into the prompt. Exactly one of this or\n* {@link exampleSelector} must be\n* provided.\n*/\nexamples?: Example[];\n\n/**\n* An {@link BaseExampleSelector} Examples to format into the prompt. Exactly one of this or\n* {@link examples} must be\n* provided.\n*/\nexampleSelector?: BaseExampleSelector;\n\n/**\n* An {@link PromptTemplate} used to format a single example.\n*/\nexamplePrompt: PromptTemplate;\n\n/**\n* String separator used to join the prefix, the examples, and suffix.\n*/\nexampleSeparator?: string;\n\n/**\n* A prompt template string to put before the examples.\n*\n* @defaultValue `\"\"`\n*/\nprefix?: string;\n\n/**\n* A prompt template string to put after the examples.\n*/\nsuffix?: string;\n\n/**\n* The format of the prompt template. Options are: 'f-string', 'jinja-2'\n*/\ntemplateFormat?: TemplateFormat;\n\n/**\n* Whether or not to try validating the template on initialization.\n*/\nvalidateTemplate?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":215,"to":261}}}}],["715",{"pageContent":"/**\n* Prompt template that contains few-shot examples.\n* @augments BasePromptTemplate\n* @augments FewShotPromptTemplateInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":440,"to":444}}}}],["716",{"pageContent":"class FewShotPromptTemplate\nextends BaseStringPromptTemplate\nimplements FewShotPromptTemplateInput\n{\nexamples?: InputValues[];\n\nexampleSelector?: BaseExampleSelector | undefined;\n\nexamplePrompt: PromptTemplate;\n\nsuffix = \"\";\n\nexampleSeparator = \"\\n\\n\";\n\nprefix = \"\";\n\ntemplateFormat: TemplateFormat = \"f-string\";\n\nvalidateTemplate = true;\n\nconstructor(input: FewShotPromptTemplateInput) {\nsuper(input);\nObject.assign(this, input);\n\nif (this.examples !== undefined && this.exampleSelector !== undefined) {\nthrow new Error(\n\"Only one of 'examples' and 'example_selector' should be provided\"\n);\n}\n\nif (this.examples === undefined && this.exampleSelector === undefined) {\nthrow new Error(\n\"One of 'examples' and 'example_selector' should be provided\"\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":652,"to":686}}}}],["717",{"pageContent":"if (this.validateTemplate) {\nlet totalInputVariables = this.inputVariables;\nif (this.partialVariables) {\ntotalInputVariables = totalInputVariables.concat(\nObject.keys(this.partialVariables)\n);\n}\ncheckValidTemplate(\nthis.prefix + this.suffix,\nthis.templateFormat,\ntotalInputVariables\n);\n}\n}\n\n_getPromptType(): \"few_shot\" {\nreturn \"few_shot\";\n}\n\nprivate async getExamples(\ninputVariables: InputValues\n): Promise<InputValues[]> {\nif (this.examples !== undefined) {\nreturn this.examples;\n}\nif (this.exampleSelector !== undefined) {\nreturn this.exampleSelector.selectExamples(inputVariables);\n}\n\nthrow new Error(\n\"One of 'examples' and 'example_selector' should be provided\"\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":875,"to":907}}}}],["718",{"pageContent":"async partial(values: PartialValues): Promise<FewShotPromptTemplate> {\nconst promptDict: FewShotPromptTemplate = { ...this };\npromptDict.inputVariables = this.inputVariables.filter(\n(iv) => !(iv in values)\n);\npromptDict.partialVariables = {\n...(this.partialVariables ?? {}),\n...values,\n};\nreturn new FewShotPromptTemplate(promptDict);\n}\n\nasync format(values: InputValues): Promise<string> {\nconst allValues = await this.mergePartialAndUserVariables(values);\nconst examples = await this.getExamples(allValues);\n\nconst exampleStrings = await Promise.all(\nexamples.map((example) => this.examplePrompt.format(example))\n);\nconst template = [this.prefix, ...exampleStrings, this.suffix].join(\nthis.exampleSeparator\n);\nreturn renderTemplate(template, this.templateFormat, allValues);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":1098,"to":1121}}}}],["719",{"pageContent":"serialize(): SerializedFewShotTemplate {\nif (this.exampleSelector || !this.examples) {\nthrow new Error(\n\"Serializing an example selector is not currently supported\"\n);\n}\nif (this.outputParser !== undefined) {\nthrow new Error(\n\"Serializing an output parser is not currently supported\"\n);\n}\nreturn {\n_type: this._getPromptType(),\ninput_variables: this.inputVariables,\nexample_prompt: this.examplePrompt.serialize(),\nexample_separator: this.exampleSeparator,\nsuffix: this.suffix,\nprefix: this.prefix,\ntemplate_format: this.templateFormat,\nexamples: this.examples,\n};\n}\n\nstatic async deserialize(\ndata: SerializedFewShotTemplate\n): Promise<FewShotPromptTemplate> {\nconst { example_prompt } = data;\nif (!example_prompt) {\nthrow new Error(\"Missing example prompt\");\n}\nconst examplePrompt = await PromptTemplate.deserialize(example_prompt);\n\nlet examples: Example[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":1308,"to":1340}}}}],["720",{"pageContent":"if (Array.isArray(data.examples)) {\nexamples = data.examples;\n} else {\nthrow new Error(\n\"Invalid examples format. Only list or string are supported.\"\n);\n}\n\nreturn new FewShotPromptTemplate({\ninputVariables: data.input_variables,\nexamplePrompt,\nexamples,\nexampleSeparator: data.example_separator,\nprefix: data.prefix,\nsuffix: data.suffix,\ntemplateFormat: data.template_format,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/few_shot.ts","loc":{"lines":{"from":1524,"to":1542}}}}],["721",{"pageContent":"export {\nBaseExampleSelector,\nBasePromptTemplate,\nBasePromptTemplateInput,\nStringPromptValue,\nBaseStringPromptTemplate,\n} from \"./base.js\";\nexport { PromptTemplate, PromptTemplateInput } from \"./prompt.js\";\nexport {\nLengthBasedExampleSelector,\nLengthBasedExampleSelectorInput,\n} from \"./selectors/LengthBasedExampleSelector.js\";\nexport {\nSemanticSimilarityExampleSelector,\nSemanticSimilarityExampleSelectorInput,\n} from \"./selectors/SemanticSimilarityExampleSelector.js\";\nexport {\nFewShotPromptTemplate,\nFewShotPromptTemplateInput,\n} from \"./few_shot.js\";\nexport {\nChatPromptTemplate,\nHumanMessagePromptTemplate,\nAIMessagePromptTemplate,\nSystemMessagePromptTemplate,\nChatMessagePromptTemplate,\nMessagesPlaceholder,\nBaseChatPromptTemplate,\n} from \"./chat.js\";\nexport {\nSerializedPromptTemplate,\nSerializedBasePromptTemplate,\nSerializedFewShotTemplate,\nSerializedMessagePromptTemplate,\nSerializedChatPromptTemplate,\n} from \"./serde.js\";\nexport {\nparseTemplate,\nrenderTemplate,\ncheckValidTemplate,\nTemplateFormat,\n} from \"./template.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/index.ts","loc":{"lines":{"from":1,"to":42}}}}],["722",{"pageContent":"import { BasePromptTemplate } from \"./base.js\";\nimport { loadFromHub } from \"../util/hub.js\";\nimport { FileLoader, loadFromFile } from \"../util/load.js\";\nimport { parseFileConfig } from \"../util/parse.js\";\n\nconst loadPromptFromFile: FileLoader<BasePromptTemplate> = (text, path) =>\nBasePromptTemplate.deserialize(parseFileConfig(text, path));\n\n/**\n* Load a prompt from {@link https://github.com/hwchase17/langchain-hub | LangchainHub} or local filesystem.\n*\n* @example\n* Loading from LangchainHub:\n* ```ts\n* import { loadPrompt } from \"langchain/prompts/load\";\n* const prompt = await loadPrompt(\"lc://prompts/hello-world/prompt.yaml\");\n* ```\n*\n* @example\n* Loading from local filesystem:\n* ```ts\n* import { loadPrompt } from \"langchain/prompts/load\";\n* const prompt = await loadPrompt(\"/path/to/prompt.json\");\n* ```\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/load.ts","loc":{"lines":{"from":1,"to":25}}}}],["723",{"pageContent":"const loadPrompt = async (uri: string): Promise<BasePromptTemplate> => {\nconst hubResult = await loadFromHub(\nuri,\nloadPromptFromFile,\n\"prompts\",\nnew Set([\"py\", \"json\", \"yaml\"])\n);\nif (hubResult) {\nreturn hubResult;\n}\n\nreturn loadFromFile(uri, loadPromptFromFile);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/load.ts","loc":{"lines":{"from":40,"to":52}}}}],["724",{"pageContent":"import { BaseStringPromptTemplate, BasePromptTemplateInput } from \"./base.js\";\nimport {\ncheckValidTemplate,\nparseTemplate,\nrenderTemplate,\nTemplateFormat,\n} from \"./template.js\";\nimport { SerializedPromptTemplate } from \"./serde.js\";\nimport { InputValues, PartialValues } from \"../schema/index.js\";\n\n/**\n* Inputs to create a {@link PromptTemplate}\n* @augments BasePromptTemplateInput\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":1,"to":14}}}}],["725",{"pageContent":"interface PromptTemplateInput extends BasePromptTemplateInput {\n/**\n* The prompt template\n*/\ntemplate: string;\n\n/**\n* The format of the prompt template. Options are 'f-string', 'jinja-2'\n*\n* @defaultValue 'f-string'\n*/\ntemplateFormat?: TemplateFormat;\n\n/**\n* Whether or not to try validating the template on initialization\n*\n* @defaultValue `true`\n*/\nvalidateTemplate?: boolean;\n}\n\n/**\n* Schema to represent a basic prompt for an LLM.\n* @augments BasePromptTemplate\n* @augments PromptTemplateInput\n*\n* @example\n* ```ts\n* import { PromptTemplate } from \"langchain/prompts\";\n*\n* const prompt = new PromptTemplate({\n*   inputVariables: [\"foo\"],\n*   template: \"Say {foo}\",\n* });\n* ```\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":183,"to":218}}}}],["726",{"pageContent":"class PromptTemplate\nextends BaseStringPromptTemplate\nimplements PromptTemplateInput\n{\ntemplate: string;\n\ntemplateFormat: TemplateFormat = \"f-string\";\n\nvalidateTemplate = true;\n\nconstructor(input: PromptTemplateInput) {\nsuper(input);\nObject.assign(this, input);\n\nif (this.validateTemplate) {\nlet totalInputVariables = this.inputVariables;\nif (this.partialVariables) {\ntotalInputVariables = totalInputVariables.concat(\nObject.keys(this.partialVariables)\n);\n}\ncheckValidTemplate(\nthis.template,\nthis.templateFormat,\ntotalInputVariables\n);\n}\n}\n\n_getPromptType(): \"prompt\" {\nreturn \"prompt\";\n}\n\nasync format(values: InputValues): Promise<string> {\nconst allValues = await this.mergePartialAndUserVariables(values);\nreturn renderTemplate(this.template, this.templateFormat, allValues);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":374,"to":410}}}}],["727",{"pageContent":"/**\n* Take examples in list format with prefix and suffix to create a prompt.\n*\n* Intendend to be used a a way to dynamically create a prompt from examples.\n*\n* @param examples - List of examples to use in the prompt.\n* @param suffix - String to go after the list of examples. Should generally set up the user's input.\n* @param inputVariables - A list of variable names the final prompt template will expect\n* @param exampleSeparator - The separator to use in between examples\n* @param prefix - String that should go before any examples. Generally includes examples.\n*\n* @returns The final prompt template generated.\n*/\nstatic fromExamples(\nexamples: string[],\nsuffix: string,\ninputVariables: string[],\nexampleSeparator = \"\\n\\n\",\nprefix = \"\"\n) {\nconst template = [prefix, ...examples, suffix].join(exampleSeparator);\nreturn new PromptTemplate({\ninputVariables,\ntemplate,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":561,"to":586}}}}],["728",{"pageContent":"/**\n* Load prompt template from a template f-string\n*/\nstatic fromTemplate(\ntemplate: string,\n{\ntemplateFormat = \"f-string\",\n...rest\n}: Omit<PromptTemplateInput, \"template\" | \"inputVariables\"> = {}\n) {\nconst names = new Set<string>();\nparseTemplate(template, templateFormat).forEach((node) => {\nif (node.type === \"variable\") {\nnames.add(node.name);\n}\n});\n\nreturn new PromptTemplate({\ninputVariables: [...names],\ntemplateFormat,\ntemplate,\n...rest,\n});\n}\n\nasync partial(values: PartialValues): Promise<PromptTemplate> {\nconst promptDict: PromptTemplateInput = { ...this };\npromptDict.inputVariables = this.inputVariables.filter(\n(iv) => !(iv in values)\n);\npromptDict.partialVariables = {\n...(this.partialVariables ?? {}),\n...values,\n};\nreturn new PromptTemplate(promptDict);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":731,"to":766}}}}],["729",{"pageContent":"serialize(): SerializedPromptTemplate {\nif (this.outputParser !== undefined) {\nthrow new Error(\n\"Cannot serialize a prompt template with an output parser\"\n);\n}\nreturn {\n_type: this._getPromptType(),\ninput_variables: this.inputVariables,\ntemplate: this.template,\ntemplate_format: this.templateFormat,\n};\n}\n\nstatic async deserialize(\ndata: SerializedPromptTemplate\n): Promise<PromptTemplate> {\nif (!data.template) {\nthrow new Error(\"Prompt template must have a template\");\n}\nconst res = new PromptTemplate({\ninputVariables: data.input_variables,\ntemplate: data.template,\ntemplateFormat: data.template_format,\n});\nreturn res;\n}\n\n// TODO(from file)\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/prompt.ts","loc":{"lines":{"from":917,"to":946}}}}],["730",{"pageContent":"import { Example } from \"../../schema/index.js\";\nimport type { BaseExampleSelector } from \"../base.js\";\nimport { PromptTemplate } from \"../prompt.js\";\n\nfunction getLengthBased(text: string): number {\nreturn text.split(/\\n| /).length;\n}\n\nexport interface LengthBasedExampleSelectorInput {\nexamplePrompt: PromptTemplate;\nmaxLength?: number;\ngetTextLength?: (text: string) => number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/LengthBasedExampleSelector.ts","loc":{"lines":{"from":1,"to":13}}}}],["731",{"pageContent":"class LengthBasedExampleSelector implements BaseExampleSelector {\nprotected examples: Example[] = [];\n\nexamplePrompt!: PromptTemplate;\n\ngetTextLength: (text: string) => number = getLengthBased;\n\nmaxLength = 2048;\n\nexampleTextLengths: number[] = [];\n\nconstructor(data: LengthBasedExampleSelectorInput) {\nthis.examplePrompt = data.examplePrompt;\nthis.maxLength = data.maxLength ?? 2048;\nthis.getTextLength = data.getTextLength ?? getLengthBased;\n}\n\nasync addExample(example: Example): Promise<void> {\nthis.examples.push(example);\nconst stringExample = await this.examplePrompt.format(example);\nthis.exampleTextLengths.push(this.getTextLength(stringExample));\n}\n\nasync calculateExampleTextLengths(\nv: number[],\nvalues: LengthBasedExampleSelector\n): Promise<number[]> {\nif (v.length > 0) {\nreturn v;\n}\n\nconst { examples, examplePrompt } = values;\nconst stringExamples = await Promise.all(\nexamples.map((eg: Example) => examplePrompt.format(eg))\n);\nreturn stringExamples.map((eg: string) => this.getTextLength(eg));\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/LengthBasedExampleSelector.ts","loc":{"lines":{"from":82,"to":118}}}}],["732",{"pageContent":"async selectExamples(inputVariables: Example): Promise<Example[]> {\nconst inputs = Object.values(inputVariables).join(\" \");\nlet remainingLength = this.maxLength - this.getTextLength(inputs);\nlet i = 0;\nconst examples: Example[] = [];\n\nwhile (remainingLength > 0 && i < this.examples.length) {\nconst newLength = remainingLength - this.exampleTextLengths[i];\nif (newLength < 0) {\nbreak;\n} else {\nexamples.push(this.examples[i]);\nremainingLength = newLength;\n}\ni += 1;\n}\n\nreturn examples;\n}\n\nstatic async fromExamples(\nexamples: Example[],\nargs: LengthBasedExampleSelectorInput\n) {\nconst selector = new LengthBasedExampleSelector(args);\nawait Promise.all(examples.map((eg) => selector.addExample(eg)));\nreturn selector;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/LengthBasedExampleSelector.ts","loc":{"lines":{"from":165,"to":193}}}}],["733",{"pageContent":"import { Embeddings } from \"../../embeddings/base.js\";\nimport { VectorStore } from \"../../vectorstores/base.js\";\nimport { Document } from \"../../document.js\";\nimport { Example } from \"../../schema/index.js\";\nimport type { BaseExampleSelector } from \"../base.js\";\n\nfunction sortedValues<T>(values: Record<string, T>): T[] {\nreturn Object.keys(values)\n.sort()\n.map((key) => values[key]);\n}\n\nexport interface SemanticSimilarityExampleSelectorInput {\nvectorStore: VectorStore;\nk?: number;\nexampleKeys?: string[];\ninputKeys?: string[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/SemanticSimilarityExampleSelector.ts","loc":{"lines":{"from":1,"to":18}}}}],["734",{"pageContent":"class SemanticSimilarityExampleSelector implements BaseExampleSelector {\nvectorStore: VectorStore;\n\nk = 4;\n\nexampleKeys?: string[];\n\ninputKeys?: string[];\n\nconstructor(data: SemanticSimilarityExampleSelectorInput) {\nthis.vectorStore = data.vectorStore;\nthis.k = data.k ?? 4;\nthis.exampleKeys = data.exampleKeys;\nthis.inputKeys = data.inputKeys;\n}\n\nasync addExample(example: Example): Promise<void> {\nconst inputKeys = this.inputKeys ?? Object.keys(example);\nconst stringExample = sortedValues(\ninputKeys.reduce(\n(acc, key) => ({ ...acc, [key]: example[key] }),\n{} as Example\n)\n).join(\" \");\n\nawait this.vectorStore.addDocuments([\nnew Document({\npageContent: stringExample,\nmetadata: { example },\n}),\n]);\n}\n\nasync selectExamples<T>(\ninputVariables: Record<string, T>\n): Promise<Example[]> {\nconst inputKeys = this.inputKeys ?? Object.keys(inputVariables);\nconst query = sortedValues(\ninputKeys.reduce(\n(acc, key) => ({ ...acc, [key]: inputVariables[key] }),\n{} as Record<string, T>\n)\n).join(\" \");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/SemanticSimilarityExampleSelector.ts","loc":{"lines":{"from":115,"to":157}}}}],["735",{"pageContent":"const exampleDocs = await this.vectorStore.similaritySearch(query, this.k);\n\nconst examples = exampleDocs.map((doc) => doc.metadata);\nif (this.exampleKeys) {\n// If example keys are provided, filter examples to those keys.\nreturn examples.map((example) =>\n(this.exampleKeys as string[]).reduce(\n(acc, key) => ({ ...acc, [key]: example[key] }),\n{}\n)\n);\n}\nreturn examples;\n}\n\nstatic async fromExamples<C extends typeof VectorStore>(\nexamples: Record<string, string>[],\nembeddings: Embeddings,\nvectorStoreCls: C,\noptions: {\nk?: number;\ninputKeys?: string[];\n} & Parameters<C[\"fromTexts\"]>[3] = {}\n): Promise<SemanticSimilarityExampleSelector> {\nconst inputKeys = options.inputKeys ?? null;\nconst stringExamples = examples.map((example) =>\nsortedValues(\ninputKeys\n? inputKeys.reduce(\n(acc, key) => ({ ...acc, [key]: example[key] }),\n{} as Record<string, string>\n)\n: example\n).join(\" \")\n);\n\nconst vectorStore = await vectorStoreCls.fromTexts(\nstringExamples,\nexamples, // metadatas\nembeddings,\noptions\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/SemanticSimilarityExampleSelector.ts","loc":{"lines":{"from":234,"to":275}}}}],["736",{"pageContent":"return new SemanticSimilarityExampleSelector({\nvectorStore,\nk: options.k ?? 4,\nexampleKeys: options.exampleKeys,\ninputKeys: options.inputKeys,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/selectors/SemanticSimilarityExampleSelector.ts","loc":{"lines":{"from":352,"to":359}}}}],["737",{"pageContent":"import type { Example } from \"../schema/index.js\";\nimport type { TemplateFormat } from \"./template.js\";\n\nexport type SerializedPromptTemplate = {\n_type?: \"prompt\";\ninput_variables: string[];\ntemplate_format?: TemplateFormat;\ntemplate?: string;\n};\n\nexport type SerializedFewShotTemplate = {\n_type: \"few_shot\";\ninput_variables: string[];\nexamples: string | Example[];\nexample_prompt?: SerializedPromptTemplate;\nexample_separator: string;\nprefix?: string;\nsuffix?: string;\ntemplate_format: TemplateFormat;\n};\n\nexport type SerializedMessagePromptTemplate = {\n_type: \"message\";\ninput_variables: string[];\n[key: string]: unknown;\n};\n\n/** Serialized Chat prompt template */\nexport type SerializedChatPromptTemplate = {\n_type?: \"chat_prompt\";\ninput_variables: string[];\ntemplate_format?: TemplateFormat;\nprompt_messages: SerializedMessagePromptTemplate[];\n};\n\nexport type SerializedBasePromptTemplate =\n| SerializedFewShotTemplate\n| SerializedPromptTemplate\n| SerializedChatPromptTemplate;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/serde.ts","loc":{"lines":{"from":1,"to":39}}}}],["738",{"pageContent":"import { InputValues } from \"../schema/index.js\";\n\nexport type TemplateFormat = \"f-string\" | \"jinja2\";\n\ntype ParsedFStringNode =\n| { type: \"literal\"; text: string }\n| { type: \"variable\"; name: string };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/template.ts","loc":{"lines":{"from":1,"to":7}}}}],["739",{"pageContent":"const parseFString = (template: string): ParsedFStringNode[] => {\n// Core logic replicated from internals of pythons built in Formatter class.\n// https://github.com/python/cpython/blob/135ec7cefbaffd516b77362ad2b2ad1025af462e/Objects/stringlib/unicode_format.h#L700-L706\nconst chars = template.split(\"\");\nconst nodes: ParsedFStringNode[] = [];\n\nconst nextBracket = (bracket: \"}\" | \"{\" | \"{}\", start: number) => {\nfor (let i = start; i < chars.length; i += 1) {\nif (bracket.includes(chars[i])) {\nreturn i;\n}\n}\nreturn -1;\n};\n\nlet i = 0;\nwhile (i < chars.length) {\nif (chars[i] === \"{\" && i + 1 < chars.length && chars[i + 1] === \"{\") {\nnodes.push({ type: \"literal\", text: \"{\" });\ni += 2;\n} else if (\nchars[i] === \"}\" &&\ni + 1 < chars.length &&\nchars[i + 1] === \"}\"\n) {\nnodes.push({ type: \"literal\", text: \"}\" });\ni += 2;\n} else if (chars[i] === \"{\") {\nconst j = nextBracket(\"}\", i);\nif (j < 0) {\nthrow new Error(\"Unclosed '{' in template.\");\n}\n\nnodes.push({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/template.ts","loc":{"lines":{"from":115,"to":148}}}}],["740",{"pageContent":": \"variable\",\nname: chars.slice(i + 1, j).join(\"\"),\n});\ni = j + 1;\n} else if (chars[i] === \"}\") {\nthrow new Error(\"Single '}' in template.\");\n} else {\nconst next = nextBracket(\"{}\", i);\nconst text = (next < 0 ? chars.slice(i) : chars.slice(i, next)).join(\"\");\nnodes.push({ type: \"literal\", text });\ni = next < 0 ? chars.length : next;\n}\n}\nreturn nodes;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/template.ts","loc":{"lines":{"from":235,"to":249}}}}],["741",{"pageContent":"const interpolateFString = (template: string, values: InputValues) =>\nparseFString(template).reduce((res, node) => {\nif (node.type === \"variable\") {\nif (node.name in values) {\nreturn res + values[node.name];\n}\nthrow new Error(`Missing value for input ${node.name}`);\n}\n\nreturn res + node.text;\n}, \"\");\n\ntype Interpolator = (template: string, values: InputValues) => string;\n\ntype Parser = (template: string) => ParsedFStringNode[];\n\nexport const DEFAULT_FORMATTER_MAPPING: Record<TemplateFormat, Interpolator> = {\n\"f-string\": interpolateFString,\njinja2: (_: string, __: InputValues) => \"\",\n};\n\nexport const DEFAULT_PARSER_MAPPING: Record<TemplateFormat, Parser> = {\n\"f-string\": parseFString,\njinja2: (_: string) => [],\n};\n\nexport const renderTemplate = (\ntemplate: string,\ntemplateFormat: TemplateFormat,\ninputValues: InputValues\n) => DEFAULT_FORMATTER_MAPPING[templateFormat](template, inputValues);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/template.ts","loc":{"lines":{"from":354,"to":384}}}}],["742",{"pageContent":"const parseTemplate = (\ntemplate: string,\ntemplateFormat: TemplateFormat\n) => DEFAULT_PARSER_MAPPING[templateFormat](template);\n\nexport const checkValidTemplate = (\ntemplate: string,\ntemplateFormat: TemplateFormat,\ninputVariables: string[]\n) => {\nif (!(templateFormat in DEFAULT_FORMATTER_MAPPING)) {\nconst validFormats = Object.keys(DEFAULT_FORMATTER_MAPPING);\nthrow new Error(`Invalid template format. Got \\`${templateFormat}\\`;\nshould be one of ${validFormats}`);\n}\ntry {\nconst dummyInputs: InputValues = inputVariables.reduce((acc, v) => {\nacc[v] = \"foo\";\nreturn acc;\n}, {} as Record<string, string>);\nrenderTemplate(template, templateFormat, dummyInputs);\n} catch {\nthrow new Error(\"Invalid prompt schema.\");\n}\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/template.ts","loc":{"lines":{"from":473,"to":497}}}}],["743",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport {\nAIMessagePromptTemplate,\nChatPromptTemplate,\nChatMessagePromptTemplate,\nHumanMessagePromptTemplate,\nSystemMessagePromptTemplate,\nMessagesPlaceholder,\n} from \"../chat.js\";\nimport { PromptTemplate } from \"../prompt.js\";\nimport {\nAIChatMessage,\nChatMessage,\nHumanChatMessage,\nSystemChatMessage,\n} from \"../../schema/index.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":1,"to":16}}}}],["744",{"pageContent":"createChatPromptTemplate(): ChatPromptTemplate {\nconst systemPrompt = new PromptTemplate({\ntemplate: \"Here's some context: {context}\",\ninputVariables: [\"context\"],\n});\nconst userPrompt = new PromptTemplate({\ntemplate: \"Hello {foo}, I'm {bar}. Thanks for the {context}\",\ninputVariables: [\"foo\", \"bar\", \"context\"],\n});\nconst aiPrompt = new PromptTemplate({\ntemplate: \"I'm an AI. I'm {foo}. I'm {bar}.\",\ninputVariables: [\"foo\", \"bar\"],\n});\nconst genericPrompt = new PromptTemplate({\ntemplate: \"I'm a generic message. I'm {foo}. I'm {bar}.\",\ninputVariables: [\"foo\", \"bar\"],\n});\nreturn new ChatPromptTemplate({\npromptMessages: [\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\nnew AIMessagePromptTemplate(aiPrompt),\nnew ChatMessagePromptTemplate(genericPrompt, \"test\"),\n],\ninputVariables: [\"context\", \"foo\", \"bar\"],\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":228,"to":254}}}}],["745",{"pageContent":"test(\"Test format\", async () => {\nconst chatPrompt = createChatPromptTemplate();\nconst messages = await chatPrompt.formatPromptValue({\ncontext: \"This is a context\",\nfoo: \"Foo\",\nbar: \"Bar\",\n});\nexpect(messages.toChatMessages()).toEqual([\nnew SystemChatMessage(\"Here's some context: This is a context\"),\nnew HumanChatMessage(\n\"Hello Foo, I'm Bar. Thanks for the This is a context\"\n),\nnew AIChatMessage(\"I'm an AI. I'm Foo. I'm Bar.\"),\nnew ChatMessage(\"I'm a generic message. I'm Foo. I'm Bar.\", \"test\"),\n]);\n});\n\ntest(\"Test serialize\", async () => {\nconst chatPrompt = createChatPromptTemplate();\nexpect(chatPrompt.serialize()).toMatchSnapshot();\n});\n\ntest(\"Test format with invalid input values\", async () => {\nconst chatPrompt = createChatPromptTemplate();\nawait expect(\nchatPrompt.formatPromptValue({\ncontext: \"This is a context\",\nfoo: \"Foo\",\n})\n).rejects.toThrow(\"Missing value for input variable `bar`\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":453,"to":483}}}}],["746",{"pageContent":"test(\"Test format with invalid input variables\", async () => {\nconst systemPrompt = new PromptTemplate({\ntemplate: \"Here's some context: {context}\",\ninputVariables: [\"context\"],\n});\nconst userPrompt = new PromptTemplate({\ntemplate: \"Hello {foo}, I'm {bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nexpect(\n() =>\nnew ChatPromptTemplate({\npromptMessages: [\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\n],\ninputVariables: [\"context\", \"foo\", \"bar\", \"baz\"],\n})\n).toThrow(\n\"Input variables `baz` are not used in any of the prompt messages.\"\n);\n\nexpect(\n() =>\nnew ChatPromptTemplate({\npromptMessages: [\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\n],\ninputVariables: [\"context\", \"foo\"],\n})\n).toThrow(\n\"Input variables `bar` are used in prompt messages but not in the prompt template.\"\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":680,"to":714}}}}],["747",{"pageContent":"test(\"Test fromPromptMessages\", async () => {\nconst systemPrompt = new PromptTemplate({\ntemplate: \"Here's some context: {context}\",\ninputVariables: [\"context\"],\n});\nconst userPrompt = new PromptTemplate({\ntemplate: \"Hello {foo}, I'm {bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\n]);\nexpect(chatPrompt.inputVariables).toEqual([\"context\", \"foo\", \"bar\"]);\nconst messages = await chatPrompt.formatPromptValue({\ncontext: \"This is a context\",\nfoo: \"Foo\",\nbar: \"Bar\",\n});\nexpect(messages.toChatMessages()).toEqual([\nnew SystemChatMessage(\"Here's some context: This is a context\"),\nnew HumanChatMessage(\"Hello Foo, I'm Bar\"),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":913,"to":936}}}}],["748",{"pageContent":"test(\"Test fromPromptMessages is composable\", async () => {\nconst systemPrompt = new PromptTemplate({\ntemplate: \"Here's some context: {context}\",\ninputVariables: [\"context\"],\n});\nconst userPrompt = new PromptTemplate({\ntemplate: \"Hello {foo}, I'm {bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nconst chatPromptInner = ChatPromptTemplate.fromPromptMessages([\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\n]);\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nchatPromptInner,\nAIMessagePromptTemplate.fromTemplate(\"I'm an AI. I'm {foo}. I'm {bar}.\"),\n]);\nexpect(chatPrompt.inputVariables).toEqual([\"context\", \"foo\", \"bar\"]);\nconst messages = await chatPrompt.formatPromptValue({\ncontext: \"This is a context\",\nfoo: \"Foo\",\nbar: \"Bar\",\n});\nexpect(messages.toChatMessages()).toEqual([\nnew SystemChatMessage(\"Here's some context: This is a context\"),\nnew HumanChatMessage(\"Hello Foo, I'm Bar\"),\nnew AIChatMessage(\"I'm an AI. I'm Foo. I'm Bar.\"),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":1137,"to":1165}}}}],["749",{"pageContent":"test(\"Test fromPromptMessages is composable with partial vars\", async () => {\nconst systemPrompt = new PromptTemplate({\ntemplate: \"Here's some context: {context}\",\ninputVariables: [\"context\"],\n});\nconst userPrompt = new PromptTemplate({\ntemplate: \"Hello {foo}, I'm {bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nconst chatPromptInner = ChatPromptTemplate.fromPromptMessages([\nnew SystemMessagePromptTemplate(systemPrompt),\nnew HumanMessagePromptTemplate(userPrompt),\n]);\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\nawait chatPromptInner.partial({\ncontext: \"This is a context\",\nfoo: \"Foo\",\n}),\nAIMessagePromptTemplate.fromTemplate(\"I'm an AI. I'm {foo}. I'm {bar}.\"),\n]);\nexpect(chatPrompt.inputVariables).toEqual([\"bar\"]);\nconst messages = await chatPrompt.formatPromptValue({\nbar: \"Bar\",\n});\nexpect(messages.toChatMessages()).toEqual([\nnew SystemChatMessage(\"Here's some context: This is a context\"),\nnew HumanChatMessage(\"Hello Foo, I'm Bar\"),\nnew AIChatMessage(\"I'm an AI. I'm Foo. I'm Bar.\"),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":1360,"to":1389}}}}],["750",{"pageContent":"test(\"Test SimpleMessagePromptTemplate\", async () => {\nconst prompt = new MessagesPlaceholder(\"foo\");\nconst values = { foo: [new HumanChatMessage(\"Hello Foo, I'm Bar\")] };\nconst messages = await prompt.formatMessages(values);\nexpect(messages).toEqual([new HumanChatMessage(\"Hello Foo, I'm Bar\")]);\n});\n\ntest(\"Test using partial\", async () => {\nconst userPrompt = new PromptTemplate({\ntemplate: \"{foo}{bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst prompt = new ChatPromptTemplate({\npromptMessages: [new HumanMessagePromptTemplate(userPrompt)],\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst partialPrompt = await prompt.partial({ foo: \"foo\" });\n\n// original prompt is not modified\nexpect(prompt.inputVariables).toEqual([\"foo\", \"bar\"]);\n// partial prompt has only remaining variables\nexpect(partialPrompt.inputVariables).toEqual([\"bar\"]);\n\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\n'[{\"text\":\"foobaz\"}]'\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/chat.test.ts","loc":{"lines":{"from":1584,"to":1612}}}}],["751",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { FewShotPromptTemplate } from \"../few_shot.js\";\nimport { LengthBasedExampleSelector } from \"../index.js\";\nimport { PromptTemplate } from \"../prompt.js\";\n\ntest(\"Test using partial\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"{foo}{bar}\");\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexamples: [],\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [\"foo\"],\npartialVariables: { bar: \"baz\" },\n});\nexpect(await prompt.format({ foo: \"foo\" })).toBe(\"foobaz\\n\");\n});\n\ntest(\"Test using full partial\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"{foo}{bar}\");\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexamples: [],\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [],\npartialVariables: { bar: \"baz\", foo: \"boo\" },\n});\nexpect(await prompt.format({})).toBe(\"boobaz\\n\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/few_shot.test.ts","loc":{"lines":{"from":1,"to":34}}}}],["752",{"pageContent":"test(\"Test partial with string\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"{foo}{bar}\");\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexamples: [],\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst partialPrompt = await prompt.partial({ foo: \"foo\" });\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\"foobaz\\n\");\nexpect(prompt.inputVariables).toEqual([\"foo\", \"bar\"]);\n});\n\ntest(\"Test partial with function\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"{foo}{bar}\");\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexamples: [],\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst partialPrompt = await prompt.partial({\nfoo: () => Promise.resolve(\"boo\"),\n});\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\"boobaz\\n\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/few_shot.test.ts","loc":{"lines":{"from":122,"to":155}}}}],["753",{"pageContent":"test(\"Test partial with function and examples\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"An example about {x}\");\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexamples: [{ x: \"foo\" }, { x: \"bar\" }],\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst partialPrompt = await prompt.partial({\nfoo: () => Promise.resolve(\"boo\"),\n});\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\n`boobaz\nAn example about foo\nAn example about bar\n`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/few_shot.test.ts","loc":{"lines":{"from":244,"to":265}}}}],["754",{"pageContent":"test(\"Test partial with function and example selector\", async () => {\nconst examplePrompt = PromptTemplate.fromTemplate(\"An example about {x}\");\nconst exampleSelector = await LengthBasedExampleSelector.fromExamples(\n[{ x: \"foo\" }, { x: \"bar\" }],\n{ examplePrompt, maxLength: 200 }\n);\nconst prompt = new FewShotPromptTemplate({\nprefix: \"{foo}{bar}\",\nexampleSelector,\nsuffix: \"\",\ntemplateFormat: \"f-string\",\nexampleSeparator: \"\\n\",\nexamplePrompt,\ninputVariables: [\"foo\", \"bar\"],\n});\n\nconst partialPrompt = await prompt.partial({\nfoo: () => Promise.resolve(\"boo\"),\n});\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\n`boobaz\nAn example about foo\nAn example about bar\n`\n);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/few_shot.test.ts","loc":{"lines":{"from":368,"to":393}}}}],["755",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport * as path from \"node:path\";\nimport { fileURLToPath } from \"node:url\";\nimport { loadPrompt } from \"../load.js\";\n\ntest(\"Load Hello World Prompt\", async () => {\nconst helloWorld = path.join(\npath.join(path.dirname(fileURLToPath(import.meta.url)), \"prompts\"),\n\"hello_world.yaml\"\n);\nconst prompt = await loadPrompt(helloWorld);\nexpect(prompt._getPromptType()).toBe(\"prompt\");\nexpect(await prompt.format({})).toBe(\"Say hello world.\");\n});\n\ntest(\"Load hub prompt\", async () => {\nconst prompt = await loadPrompt(\n\"lc@abb92d8://prompts/hello-world/prompt.yaml\"\n);\nexpect(prompt._getPromptType()).toBe(\"prompt\");\nexpect(await prompt.format({})).toBe(\"Say hello world.\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/load.int.test.ts","loc":{"lines":{"from":1,"to":22}}}}],["756",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { PromptTemplate } from \"../prompt.js\";\n\ntest(\"Test using partial\", async () => {\nconst prompt = new PromptTemplate({\ntemplate: \"{foo}{bar}\",\ninputVariables: [\"foo\"],\npartialVariables: { bar: \"baz\" },\n});\nexpect(await prompt.format({ foo: \"foo\" })).toBe(\"foobaz\");\n});\n\ntest(\"Test using full partial\", async () => {\nconst prompt = new PromptTemplate({\ntemplate: \"{foo}{bar}\",\ninputVariables: [],\npartialVariables: { bar: \"baz\", foo: \"boo\" },\n});\nexpect(await prompt.format({})).toBe(\"boobaz\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/prompt.test.ts","loc":{"lines":{"from":1,"to":20}}}}],["757",{"pageContent":"test(\"Test partial\", async () => {\nconst prompt = new PromptTemplate({\ntemplate: \"{foo}{bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nexpect(prompt.inputVariables).toEqual([\"foo\", \"bar\"]);\nconst partialPrompt = await prompt.partial({ foo: \"foo\" });\n// original prompt is not modified\nexpect(prompt.inputVariables).toEqual([\"foo\", \"bar\"]);\n// partial prompt has only remaining variables\nexpect(partialPrompt.inputVariables).toEqual([\"bar\"]);\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\"foobaz\");\n});\n\ntest(\"Test partial with function\", async () => {\nconst prompt = new PromptTemplate({\ntemplate: \"{foo}{bar}\",\ninputVariables: [\"foo\", \"bar\"],\n});\nconst partialPrompt = await prompt.partial({\nfoo: () => Promise.resolve(\"boo\"),\n});\nexpect(await partialPrompt.format({ bar: \"baz\" })).toBe(\"boobaz\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/prompt.test.ts","loc":{"lines":{"from":46,"to":69}}}}],["758",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";\nimport { LengthBasedExampleSelector } from \"../selectors/LengthBasedExampleSelector.js\";\nimport { SemanticSimilarityExampleSelector } from \"../selectors/SemanticSimilarityExampleSelector.js\";\nimport { HNSWLib } from \"../../vectorstores/hnswlib.js\";\nimport { PromptTemplate } from \"../prompt.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/selectors.test.ts","loc":{"lines":{"from":1,"to":6}}}}],["759",{"pageContent":"test(\"Test using LengthBasedExampleSelector\", async () => {\nconst prompt = new PromptTemplate({\ntemplate: \"{foo} {bar}\",\ninputVariables: [\"foo\"],\npartialVariables: { bar: \"baz\" },\n});\nconst selector = await LengthBasedExampleSelector.fromExamples(\n[{ foo: \"one one one\" }],\n{\nexamplePrompt: prompt,\nmaxLength: 10,\n}\n);\nawait selector.addExample({ foo: \"one two three\" });\nawait selector.addExample({ foo: \"four five six\" });\nawait selector.addExample({ foo: \"seven eight nine\" });\nawait selector.addExample({ foo: \"ten eleven twelve\" });\nconst chosen = await selector.selectExamples({ foo: \"hello\", bar: \"world\" });\nexpect(chosen).toStrictEqual([\n{ foo: \"one one one\" },\n{ foo: \"one two three\" },\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/selectors.test.ts","loc":{"lines":{"from":43,"to":65}}}}],["760",{"pageContent":"test(\"Test using SemanticSimilarityExampleSelector\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\", \"bye\", \"hi\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\nnew FakeEmbeddings() // not using  OpenAIEmbeddings() because would be extra dependency\n);\nconst selector = new SemanticSimilarityExampleSelector({\nvectorStore,\n});\nconst chosen = await selector.selectExamples({ id: 1 });\nexpect(chosen).toEqual([{ id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/selectors.test.ts","loc":{"lines":{"from":92,"to":103}}}}],["761",{"pageContent":"import { expect, test, describe } from \"@jest/globals\";\nimport { interpolateFString } from \"../template.js\";\n\ndescribe.each([\n[\"{foo}\", { foo: \"bar\" }, \"bar\"],\n[\"pre{foo}post\", { foo: \"bar\" }, \"prebarpost\"],\n[\"{{pre{foo}post}}\", { foo: \"bar\" }, \"{prebarpost}\"],\n[\"text\", {}, \"text\"],\n[\"}}{{\", {}, \"}{\"],\n[\"{first}_{second}\", { first: \"foo\", second: \"bar\" }, \"foo_bar\"],\n])(\"Valid f-string\", (template, variables, result) => {\ntest(`Interpolation works: ${template}`, () => {\nexpect(interpolateFString(template, variables)).toBe(result);\n});\n});\n\ndescribe.each([\n[\"{\", {}],\n[\"}\", {}],\n[\"{foo\", {}],\n[\"foo}\", {}],\n])(\"Invalid f-string\", (template, variables) => {\ntest(`Interpolation throws: ${template}`, () => {\nexpect(() => interpolateFString(template, variables)).toThrow();\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/prompts/tests/template.test.ts","loc":{"lines":{"from":1,"to":26}}}}],["762",{"pageContent":"import { BaseDocumentCompressor } from \"./document_compressors/index.js\";\nimport { Document } from \"../document.js\";\nimport { BaseRetriever } from \"../schema/index.js\";\n\nexport interface ContextualCompressionRetrieverArgs {\nbaseCompressor: BaseDocumentCompressor;\nbaseRetriever: BaseRetriever;\n}\n\nexport class ContextualCompressionRetriever extends BaseRetriever {\nbaseCompressor: BaseDocumentCompressor;\n\nbaseRetriever: BaseRetriever;\n\nconstructor({\nbaseCompressor,\nbaseRetriever,\n}: ContextualCompressionRetrieverArgs) {\nsuper();\n\nthis.baseCompressor = baseCompressor;\nthis.baseRetriever = baseRetriever;\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nconst docs = await this.baseRetriever.getRelevantDocuments(query);\nconst compressedDocs = await this.baseCompressor.compressDocuments(\ndocs,\nquery\n);\nreturn compressedDocs;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/contextual_compression.ts","loc":{"lines":{"from":1,"to":33}}}}],["763",{"pageContent":"import { BaseRetriever } from \"../schema/index.js\";\nimport { Document } from \"../document.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../util/async_caller.js\";\n\nexport interface DataberryRetrieverArgs extends AsyncCallerParams {\ndatastoreUrl: string;\ntopK?: number;\napiKey?: string;\n}\n\ninterface Berry {\ntext: string;\nscore: number;\nsource?: string;\n[key: string]: unknown;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/databerry.ts","loc":{"lines":{"from":1,"to":16}}}}],["764",{"pageContent":"class DataberryRetriever extends BaseRetriever {\ncaller: AsyncCaller;\n\ndatastoreUrl: string;\n\ntopK?: number;\n\napiKey?: string;\n\nconstructor({ datastoreUrl, apiKey, topK, ...rest }: DataberryRetrieverArgs) {\nsuper();\n\nthis.caller = new AsyncCaller(rest);\nthis.datastoreUrl = datastoreUrl;\nthis.apiKey = apiKey;\nthis.topK = topK;\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nconst r = await this.caller.call(fetch, this.datastoreUrl, {\nmethod: \"POST\",\nbody: JSON.stringify({\nquery,\n...(this.topK ? { topK: this.topK } : {}),\n}),\nheaders: {\n\"Content-Type\": \"application/json\",\n...(this.apiKey ? { Authorization: `Bearer ${this.apiKey}` } : {}),\n},\n});\n\nconst { results } = (await r.json()) as { results: Berry[] };\n\nreturn results.map(\n({ text, score, source, ...rest }) =>\nnew Document({\npageContent: text,\nmetadata: {\nscore,\nsource,\n...rest,\n},\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/databerry.ts","loc":{"lines":{"from":64,"to":109}}}}],["765",{"pageContent":"import { Document } from \"../../document.js\";\n\n/**\n* Base Document Compression class. All compressors should extend this class.\n*/\nexport abstract class BaseDocumentCompressor {\nabstract compressDocuments(\ndocuments: Document[],\nquery: string\n): Promise<Document[]>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/document_compressors/index.ts","loc":{"lines":{"from":1,"to":11}}}}],["766",{"pageContent":"import { Document } from \"../document.js\";\nimport { BasePromptTemplate, StringPromptValue } from \"../prompts/base.js\";\nimport { PromptTemplate } from \"../prompts/prompt.js\";\nimport {\nVectorStore,\nVectorStoreRetriever,\nVectorStoreRetrieverInput,\n} from \"../vectorstores/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { BasePromptValue } from \"../schema/index.js\";\n\nexport type PromptKey =\n| \"websearch\"\n| \"scifact\"\n| \"arguana\"\n| \"trec-covid\"\n| \"fiqa\"\n| \"dbpedia-entity\"\n| \"trec-news\"\n| \"mr-tydi\";\n\nexport interface HydeRetrieverOptions<V extends VectorStore>\nextends VectorStoreRetrieverInput<V> {\nllm: BaseLanguageModel;\npromptTemplate?: BasePromptTemplate | PromptKey;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":1,"to":26}}}}],["767",{"pageContent":"class HydeRetriever<\nV extends VectorStore = VectorStore\n> extends VectorStoreRetriever<V> {\nllm: BaseLanguageModel;\n\npromptTemplate?: BasePromptTemplate;\n\nconstructor(fields: HydeRetrieverOptions<V>) {\nsuper(fields);\nthis.llm = fields.llm;\nthis.promptTemplate =","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":125,"to":135}}}}],["768",{"pageContent":"of fields.promptTemplate === \"string\"\n? getPromptTemplateFromKey(fields.promptTemplate)\n: fields.promptTemplate;\nif (this.promptTemplate) {\nconst { inputVariables } = this.promptTemplate;\nif (inputVariables.length !== 1 && inputVariables[0] !== \"question\") {\nthrow new Error(\n`Prompt template must accept a single input variable 'question'. Invalid input variables for prompt template: ${inputVariables}`\n);\n}\n}\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nlet value: BasePromptValue = new StringPromptValue(query);\n\n// Use a custom template if provided\nif (this.promptTemplate) {\nvalue = await this.promptTemplate.formatPromptValue({ question: query });\n}\n\n// Get a hypothetical answer from the LLM\nconst res = await this.llm.generatePrompt([value]);\nconst answer = res.generations[0][0].text;\n\n// Retrieve relevant documents based on the hypothetical answer\nconst results = await this.vectorStore.similaritySearch(\nanswer,\nthis.k,\nthis.filter\n);\n\nreturn results;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":251,"to":285}}}}],["769",{"pageContent":"function getPromptTemplateFromKey(key: PromptKey): BasePromptTemplate {\nlet template: string;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":372,"to":373}}}}],["770",{"pageContent":"switch (key) {\ncase \"websearch\":\ntemplate = `Please write a passage to answer the question \nQuestion: {question}\nPassage:`;\nbreak;\ncase \"scifact\":\ntemplate = `Please write a scientific paper passage to support/refute the claim \nClaim: {question}\nPassage:`;\nbreak;\ncase \"arguana\":\ntemplate = `Please write a counter argument for the passage \nPassage: {question}\nCounter Argument:`;\nbreak;\ncase \"trec-covid\":\ntemplate = `Please write a scientific paper passage to answer the question\nQuestion: {question}\nPassage:`;\nbreak;\ncase \"fiqa\":\ntemplate = `Please write a financial article passage to answer the question\nQuestion: {question}\nPassage:`;\nbreak;\ncase \"dbpedia-entity\":\ntemplate = `Please write a passage to answer the question.\nQuestion: {question}\nPassage:`;\nbreak;\ncase \"trec-news\":\ntemplate = `Please write a news passage about the topic.\nTopic: {question}\nPassage:`;\nbreak;\ncase \"mr-tydi\":\ntemplate = `Please write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail.\nQuestion: {question}\nPassage:`;\nbreak;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":495,"to":535}}}}],["771",{"pageContent":":\nthrow new Error(`Invalid prompt key: ${key}`);\n}\n\nreturn PromptTemplate.fromTemplate(template);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/hyde.ts","loc":{"lines":{"from":620,"to":625}}}}],["772",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/retrievers' is deprecated. Import from eg. 'langchain/retrievers/remote' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport { RemoteRetriever } from \"./remote/base.js\";\nexport { ChatGPTPluginRetriever } from \"./remote/chatgpt-plugin.js\";\nexport {\nSupabaseHybridSearch,\nSupabaseHybridSearchParams,\n} from \"./supabase.js\";\nexport { RemoteLangChainRetriever } from \"./remote/remote-retriever.js\";\nexport { MetalRetriever } from \"./metal.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/index.ts","loc":{"lines":{"from":1,"to":12}}}}],["773",{"pageContent":"import { BaseRetriever } from \"../schema/index.js\";\nimport { Document } from \"../document.js\";\n\nexport interface MetalRetrieverFields {\nclient: import(\"@getmetal/metal-sdk\").default;\n}\n\ninterface ResponseItem {\ntext: string;\n[key: string]: unknown;\n}\n\nexport class MetalRetriever extends BaseRetriever {\nprivate client: import(\"@getmetal/metal-sdk\").default;\n\nconstructor(fields: MetalRetrieverFields) {\nsuper();\n\nthis.client = fields.client;\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nconst res = await this.client.search({ text: query });\n\nconst items = (\"data\" in res ? res.data : res) as ResponseItem[];\nreturn items.map(\n({ text, metadata }) =>\nnew Document({\npageContent: text,\nmetadata: metadata as Record<string, unknown>,\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/metal.ts","loc":{"lines":{"from":1,"to":34}}}}],["774",{"pageContent":"import { BaseRetriever } from \"../../schema/index.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../../util/async_caller.js\";\nimport { Document } from \"../../document.js\";\n\nexport type RemoteRetrieverAuth = false | { bearer: string };\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type RemoteRetrieverValues = Record<string, any>;\n\nexport interface RemoteRetrieverParams extends AsyncCallerParams {\n/**\n* The URL of the remote retriever server\n*/\nurl: string;\n\n/**\n* The authentication method to use, currently implemented is\n* - false: no authentication\n* - { bearer: string }: Bearer token authentication\n*/\nauth: RemoteRetrieverAuth;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/base.ts","loc":{"lines":{"from":1,"to":21}}}}],["775",{"pageContent":"abstract class RemoteRetriever\nextends BaseRetriever\nimplements RemoteRetrieverParams\n{\nurl: string;\n\nauth: RemoteRetrieverAuth;\n\nheaders: Record<string, string>;\n\nasyncCaller: AsyncCaller;\n\nconstructor({ url, auth, ...rest }: RemoteRetrieverParams) {\nsuper();\nthis.url = url;\nthis.auth = auth;\nthis.headers = {\nAccept: \"application/json\",\n\"Content-Type\": \"application/json\",\n...(this.auth && this.auth.bearer\n? { Authorization: `Bearer ${this.auth.bearer}` }\n: {}),\n};\nthis.asyncCaller = new AsyncCaller(rest);\n}\n\nabstract createJsonBody(query: string): RemoteRetrieverValues;\n\nabstract processJsonResponse(json: RemoteRetrieverValues): Document[];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/base.ts","loc":{"lines":{"from":71,"to":99}}}}],["776",{"pageContent":"async getRelevantDocuments(query: string): Promise<Document[]> {\nconst body = this.createJsonBody(query);\nconst response = await this.asyncCaller.call(() =>\nfetch(this.url, {\nmethod: \"POST\",\nheaders: this.headers,\nbody: JSON.stringify(body),\n})\n);\nif (!response.ok) {\nthrow new Error(\n`Failed to retrieve documents from ${this.url}: ${response.status} ${response.statusText}`\n);\n}\nconst json = await response.json();\nreturn this.processJsonResponse(json);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/base.ts","loc":{"lines":{"from":151,"to":168}}}}],["777",{"pageContent":"import { Document } from \"../../document.js\";\nimport {\nRemoteRetriever,\nRemoteRetrieverParams,\nRemoteRetrieverValues,\n} from \"./base.js\";\n\nexport interface ChatGPTPluginRetrieverFilter {\ndocument_id?: string;\nsource?: string;\nsource_id?: string;\nauthor?: string;\nstart_date?: string;\nend_date?: string;\n}\n\nexport interface ChatGPTPluginRetrieverParams extends RemoteRetrieverParams {\n/**\n* The number of results to request from the ChatGPTRetrievalPlugin server\n*/\ntopK?: number;\n\n/**\n* The filter to use when querying the ChatGPTRetrievalPlugin server\n*/\nfilter?: ChatGPTPluginRetrieverFilter;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/chatgpt-plugin.ts","loc":{"lines":{"from":1,"to":27}}}}],["778",{"pageContent":"class ChatGPTPluginRetriever\nextends RemoteRetriever\nimplements ChatGPTPluginRetrieverParams\n{\ntopK: number;\n\nfilter?: ChatGPTPluginRetrieverFilter;\n\nconstructor({ topK = 4, filter, ...rest }: ChatGPTPluginRetrieverParams) {\nsuper(rest);\nthis.topK = topK;\nthis.filter = filter;\n}\n\ncreateJsonBody(query: string): RemoteRetrieverValues {\nreturn {\nqueries: [\n{\nquery,\ntop_k: this.topK,\nfilter: this.filter,\n},\n],\n};\n}\n\nprocessJsonResponse(json: RemoteRetrieverValues): Document[] {\nconst results = json?.results?.[0]?.results;\n\nif (!results) {\n// Note an empty array of results would not fall into this case\nthrow new Error(\"No results returned from ChatGPTPluginRetriever\");\n}\n\nreturn results.map(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n(result: any) =>\nnew Document({\npageContent: result.text,\nmetadata: result.metadata,\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/chatgpt-plugin.ts","loc":{"lines":{"from":73,"to":116}}}}],["779",{"pageContent":"export {\nRemoteRetriever,\nRemoteRetrieverParams,\nRemoteRetrieverAuth,\nRemoteRetrieverValues,\n} from \"./base.js\";\nexport {\nChatGPTPluginRetriever,\nChatGPTPluginRetrieverFilter,\nChatGPTPluginRetrieverParams,\n} from \"./chatgpt-plugin.js\";\nexport {\nRemoteLangChainRetriever,\nRemoteLangChainRetrieverParams,\n} from \"./remote-retriever.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/index.ts","loc":{"lines":{"from":1,"to":15}}}}],["780",{"pageContent":"import { Document } from \"../../document.js\";\nimport {\nRemoteRetriever,\nRemoteRetrieverParams,\nRemoteRetrieverValues,\n} from \"./base.js\";\n\nexport interface RemoteLangChainRetrieverParams extends RemoteRetrieverParams {\n/**\n* The key in the JSON body to put the query in\n*/\ninputKey?: string;\n/**\n* The key in the JSON response to get the response from\n*/\nresponseKey?: string;\n/**\n* The key in the JSON response to get the page content from\n*/\npageContentKey?: string;\n/**\n* The key in the JSON response to get the metadata from\n*/\nmetadataKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/remote-retriever.ts","loc":{"lines":{"from":1,"to":25}}}}],["781",{"pageContent":"class RemoteLangChainRetriever\nextends RemoteRetriever\nimplements RemoteLangChainRetrieverParams\n{\ninputKey: string;\n\nresponseKey: string;\n\npageContentKey: string;\n\nmetadataKey: string;\n\nconstructor({\ninputKey = \"message\",\nresponseKey = \"response\",\npageContentKey = \"page_content\",\nmetadataKey = \"metadata\",\n...rest\n}: RemoteLangChainRetrieverParams) {\nsuper(rest);\nthis.inputKey = inputKey;\nthis.responseKey = responseKey;\nthis.pageContentKey = pageContentKey;\nthis.metadataKey = metadataKey;\n}\n\ncreateJsonBody(query: string): RemoteRetrieverValues {\nreturn {\n[this.inputKey]: query,\n};\n}\n\nprocessJsonResponse(json: RemoteRetrieverValues): Document[] {\nreturn json[this.responseKey].map(\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n(r: any) =>\nnew Document({\npageContent: r[this.pageContentKey],\nmetadata: r[this.metadataKey],\n})\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/remote/remote-retriever.ts","loc":{"lines":{"from":72,"to":114}}}}],["782",{"pageContent":"import type { SupabaseClient } from \"@supabase/supabase-js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\nimport { BaseRetriever } from \"../schema/index.js\";\n\ninterface SearchEmbeddingsParams {\nquery_embedding: number[];\nmatch_count: number; // int\n}\n\ninterface SearchKeywordParams {\nquery_text: string;\nmatch_count: number; // int\n}\n\ninterface SearchResponseRow {\nid: number;\ncontent: string;\nmetadata: object;\nsimilarity: number;\n}\n\ntype SearchResult = [Document, number, number];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/supabase.ts","loc":{"lines":{"from":1,"to":23}}}}],["783",{"pageContent":"interface SupabaseLibArgs {\nclient: SupabaseClient;\n/**\n* The table name on Supabase. Defaults to \"documents\".\n*/\ntableName?: string;\n/**\n* The name of the Similarity search function on Supabase. Defaults to \"match_documents\".\n*/\nsimilarityQueryName?: string;\n/**\n* The name of the Keyword search function on Supabase. Defaults to \"kw_match_documents\".\n*/\nkeywordQueryName?: string;\n/**\n* The number of documents to return from the similarity search. Defaults to 2.\n*/\nsimilarityK?: number;\n/**\n* The number of documents to return from the keyword search. Defaults to 2.\n*/\nkeywordK?: number;\n}\n\nexport interface SupabaseHybridSearchParams {\nquery: string;\nsimilarityK: number;\nkeywordK: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/supabase.ts","loc":{"lines":{"from":183,"to":211}}}}],["784",{"pageContent":"class SupabaseHybridSearch extends BaseRetriever {\nsimilarityK: number;\n\nquery: string;\n\nkeywordK: number;\n\nsimilarityQueryName: string;\n\nclient: SupabaseClient;\n\ntableName: string;\n\nkeywordQueryName: string;\n\nembeddings: Embeddings;\n\nconstructor(embeddings: Embeddings, args: SupabaseLibArgs) {\nsuper();\nthis.embeddings = embeddings;\nthis.client = args.client;\nthis.tableName = args.tableName || \"documents\";\nthis.similarityQueryName = args.similarityQueryName || \"match_documents\";\nthis.keywordQueryName = args.keywordQueryName || \"kw_match_documents\";\nthis.similarityK = args.similarityK || 2;\nthis.keywordK = args.keywordK || 2;\n}\n\nprotected async similaritySearch(\nquery: string,\nk: number\n): Promise<SearchResult[]> {\nconst embeddedQuery = await this.embeddings.embedQuery(query);\n\nconst matchDocumentsParams: SearchEmbeddingsParams = {\nquery_embedding: embeddedQuery,\nmatch_count: k,\n};\n\nconst { data: searches, error } = await this.client.rpc(\nthis.similarityQueryName,\nmatchDocumentsParams\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/supabase.ts","loc":{"lines":{"from":364,"to":406}}}}],["785",{"pageContent":"if (error) {\nthrow new Error(\n`Error searching for documents: ${error.code} ${error.message} ${error.details}`\n);\n}\n\nreturn (searches as SearchResponseRow[]).map((resp) => [\nnew Document({\nmetadata: resp.metadata,\npageContent: resp.content,\n}),\nresp.similarity,\nresp.id,\n]);\n}\n\nprotected async keywordSearch(\nquery: string,\nk: number\n): Promise<SearchResult[]> {\nconst kwMatchDocumentsParams: SearchKeywordParams = {\nquery_text: query,\nmatch_count: k,\n};\n\nconst { data: searches, error } = await this.client.rpc(\nthis.keywordQueryName,\nkwMatchDocumentsParams\n);\n\nif (error) {\nthrow new Error(\n`Error searching for documents: ${error.code} ${error.message} ${error.details}`\n);\n}\n\nreturn (searches as SearchResponseRow[]).map((resp) => [\nnew Document({\nmetadata: resp.metadata,\npageContent: resp.content,\n}),\nresp.similarity * 10,\nresp.id,\n]);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/supabase.ts","loc":{"lines":{"from":549,"to":593}}}}],["786",{"pageContent":"protected async hybridSearch(\nquery: string,\nsimilarityK: number,\nkeywordK: number\n): Promise<SearchResult[]> {\nconst similarity_search = this.similaritySearch(query, similarityK);\n\nconst keyword_search = this.keywordSearch(query, keywordK);\n\nreturn Promise.all([similarity_search, keyword_search])\n.then((results) => results.flat())\n.then((results) => {\nconst picks = new Map<number, SearchResult>();\n\nresults.forEach((result) => {\nconst id = result[2];\nconst nextScore = result[1];\nconst prevScore = picks.get(id)?.[1];\n\nif (prevScore === undefined || nextScore > prevScore) {\npicks.set(id, result);\n}\n});\n\nreturn Array.from(picks.values());\n})\n.then((results) => results.sort((a, b) => b[1] - a[1]));\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nconst searchResults = await this.hybridSearch(\nquery,\nthis.similarityK,\nthis.keywordK\n);\n\nreturn searchResults.map(([doc]) => doc);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/supabase.ts","loc":{"lines":{"from":741,"to":779}}}}],["787",{"pageContent":"import { expect, test } from \"@jest/globals\";\nimport { HydeRetriever } from \"../hyde.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { OpenAI } from \"../../llms/openai.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Hyde retriever\", async () => {\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = new MemoryVectorStore(embeddings);\nconst llm = new OpenAI();\nconst retriever = new HydeRetriever({\nvectorStore,\nllm,\nk: 1,\n});\n\nawait vectorStore.addDocuments(\n[\n\"My name is John.\",\n\"My name is Bob.\",\n\"My favourite food is pizza.\",\n\"My favourite food is pasta.\",\n].map((pageContent) => new Document({ pageContent }))\n);\n\nconst results = await retriever.getRelevantDocuments(\n\"What is my favourite food?\"\n);\n\nexpect(results.length).toBe(1);\nconsole.log(results);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/hyde.int.test.ts","loc":{"lines":{"from":1,"to":33}}}}],["788",{"pageContent":"test(\"Hyde retriever with default prompt template\", async () => {\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = new MemoryVectorStore(embeddings);\nconst llm = new OpenAI();\nconst retriever = new HydeRetriever({\nvectorStore,\nllm,\nk: 1,\npromptTemplate: \"websearch\",\n});\n\nawait vectorStore.addDocuments(\n[\n\"My name is John.\",\n\"My name is Bob.\",\n\"My favourite food is pizza.\",\n\"My favourite food is pasta.\",\n].map((pageContent) => new Document({ pageContent }))\n);\n\nconst results = await retriever.getRelevantDocuments(\n\"What is my favourite food?\"\n);\n\nexpect(results.length).toBe(1);\nconsole.log(results);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/hyde.int.test.ts","loc":{"lines":{"from":66,"to":92}}}}],["789",{"pageContent":"/* eslint-disable no-process-env */\n/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport { test, expect } from \"@jest/globals\";\nimport Metal from \"@getmetal/metal-sdk\";\n\nimport { MetalRetriever } from \"../metal.js\";\n\ntest(\"MetalRetriever\", async () => {\nconst MetalSDK = Metal.default;\nconst client = new MetalSDK(\nprocess.env.METAL_API_KEY!,\nprocess.env.METAL_CLIENT_ID!,\nprocess.env.METAL_INDEX_ID\n);\nconst retriever = new MetalRetriever({ client });\n\nconst docs = await retriever.getRelevantDocuments(\"hello\");\n\nexpect(docs.length).toBeGreaterThan(0);\n\nconsole.log(docs);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/metal.int.test.ts","loc":{"lines":{"from":1,"to":22}}}}],["790",{"pageContent":"/* eslint-disable no-process-env */\n/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport { test, expect } from \"@jest/globals\";\nimport { createClient } from \"@supabase/supabase-js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { SupabaseHybridSearch } from \"../supabase.js\";\n\ntest(\"Supabase hybrid keyword search\", async () => {\nconst client = createClient(\nprocess.env.SUPABASE_URL!,\nprocess.env.SUPABASE_PRIVATE_KEY!\n);\n\nconst embeddings = new OpenAIEmbeddings();\n\nconst retriever = new SupabaseHybridSearch(embeddings, {\nclient,\nsimilarityK: 2,\nkeywordK: 2,\n});\n\nexpect(retriever).toBeDefined();\n\nconst results = await retriever.getRelevantDocuments(\"hello bye\");\n\nexpect(results.length).toBeGreaterThan(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/retrievers/tests/supabase.int.test.ts","loc":{"lines":{"from":1,"to":27}}}}],["791",{"pageContent":"import { Document } from \"../document.js\";\n\nexport const RUN_KEY = \"__run\";\n\nexport type Example = Record<string, string>;\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type InputValues = Record<string, any>;\n\nexport type PartialValues = Record<\nstring,\nstring | (() => Promise<string>) | (() => string)\n>;\n\n/**\n* Output of a single generation.\n*/\nexport interface Generation {\n/**\n* Generated text output\n*/\ntext: string;\n/**\n* Raw generation info response from the provider.\n* May include things like reason for finishing (e.g. in {@link OpenAI})\n*/\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ngenerationInfo?: Record<string, any>;\n}\n\n/**\n* Contains all relevant information returned by an LLM.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/schema/index.ts","loc":{"lines":{"from":1,"to":33}}}}],["792",{"pageContent":"type LLMResult = {\n/**\n* List of the things generated. Each input could have multiple {@link Generation | generations}, hence this is a list of lists.\n*/\ngenerations: Generation[][];\n/**\n* Dictionary of arbitrary LLM-provider specific output.\n*/\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nllmOutput?: Record<string, any>;\n/**\n* Dictionary of run metadata\n*/\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n[RUN_KEY]?: Record<string, any>;\n};\nexport type MessageType = \"human\" | \"ai\" | \"generic\" | \"system\";\n\nexport abstract class BaseChatMessage {\n/** The text of the message. */\ntext: string;\n\n/** The name of the message sender in a multi-user chat. */\nname?: string;\n\n/** The type of the message. */\nabstract _getType(): MessageType;\n\nconstructor(text: string) {\nthis.text = text;\n}\n}\n\nexport class HumanChatMessage extends BaseChatMessage {\n_getType(): MessageType {\nreturn \"human\";\n}\n}\n\nexport class AIChatMessage extends BaseChatMessage {\n_getType(): MessageType {\nreturn \"ai\";\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/schema/index.ts","loc":{"lines":{"from":165,"to":208}}}}],["793",{"pageContent":"class SystemChatMessage extends BaseChatMessage {\n_getType(): MessageType {\nreturn \"system\";\n}\n}\n\nexport class ChatMessage extends BaseChatMessage {\nrole: string;\n\nconstructor(text: string, role: string) {\nsuper(text);\nthis.role = role;\n}\n\n_getType(): MessageType {\nreturn \"generic\";\n}\n}\n\nexport interface ChatGeneration extends Generation {\nmessage: BaseChatMessage;\n}\n\nexport interface ChatResult {\ngenerations: ChatGeneration[];\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nllmOutput?: Record<string, any>;\n}\n\n/**\n* Base PromptValue class. All prompt values should extend this class.\n*/\nexport abstract class BasePromptValue {\nabstract toString(): string;\n\nabstract toChatMessages(): BaseChatMessage[];\n}\n\nexport type AgentAction = {\ntool: string;\ntoolInput: string;\nlog: string;\n};\n\nexport type AgentFinish = {\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nreturnValues: Record<string, any>;\nlog: string;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/schema/index.ts","loc":{"lines":{"from":332,"to":381}}}}],["794",{"pageContent":"type AgentStep = {\naction: AgentAction;\nobservation: string;\n};\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type ChainValues = Record<string, any>;\n\n/**\n* Base Index class. All indexes should extend this class.\n*/\nexport abstract class BaseRetriever {\nabstract getRelevantDocuments(query: string): Promise<Document[]>;\n}\n\nexport abstract class BaseChatMessageHistory {\npublic abstract getMessages(): Promise<BaseChatMessage[]>;\n\npublic abstract addUserMessage(message: string): Promise<void>;\n\npublic abstract addAIChatMessage(message: string): Promise<void>;\n\npublic abstract clear(): Promise<void>;\n}\n\nexport abstract class BaseCache<T = Generation[]> {\nabstract lookup(prompt: string, llmKey: string): Promise<T | null>;\n\nabstract update(prompt: string, llmKey: string, value: T): Promise<void>;\n}\n\nexport abstract class BaseFileStore {\nabstract readFile(path: string): Promise<string>;\n\nabstract writeFile(path: string, contents: string): Promise<void>;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/schema/index.ts","loc":{"lines":{"from":507,"to":542}}}}],["795",{"pageContent":"import { Callbacks } from \"../callbacks/manager.js\";\nimport { BasePromptValue } from \"./index.js\";\n\n/** Class to parse the output of an LLM call.\n*/\nexport abstract class BaseOutputParser<T = unknown> {\n/**\n* Parse the output of an LLM call.\n*\n* @param text - LLM output to parse.\n* @returns Parsed output.\n*/\nabstract parse(text: string, callbacks?: Callbacks): Promise<T>;\n\nasync parseWithPrompt(\ntext: string,\n_prompt: BasePromptValue,\ncallbacks?: Callbacks\n): Promise<T> {\nreturn this.parse(text, callbacks);\n}\n\n/**\n* Return a string describing the format of the output.\n* @returns Format instructions.\n* @example\n* ```json\n* {\n*  \"foo\": \"bar\"\n* }\n* ```\n*/\nabstract getFormatInstructions(): string;\n\n/**\n* Return the string type key uniquely identifying this class of parser\n*/\n_type(): string {\nthrow new Error(\"_type not implemented\");\n}\n}\n\nexport class OutputParserException extends Error {\nconstructor(message: string) {\nsuper(message);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/schema/output_parser.ts","loc":{"lines":{"from":1,"to":47}}}}],["796",{"pageContent":"import type { DataSource as DataSourceT, DataSourceOptions } from \"typeorm\";\nimport {\ngenerateTableInfoFromTables,\ngetTableAndColumnsName,\nSerializedSqlDatabase,\nSqlDatabaseDataSourceParams,\nSqlDatabaseOptionsParams,\nSqlTable,\nverifyIgnoreTablesExistInDatabase,\nverifyIncludeTablesExistInDatabase,\nverifyListTablesExistInDatabase,\n} from \"./util/sql_utils.js\";\n\nexport { SqlDatabaseDataSourceParams, SqlDatabaseOptionsParams };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":1,"to":14}}}}],["797",{"pageContent":"class SqlDatabase\nimplements SqlDatabaseOptionsParams, SqlDatabaseDataSourceParams\n{\nappDataSourceOptions: DataSourceOptions;\n\nappDataSource: DataSourceT;\n\nallTables: Array<SqlTable> = [];\n\nincludesTables: Array<string> = [];\n\nignoreTables: Array<string> = [];\n\nsampleRowsInTableInfo = 3;\n\nprotected constructor(fields: SqlDatabaseDataSourceParams) {\nthis.appDataSource = fields.appDataSource;\nthis.appDataSourceOptions = fields.appDataSource.options;\nif (fields?.includesTables && fields?.ignoreTables) {\nthrow new Error(\"Cannot specify both include_tables and ignoreTables\");\n}\nthis.includesTables = fields?.includesTables ?? [];\nthis.ignoreTables = fields?.ignoreTables ?? [];\nthis.sampleRowsInTableInfo =\nfields?.sampleRowsInTableInfo ?? this.sampleRowsInTableInfo;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":159,"to":184}}}}],["798",{"pageContent":"static async fromDataSourceParams(\nfields: SqlDatabaseDataSourceParams\n): Promise<SqlDatabase> {\nconst sqlDatabase = new SqlDatabase(fields);\nif (!sqlDatabase.appDataSource.isInitialized) {\nawait sqlDatabase.appDataSource.initialize();\n}\nsqlDatabase.allTables = await getTableAndColumnsName(\nsqlDatabase.appDataSource\n);\nverifyIncludeTablesExistInDatabase(\nsqlDatabase.allTables,\nsqlDatabase.includesTables\n);\nverifyIgnoreTablesExistInDatabase(\nsqlDatabase.allTables,\nsqlDatabase.ignoreTables\n);\nreturn sqlDatabase;\n}\n\nstatic async fromOptionsParams(\nfields: SqlDatabaseOptionsParams\n): Promise<SqlDatabase> {\nconst { DataSource } = await import(\"typeorm\");\nconst dataSource = new DataSource(fields.appDataSourceOptions);\nreturn SqlDatabase.fromDataSourceParams({\n...fields,\nappDataSource: dataSource,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":312,"to":342}}}}],["799",{"pageContent":"/**\n* Get information about specified tables.\n*\n* Follows best practices as specified in: Rajkumar et al, 2022\n* (https://arxiv.org/abs/2204.00498)\n*\n* If `sample_rows_in_table_info`, the specified number of sample rows will be\n* appended to each table description. This can increase performance as\n* demonstrated in the paper.\n*/\nasync getTableInfo(targetTables?: Array<string>): Promise<string> {\nlet selectedTables =\nthis.includesTables.length > 0\n? this.allTables.filter((currentTable) =>\nthis.includesTables.includes(currentTable.tableName)\n)\n: this.allTables;\n\nif (this.ignoreTables.length > 0) {\nselectedTables = selectedTables.filter(\n(currentTable) => !this.ignoreTables.includes(currentTable.tableName)\n);\n}\n\nif (targetTables && targetTables.length > 0) {\nverifyListTablesExistInDatabase(\nthis.allTables,\ntargetTables,\n\"Wrong target table name:\"\n);\nselectedTables = this.allTables.filter((currentTable) =>\ntargetTables.includes(currentTable.tableName)\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":470,"to":503}}}}],["800",{"pageContent":"return generateTableInfoFromTables(\nselectedTables,\nthis.appDataSource,\nthis.sampleRowsInTableInfo\n);\n}\n\n/**\n* Execute a SQL command and return a string representing the results.\n* If the statement returns rows, a string of the results is returned.\n* If the statement returns no rows, an empty string is returned.\n*/\nasync run(command: string, fetch: \"all\" | \"one\" = \"all\"): Promise<string> {\n// TODO: Potential security issue here\nconst res = await this.appDataSource.query(command);\n\nif (fetch === \"all\") {\nreturn JSON.stringify(res);\n}\n\nif (res?.length > 0) {\nreturn JSON.stringify(res[0]);\n}\n\nreturn \"\";\n}\n\nserialize(): SerializedSqlDatabase {\nreturn {\n_type: \"sql_database\",\nappDataSourceOptions: this.appDataSourceOptions,\nincludesTables: this.includesTables,\nignoreTables: this.ignoreTables,\nsampleRowsInTableInfo: this.sampleRowsInTableInfo,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":628,"to":663}}}}],["801",{"pageContent":"/** @ignore */\nstatic async imports() {\ntry {\nconst { DataSource } = await import(\"typeorm\");\nreturn { DataSource };\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Failed to load typeorm. Please install it with eg. `yarn add typeorm`.\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/sql_db.ts","loc":{"lines":{"from":790,"to":802}}}}],["802",{"pageContent":"import { BaseFileStore } from \"../../schema/index.js\";\n\nexport class InMemoryFileStore extends BaseFileStore {\nprivate files: Map<string, string> = new Map();\n\nasync readFile(path: string): Promise<string> {\nconst contents = this.files.get(path);\nif (contents === undefined) {\nthrow new Error(`File not found: ${path}`);\n}\nreturn contents;\n}\n\nasync writeFile(path: string, contents: string): Promise<void> {\nthis.files.set(path, contents);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/stores/file/in_memory.ts","loc":{"lines":{"from":1,"to":17}}}}],["803",{"pageContent":"import * as fs from \"node:fs/promises\";\nimport { mkdtempSync } from \"node:fs\";\nimport { join } from \"node:path\";\n\nimport { BaseFileStore } from \"../../schema/index.js\";\n\nexport class NodeFileStore extends BaseFileStore {\nconstructor(public basePath: string = mkdtempSync(\"langchain-\")) {\nsuper();\n}\n\nasync readFile(path: string): Promise<string> {\nreturn await fs.readFile(join(this.basePath, path), \"utf8\");\n}\n\nasync writeFile(path: string, contents: string): Promise<void> {\nawait fs.writeFile(join(this.basePath, path), contents, \"utf8\");\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/stores/file/node.ts","loc":{"lines":{"from":1,"to":19}}}}],["804",{"pageContent":"import {\nHumanChatMessage,\nAIChatMessage,\nBaseChatMessage,\nBaseChatMessageHistory,\n} from \"../../schema/index.js\";\n\nexport class ChatMessageHistory extends BaseChatMessageHistory {\nprivate messages: BaseChatMessage[] = [];\n\nconstructor(messages?: BaseChatMessage[]) {\nsuper();\nthis.messages = messages ?? [];\n}\n\nasync getMessages(): Promise<BaseChatMessage[]> {\nreturn this.messages;\n}\n\nasync addUserMessage(message: string) {\nthis.messages.push(new HumanChatMessage(message));\n}\n\nasync addAIChatMessage(message: string) {\nthis.messages.push(new AIChatMessage(message));\n}\n\nasync clear() {\nthis.messages = [];\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/stores/message/in_memory.ts","loc":{"lines":{"from":1,"to":31}}}}],["805",{"pageContent":"import { test, expect, beforeEach, afterEach } from \"@jest/globals\";\nimport { DataSource } from \"typeorm\";\nimport { SqlDatabase } from \"../sql_db.js\";\n\nlet datasource: DataSource;\n\nbeforeEach(async () => {\ndatasource = new DataSource({","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/sql_database.int.test.ts","loc":{"lines":{"from":1,"to":8}}}}],["806",{"pageContent":": \"sqlite\",\ndatabase: \":memory:\",\nsynchronize: true,\n});\nawait datasource.initialize();\n\nawait datasource.query(`\nCREATE TABLE products (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Apple', 100);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Banana', 200);\n`);\nawait datasource.query(`\nINSERT INTO products (name, price) VALUES ('Orange', 300);\n`);\nawait datasource.query(`\nCREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, age INTEGER);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Alice', 20);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Bob', 21);\n`);\nawait datasource.query(`\nINSERT INTO users (name, age) VALUES ('Charlie', 22);\n`);\n});\n\nafterEach(async () => {\nawait datasource.destroy();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/sql_database.int.test.ts","loc":{"lines":{"from":145,"to":179}}}}],["807",{"pageContent":"test(\"Test getTableInfo\", async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\nconst result = await db.getTableInfo([\"users\", \"products\"]);\nconst expectStr = `\nCREATE TABLE products (\nid INTEGER , name TEXT , price INTEGER ) \nSELECT * FROM \"products\" LIMIT 3;\nid name price\n1 Apple 100\n2 Banana 200\n3 Orange 300\nCREATE TABLE users (\nid INTEGER , name TEXT , age INTEGER ) \nSELECT * FROM \"users\" LIMIT 3;\nid name age\n1 Alice 20\n2 Bob 21\n3 Charlie 22`;\nexpect(result.trim()).toBe(expectStr.trim());\n});\n\ntest(\"Test getTableInfo with less tables than in db\", async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\nconst result = await db.getTableInfo([\"products\"]);\nconst expectStr = `\nCREATE TABLE products (\nid INTEGER , name TEXT , price INTEGER ) \nSELECT * FROM \"products\" LIMIT 3;\nid name price\n1 Apple 100\n2 Banana 200\n3 Orange 300`;\nexpect(result.trim()).toBe(expectStr.trim());\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/sql_database.int.test.ts","loc":{"lines":{"from":296,"to":333}}}}],["808",{"pageContent":"test(\"Test getTableInfo with includes tables\", async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\nincludesTables: [\"products\"],\n});\nconst result = await db.getTableInfo();\nconst expectStr = `\nCREATE TABLE products (\nid INTEGER , name TEXT , price INTEGER ) \nSELECT * FROM \"products\" LIMIT 3;\nid name price\n1 Apple 100\n2 Banana 200\n3 Orange 300`;\nexpect(result.trim()).toBe(expectStr.trim());\n});\n\ntest(\"Test getTableInfo with ignoreTables\", async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\nignoreTables: [\"users\"],\n});\nconst result = await db.getTableInfo();\nconst expectStr = `\nCREATE TABLE products (\nid INTEGER , name TEXT , price INTEGER ) \nSELECT * FROM \"products\" LIMIT 3;\nid name price\n1 Apple 100\n2 Banana 200\n3 Orange 300`;\nexpect(result.trim()).toBe(expectStr.trim());\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/sql_database.int.test.ts","loc":{"lines":{"from":447,"to":479}}}}],["809",{"pageContent":"test(\"Test getTableInfo with error\", async () => {\nawait expect(async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\nawait db.getTableInfo([\"users\", \"productss\"]);\n}).rejects.toThrow(\n\"Wrong target table name: the table productss was not found in the database\"\n);\n});\n\ntest(\"Test run\", async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\nconst result = await db.run(\"SELECT * FROM users\");\nconst expectStr = `[{\"id\":1,\"name\":\"Alice\",\"age\":20},{\"id\":2,\"name\":\"Bob\",\"age\":21},{\"id\":3,\"name\":\"Charlie\",\"age\":22}]`;\nexpect(result.trim()).toBe(expectStr.trim());\n});\n\ntest(\"Test run with error\", async () => {\nawait expect(async () => {\nconst db = await SqlDatabase.fromDataSourceParams({\nappDataSource: datasource,\n});\nawait db.run(\"SELECT * FROM userss\");\n}).rejects.toThrow(\"SQLITE_ERROR: no such table: userss\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/sql_database.int.test.ts","loc":{"lines":{"from":596,"to":623}}}}],["810",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { Document } from \"../document.js\";\nimport {\nCharacterTextSplitter,\nMarkdownTextSplitter,\nRecursiveCharacterTextSplitter,\nTokenTextSplitter,\n} from \"../text_splitter.js\";\n\ntest(\"Test splitting by character count.\", async () => {\nconst text = \"foo bar baz 123\";\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 7,\nchunkOverlap: 3,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"foo bar\", \"bar baz\", \"baz 123\"];\nexpect(output).toEqual(expectedOutput);\n});\n\ntest(\"Test splitting by character count doesn't create empty documents.\", async () => {\nconst text = \"foo  bar\";\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 2,\nchunkOverlap: 0,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"foo\", \"bar\"];\nexpect(output).toEqual(expectedOutput);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["811",{"pageContent":"test(\"Test splitting by character count on long words.\", async () => {\nconst text = \"foo bar baz a a\";\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 3,\nchunkOverlap: 1,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"foo\", \"bar\", \"baz\", \"a a\"];\nexpect(output).toEqual(expectedOutput);\n});\n\ntest(\"Test splitting by character count when shorter words are first.\", async () => {\nconst text = \"a a foo bar baz\";\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 3,\nchunkOverlap: 1,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"a a\", \"foo\", \"bar\", \"baz\"];\nexpect(output).toEqual(expectedOutput);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":210,"to":232}}}}],["812",{"pageContent":"test(\"Test splitting by characters when splits not found easily.\", async () => {\nconst text = \"foo bar baz 123\";\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 1,\nchunkOverlap: 0,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"foo\", \"bar\", \"baz\", \"123\"];\nexpect(output).toEqual(expectedOutput);\n});\n\ntest(\"Test invalid arguments.\", () => {\nexpect(() => {\nconst res = new CharacterTextSplitter({ chunkSize: 2, chunkOverlap: 4 });\nconsole.log(res);\n}).toThrow();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":417,"to":434}}}}],["813",{"pageContent":"test(\"Test create documents method.\", async () => {\nconst texts = [\"foo bar\", \"baz\"];\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 3,\nchunkOverlap: 0,\n});\nconst docs = await splitter.createDocuments(texts);\nconst metadata = { loc: { lines: { from: 1, to: 1 } } };\nconst expectedDocs = [\nnew Document({ pageContent: \"foo\", metadata }),\nnew Document({ pageContent: \"bar\", metadata }),\nnew Document({ pageContent: \"baz\", metadata }),\n];\nexpect(docs).toEqual(expectedDocs);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":624,"to":639}}}}],["814",{"pageContent":"test(\"Test create documents with metadata method.\", async () => {\nconst texts = [\"foo bar\", \"baz\"];\nconst splitter = new CharacterTextSplitter({\nseparator: \" \",\nchunkSize: 3,\nchunkOverlap: 0,\n});\nconst docs = await splitter.createDocuments(texts, [\n{ source: \"1\" },\n{ source: \"2\" },\n]);\nconst loc = { lines: { from: 1, to: 1 } };\nconst expectedDocs = [\nnew Document({ pageContent: \"foo\", metadata: { source: \"1\", loc } }),\nnew Document({\npageContent: \"bar\",\nmetadata: { source: \"1\", loc },\n}),\nnew Document({ pageContent: \"baz\", metadata: { source: \"2\", loc } }),\n];\nexpect(docs).toEqual(expectedDocs);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":829,"to":850}}}}],["815",{"pageContent":"test(\"Test iterative text splitter.\", async () => {\nconst text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\nThis is a weird text to write, but gotta test the splittingggg some how.\\n\\n\nBye!\\n\\n-H.`;\nconst splitter = new RecursiveCharacterTextSplitter({\nchunkSize: 10,\nchunkOverlap: 1,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\n\"Hi.\",\n\"I'm\",\n\"Harrison.\",\n\"How? Are?\",\n\"You?\",\n\"Okay then f\",\n\"f f f f.\",\n\"This is a\",\n\"a weird\",\n\"text to\",\n\"write, but\",\n\"gotta test\",\n\"the\",\n\"splitting\",\n\"gggg\",\n\"some how.\",\n\"Bye!\\n\\n-H.\",\n];\nexpect(output).toEqual(expectedOutput);\n});\n\ntest(\"Token text splitter\", async () => {\nconst text = \"foo bar baz a a\";\nconst splitter = new TokenTextSplitter({\nencodingName: \"r50k_base\",\nchunkSize: 3,\nchunkOverlap: 0,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\"foo bar b\", \"az a a\"];\n\nexpect(output).toEqual(expectedOutput);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":1036,"to":1078}}}}],["816",{"pageContent":"test(\"Test markdown text splitter.\", async () => {\nconst text =\n\"#  LangChain\\n\" +\n\"\\n\" +\n\" Building applications with LLMs through composability \\n\" +\n\"\\n\" +\n\"## Quick Install\\n\" +\n\"\\n\" +\n\"```bash\\n\" +\n\"# Hopefully this code block isn't split\\n\" +\n\"pip install langchain\\n\" +\n\"```\\n\" +\n\"\\n\" +\n\"As an open source project in a rapidly developing field, we are extremely open to contributions.\";\nconst splitter = new MarkdownTextSplitter({\nchunkSize: 100,\nchunkOverlap: 0,\n});\nconst output = await splitter.splitText(text);\nconst expectedOutput = [\n\"#  LangChain\\n\\n Building applications with LLMs through composability \",\n\"Quick Install\\n\\n```bash\\n# Hopefully this code block isn't split\\npip install langchain\",\n\"As an open source project in a rapidly developing field, we are extremely open to contributions.\",\n];\nexpect(output).toEqual(expectedOutput);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":1255,"to":1280}}}}],["817",{"pageContent":"test(\"Test lines loc on iterative text splitter.\", async () => {\nconst text = `Hi.\\nI'm Harrison.\\n\\nHow?\\na\\nb`;\nconst splitter = new RecursiveCharacterTextSplitter({\nchunkSize: 20,\nchunkOverlap: 1,\n});\nconst docs = await splitter.createDocuments([text]);\n\nconst expectedDocs = [\nnew Document({\npageContent: \"Hi.\\nI'm Harrison.\",\nmetadata: { loc: { lines: { from: 1, to: 2 } } },\n}),\nnew Document({\npageContent: \"How?\\na\\nb\",\nmetadata: { loc: { lines: { from: 4, to: 6 } } },\n}),\n];\n\nexpect(docs).toEqual(expectedDocs);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tests/text_splitter.test.ts","loc":{"lines":{"from":1458,"to":1478}}}}],["818",{"pageContent":"import type * as tiktoken from \"@dqbd/tiktoken\";\nimport { Document } from \"./document.js\";\n\nexport interface TextSplitterParams {\nchunkSize: number;\n\nchunkOverlap: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":1,"to":8}}}}],["819",{"pageContent":"abstract class TextSplitter implements TextSplitterParams {\nchunkSize = 1000;\n\nchunkOverlap = 200;\n\nconstructor(fields?: Partial<TextSplitterParams>) {\nthis.chunkSize = fields?.chunkSize ?? this.chunkSize;\nthis.chunkOverlap = fields?.chunkOverlap ?? this.chunkOverlap;\nif (this.chunkOverlap >= this.chunkSize) {\nthrow new Error(\"Cannot have chunkOverlap >= chunkSize\");\n}\n}\n\nabstract splitText(text: string): Promise<string[]>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":336,"to":349}}}}],["820",{"pageContent":"async createDocuments(\ntexts: string[],\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nmetadatas: Record<string, any>[] = []\n): Promise<Document[]> {\nconst _metadatas =\nmetadatas.length > 0 ? metadatas : new Array(texts.length).fill({});\nconst documents = new Array<Document>();\nfor (let i = 0; i < texts.length; i += 1) {\nconst text = texts[i];\nlet lineCounterIndex = 1;\nlet prevChunk = null;\nfor (const chunk of await this.splitText(text)) {\n// we need to count the \\n that are in the text before getting removed by the splitting\nlet numberOfIntermediateNewLines = 0;\nif (prevChunk) {\nconst indexChunk = text.indexOf(chunk);\nconst indexEndPrevChunk = text.indexOf(prevChunk) + prevChunk.length;\nconst removedNewlinesFromSplittingText = text.slice(\nindexEndPrevChunk,\nindexChunk\n);\nnumberOfIntermediateNewLines = (\nremovedNewlinesFromSplittingText.match(/\\n/g) || []\n).length;\n}\nlineCounterIndex += numberOfIntermediateNewLines;\nconst newLinesCount = (chunk.match(/\\n/g) || []).length;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":667,"to":694}}}}],["821",{"pageContent":"const loc =\n_metadatas[i].loc && typeof _metadatas[i].loc === \"object\"\n? { ..._metadatas[i].loc }\n: {};\nloc.lines = {\nfrom: lineCounterIndex,\nto: lineCounterIndex + newLinesCount,\n};\nconst metadataWithLinesNumber = {\n..._metadatas[i],\nloc,\n};\ndocuments.push(\nnew Document({\npageContent: chunk,\nmetadata: metadataWithLinesNumber,\n})\n);\nlineCounterIndex += newLinesCount;\nprevChunk = chunk;\n}\n}\nreturn documents;\n}\n\nasync splitDocuments(documents: Document[]): Promise<Document[]> {\nconst selectedDocuments = documents.filter(\n(doc) => doc.pageContent !== undefined\n);\nconst texts = selectedDocuments.map((doc) => doc.pageContent);\nconst metadatas = selectedDocuments.map((doc) => doc.metadata);\nreturn this.createDocuments(texts, metadatas);\n}\n\nprivate joinDocs(docs: string[], separator: string): string | null {\nconst text = docs.join(separator).trim();\nreturn text === \"\" ? null : text;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":996,"to":1033}}}}],["822",{"pageContent":"mergeSplits(splits: string[], separator: string): string[] {\nconst docs: string[] = [];\nconst currentDoc: string[] = [];\nlet total = 0;\nfor (const d of splits) {\nconst _len = d.length;\nif (total + _len >= this.chunkSize) {\nif (total > this.chunkSize) {\nconsole.warn(\n`Created a chunk of size ${total}, +\nwhich is longer than the specified ${this.chunkSize}`\n);\n}\nif (currentDoc.length > 0) {\nconst doc = this.joinDocs(currentDoc, separator);\nif (doc !== null) {\ndocs.push(doc);\n}\n// Keep on popping if:\n// - we have a larger chunk than in the chunk overlap\n// - or if we still have any chunks and the length is long\nwhile (\ntotal > this.chunkOverlap ||\n(total + _len > this.chunkSize && total > 0)\n) {\ntotal -= currentDoc[0].length;\ncurrentDoc.shift();\n}\n}\n}\ncurrentDoc.push(d);\ntotal += _len;\n}\nconst doc = this.joinDocs(currentDoc, separator);\nif (doc !== null) {\ndocs.push(doc);\n}\nreturn docs;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":1337,"to":1376}}}}],["823",{"pageContent":"interface CharacterTextSplitterParams extends TextSplitterParams {\nseparator: string;\n}\n\nexport class CharacterTextSplitter\nextends TextSplitter\nimplements CharacterTextSplitterParams\n{\nseparator = \"\\n\\n\";\n\nconstructor(fields?: Partial<CharacterTextSplitterParams>) {\nsuper(fields);\nthis.separator = fields?.separator ?? this.separator;\n}\n\nasync splitText(text: string): Promise<string[]> {\n// First we naively split the large input into a bunch of smaller ones.\nlet splits: string[];\nif (this.separator) {\nsplits = text.split(this.separator);\n} else {\nsplits = text.split(\"\");\n}\nreturn this.mergeSplits(splits, this.separator);\n}\n}\n\nexport interface RecursiveCharacterTextSplitterParams\nextends TextSplitterParams {\nseparators: string[];\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":1680,"to":1710}}}}],["824",{"pageContent":"class RecursiveCharacterTextSplitter\nextends TextSplitter\nimplements RecursiveCharacterTextSplitterParams\n{\nseparators: string[] = [\"\\n\\n\", \"\\n\", \" \", \"\"];\n\nconstructor(fields?: Partial<RecursiveCharacterTextSplitterParams>) {\nsuper(fields);\nthis.separators = fields?.separators ?? this.separators;\n}\n\nasync splitText(text: string): Promise<string[]> {\nconst finalChunks: string[] = [];\n\n// Get appropriate separator to use\nlet separator: string = this.separators[this.separators.length - 1];\nfor (const s of this.separators) {\nif (s === \"\") {\nseparator = s;\nbreak;\n}\nif (text.includes(s)) {\nseparator = s;\nbreak;\n}\n}\n\n// Now that we have the separator, split the text\nlet splits: string[];\nif (separator) {\nsplits = text.split(separator);\n} else {\nsplits = text.split(\"\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":2018,"to":2051}}}}],["825",{"pageContent":"// Now go merging things, recursively splitting longer texts.\nlet goodSplits: string[] = [];\nfor (const s of splits) {\nif (s.length < this.chunkSize) {\ngoodSplits.push(s);\n} else {\nif (goodSplits.length) {\nconst mergedText = this.mergeSplits(goodSplits, separator);\nfinalChunks.push(...mergedText);\ngoodSplits = [];\n}\nconst otherInfo = await this.splitText(s);\nfinalChunks.push(...otherInfo);\n}\n}\nif (goodSplits.length) {\nconst mergedText = this.mergeSplits(goodSplits, separator);\nfinalChunks.push(...mergedText);\n}\nreturn finalChunks;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":2358,"to":2379}}}}],["826",{"pageContent":"interface TokenTextSplitterParams extends TextSplitterParams {\nencodingName: tiktoken.TiktokenEncoding;\nallowedSpecial: \"all\" | Array<string>;\ndisallowedSpecial: \"all\" | Array<string>;\n}\n\n/**\n* Implementation of splitter which looks at tokens.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":2695,"to":2703}}}}],["827",{"pageContent":"class TokenTextSplitter\nextends TextSplitter\nimplements TokenTextSplitterParams\n{\nencodingName: tiktoken.TiktokenEncoding;\n\nallowedSpecial: \"all\" | Array<string>;\n\ndisallowedSpecial: \"all\" | Array<string>;\n\nprivate tokenizer: tiktoken.Tiktoken;\n\nprivate registry: FinalizationRegistry<tiktoken.Tiktoken>;\n\nconstructor(fields?: Partial<TokenTextSplitterParams>) {\nsuper(fields);\n\nthis.encodingName = fields?.encodingName ?? \"gpt2\";\nthis.allowedSpecial = fields?.allowedSpecial ?? [];\nthis.disallowedSpecial = fields?.disallowedSpecial ?? \"all\";\n}\n\nasync splitText(text: string): Promise<string[]> {\nif (!this.tokenizer) {\nconst tiktoken = await TokenTextSplitter.imports();\nthis.tokenizer = tiktoken.get_encoding(this.encodingName);\n// We need to register a finalizer to free the tokenizer when the\n// splitter is garbage collected.\nthis.registry = new FinalizationRegistry((t) => t.free());\nthis.registry.register(this, this.tokenizer);\n}\n\nconst splits: string[] = [];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":3028,"to":3060}}}}],["828",{"pageContent":"const input_ids = this.tokenizer.encode(\ntext,\nthis.allowedSpecial,\nthis.disallowedSpecial\n);\n\nlet start_idx = 0;\nlet cur_idx = Math.min(start_idx + this.chunkSize, input_ids.length);\nlet chunk_ids = input_ids.slice(start_idx, cur_idx);\n\nconst decoder = new TextDecoder();\n\nwhile (start_idx < input_ids.length) {\nsplits.push(decoder.decode(this.tokenizer.decode(chunk_ids)));\n\nstart_idx += this.chunkSize - this.chunkOverlap;\ncur_idx = Math.min(start_idx + this.chunkSize, input_ids.length);\nchunk_ids = input_ids.slice(start_idx, cur_idx);\n}\n\nreturn splits;\n}\n\nstatic async imports(): Promise<typeof tiktoken> {\ntry {\nreturn await import(\"@dqbd/tiktoken\");\n} catch (err) {\nconsole.error(err);\nthrow new Error(\n\"Please install @dqbd/tiktoken as a dependency with, e.g. `npm install -S @dqbd/tiktoken`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":3363,"to":3396}}}}],["829",{"pageContent":"type MarkdownTextSplitterParams = TextSplitterParams;\n\nexport class MarkdownTextSplitter\nextends RecursiveCharacterTextSplitter\nimplements MarkdownTextSplitterParams\n{\nseparators: string[] = [\n// First, try to split along Markdown headings (starting with level 2)\n\"\\n## \",\n\"\\n### \",\n\"\\n#### \",\n\"\\n##### \",\n\"\\n###### \",\n// Note the alternative syntax for headings (below) is not handled here\n// Heading level 2\n// ---------------\n// End of code block\n\"```\\n\\n\",\n// Horizontal lines\n\"\\n\\n***\\n\\n\",\n\"\\n\\n---\\n\\n\",\n\"\\n\\n___\\n\\n\",\n// Note that this splitter doesn't handle horizontal lines defined\n// by *three or more* of ***, ---, or ___, but this is not handled\n\"\\n\\n\",\n\"\\n\",\n\" \",\n\"\",\n];\n\nconstructor(fields?: Partial<MarkdownTextSplitterParams>) {\nsuper(fields);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/text_splitter.ts","loc":{"lines":{"from":3702,"to":3735}}}}],["830",{"pageContent":"import { Tool } from \"./base.js\";\n\nexport interface AIPluginToolParams {\nname: string;\ndescription: string;\napiSpec: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aiplugin.ts","loc":{"lines":{"from":1,"to":7}}}}],["831",{"pageContent":"class AIPluginTool extends Tool implements AIPluginToolParams {\nprivate _name: string;\n\nprivate _description: string;\n\napiSpec: string;\n\nget name() {\nreturn this._name;\n}\n\nget description() {\nreturn this._description;\n}\n\nconstructor(params: AIPluginToolParams) {\nsuper();\nthis._name = params.name;\nthis._description = params.description;\nthis.apiSpec = params.apiSpec;\n}\n\n/** @ignore */\nasync _call(_input: string) {\nreturn this.apiSpec;\n}\n\nstatic async fromPluginUrl(url: string) {\nconst aiPluginRes = await fetch(url);\nif (!aiPluginRes.ok) {\nthrow new Error(\n`Failed to fetch plugin from ${url} with status ${aiPluginRes.status}`\n);\n}\nconst aiPluginJson = await aiPluginRes.json();\n\nconst apiUrlRes = await fetch(aiPluginJson.api.url);\nif (!apiUrlRes.ok) {\nthrow new Error(\n`Failed to fetch API spec from ${aiPluginJson.api.url} with status ${apiUrlRes.status}`\n);\n}\nconst apiUrlJson = await apiUrlRes.text();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aiplugin.ts","loc":{"lines":{"from":62,"to":104}}}}],["832",{"pageContent":"return new AIPluginTool({\nname: aiPluginJson.name_for_model,\ndescription: `Call this tool to get the OpenAPI spec (and usage guide) for interacting with the ${aiPluginJson.name_for_human} API. You should only call this ONCE! What is the ${aiPluginJson.name_for_human} API useful for? ${aiPluginJson.description_for_human}`,\napiSpec: `Usage Guide: ${aiPluginJson.description_for_model}\n\nOpenAPI Spec: ${apiUrlJson}`,\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aiplugin.ts","loc":{"lines":{"from":122,"to":130}}}}],["833",{"pageContent":"import { DynamicTool, DynamicToolInput } from \"./dynamic.js\";\n\ninterface LambdaConfig {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":1,"to":3}}}}],["834",{"pageContent":"Name: string;\nregion?: string;\naccessKeyId?: string;\nsecretAccessKey?: string;\n}\n\ninterface LambdaClientConstructorArgs {\nregion?: string;\ncredentials?: {\naccessKeyId: string;\nsecretAccessKey: string;\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":98,"to":110}}}}],["835",{"pageContent":"AWSLambda extends DynamicTool {\nprivate lambdaConfig: LambdaConfig;\n\nconstructor({\nname,\ndescription,\n...rest\n}: LambdaConfig & Omit<DynamicToolInput, \"func\">) {\nsuper({\nname,\ndescription,\nfunc: async (input: string) => this._func(input),\n});\n\nthis.lambdaConfig = rest;\n}\n\n/** @ignore */\nasync _func(input: string): Promise<string> {\nconst { Client, Invoker } = await LambdaImports();\n\nconst clientConstructorArgs: LambdaClientConstructorArgs = {};\n\nif (this.lambdaConfig.region) {\nclientConstructorArgs.region = this.lambdaConfig.region;\n}\n\nif (this.lambdaConfig.accessKeyId && this.lambdaConfig.secretAccessKey) {\nclientConstructorArgs.credentials = {\naccessKeyId: this.lambdaConfig.accessKeyId,\nsecretAccessKey: this.lambdaConfig.secretAccessKey,\n};\n}\n\nconst lambdaClient = new Client(clientConstructorArgs);\n\nreturn new Promise((resolve) => {\nconst payloadUint8Array = new TextEncoder().encode(JSON.stringify(input));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":198,"to":235}}}}],["836",{"pageContent":"const command = new Invoker({\nFunctionName: this.lambdaConfig.functionName,\nInvocationType: \"RequestResponse\",\nPayload: payloadUint8Array,\n});\n\nlambdaClient\n.send(command)\n.then((response) => {\nconst responseData = JSON.parse(\nnew TextDecoder().decode(response.Payload)\n);\n\nresolve(responseData.body ? responseData.body : \"request completed.\");\n})\n.catch((error: Error) => {\nconsole.error(\"Error invoking Lambda function:\", error);\nresolve(\"failed to complete request\");\n});\n});\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":291,"to":312}}}}],["837",{"pageContent":"LambdaImports() {\ntry {\nconst { LambdaClient, InvokeCommand } = await import(\n\"@aws-sdk/client-lambda\"\n);\n\nreturn {\nClient: LambdaClient as typeof LambdaClient,\nInvoker: InvokeCommand as typeof InvokeCommand,\n};\n} catch (e) {\nconsole.error(e);\nthrow new Error(\n\"Failed to load @aws-sdk/client-lambda'. Please install it eg. `yarn add @aws-sdk/client-lambda`.\"\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":385,"to":401}}}}],["838",{"pageContent":"{ AWSLambda };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/aws_lambda.ts","loc":{"lines":{"from":480,"to":480}}}}],["839",{"pageContent":"import { z } from \"zod\";\nimport {\nCallbackManager,\nCallbackManagerForToolRun,\nCallbacks,\n} from \"../callbacks/manager.js\";\nimport { BaseLangChain, BaseLangChainParams } from \"../base_language/index.js\";\n\nexport interface ToolParams extends BaseLangChainParams {}\n\n/**\n* Base class for Tools that accept input of any shape defined by a Zod schema.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/base.ts","loc":{"lines":{"from":1,"to":13}}}}],["840",{"pageContent":"abstract class StructuredTool<\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nT extends z.ZodObject<any, any, any, any> = z.ZodObject<any, any, any, any>\n> extends BaseLangChain {\nabstract schema: T | z.ZodEffects<T>;\n\nconstructor(fields?: ToolParams) {\nsuper(fields ?? {});\n}\n\nprotected abstract _call(\narg: z.output<T>,\nrunManager?: CallbackManagerForToolRun\n): Promise<string>;\n\nasync call(\narg: (z.output<T> extends string ? string : never) | z.input<T>,\ncallbacks?: Callbacks\n): Promise<string> {\nconst parsed = await this.schema.parseAsync(arg);\nconst callbackManager_ = await CallbackManager.configure(\ncallbacks,\nthis.callbacks,\n{ verbose: this.verbose }\n);\nconst runManager = await callbackManager_?.handleToolStart(\n{ name: this.name },","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/base.ts","loc":{"lines":{"from":83,"to":109}}}}],["841",{"pageContent":"of parsed === \"string\" ? parsed : JSON.stringify(parsed)\n);\nlet result;\ntry {\nresult = await this._call(parsed, runManager);\n} catch (e) {\nawait runManager?.handleToolError(e);\nthrow e;\n}\nawait runManager?.handleToolEnd(result);\nreturn result;\n}\n\nabstract name: string;\n\nabstract description: string;\n\nreturnDirect = false;\n}\n\n/**\n* Base class for Tools that accept input as a string.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/base.ts","loc":{"lines":{"from":165,"to":187}}}}],["842",{"pageContent":"abstract class Tool extends StructuredTool {\nschema = z\n.object({ input: z.string().optional() })\n.transform((obj) => obj.input);\n\nconstructor(verbose?: boolean, callbacks?: Callbacks) {\nsuper({ verbose, callbacks });\n}\n\ncall(\narg: string | undefined | z.input<this[\"schema\"]>,\ncallbacks?: Callbacks\n): Promise<string> {\nreturn super.call(\ntypeof arg === \"string\" || !arg ? { input: arg } : arg,\ncallbacks\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/base.ts","loc":{"lines":{"from":255,"to":273}}}}],["843",{"pageContent":"import { Tool } from \"./base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/bingserpapi.ts","loc":{"lines":{"from":1,"to":1}}}}],["844",{"pageContent":"BingSerpAPI extends Tool {\nname = \"bing-search\";\n\ndescription =\n\"a search engine. useful for when you need to answer questions about current events. input should be a search query.\";\n\nkey: string;\n\nparams: Record<string, string>;\n\nconstructor(\napiKey: string | undefined = typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.BingApiKey\n: undefined,\nparams: Record<string, string> = {}\n) {\nsuper();\n\nif (!apiKey) {\nthrow new Error(\n\"BingSerpAPI API key not set. You can set it as BingApiKey in your .env file.\"\n);\n}\n\nthis.key = apiKey;\nthis.params = params;\n}\n\n/** @ignore */\nasync _call(input: string): Promise<string> {\nconst headers = { \"Ocp-Apim-Subscription-Key\": this.key };\nconst params = { q: input, textDecorations: \"true\", textFormat: \"HTML\" };\nconst searchUrl = new URL(\"https://api.bing.microsoft.com/v7.0/search\");\n\nObject.entries(params).forEach(([key, value]) => {\nsearchUrl.searchParams.append(key, value);\n});\n\nconst response = await fetch(searchUrl, { headers });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/bingserpapi.ts","loc":{"lines":{"from":62,"to":101}}}}],["845",{"pageContent":"if (!response.ok) {\nthrow new Error(`HTTP error ${response.status}`);\n}\n\nconst res = await response.json();\nconst results: [] = res.webPages.value;\n\nif (results.length === 0) {\nreturn \"No good results found.\";\n}\nconst snippets = results\n.map((result: { snippet: string }) => result.snippet)\n.join(\" \");\n\nreturn snippets;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/bingserpapi.ts","loc":{"lines":{"from":125,"to":141}}}}],["846",{"pageContent":"{ BingSerpAPI };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/bingserpapi.ts","loc":{"lines":{"from":189,"to":189}}}}],["847",{"pageContent":"import { Parser } from \"expr-eval\";\n\nimport { Tool } from \"./base.js\";\n\nexport class Calculator extends Tool {\nname = \"calculator\";\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nreturn Parser.evaluate(input).toString();\n} catch (error) {\nreturn \"I don't know how to do that.\";\n}\n}\n\ndescription = `Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/calculator.ts","loc":{"lines":{"from":1,"to":18}}}}],["848",{"pageContent":"import { DynamicTool, DynamicToolInput } from \"./dynamic.js\";\nimport { BaseChain } from \"../chains/base.js\";\n\nexport interface ChainToolInput extends Omit<DynamicToolInput, \"func\"> {\nchain: BaseChain;\n}\n\nexport class ChainTool extends DynamicTool {\nchain: BaseChain;\n\nconstructor({ chain, ...rest }: ChainToolInput) {\nsuper({\n...rest,\nfunc: async (input, runManager) =>\nchain.run(input, runManager?.getChild()),\n});\nthis.chain = chain;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/chain.ts","loc":{"lines":{"from":1,"to":19}}}}],["849",{"pageContent":"import { Tool } from \"./base.js\";\n\nclass DadJokeAPI extends Tool {\nname: string;\n\ndescription: string;\n\nconstructor() {\nsuper();\nthis.name = \"dadjoke\";\nthis.description =\n\"a dad joke generator. get a dad joke about a specific topic. input should be a search term.\";\n}\n\n/** @ignore */\nasync _call(input: string): Promise<string> {\nconst headers = { Accept: \"application/json\" };\nconst searchUrl = `https://icanhazdadjoke.com/search?term=${input}`;\n\nconst response = await fetch(searchUrl, { headers });\n\nif (!response.ok) {\nthrow new Error(`HTTP error ${response.status}`);\n}\n\nconst data = await response.json();\nconst jokes = data.results;\n\nif (jokes.length === 0) {\nreturn `No dad jokes found about ${input}`;\n}\n\nconst randomIndex = Math.floor(Math.random() * jokes.length);\nconst randomJoke = jokes[randomIndex].joke;\n\nreturn randomJoke;\n}\n}\n\nexport { DadJokeAPI };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/dadjokeapi.ts","loc":{"lines":{"from":1,"to":40}}}}],["850",{"pageContent":"import { CallbackManagerForToolRun, Callbacks } from \"../callbacks/manager.js\";\nimport { Tool } from \"./base.js\";\n\nexport interface DynamicToolInput {\nname: string;\ndescription: string;\nfunc: (\ninput: string,\nrunManager?: CallbackManagerForToolRun\n) => Promise<string>;\nreturnDirect?: boolean;\nverbose?: boolean;\ncallbacks?: Callbacks;\n}\n\n/**\n* A tool that can be created dynamically from a function, name, and description.\n*/\nexport class DynamicTool extends Tool {\nname: string;\n\ndescription: string;\n\nfunc: DynamicToolInput[\"func\"];\n\nconstructor(fields: DynamicToolInput) {\nsuper(fields.verbose, fields.callbacks);\nthis.name = fields.name;\nthis.description = fields.description;\nthis.func = fields.func;\nthis.returnDirect = fields.returnDirect ?? this.returnDirect;\n}\n\n/** @ignore */\nasync _call(\ninput: string,\nrunManager?: CallbackManagerForToolRun\n): Promise<string> {\nreturn this.func(input, runManager);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/dynamic.ts","loc":{"lines":{"from":1,"to":41}}}}],["851",{"pageContent":"import { z } from \"zod\";\nimport { BaseFileStore } from \"../schema/index.js\";\nimport { StructuredTool, ToolParams } from \"./base.js\";\n\ninterface ReadFileParams extends ToolParams {\nstore: BaseFileStore;\n}\n\nexport class ReadFileTool extends StructuredTool {\nschema = z.object({\nfile_path: z.string().describe(\"name of file\"),\n});\n\nname = \"read_file\";\n\ndescription = \"Read file from disk\";\n\nstore: BaseFileStore;\n\nconstructor({ store, ...rest }: ReadFileParams) {\nsuper(rest);\n\nthis.store = store;\n}\n\nasync _call({ file_path }: z.infer<typeof this.schema>) {\nreturn await this.store.readFile(file_path);\n}\n}\n\ninterface WriteFileParams extends ToolParams {\nstore: BaseFileStore;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/fs.ts","loc":{"lines":{"from":1,"to":33}}}}],["852",{"pageContent":"class WriteFileTool extends StructuredTool {\nschema = z.object({\nfile_path: z.string().describe(\"name of file\"),\ntext: z.string().describe(\"text to write to file\"),\n});\n\nname = \"write_file\";\n\ndescription = \"Write file from disk\";\n\nstore: BaseFileStore;\n\nconstructor({ store, ...rest }: WriteFileParams) {\nsuper(rest);\n\nthis.store = store;\n}\n\nasync _call({ file_path, text }: z.infer<typeof this.schema>) {\nawait this.store.writeFile(file_path, text);\nreturn \"File written to successfully.\";\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/fs.ts","loc":{"lines":{"from":59,"to":81}}}}],["853",{"pageContent":"/** From https://github.com/SidU/teams-langchain-js/wiki/Connecting-IFTTT-Services.\n\n# Creating a webhook\n- Go to https://ifttt.com/create\n\n# Configuring the \"If This\"\n- Click on the \"If This\" button in the IFTTT interface.\n- Search for \"Webhooks\" in the search bar.\n- Choose the first option for \"Receive a web request with a JSON payload.\"\n- Choose an Event Name that is specific to the service you plan to connect to.\nThis will make it easier for you to manage the webhook URL.\nFor example, if you're connecting to Spotify, you could use \"Spotify\" as your\nEvent Name.\n- Click the \"Create Trigger\" button to save your settings and create your webhook.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/IFTTTWebhook.ts","loc":{"lines":{"from":1,"to":14}}}}],["854",{"pageContent":"# Configuring the \"Then That\"\n- Tap on the \"Then That\" button in the IFTTT interface.\n- Search for the service you want to connect, such as Spotify.\n- Choose an action from the service, such as \"Add track to a playlist\".\n- Configure the action by specifying the necessary details, such as the playlist name,\ne.g., \"Songs from AI\".\n- Reference the JSON Payload received by the Webhook in your action. For the Spotify\nscenario, choose \"{{JsonPayload}}\" as your search query.\n- Tap the \"Create Action\" button to save your action settings.\n- Once you have finished configuring your action, click the \"Finish\" button to\ncomplete the setup.\n- Congratulations! You have successfully connected the Webhook to the desired\nservice, and you're ready to start receiving data and triggering actions ","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/IFTTTWebhook.ts","loc":{"lines":{"from":16,"to":28}}}}],["855",{"pageContent":"# Finishing up\n- To get your webhook URL go to https://ifttt.com/maker_webhooks/settings\n- Copy the IFTTT key value from there. The URL is of the form\nhttps://maker.ifttt.com/use/YOUR_IFTTT_KEY. Grab the YOUR_IFTTT_KEY value.\n*/\nimport { Tool } from \"./base.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/IFTTTWebhook.ts","loc":{"lines":{"from":69,"to":74}}}}],["856",{"pageContent":"class IFTTTWebhook extends Tool {\nprivate url: string;\n\nname: string;\n\ndescription: string;\n\nconstructor(url: string, name: string, description: string) {\nsuper();\nthis.url = url;\nthis.name = name;\nthis.description = description;\n}\n\n/** @ignore */\nasync _call(input: string): Promise<string> {\nconst headers = { \"Content-Type\": \"application/json\" };\nconst body = JSON.stringify({ this: input });\n\nconst response = await fetch(this.url, {\nmethod: \"POST\",\nheaders,\nbody,\n});\n\nif (!response.ok) {\nthrow new Error(`HTTP error ${response.status}`);\n}\n\nconst result = await response.text();\nreturn result;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/IFTTTWebhook.ts","loc":{"lines":{"from":135,"to":167}}}}],["857",{"pageContent":"export { SerpAPI, SerpAPIParameters } from \"./serpapi.js\";\nexport { DadJokeAPI } from \"./dadjokeapi.js\";\nexport { BingSerpAPI } from \"./bingserpapi.js\";\nexport { Tool, ToolParams, StructuredTool } from \"./base.js\";\nexport { DynamicTool, DynamicToolInput } from \"./dynamic.js\";\nexport { IFTTTWebhook } from \"./IFTTTWebhook.js\";\nexport { ChainTool, ChainToolInput } from \"./chain.js\";\nexport {\nQuerySqlTool,\nInfoSqlTool,\nListTablesSqlTool,\nQueryCheckerTool,\n} from \"./sql.js\";\nexport {\nJsonSpec,\nJsonListKeysTool,\nJsonGetValueTool,\nJsonObject,\nJson,\n} from \"./json.js\";\nexport { RequestsGetTool, RequestsPostTool } from \"./requests.js\";\nexport { VectorStoreQATool } from \"./vectorstore.js\";\nexport {\nZapierNLARunAction,\nZapierNLAWrapper,\nZapiterNLAWrapperParams,\n} from \"./zapier.js\";\nexport { Serper, SerperParameters } from \"./serper.js\";\nexport { AIPluginTool } from \"./aiplugin.js\";\nexport { ReadFileTool, WriteFileTool } from \"./fs.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/index.ts","loc":{"lines":{"from":1,"to":30}}}}],["858",{"pageContent":"import jsonpointer from \"jsonpointer\";\nimport { Tool } from \"./base.js\";\n\nexport type Json =\n| string\n| number\n| boolean\n| null\n| { [key: string]: Json }\n| Json[];\n\nexport type JsonObject = { [key: string]: Json };","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/json.ts","loc":{"lines":{"from":1,"to":12}}}}],["859",{"pageContent":"class JsonSpec {\nobj: JsonObject;\n\nmaxValueLength = 4000;\n\nconstructor(obj: JsonObject, max_value_length = 4000) {\nthis.obj = obj;\nthis.maxValueLength = max_value_length;\n}\n\npublic getKeys(input: string): string {\nconst pointer = jsonpointer.compile(input);\nconst res = pointer.get(this.obj) as Json;\nif (typeof res === \"object\" && !Array.isArray(res) && res !== null) {\nreturn Object.keys(res).join(\", \");\n}\n\nthrow new Error(\n`Value at ${input} is not a dictionary, get the value directly instead.`\n);\n}\n\npublic getValue(input: string): string {\nconst pointer = jsonpointer.compile(input);\nconst res = pointer.get(this.obj) as Json;\n\nif (res === null || res === undefined) {\nthrow new Error(`Value at ${input} is null or undefined.`);\n}\n\nconst str = typeof res === \"object\" ? JSON.stringify(res) : res.toString();\nif (","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/json.ts","loc":{"lines":{"from":100,"to":131}}}}],["860",{"pageContent":"of res === \"object\" &&\n!Array.isArray(res) &&\nstr.length > this.maxValueLength\n) {\nreturn `Value is a large dictionary, should explore its keys directly.`;\n}\n\nif (str.length > this.maxValueLength) {\nreturn `${str.slice(0, this.maxValueLength)}...`;\n}\nreturn str;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/json.ts","loc":{"lines":{"from":195,"to":207}}}}],["861",{"pageContent":"class JsonListKeysTool extends Tool {\nname = \"json_list_keys\";\n\nconstructor(public jsonSpec: JsonSpec) {\nsuper();\n}\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nreturn this.jsonSpec.getKeys(input);\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Can be used to list all keys at a given path. \nBefore calling this you should be SURE that the path to this exists.\nThe input is a text representation of the path to the json as json pointer syntax (e.g. /key1/0/key2).`;\n}\n\nexport class JsonGetValueTool extends Tool {\nname = \"json_get_value\";\n\nconstructor(public jsonSpec: JsonSpec) {\nsuper();\n}\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nreturn this.jsonSpec.getValue(input);\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Can be used to see value in string format at a given path.\nBefore calling this you should be SURE that the path to this exists.\nThe input is a text representation of the path to the json as json pointer syntax (e.g. /key1/0/key2).`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/json.ts","loc":{"lines":{"from":292,"to":332}}}}],["862",{"pageContent":"import { Tool } from \"./base.js\";\n\nexport interface Headers {\n[key: string]: string;\n}\n\nexport interface RequestTool {\nheaders: Headers;\n}\n\nexport class RequestsGetTool extends Tool implements RequestTool {\nname = \"requests_get\";\n\nconstructor(public headers: Headers = {}) {\nsuper();\n}\n\n/** @ignore */\nasync _call(input: string) {\nconst res = await fetch(input, {\nheaders: this.headers,\n});\nreturn res.text();\n}\n\ndescription = `A portal to the internet. Use this when you need to get specific content from a website. \nInput should be a  url (i.e. https://www.google.com). The output will be the text response of the GET request.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/requests.ts","loc":{"lines":{"from":1,"to":28}}}}],["863",{"pageContent":"class RequestsPostTool extends Tool implements RequestTool {\nname = \"requests_post\";\n\nconstructor(public headers: Headers = {}) {\nsuper();\n}\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nconst { url, data } = JSON.parse(input);\nconst res = await fetch(url, {\nmethod: \"POST\",\nheaders: this.headers,\nbody: JSON.stringify(data),\n});\nreturn res.text();\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Use this when you want to POST to a website.\nInput should be a json string with two keys: \"url\" and \"data\".\nThe value of \"url\" should be a string, and the value of \"data\" should be a dictionary of \nkey-value pairs you want to POST to the url as a JSON body.\nBe careful to always use double quotes for strings in the json string\nThe output will be the text response of the POST request.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/requests.ts","loc":{"lines":{"from":59,"to":87}}}}],["864",{"pageContent":"import { Tool } from \"./base.js\";\n\n/**\n* This does not use the `serpapi` package because it appears to cause issues\n* when used in `jest` tests. Part of the issue seems to be that the `serpapi`\n* package imports a wasm module to use instead of native `fetch`, which we\n* don't want anyway.\n*\n* NOTE: you must provide location, gl and hl or your region and language will\n* may not match your location, and will not be deterministic.\n*/\n\n// Copied over from `serpapi` package","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":1,"to":13}}}}],["865",{"pageContent":"BaseParameters {\n/**\n* Parameter defines the device to use to get the results. It can be set to\n* `desktop` (default) to use a regular browser, `tablet` to use a tablet browser\n* (currently using iPads), or `mobile` to use a mobile browser (currently\n* using iPhones).\n*/\ndevice?: \"desktop\" | \"tablet\" | \"mobile\";\n/**\n* Parameter will force SerpApi to fetch the Google results even if a cached\n* version is already present. A cache is served only if the query and all\n* parameters are exactly the same. Cache expires after 1h. Cached searches\n* are free, and are not counted towards your searches per month. It can be set\n* to `false` (default) to allow results from the cache, or `true` to disallow\n* results from the cache. `no_cache` and `async` parameters should not be used together.\n*/\nno_cache?: boolean;\n/**\n* Specify the client-side timeout of the request. In milliseconds.\n*/\ntimeout?: number;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":386,"to":407}}}}],["866",{"pageContent":"interface SerpAPIParameters extends BaseParameters {\n/**\n* Search Query\n* Parameter defines the query you want to search. You can use anything that you\n* would use in a regular Google search. e.g. `inurl:`, `site:`, `intitle:`. We\n* also support advanced search query parameters such as as_dt and as_eq. See the\n* [full list](https://serpapi.com/advanced-google-query-parameters) of supported\n* advanced search query parameters.\n*/\nq: string;\n/**\n* Location\n* Parameter defines from where you want the search to originate. If several\n* locations match the location requested, we'll pick the most popular one. Head to\n* [/locations.json API](https://serpapi.com/locations-api) if you need more\n* precise control. location and uule parameters can't be used together. Avoid\n* utilizing location when setting the location outside the U.S. when using Google\n* Shopping and/or Google Product API.\n*/\nlocation?: string;\n/**\n* Encoded Location\n* Parameter is the Google encoded location you want to use for the search. uule","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":770,"to":792}}}}],["867",{"pageContent":"* and location parameters can't be used together.\n*/\nuule?: string;\n/**\n* Google Place ID\n* Parameter defines the id (`CID`) of the Google My Business listing you want to\n* scrape. Also known as Google Place ID.\n*/\nludocid?: string;\n/**\n* Additional Google Place ID\n* Parameter that you might have to use to force the knowledge graph map view to\n* show up. You can find the lsig ID by using our [Local Pack\n* API](https://serpapi.com/local-pack) or [Places Results\n* API](https://serpapi.com/places-results).\n* lsig ID is also available via a redirect Google uses within [Google My\n* Business](https://www.google.com/business/).\n*/\nlsig?: string;\n/**\n* Google Knowledge Graph ID\n* Parameter defines the id (`KGMID`) of the Google Knowledge Graph listing you\n* want to scrape. Also known as Google Knowledge Graph ID. Searches with kgmid\n* parameter will return results for the originally encrypted search parameters.\n* For some searches, kgmid may override all other parameters except start, and num\n* parameters.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":1153,"to":1179}}}}],["868",{"pageContent":"kgmid?: string;\n/**\n* Google Cached Search Parameters ID\n* Parameter defines the cached search parameters of the Google Search you want to\n* scrape. Searches with si parameter will return results for the originally\n* encrypted search parameters. For some searches, si may override all other\n* parameters except start, and num parameters. si can be used to scrape Google\n* Knowledge Graph Tabs.\n*/\nsi?: string;\n/**\n* Domain\n* Parameter defines the Google domain to use. It defaults to `google.com`. Head to\n* the [Google domains page](https://serpapi.com/google-domains) for a full list of\n* supported Google domains.\n*/\ngoogle_domain?: string;\n/**\n* Country\n* Parameter defines the country to use for the Google search. It's a two-letter\n* country code. (e.g., `us` for the United States, `uk` for United Kingdom, or\n* `fr` for France). Head to the [Google countries\n* page](https://serpapi.com/google-countries) for a full list of supported Google\n* countries.\n*/\ngl?: string;\n/**\n* Language","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":1540,"to":1567}}}}],["869",{"pageContent":"* Parameter defines the language to use for the Google search. It's a two-letter\n* language code. (e.g., `en` for English, `es` for Spanish, or `fr` for French).\n* Head to the [Google languages page](https://serpapi.com/google-languages) for a\n* full list of supported Google languages.\n*/\nhl?: string;\n/**\n* Set Multiple Languages\n* Parameter defines one or multiple languages to limit the search to. It uses\n* `lang_{two-letter language code}` to specify languages and `|` as a delimiter.\n* (e.g., `lang_fr|lang_de` will only search French and German pages). Head to the\n* [Google lr languages page](https://serpapi.com/google-lr-languages) for a full\n* list of supported languages.\n*/\nlr?: string;\n/**\n* as_dt\n* Parameter controls whether to include or exclude results from the site named in\n* the as_sitesearch parameter.\n*/\nas_dt?: string;\n/**\n* as_epq\n* Parameter identifies a phrase that all documents in the search results must\n* contain. You can also use the [phrase","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":1928,"to":1952}}}}],["870",{"pageContent":"* search](https://developers.google.com/custom-search/docs/xml_results#PhraseSearchqt)\n* query term to search for a phrase.\n*/\nas_epq?: string;\n/**\n* as_eq\n* Parameter identifies a word or phrase that should not appear in any documents in\n* the search results. You can also use the [exclude\n* query](https://developers.google.com/custom-search/docs/xml_results#Excludeqt)\n* term to ensure that a particular word or phrase will not appear in the documents\n* in a set of search results.\n*/\nas_eq?: string;\n/**\n* as_lq\n* Parameter specifies that all search results should contain a link to a\n* particular URL. You can also use the\n* [link:](https://developers.google.com/custom-search/docs/xml_results#BackLinksqt)\n* query term for this type of query.\n*/\nas_lq?: string;\n/**\n* as_nlo\n* Parameter specifies the starting value for a search range. Use as_nlo and as_nhi\n* to append an inclusive search range.\n*/\nas_nlo?: string;\n/**\n* as_nhi\n* Parameter specifies the ending value for a search range. Use as_nlo and as_nhi","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":2314,"to":2343}}}}],["871",{"pageContent":"* to append an inclusive search range.\n*/\nas_nhi?: string;\n/**\n* as_oq\n* Parameter provides additional search terms to check for in a document, where\n* each document in the search results must contain at least one of the additional\n* search terms. You can also use the [Boolean\n* OR](https://developers.google.com/custom-search/docs/xml_results#BooleanOrqt)\n* query term for this type of query.\n*/\nas_oq?: string;\n/**\n* as_q\n* Parameter provides search terms to check for in a document. This parameter is\n* also commonly used to allow users to specify additional terms to search for\n* within a set of search results.\n*/\nas_q?: string;\n/**\n* as_qdr\n* Parameter requests search results from a specified time period (quick date\n* range). The following values are supported:\n* `d[number]`: requests results from the specified number of past days. Example\n* for the past 10 days: `as_qdr=d10`\n* `w[number]`: requests results from the specified number of past weeks.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":2704,"to":2729}}}}],["872",{"pageContent":"* `m[number]`: requests results from the specified number of past months.\n* `y[number]`: requests results from the specified number of past years. Example\n* for the past year: `as_qdr=y`\n*/\nas_qdr?: string;\n/**\n* as_rq\n* Parameter specifies that all search results should be pages that are related to\n* the specified URL. The parameter value should be a URL. You can also use the\n* [related:](https://developers.google.com/custom-search/docs/xml_results#RelatedLinksqt)\n* query term for this type of query.\n*/\nas_rq?: string;\n/**\n* as_sitesearch\n* Parameter allows you to specify that all search results should be pages from a\n* given site. By setting the as_dt parameter, you can also use it to exclude pages\n* from a given site from your search resutls.\n*/\nas_sitesearch?: string;\n/**\n* Advanced Search Parameters\n* (to be searched) parameter defines advanced search parameters that aren't\n* possible in the regular query field. (e.g., advanced search for patents, dates,\n* news, videos, images, apps, or text contents).","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":3091,"to":3115}}}}],["873",{"pageContent":"*/\ntbs?: string;\n/**\n* Adult Content Filtering\n* Parameter defines the level of filtering for adult content. It can be set to\n* `active`, or `off` (default).\n*/\nsafe?: string;\n/**\n* Exclude Auto-corrected Results\n* Parameter defines the exclusion of results from an auto-corrected query that is\n* spelled wrong. It can be set to `1` to exclude these results, or `0` to include\n* them (default).\n*/\nnfpr?: string;\n/**\n* Results Filtering\n* Parameter defines if the filters for 'Similar Results' and 'Omitted Results' are\n* on or off. It can be set to `1` (default) to enable these filters, or `0` to\n* disable these filters.\n*/\nfilter?: string;\n/**\n* Search Type\n* (to be matched) parameter defines the type of search you want to do.\n* It can be set to:\n* `(no tbm parameter)`: regular Google Search,\n* `isch`: [Google Images API](https://serpapi.com/images-results),\n* `lcl` - [Google Local API](https://serpapi.com/local-results)\n* `vid`: [Google Videos API](https://serpapi.com/videos-results),","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":3476,"to":3505}}}}],["874",{"pageContent":"* `nws`: [Google News API](https://serpapi.com/news-results),\n* `shop`: [Google Shopping API](https://serpapi.com/shopping-results),\n* or any other Google service.\n*/\ntbm?: string;\n/**\n* Result Offset\n* Parameter defines the result offset. It skips the given number of results. It's\n* used for pagination. (e.g., `0` (default) is the first page of results, `10` is\n* the 2nd page of results, `20` is the 3rd page of results, etc.).\n* Google Local Results only accepts multiples of `20`(e.g. `20` for the second\n* page results, `40` for the third page results, etc.) as the start value.\n*/\nstart?: number;\n/**\n* Number of Results\n* Parameter defines the maximum number of results to return. (e.g., `10` (default)\n* returns 10 results, `40` returns 40 results, and `100` returns 100 results).\n*/\nnum?: string;\n/**\n* Page Number (images)\n* Parameter defines the page number for [Google\n* Images](https://serpapi.com/images-results). There are 100 images per page. This","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":3866,"to":3889}}}}],["875",{"pageContent":"* parameter is equivalent to start (offset) = ijn * 100. This parameter works only\n* for [Google Images](https://serpapi.com/images-results) (set tbm to `isch`).\n*/\nijn?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":4251,"to":4255}}}}],["876",{"pageContent":"UrlParameters = Record<\nstring,\nstring | number | boolean | undefined | null\n>;\n\n/**\n* Wrapper around SerpAPI.\n*\n* To use, you should have the `serpapi` package installed and the SERPAPI_API_KEY environment variable set.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":4636,"to":4645}}}}],["877",{"pageContent":"class SerpAPI extends Tool {\nprotected key: string;\n\nprotected params: Partial<SerpAPIParameters>;\n\nprotected baseUrl: string;\n\nconstructor(\napiKey: string | undefined = typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.SERPAPI_API_KEY\n: undefined,\nparams: Partial<SerpAPIParameters> = {},\nbaseUrl = \"https://serpapi.com\"\n) {\nsuper();\n\nif (!apiKey) {\nthrow new Error(\n\"SerpAPI API key not set. You can set it as SERPAPI_API_KEY in your .env file, or pass it to SerpAPI.\"\n);\n}\n\nthis.key = apiKey;\nthis.params = params;\nthis.baseUrl = baseUrl;\n}\n\nname = \"search\";\n\nprotected buildUrl<P extends UrlParameters>(\npath: string,\nparameters: P,\nbaseUrl: string\n): string {\nconst nonUndefinedParams: [string, string][] = Object.entries(parameters)\n.filter(([_, value]) => value !== undefined)\n.map(([key, value]) => [key, `${value}`]);\nconst searchParams = new URLSearchParams(nonUndefinedParams);\nreturn `${baseUrl}/${path}?${searchParams}`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":5025,"to":5065}}}}],["878",{"pageContent":"/** @ignore */\nasync _call(input: string) {\nconst { timeout, ...params } = this.params;\nconst resp = await fetch(\nthis.buildUrl(\n\"search\",\n{\n...params,\napi_key: this.key,\nq: input,\n},\nthis.baseUrl\n),\n{\nsignal: timeout ? AbortSignal.timeout(timeout) : undefined,\n}\n);\n\nconst res = await resp.json();\n\nif (res.error) {\nthrow new Error(`Got error from serpAPI: ${res.error}`);\n}\n\nif (res.answer_box?.answer) {\nreturn res.answer_box.answer;\n}\n\nif (res.answer_box?.snippet) {\nreturn res.answer_box.snippet;\n}\n\nif (res.answer_box?.snippet_highlighted_words) {\nreturn res.answer_box.snippet_highlighted_words[0];\n}\n\nif (res.sports_results?.game_spotlight) {\nreturn res.sports_results.game_spotlight;\n}\n\nif (res.knowledge_graph?.description) {\nreturn res.knowledge_graph.description;\n}\n\nif (res.organic_results?.[0]?.snippet) {\nreturn res.organic_results[0].snippet;\n}\n\nreturn \"No good search result found\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":5427,"to":5476}}}}],["879",{"pageContent":"description =\n\"a search engine. useful for when you need to answer questions about current events. input should be a search query.\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serpapi.ts","loc":{"lines":{"from":5839,"to":5841}}}}],["880",{"pageContent":"import { Tool } from \"./base.js\";\n\nexport type SerperParameters = {\ngl?: string;\nhl?: string;\n};\n\n/**\n* Wrapper around serper.\n*\n* You can create a free API key at https://serper.dev.\n*\n* To use, you should have the SERPER_API_KEY environment variable set.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serper.ts","loc":{"lines":{"from":1,"to":14}}}}],["881",{"pageContent":"class Serper extends Tool {\nprotected key: string;\n\nprotected params: Partial<SerperParameters>;\n\nconstructor(\napiKey: string | undefined = typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.SERPER_API_KEY\n: undefined,\nparams: Partial<SerperParameters> = {}\n) {\nsuper();\n\nif (!apiKey) {\nthrow new Error(\n\"Serper API key not set. You can set it as SERPER_API_KEY in your .env file, or pass it to Serper.\"\n);\n}\n\nthis.key = apiKey;\nthis.params = params;\n}\n\nname = \"search\";\n\n/** @ignore */\nasync _call(input: string) {\nconst options = {\nmethod: \"POST\",\nheaders: {\n\"X-API-KEY\": this.key,\n\"Content-Type\": \"application/json\",\n},\nbody: JSON.stringify({\nq: input,\n...this.params,\n}),\n};\n\nconst res = await fetch(\"https://google.serper.dev/search\", options);\n\nif (!res.ok) {\nthrow new Error(`Got ${res.status} error from serper: ${res.statusText}`);\n}\n\nconst json = await res.json();\n\nif (json.answerBox?.answer) {\nreturn json.answerBox.answer;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serper.ts","loc":{"lines":{"from":93,"to":143}}}}],["882",{"pageContent":"if (json.answerBox?.snippet) {\nreturn json.answerBox.snippet;\n}\n\nif (json.answerBox?.snippet_highlighted_words) {\nreturn json.answerBox.snippet_highlighted_words[0];\n}\n\nif (json.sportsResults?.game_spotlight) {\nreturn json.sportsResults.game_spotlight;\n}\n\nif (json.knowledgeGraph?.description) {\nreturn json.knowledgeGraph.description;\n}\n\nif (json.organic?.[0]?.snippet) {\nreturn json.organic[0].snippet;\n}\n\nreturn \"No good search result found\";\n}\n\ndescription =\n\"a search engine. useful for when you need to answer questions about current events. input should be a search query.\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/serper.ts","loc":{"lines":{"from":190,"to":215}}}}],["883",{"pageContent":"import { Tool } from \"./base.js\";\nimport { OpenAI } from \"../llms/openai.js\";\nimport { LLMChain } from \"../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../prompts/prompt.js\";\nimport type { SqlDatabase } from \"../sql_db.js\";\nimport { SqlTable } from \"../util/sql_utils.js\";\n\ninterface SqlTool {\ndb: SqlDatabase;\n}\n\nexport class QuerySqlTool extends Tool implements SqlTool {\nname = \"query-sql\";\n\ndb: SqlDatabase;\n\nconstructor(db: SqlDatabase) {\nsuper();\nthis.db = db;\n}\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nreturn await this.db.run(input);\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Input to this tool is a detailed and correct SQL query, output is a result from the database.\nIf the query is not correct, an error message will be returned. \nIf an error is returned, rewrite the query, check the query, and try again.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/sql.ts","loc":{"lines":{"from":1,"to":34}}}}],["884",{"pageContent":"class InfoSqlTool extends Tool implements SqlTool {\nname = \"info-sql\";\n\ndb: SqlDatabase;\n\nconstructor(db: SqlDatabase) {\nsuper();\nthis.db = db;\n}\n\n/** @ignore */\nasync _call(input: string) {\ntry {\nconst tables = input.split(\",\").map((table) => table.trim());\nreturn await this.db.getTableInfo(tables);\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables.\nBe sure that the tables actually exist by calling list-tables-sql first!\n\nExample Input: \"table1, table2, table3.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/sql.ts","loc":{"lines":{"from":128,"to":152}}}}],["885",{"pageContent":"class ListTablesSqlTool extends Tool implements SqlTool {\nname = \"list-tables-sql\";\n\ndb: SqlDatabase;\n\nconstructor(db: SqlDatabase) {\nsuper();\nthis.db = db;\n}\n\n/** @ignore */\nasync _call(_: string) {\ntry {\nconst tables = this.db.allTables.map(\n(table: SqlTable) => table.tableName\n);\nreturn tables.join(\", \");\n} catch (error) {\nreturn `${error}`;\n}\n}\n\ndescription = `Input is an empty string, output is a comma separated list of tables in the database.`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/sql.ts","loc":{"lines":{"from":254,"to":277}}}}],["886",{"pageContent":"class QueryCheckerTool extends Tool {\nname = \"query-checker\";\n\ntemplate = `\n{query}\nDouble check the sqlite query above for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\n\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.`;\n\nllmChain: LLMChain;\n\nconstructor(llmChain?: LLMChain) {\nsuper();\nif (llmChain) {\nthis.llmChain = llmChain;\n} else {\nconst model = new OpenAI({ temperature: 0 });\nconst prompt = new PromptTemplate({\ntemplate: this.template,\ninputVariables: [\"query\"],\n});\nthis.llmChain = new LLMChain({ llm: model, prompt });\n}\n}\n\n/** @ignore */\nasync _call(input: string) {\nreturn this.llmChain.predict({ query: input });\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/sql.ts","loc":{"lines":{"from":387,"to":423}}}}],["887",{"pageContent":"description = `Use this tool to double check if your query is correct before executing it.\nAlways use this tool before executing a query with query-sql!`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/sql.ts","loc":{"lines":{"from":512,"to":514}}}}],["888",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { AIPluginTool } from \"../aiplugin.js\";\n\ntest(\"AIPluginTool\", async () => {\nconst tool = await AIPluginTool.fromPluginUrl(\n\"https://www.klarna.com/.well-known/ai-plugin.json\"\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":1,"to":7}}}}],["889",{"pageContent":"expect(await tool.call(undefined)).toMatchInlineSnapshot(`","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":10,"to":10}}}}],["890",{"pageContent":"\"Usage Guide: Assistant uses the Klarna plugin to get relevant product suggestions for any shopping or product discovery purpose. Assistant will reply with the following 3 paragraphs 1) Search Results 2) Product Comparison of the Search Results 3) Followup Questions. The first paragraph contains a list of the products with their attributes listed clearly and concisely as bullet points under the product, together with a link to the product and an explanation. Links will always be returned and should be shown to the user. The second paragraph compares the results returned in a summary sentence starting with \"In summary\". Assistant comparisons consider only the most important features of the products that will help them fit the users request, and each product mention is brief, short and concise. In the third paragraph assistant always asks helpful follow-up questions and end with a question mark. When assistant is asking a follow-up question, it uses it's product expertise to provide information pertaining to the subject of the user's request that may guide them in their search for the right product.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":11,"to":11}}}}],["891",{"pageContent":"OpenAPI Spec: {\"openapi\":\"3.0.1\",\"info\":{\"version\":\"v0\",\"title\":\"Open AI Klarna product Api\"},\"servers\":[{\"url\":\"https://www.klarna.com/us/shopping\"}],\"tags\":[{\"name\":\"open-ai-product-endpoint\",\"description\":\"Open AI Product Endpoint. Query for products.\"}],\"paths\":{\"/public/openai/v0/products\":{\"get\":{\"tags\":[\"open-ai-product-endpoint\"],\"summary\":\"API for fetching Klarna product information\",\"operationId\":\"productsUsingGET\",\"parameters\":[{\"name\":\"q\",\"in\":\"query\",\"description\":\"A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started.\",\"required\":true,\"schema\":{\"type\":\"string\"}},{\"name\":\"size\",\"in\":\"query\",\"description\":\"number of products","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":13,"to":13}}}}],["892",{"pageContent":"returned\",\"required\":false,\"schema\":{\"type\":\"integer\"}},{\"name\":\"min_price\",\"in\":\"query\",\"description\":\"(Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\",\"required\":false,\"schema\":{\"type\":\"integer\"}},{\"name\":\"max_price\",\"in\":\"query\",\"description\":\"(Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\",\"required\":false,\"schema\":{\"type\":\"integer\"}}],\"responses\":{\"200\":{\"description\":\"Products found\",\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/ProductResponse\"}}}},\"503\":{\"description\":\"one or more services are","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":13,"to":13}}}}],["893",{"pageContent":"unavailable\"}},\"deprecated\":false}}},\"components\":{\"schemas\":{\"Product\":{\"type\":\"object\",\"properties\":{\"attributes\":{\"type\":\"array\",\"items\":{\"type\":\"string\"}},\"name\":{\"type\":\"string\"},\"price\":{\"type\":\"string\"},\"url\":{\"type\":\"string\"}},\"title\":\"Product\"},\"ProductResponse\":{\"type\":\"object\",\"properties\":{\"products\":{\"type\":\"array\",\"items\":{\"$ref\":\"#/components/schemas/Product\"}}},\"title\":\"ProductResponse\"}}}}\"","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":13,"to":13}}}}],["894",{"pageContent":"`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":14,"to":14}}}}],["895",{"pageContent":"expect(await tool.call({})).toMatch(/Usage Guide/);\n\nexpect(await tool.call(\"\")).toMatch(/OpenAPI Spec/);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/aiplugin.int.test.ts","loc":{"lines":{"from":19,"to":22}}}}],["896",{"pageContent":"import { test, expect, jest } from \"@jest/globals\";\n\nimport { ChainTool } from \"../chain.js\";\nimport { LLMChain } from \"../../chains/llm_chain.js\";\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\nimport { LLM } from \"../../llms/base.js\";\nimport { VectorDBQAChain } from \"../../chains/vector_db_qa.js\";\nimport { MemoryVectorStore } from \"../../vectorstores/memory.js\";\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":1,"to":9}}}}],["897",{"pageContent":"FakeLLM extends LLM {\n_llmType() {\nreturn \"fake\";\n}\n\nasync _call(prompt: string): Promise<string> {\nreturn prompt;\n}\n}\n\ntest(\"chain tool with llm chain and local callback\", async () => {\nconst calls: string[] = [];\nconst handleToolStart = jest.fn(() => {\ncalls.push(\"tool start\");\n});\nconst handleToolEnd = jest.fn(() => {\ncalls.push(\"tool end\");\n});\nconst handleLLMStart = jest.fn(() => {\ncalls.push(\"llm start\");\n});\nconst handleLLMEnd = jest.fn(() => {\ncalls.push(\"llm end\");\n});\nconst handleChainStart = jest.fn(() => {\ncalls.push(\"chain start\");\n});\nconst handleChainEnd = jest.fn(() => {\ncalls.push(\"chain end\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":140,"to":169}}}}],["898",{"pageContent":"const chain = new LLMChain({\nllm: new FakeLLM({}),\nprompt: PromptTemplate.fromTemplate(\"hello world\"),\n});\nconst tool = new ChainTool({ chain, name: \"fake\", description: \"fake\" });\nconst result = await tool.call(\"hi\", [\n{\nhandleToolStart,\nhandleToolEnd,\nhandleLLMStart,\nhandleLLMEnd,\nhandleChainStart,\nhandleChainEnd,\n},\n]);\nexpect(result).toMatchInlineSnapshot(`\"hello world\"`);\nexpect(handleToolStart).toBeCalledTimes(1);\nexpect(handleToolEnd).toBeCalledTimes(1);\nexpect(handleLLMStart).toBeCalledTimes(1);\nexpect(handleLLMEnd).toBeCalledTimes(1);\nexpect(handleChainStart).toBeCalledTimes(1);\nexpect(handleChainEnd).toBeCalledTimes(1);\nexpect(calls).toMatchInlineSnapshot(`\n[\n\"tool start\",\n\"chain start\",\n\"llm start\",\n\"llm end\",\n\"chain end\",\n\"tool end\",\n]\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":288,"to":320}}}}],["899",{"pageContent":"test(\"chain tool with vectordbqa chain\", async () => {\nconst calls: string[] = [];\nconst handleToolStart = jest.fn(() => {\ncalls.push(\"tool start\");\n});\nconst handleToolEnd = jest.fn(() => {\ncalls.push(\"tool end\");\n});\nconst handleLLMStart = jest.fn(() => {\ncalls.push(\"llm start\");\n});\nconst handleLLMEnd = jest.fn(() => {\ncalls.push(\"llm end\");\n});\nconst handleChainStart = jest.fn(() => {\ncalls.push(\"chain start\");\n});\nconst handleChainEnd = jest.fn(() => {\ncalls.push(\"chain end\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":434,"to":453}}}}],["900",{"pageContent":"const chain = VectorDBQAChain.fromLLM(\nnew FakeLLM({}),\nawait MemoryVectorStore.fromExistingIndex(new FakeEmbeddings())\n);\nconst tool = new ChainTool({ chain, name: \"fake\", description: \"fake\" });\nconst result = await tool.call(\"hi\", [\n{\nhandleToolStart,\nhandleToolEnd,\nhandleLLMStart,\nhandleLLMEnd,\nhandleChainStart,\nhandleChainEnd,\n},\n]);\nexpect(result).toMatchInlineSnapshot(`\n\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":580,"to":596}}}}],["901",{"pageContent":"Question: hi\nHelpful Answer:\"\n`);\nexpect(handleToolStart).toBeCalledTimes(1);\nexpect(handleToolEnd).toBeCalledTimes(1);\nexpect(handleLLMStart).toBeCalledTimes(1);\nexpect(handleLLMEnd).toBeCalledTimes(1);\nexpect(handleChainStart).toBeCalledTimes(3);\nexpect(handleChainEnd).toBeCalledTimes(3);\nexpect(calls).toMatchInlineSnapshot(`\n[\n\"tool start\",\n\"chain start\",\n\"chain start\",\n\"chain start\",\n\"llm start\",\n\"llm end\",\n\"chain end\",\n\"chain end\",\n\"chain end\",\n\"tool end\",\n]\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/chain.test.ts","loc":{"lines":{"from":720,"to":743}}}}],["902",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { SerpAPI } from \"../../tools/serpapi.js\";\n\ndescribe(\"serp api test suite\", () => {\nclass SerpApiUrlTester extends SerpAPI {\ntestThisUrl(): string {\nreturn this.buildUrl(\"search\", this.params, this.baseUrl);\n}\n}\n\ntest(\"Test default url\", async () => {\nconst serpApi = new SerpApiUrlTester(\n\"Not a real key but constructor error if not set\",\n{\nhl: \"en\",\ngl: \"us\",\n}\n);\nexpect(serpApi.testThisUrl()).toEqual(\n\"https://serpapi.com/search?hl=en&gl=us\"\n);\n});\n\ntest(\"Test override url\", async () => {\nconst serpApiProxied = new SerpApiUrlTester(\n\"Not a real key but constructor error if not set\",\n{\ngl: \"us\",\n},\n\"https://totallyProxied.com\"\n);\n\nexpect(\nserpApiProxied.testThisUrl() === \"https://totallyProxied.com/search?gl=us\"\n);\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/serpapi.test.ts","loc":{"lines":{"from":1,"to":37}}}}],["903",{"pageContent":"import { test, expect, describe } from \"@jest/globals\";\nimport { WebBrowser } from \"../webbrowser.js\";\nimport { ChatOpenAI } from \"../../chat_models/openai.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport fetchAdapter from \"../../util/axios-fetch-adapter.js\";\n\ndescribe(\"webbrowser Test suite\", () => {\ntest(\"get word of the day\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://www.merriam-webster.com/word-of-the-day\",\"word of the day\"`\n);\n\nexpect(result).toContain(\"Word of the Day:\");\n});\n\ntest(\"get a summary of the page when empty request with fetch adapter\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.int.test.ts","loc":{"lines":{"from":1,"to":22}}}}],["904",{"pageContent":"const browser = new WebBrowser({\nmodel,\nembeddings,\naxiosConfig: {\nadapter: fetchAdapter,\n},\n});\nconst result = await browser.call(\n`\"https://www.merriam-webster.com/word-of-the-day\",\"\"`\n);\n\n// fuzzy, sometimes its capped and others not\nexpect(result).toMatch(/word of the day/i);\n});\n\ntest(\"error no url\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(`\"\",\"\"`);\n\nexpect(result).toEqual(\"TypeError [ERR_INVALID_URL]: Invalid URL\");\n});\n\ntest(\"error no protocol or malformed\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"www.merriam-webster.com/word-of-the-day\",\"word of the day\"`\n);\n\nexpect(result).toEqual(\"TypeError [ERR_INVALID_URL]: Invalid URL\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.int.test.ts","loc":{"lines":{"from":124,"to":159}}}}],["905",{"pageContent":"test(\"error bad site\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://www.hDjRBKoAD0EIbF29TWM4rbXDGGM5Nhy4uzNEAdDS.com\",\"word of the day\"`\n);\n\nexpect(result).toEqual(\n\"Error: getaddrinfo ENOTFOUND www.hdjrbkoad0eibf29twm4rbxdggm5nhy4uzneadds.com\"\n);\n});\n\ntest(\"get a summary of a page that detects scraping\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://www.musicgateway.com/spotify-pre-save\",\"\"`\n);\n\nexpect(result).not.toEqual(\"Error: http response 403\");\n});\n\n// cant we figure the headers to fix this?\ntest.skip(\"get a summary of a page that detects scraping 2\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.int.test.ts","loc":{"lines":{"from":257,"to":286}}}}],["906",{"pageContent":"const browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://parade.com/991228/marynliles/couples-goals\",\"\"`\n);\nexpect(result).not.toEqual(\"Error: http response 403\");\n});\n\ntest(\"get a summary of a page that rejects unauthorized\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();\n\nconst browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://firstround.com/review/how-to-fix-the-co-founder-fights-youre-sick-of-having-lessons-from-couples-therapist-esther-perel\",\"\"`\n);\n\nexpect(result).toContain(\"Esther Perel\");\n});\n\n// other urls that have done this too\n// \"https://wsimag.com/economy-and-politics/15473-power-and-money\",\n// \"https://thriveglobal.com/stories/sleep-what-to-do-what-not-to-do\",\ntest(\"get a summary of a page that redirects too many times\", async () => {\nconst model = new ChatOpenAI({ temperature: 0 });\nconst embeddings = new OpenAIEmbeddings();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.int.test.ts","loc":{"lines":{"from":382,"to":406}}}}],["907",{"pageContent":"const browser = new WebBrowser({ model, embeddings });\nconst result = await browser.call(\n`\"https://www.healtheuropa.eu/why-mdma-must-be-reclassified-as-a-schedule-2-drug/95780\",\"\"`\n);\nexpect(result).toContain(\"Beckley Foundation\");\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.int.test.ts","loc":{"lines":{"from":502,"to":508}}}}],["908",{"pageContent":"import { test, expect, describe } from \"@jest/globals\";\nimport { readFileSync } from \"fs\";\nimport { getText, parseInputs } from \"../webbrowser.js\";\n\ndescribe(\"webbrowser Test suite\", () => {\nconst html = readFileSync(\"./src/tools/fixtures/wordoftheday.html\", \"utf8\");\n\ntest(\"parse html to text and links\", async () => {\nconst baseUrl = \"https://www.merriam-webster.com/word-of-the-day\";\nconst text = getText(html, baseUrl, false);\nexpect(text).toContain(\"Word of the Day: Foible\");\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.test.ts","loc":{"lines":{"from":1,"to":12}}}}],["909",{"pageContent":"test(\"parseInputs\", () => {\nexpect(\nparseInputs(`\"https://www.merriam-webster.com/word-of-the-day\",\"\"`)\n).toEqual([\"https://www.merriam-webster.com/word-of-the-day\", \"\"]);\nexpect(\nparseInputs(\n`\"https://www.merriam-webster.com/word-of-the-day\",\"word of the day\"`\n)\n).toEqual([\n\"https://www.merriam-webster.com/word-of-the-day\",\n\"word of the day\",\n]);\nexpect(\nparseInputs(`\"https://www.merriam-webster.com/word-of-the-day\",\"`)\n).toEqual([\"https://www.merriam-webster.com/word-of-the-day\", \"\"]);\nexpect(\nparseInputs(`\"https://www.merriam-webster.com/word-of-the-day\",`)\n).toEqual([\"https://www.merriam-webster.com/word-of-the-day\", \"\"]);\nexpect(\nparseInputs(`\"https://www.merriam-webster.com/word-of-the-day\"`)\n).toEqual([\"https://www.merriam-webster.com/word-of-the-day\", undefined]);\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/tests/webbrowser.test.ts","loc":{"lines":{"from":37,"to":59}}}}],["910",{"pageContent":"import { VectorStore } from \"../vectorstores/base.js\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { VectorDBQAChain } from \"../chains/vector_db_qa.js\";\nimport { Tool } from \"./base.js\";\n\ninterface VectorStoreTool {\nvectorStore: VectorStore;\nllm: BaseLanguageModel;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/vectorstore.ts","loc":{"lines":{"from":1,"to":9}}}}],["911",{"pageContent":"class VectorStoreQATool extends Tool implements VectorStoreTool {\nvectorStore: VectorStore;\n\nllm: BaseLanguageModel;\n\nname: string;\n\ndescription: string;\n\nchain: VectorDBQAChain;\n\nconstructor(name: string, description: string, fields: VectorStoreTool) {\nsuper();\nthis.name = name;\nthis.description = description;\nthis.vectorStore = fields.vectorStore;\nthis.llm = fields.llm;\nthis.chain = VectorDBQAChain.fromLLM(this.llm, this.vectorStore);\n}\n\nstatic getDescription(name: string, description: string): string {\nreturn `Useful for when you need to answer questions about ${name}. Whenever you need information about ${description} you should ALWAYS use this. Input should be a fully formed question.`;\n}\n\n/** @ignore */\nasync _call(input: string) {\nreturn this.chain.run(input);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/vectorstore.ts","loc":{"lines":{"from":40,"to":68}}}}],["912",{"pageContent":"import axiosMod, { AxiosRequestConfig, AxiosStatic } from \"axios\";\nimport { isNode } from \"browser-or-node\";\nimport * as cheerio from \"cheerio\";\nimport { BaseLanguageModel } from \"../base_language/index.js\";\nimport { RecursiveCharacterTextSplitter } from \"../text_splitter.js\";\nimport { MemoryVectorStore } from \"../vectorstores/memory.js\";\nimport { StringPromptValue } from \"../prompts/base.js\";\nimport { Document } from \"../document.js\";\nimport { Tool, ToolParams } from \"./base.js\";\nimport {\nCallbackManager,\nCallbackManagerForToolRun,\n} from \"../callbacks/manager.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1,"to":15}}}}],["913",{"pageContent":"const parseInputs = (inputs: string): [string, string] => {\nconst [baseUrl, task] = inputs.split(\",\").map((input) => {\nlet t = input.trim();\nt = t.startsWith('\"') ? t.slice(1) : t;\nt = t.endsWith('\"') ? t.slice(0, -1) : t;\n// it likes to put / at the end of urls, wont matter for task\nt = t.endsWith(\"/\") ? t.slice(0, -1) : t;\nreturn t.trim();\n});\n\nreturn [baseUrl, task];\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":249,"to":260}}}}],["914",{"pageContent":"const getText = (\nhtml: string,\nbaseUrl: string,\nsummary: boolean\n): string => {\n// scriptingEnabled so noscript elements are parsed\nconst $ = cheerio.load(html, { scriptingEnabled: true });\n\nlet text = \"\";\n\n// lets only get the body if its a summary, dont need to summarize header or footer etc\nconst rootElement = summary ? \"body \" : \"*\";\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n$(`${rootElement}:not(style):not(script):not(svg)`).each((_i, elem: any) => {\n// we dont want duplicated content as we drill down so remove children\nlet content = $(elem).clone().children().remove().end().text().trim();\nconst $el = $(elem);\n\n// if its an ahref, print the content and url\nlet href = $el.attr(\"href\");\nif ($el.prop(\"tagName\")?.toLowerCase() === \"a\" && href) {\nif (!href.startsWith(\"http\")) {\ntry {\nhref = new URL(href, baseUrl).toString();\n} catch {\n// if this fails thats fine, just no url for this\nhref = \"\";\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":502,"to":531}}}}],["915",{"pageContent":"const imgAlt = $el.find(\"img[alt]\").attr(\"alt\")?.trim();\nif (imgAlt) {\ncontent += ` ${imgAlt}`;\n}\n\ntext += ` [${content}](${href})`;\n}\n// otherwise just print the content\nelse if (content !== \"\") {\ntext += ` ${content}`;\n}\n});\n\nreturn text.trim().replace(/\\n+/g, \" \");\n};\n\nconst getHtml = async (\nbaseUrl: string,\nh: Headers,\nconfig: AxiosRequestConfig\n) => {\nconst axios = (\n\"default\" in axiosMod ? axiosMod.default : axiosMod\n) as AxiosStatic;\n\nconst domain = new URL(baseUrl).hostname;\n\nconst headers = { ...h };\n// these appear to be positional, which means they have to exist in the headers passed in\nheaders.Host = domain;\nheaders[\"Alt-Used\"] = domain;\n\nlet htmlResponse;\ntry {\nhtmlResponse = await axios.get(baseUrl, {\n...config,\nheaders,\n});\n} catch (e) {\nif (axios.isAxiosError(e) && e.response && e.response.status) {\nthrow new Error(`http response ${e.response.status}`);\n}\nthrow e;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":758,"to":801}}}}],["916",{"pageContent":"const allowedContentTypes = [\n\"text/html\",\n\"application/json\",\n\"application/xml\",\n\"application/javascript\",\n\"text/plain\",\n];\n\nconst contentType = htmlResponse.headers[\"content-type\"];\nconst contentTypeArray = contentType.split(\";\");\nif (\ncontentTypeArray[0] &&\n!allowedContentTypes.includes(contentTypeArray[0])\n) {\nthrow new Error(\"returned page was not utf8\");\n}\nreturn htmlResponse.data;\n};\n\nconst DEFAULT_HEADERS = {\nAccept:\n\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n\"Accept-Encoding\": \"gzip, deflate\",\n\"Accept-Language\": \"en-US,en;q=0.5\",\n\"Alt-Used\": \"LEAVE-THIS-KEY-SET-BY-TOOL\",\nConnection: \"keep-alive\",\nHost: \"LEAVE-THIS-KEY-SET-BY-TOOL\",\nReferer: \"https://www.google.com/\",\n\"Sec-Fetch-Dest\": \"document\",\n\"Sec-Fetch-Mode\": \"navigate\",\n\"Sec-Fetch-Site\": \"cross-site\",\n\"Upgrade-Insecure-Requests\": \"1\",\n\"User-Agent\":\n\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/111.0\",\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1029,"to":1063}}}}],["917",{"pageContent":"// eslint-disable-next-line @typescript-eslint/no-explicit-any","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1084,"to":1084}}}}],["918",{"pageContent":"Headers = Record<string, any>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1184,"to":1184}}}}],["919",{"pageContent":"interface WebBrowserArgs extends ToolParams {\nmodel: BaseLanguageModel;\n\nembeddings: Embeddings;\n\nheaders?: Headers;\n\naxiosConfig?: Omit<AxiosRequestConfig, \"url\">;\n\n/** @deprecated */\ncallbackManager?: CallbackManager;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1290,"to":1301}}}}],["920",{"pageContent":"class WebBrowser extends Tool {\nprivate model: BaseLanguageModel;\n\nprivate embeddings: Embeddings;\n\nprivate headers: Headers;\n\nprivate axiosConfig: Omit<AxiosRequestConfig, \"url\">;\n\nconstructor({\nmodel,\nheaders,\nembeddings,\nverbose,\ncallbacks,\ncallbackManager,\naxiosConfig,\n}: WebBrowserArgs) {\nsuper(verbose, callbacks ?? callbackManager);\n\nthis.model = model;\nthis.embeddings = embeddings;\nthis.headers = headers || DEFAULT_HEADERS;\nthis.axiosConfig = {\nwithCredentials: true,\nadapter: isNode ? undefined : fetchAdapter,\n...axiosConfig,\n};\n}\n\n/** @ignore */\nasync _call(inputs: string, runManager?: CallbackManagerForToolRun) {\nconst [baseUrl, task] = parseInputs(inputs);\nconst doSummary = !task;\n\nlet text;\ntry {\nconst html = await getHtml(baseUrl, this.headers, this.axiosConfig);\ntext = getText(html, baseUrl, doSummary);\n} catch (e) {\nif (e) {\nreturn e.toString();\n}\nreturn \"There was a problem connecting to the site\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1545,"to":1589}}}}],["921",{"pageContent":"const textSplitter = new RecursiveCharacterTextSplitter({\nchunkSize: 2000,\nchunkOverlap: 200,\n});\nconst texts = await textSplitter.splitText(text);\n\nlet context;\n// if we want a summary grab first 4\nif (doSummary) {\ncontext = texts.slice(0, 4).join(\"\\n\");\n}\n// search term well embed and grab top 4\nelse {\nconst docs = texts.map(\n(pageContent) =>\nnew Document({\npageContent,\nmetadata: [],\n})\n);\n\nconst vectorStore = await MemoryVectorStore.fromDocuments(\ndocs,\nthis.embeddings\n);\nconst results = await vectorStore.similaritySearch(task, 4);\ncontext = results.map((res) => res.pageContent).join(\"\\n\");\n}\n\nconst input = `Text:${context}\\n\\nI need ${\ndoSummary ? \"a summary\" : task\n} from the above text, also provide up to 5 markdown links from within that would be of interest (always including URL and text). Links should be provided, if present, in markdown syntax as a list under the heading \"Relevant Links:\".`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":1817,"to":1848}}}}],["922",{"pageContent":"const res = await this.model.generatePrompt(\n[new StringPromptValue(input)],\nundefined,\nrunManager?.getChild()\n);\n\nreturn res.generations[0][0].text;\n}\n\nname = \"web-browser\";\n\ndescription = `useful for when you need to find something on or summarize a webpage. input should be a comma seperated list of \"ONE valid http URL including protocol\",\"what you want to find on the page or empty string for a summary\".`;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/webbrowser.ts","loc":{"lines":{"from":2076,"to":2088}}}}],["923",{"pageContent":"import { Tool } from \"./base.js\";\nimport { renderTemplate } from \"../prompts/template.js\";\nimport { AsyncCaller, AsyncCallerParams } from \"../util/async_caller.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":1,"to":3}}}}],["924",{"pageContent":"const zapierNLABaseDescription: string =\n\"A wrapper around Zapier NLA actions. \" +\n\"The input to this tool is a natural language instruction, \" +\n'for example \"get the latest email from my bank\" or ' +\n'\"send a slack message to the #general channel\". ' +\n\"Each tool will have params associated with it that are specified as a list. You MUST take into account the params when creating the instruction. \" +\n\"For example, if the params are ['Message_Text', 'Channel'], your instruction should be something like 'send a slack message to the #general channel with the text hello world'. \" +\n\"Another example: if the params are ['Calendar', 'Search_Term'], your instruction should be something like 'find the meeting in my personal calendar at 3pm'. \" +\n\"Do not make up params, they will be explicitly specified in the tool description. \" +\n\"If you do not have enough information to fill in the params, just say 'not enough information provided in the instruction, missing <param>'. \" +","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":228,"to":237}}}}],["925",{"pageContent":"\"If you get a none or null response, STOP EXECUTION, do not try to another tool! \" +\n\"This tool specifically used for: {zapier_description}, \" +\n\"and has params: {params}\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":452,"to":454}}}}],["926",{"pageContent":"// eslint-disable-next-line @typescript-eslint/no-explicit-any","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":468,"to":468}}}}],["927",{"pageContent":"type ZapierValues = Record<string, any>;\n\nexport interface ZapiterNLAWrapperParams extends AsyncCallerParams {\napiKey?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":677,"to":681}}}}],["928",{"pageContent":"class ZapierNLAWrapper {\nzapierNlaApiKey: string;\n\nzapierNlaApiBase = \"https://nla.zapier.com/api/v1/\";\n\ncaller: AsyncCaller;\n\nconstructor(params?: string | ZapiterNLAWrapperParams) {\nconst zapierNlaApiKey =\ntypeof params === \"string\" ? params : params?.apiKey;\nconst apiKey =\nzapierNlaApiKey ??\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.ZAPIER_NLA_API_KEY\n: undefined);\nif (!apiKey) {\nthrow new Error(\"ZAPIER_NLA_API_KEY not set\");\n}\nthis.zapierNlaApiKey = apiKey;\nthis.caller = new AsyncCaller(","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":906,"to":926}}}}],["929",{"pageContent":"of params === \"string\" ? {} : params ?? {}\n);\n}\n\nprotected _getHeaders(): Record<string, string> {\nreturn {\n\"Content-Type\": \"application/json\",\nAccept: \"application/json\",\n\"x-api-key\": this.zapierNlaApiKey,\n};\n}\n\nprotected async _getActionRequest(\nactionId: string,\ninstructions: string,\nparams?: ZapierValues\n): Promise<ZapierValues> {\nconst data = params ?? {};\ndata.instructions = instructions;\nconst headers = this._getHeaders();\n\n// add api key to params\nconst resp = await this.caller.call(\nfetch,\n`${this.zapierNlaApiBase}exposed/${actionId}/execute/`,\n{\nmethod: \"POST\",\nheaders,\nbody: JSON.stringify(data),\n}\n);\n\nif (!resp.ok) {\nthrow new Error(\n`Failed to execute action ${actionId} with instructions ${instructions}`\n);\n}\n\nconst jsonResp = await resp.json();\n\nif (jsonResp.status === \"error\") {\nthrow new Error(`Error from Zapier: ${jsonResp.error}`);\n}\n\nreturn jsonResp;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":1144,"to":1189}}}}],["930",{"pageContent":"/**\n* Executes an action that is identified by action_id, must be exposed\n* (enabled) by the current user (associated with the set api_key). Change\n* your exposed actions here: https://nla.zapier.com/demo/start/\n* @param actionId\n* @param instructions\n* @param params\n*/\nasync runAction(\nactionId: string,\ninstructions: string,\nparams?: ZapierValues\n): Promise<ZapierValues> {\nconst resp = await this._getActionRequest(actionId, instructions, params);\nreturn resp.status === \"error\" ? resp.error : resp.result;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":1405,"to":1420}}}}],["931",{"pageContent":"/**\n* Same as run, but instead of actually executing the action, will\n* instead return a preview of params that have been guessed by the AI in\n* case you need to explicitly review before executing.\n* @param actionId\n* @param instructions\n* @param params\n*/\nasync previewAction(\nactionId: string,\ninstructions: string,\nparams?: ZapierValues\n): Promise<ZapierValues> {\nconst data = params ?? {};\ndata.preview_only = true;\nconst resp = await this._getActionRequest(actionId, instructions, data);\nreturn resp.input_params;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":1638,"to":1655}}}}],["932",{"pageContent":"/**\n* Returns a list of all exposed (enabled) actions associated with\n* current user (associated with the set api_key). Change your exposed\n* actions here: https://nla.zapier.com/demo/start/\n*/\nasync listActions(): Promise<ZapierValues[]> {\nconst headers = this._getHeaders();\nconst resp = await this.caller.call(\nfetch,\n`${this.zapierNlaApiBase}exposed/`,\n{\nmethod: \"GET\",\nheaders,\n}\n);\nif (!resp.ok) {\nthrow new Error(\"Failed to list actions\");\n}\nreturn (await resp.json()).results;\n}\n\n/**\n* Same as run, but returns a stringified version of the result.\n* @param actionId\n* @param instructions\n* @param params\n*/\nasync runAsString(\nactionId: string,\ninstructions: string,\nparams?: ZapierValues\n): Promise<string> {\nconst result = await this.runAction(actionId, instructions, params);\nreturn JSON.stringify(result);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":1873,"to":1907}}}}],["933",{"pageContent":"/**\n* Same as preview, but returns a stringified version of the result.\n* @param actionId\n* @param instructions\n* @param params\n*/\nasync previewAsString(\nactionId: string,\ninstructions: string,\nparams?: ZapierValues\n): Promise<string> {\nconst result = await this.previewAction(actionId, instructions, params);\nreturn JSON.stringify(result);\n}\n\n/**\n* Same as list, but returns a stringified version of the result.\n*/\nasync listActionsAsString(): Promise<string> {\nconst result = await this.listActions();\nreturn JSON.stringify(result);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":2123,"to":2145}}}}],["934",{"pageContent":"class ZapierNLARunAction extends Tool {\napiWrapper: ZapierNLAWrapper;\n\nactionId: string;\n\nparams?: ZapierValues;\n\nname: string;\n\ndescription: string;\n\nconstructor(\napiWrapper: ZapierNLAWrapper,\nactionId: string,\nzapierDescription: string,\nparamsSchema: ZapierValues,\nparams?: ZapierValues\n) {\nsuper();\nthis.apiWrapper = apiWrapper;\nthis.actionId = actionId;\nthis.params = params;\nthis.name = zapierDescription;\nconst paramsSchemaWithoutInstructions = { ...paramsSchema };\ndelete paramsSchemaWithoutInstructions.instructions;\nconst paramsSchemaKeysString = JSON.stringify(\nObject.keys(paramsSchemaWithoutInstructions)\n);\nthis.description = renderTemplate(zapierNLABaseDescription, \"f-string\", {\nzapier_description: zapierDescription,\nparams: paramsSchemaKeysString,\n});\n}\n\n/** @ignore */\nasync _call(arg: string): Promise<string> {\nreturn this.apiWrapper.runAsString(this.actionId, arg, this.params);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/tools/zapier.ts","loc":{"lines":{"from":2363,"to":2401}}}}],["935",{"pageContent":"/**\n* Type definitions adapted from pdfjs-dist\n* https://github.com/mozilla/pdfjs-dist/blob/master/types/src/display/api.d.ts\n*/\n\ndeclare module \"pdf-parse/lib/pdf.js/v1.10.100/build/pdf.js\" {\nexport type TypedArray =\n| Int8Array\n| Uint8Array\n| Uint8ClampedArray\n| Int16Array\n| Uint16Array\n| Int32Array\n| Uint32Array\n| Float32Array\n| Float64Array;\nexport type BinaryData = TypedArray | ArrayBuffer | Array<number> | string;\nexport type RefProxy = {\nnum: number;\ngen: number;\n};\n/**\n* Document initialization / loading parameters object.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":1,"to":24}}}}],["936",{"pageContent":"type DocumentInitParameters = {\n/**\n* - The URL of the PDF.\n*/\nurl?: string | URL | undefined;\n/**\n* - Binary PDF data.\n* Use TypedArrays (Uint8Array) to improve the memory usage. If PDF data is\n* BASE64-encoded, use `atob()` to convert it to a binary string first.\n*\n* NOTE: If TypedArrays are used they will generally be transferred to the\n* worker-thread. This will help reduce main-thread memory usage, however\n* it will take ownership of the TypedArrays.\n*/\ndata?: BinaryData | undefined;\n/**\n* - Basic authentication headers.\n*/\nhttpHeaders?: Object | undefined;\n/**\n* - Indicates whether or not\n* cross-site Access-Control requests should be made using credentials such\n* as cookies or authorization headers. The default is `false`.\n*/\nwithCredentials?: boolean | undefined;\n/**\n* - For decrypting password-protected PDFs.\n*/\npassword?: string | undefined;\n/**\n* - The PDF file length. It's used for progress\n* reports and range requests operations.\n*/\nlength?: number | undefined;\n/**\n* - Allows for using a custom range","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":1411,"to":1446}}}}],["937",{"pageContent":"* transport implementation.\n*/\nrange?: PDFDataRangeTransport | undefined;\n/**\n* - Specify maximum number of bytes fetched\n* per range request. The default value is {@link DEFAULT_RANGE_CHUNK_SIZE }.\n*/\nrangeChunkSize?: number | undefined;\n/**\n* - The worker that will be used for loading and\n* parsing the PDF data.\n*/\nworker?: PDFWorker | undefined;\n/**\n* - Controls the logging level; the constants\n* from {@link VerbosityLevel } should be used.\n*/\nverbosity?: number | undefined;\n/**\n* - The base URL of the document, used when\n* attempting to recover valid absolute URLs for annotations, and outline\n* items, that (incorrectly) only specify relative URLs.\n*/\ndocBaseUrl?: string | undefined;\n/**\n* - The URL where the predefined Adobe CMaps are\n* located. Include the trailing slash.\n*/\ncMapUrl?: string | undefined;\n/**\n* - Specifies if the Adobe CMaps are binary\n* packed or not. The default value is `true`.\n*/\ncMapPacked?: boolean | undefined;\n/**\n* - The factory that will be used when","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":2819,"to":2854}}}}],["938",{"pageContent":"* reading built-in CMap files. Providing a custom factory is useful for\n* environments without Fetch API or `XMLHttpRequest` support, such as\n* Node.js. The default value is {DOMCMapReaderFactory}.\n*/\nCMapReaderFactory?: Object | undefined;\n/**\n* - When `true`, fonts that aren't\n* embedded in the PDF document will fallback to a system font.\n* The default value is `true` in web environments and `false` in Node.js;\n* unless `disableFontFace === true` in which case this defaults to `false`\n* regardless of the environment (to prevent completely broken fonts).\n*/\nuseSystemFonts?: boolean | undefined;\n/**\n* - The URL where the standard font\n* files are located. Include the trailing slash.\n*/\nstandardFontDataUrl?: string | undefined;\n/**\n* - The factory that will be used\n* when reading the standard font files. Providing a custom factory is useful\n* for environments without Fetch API or `XMLHttpRequest` support, such as\n* Node.js. The default value is {DOMStandardFontDataFactory}.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":4228,"to":4251}}}}],["939",{"pageContent":"StandardFontDataFactory?: Object | undefined;\n/**\n* - Enable using the Fetch API in the\n* worker-thread when reading CMap and standard font files. When `true`,\n* the `CMapReaderFactory` and `StandardFontDataFactory` options are ignored.\n* The default value is `true` in web environments and `false` in Node.js.\n*/\nuseWorkerFetch?: boolean | undefined;\n/**\n* - Reject certain promises, e.g.\n* `getOperatorList`, `getTextContent`, and `RenderTask`, when the associated\n* PDF data cannot be successfully parsed, instead of attempting to recover\n* whatever possible of the data. The default value is `false`.\n*/\nstopAtErrors?: boolean | undefined;\n/**\n* - The maximum allowed image size in total\n* pixels, i.e. width * height. Images above this value will not be rendered.\n* Use -1 for no limit, which is also the default value.\n*/\nmaxImageSize?: number | undefined;\n/**\n* - Determines if we can evaluate strings\n* as JavaScript. Primarily used to improve performance of font rendering, and","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":5625,"to":5648}}}}],["940",{"pageContent":"* when parsing PDF functions. The default value is `true`.\n*/\nisEvalSupported?: boolean | undefined;\n/**\n* - Determines if we can use\n* `OffscreenCanvas` in the worker. Primarily used to improve performance of\n* image conversion/rendering.\n* The default value is `true` in web environments and `false` in Node.js.\n*/\nisOffscreenCanvasSupported?: boolean | undefined;\n/**\n* - The integer value is used to\n* know when an image must be resized (uses `OffscreenCanvas` in the worker).\n* If it's -1 then a possibly slow algorithm is used to guess the max value.\n*/\ncanvasMaxAreaInBytes?: boolean | undefined;\n/**\n* - By default fonts are converted to\n* OpenType fonts and loaded via the Font Loading API or `@font-face` rules.\n* If disabled, fonts will be rendered using a built-in font renderer that\n* constructs the glyphs with primitive path commands.\n* The default value is `false` in web environments and `true` in Node.js.\n*/\ndisableFontFace?: boolean | undefined;\n/**\n* - Include additional properties,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":7022,"to":7047}}}}],["941",{"pageContent":"* which are unused during rendering of PDF documents, when exporting the\n* parsed font data from the worker-thread. This may be useful for debugging\n* purposes (and backwards compatibility), but note that it will lead to\n* increased memory usage. The default value is `false`.\n*/\nfontExtraProperties?: boolean | undefined;\n/**\n* - Render Xfa forms if any.\n* The default value is `false`.\n*/\nenableXfa?: boolean | undefined;\n/**\n* - Specify an explicit document\n* context to create elements with and to load resources, such as fonts,\n* into. Defaults to the current document.\n*/\nownerDocument?: HTMLDocument | undefined;\n/**\n* - Disable range request loading of PDF\n* files. When enabled, and if the server supports partial content requests,\n* then the PDF will be fetched in chunks. The default value is `false`.\n*/\ndisableRange?: boolean | undefined;\n/**\n* - Disable streaming of PDF file data.\n* By default PDF.js attempts to load PDF files in chunks. The default value\n* is `false`.\n*/\ndisableStream?: boolean | undefined;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":8420,"to":8448}}}}],["942",{"pageContent":"/**\n* - Disable pre-fetching of PDF file\n* data. When range requests are enabled PDF.js will automatically keep\n* fetching more data even if it isn't needed to display the current page.\n* The default value is `false`.\n*\n* NOTE: It is also necessary to disable streaming, see above, in order for\n* disabling of pre-fetching to work correctly.\n*/\ndisableAutoFetch?: boolean | undefined;\n/**\n* - Enables special hooks for debugging PDF.js\n* (see `web/debugger.js`). The default value is `false`.\n*/\npdfBug?: boolean | undefined;\n/**\n* - The factory instance that will be used\n* when creating canvases. The default value is {new DOMCanvasFactory()}.\n*/\ncanvasFactory?: Object | undefined;\n/**\n* - A factory instance that will be used\n* to create SVG filters when rendering some images on the main canvas.\n*/\nfilterFactory?: Object | undefined;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":9821,"to":9846}}}}],["943",{"pageContent":"type OnProgressParameters = {\n/**\n* - Currently loaded number of bytes.\n*/\nloaded: number;\n/**\n* - Total number of bytes in the PDF file.\n*/\ntotal: number;\n};\n/**\n* Page getViewport parameters.\n*/\nexport type GetViewportParameters = {\n/**\n* - The desired scale of the viewport.\n*/\nscale: number;\n/**\n* - The desired rotation, in degrees, of\n* the viewport. If omitted it defaults to the page rotation.\n*/\nrotation?: number | undefined;\n/**\n* - The horizontal, i.e. x-axis, offset.\n* The default value is `0`.\n*/\noffsetX?: number | undefined;\n/**\n* - The vertical, i.e. y-axis, offset.\n* The default value is `0`.\n*/\noffsetY?: number | undefined;\n/**\n* - If true, the y-axis will not be\n* flipped. The default value is `false`.\n*/\ndontFlip?: boolean | undefined;\n};\n/**\n* Page getTextContent parameters.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":11222,"to":11263}}}}],["944",{"pageContent":"type getTextContentParameters = {\n/**\n* - When true include marked\n* content items in the items array of TextContent. The default is `false`.\n*/\nincludeMarkedContent?: boolean | undefined;\n};\n/**\n* Page text content.\n*/\nexport type TextContent = {\n/**\n* - Array of\n* {@link TextItem } and {@link TextMarkedContent } objects. TextMarkedContent\n* items are included when includeMarkedContent is true.\n*/\nitems: Array<TextItem | TextMarkedContent>;\n/**\n* - {@link TextStyle } objects,\n* indexed by font name.\n*/\nstyles: {\n[x: string]: TextStyle;\n};\n};\n/**\n* Page text content part.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":12640,"to":12667}}}}],["945",{"pageContent":"type TextItem = {\n/**\n* - Text content.\n*/\nstr: string;\n/**\n* - Text direction: 'ttb', 'ltr' or 'rtl'.\n*/\ndir: string;\n/**\n* - Transformation matrix.\n*/\ntransform: Array<any>;\n/**\n* - Width in device space.\n*/\nwidth: number;\n/**\n* - Height in device space.\n*/\nheight: number;\n/**\n* - Font name used by PDF.js for converted font.\n*/\nfontName: string;\n/**\n* - Indicating if the text content is followed by a\n* line-break.\n*/\nhasEOL: boolean;\n};\n/**\n* Page text marked content part.\n*/\nexport type TextMarkedContent = {\n/**\n* - Either 'beginMarkedContent',\n* 'beginMarkedContentProps', or 'endMarkedContent'.\n*/\ntype: string;\n/**\n* - The marked content identifier. Only used for type\n* 'beginMarkedContentProps'.\n*/\nid: string;\n};\n/**\n* Text style.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":14053,"to":14101}}}}],["946",{"pageContent":"type TextStyle = {\n/**\n* - Font ascent.\n*/\nascent: number;\n/**\n* - Font descent.\n*/\ndescent: number;\n/**\n* - Whether or not the text is in vertical mode.\n*/\nvertical: boolean;\n/**\n* - The possible font family.\n*/\nfontFamily: string;\n};\n/**\n* Page annotation parameters.\n*/\nexport type GetAnnotationsParameters = {\n/**\n* - Determines the annotations that are fetched,\n* can be 'display' (viewable annotations), 'print' (printable annotations),\n* or 'any' (all annotations). The default value is 'display'.\n*/\nintent?: string | undefined;\n};\n/**\n* Page render parameters.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":15479,"to":15510}}}}],["947",{"pageContent":"type RenderParameters = {\n/**\n* - A 2D context of a DOM\n* Canvas object.\n*/\ncanvasContext: CanvasRenderingContext2D;\n/**\n* - Rendering viewport obtained by calling\n* the `PDFPageProxy.getViewport` method.\n*/\nviewport: PageViewport;\n/**\n* - Rendering intent, can be 'display', 'print',\n* or 'any'. The default value is 'display'.\n*/\nintent?: string | undefined;\n/**\n* Controls which annotations are rendered\n* onto the canvas, for annotations with appearance-data; the values from\n* {@link AnnotationMode } should be used. The following values are supported:\n* - `AnnotationMode.DISABLE`, which disables all annotations.\n* - `AnnotationMode.ENABLE`, which includes all possible annotations (thus\n* it also depends on the `intent`-option, see above).\n* - `AnnotationMode.ENABLE_FORMS`, which excludes annotations that contain\n* interactive form elements (those will be rendered in the display layer).\n* - `AnnotationMode.ENABLE_STORAGE`, which includes all possible annotations","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":16896,"to":16921}}}}],["948",{"pageContent":"* (as above) but where interactive form elements are updated with data\n* from the {@link AnnotationStorage }-instance; useful e.g. for printing.\n* The default value is `AnnotationMode.ENABLE`.\n*/\nannotationMode?: number | undefined;\n/**\n* - Additional transform, applied just\n* before viewport transform.\n*/\ntransform?: any[] | undefined;\n/**\n* - Background\n* to use for the canvas.\n* Any valid `canvas.fillStyle` can be used: a `DOMString` parsed as CSS\n* <color> value, a `CanvasGradient` object (a linear or radial gradient) or\n* a `CanvasPattern` object (a repetitive image). The default value is\n* 'rgb(255,255,255)'.\n*\n* NOTE: This option may be partially, or completely, ignored when the\n* `pageColors`-option is used.\n*/\nbackground?: string | CanvasGradient | CanvasPattern | undefined;\n/**\n* - Overwrites background and foreground colors\n* with user defined ones in order to improve readability in high contrast\n* mode.\n*/\npageColors?: Object | undefined;\n/**\n* -","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":18295,"to":18324}}}}],["949",{"pageContent":"* A promise that should resolve with an {@link OptionalContentConfig }created from `PDFDocumentProxy.getOptionalContentConfig`. If `null`,\n* the configuration will be fetched automatically with the default visibility\n* states set.\n*/\noptionalContentConfigPromise?: Promise<OptionalContentConfig> | undefined;\n/**\n* - Map some\n* annotation ids with canvases used to render them.\n*/\nannotationCanvasMap?: Map<string, HTMLCanvasElement> | undefined;\nprintAnnotationStorage?: PrintAnnotationStorage | undefined;\n};\n/**\n* Page getOperatorList parameters.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":19698,"to":19712}}}}],["950",{"pageContent":"type GetOperatorListParameters = {\n/**\n* - Rendering intent, can be 'display', 'print',\n* or 'any'. The default value is 'display'.\n*/\nintent?: string | undefined;\n/**\n* Controls which annotations are included\n* in the operatorList, for annotations with appearance-data; the values from\n* {@link AnnotationMode } should be used. The following values are supported:\n* - `AnnotationMode.DISABLE`, which disables all annotations.\n* - `AnnotationMode.ENABLE`, which includes all possible annotations (thus\n* it also depends on the `intent`-option, see above).\n* - `AnnotationMode.ENABLE_FORMS`, which excludes annotations that contain\n* interactive form elements (those will be rendered in the display layer).\n* - `AnnotationMode.ENABLE_STORAGE`, which includes all possible annotations\n* (as above) but where interactive form elements are updated with data\n* from the {@link AnnotationStorage }-instance; useful e.g. for printing.\n* The default value is `AnnotationMode.ENABLE`.\n*/\nannotationMode?: number | undefined;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":21098,"to":21118}}}}],["951",{"pageContent":"printAnnotationStorage?: PrintAnnotationStorage | undefined;\n};\n/**\n* Structure tree node. The root node will have a role \"Root\".\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":22491,"to":22495}}}}],["952",{"pageContent":"type StructTreeNode = {\n/**\n* - Array of\n* {@link StructTreeNode } and {@link StructTreeContent } objects.\n*/\nchildren: Array<StructTreeNode | StructTreeContent>;\n/**\n* - element's role, already mapped if a role map exists\n* in the PDF.\n*/\nrole: string;\n};\n/**\n* Structure tree content.\n*/\nexport type StructTreeContent = {\n/**\n* - either \"content\" for page and stream structure\n* elements or \"object\" for object references.\n*/\ntype: string;\n/**\n* - unique id that will map to the text layer.\n*/\nid: string;\n};\n/**\n* PDF page operator list.\n*/\nexport type PDFOperatorList = {\n/**\n* - Array containing the operator functions.\n*/\nfnArray: Array<number>;\n/**\n* - Array containing the arguments of the\n* functions.\n*/\nargsArray: Array<any>;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":23900,"to":23939}}}}],["953",{"pageContent":"type PDFWorkerParameters = {\n/**\n* - The name of the worker.\n*/\nname?: string | undefined;\n/**\n* - The `workerPort` object.\n*/\nport?: Worker | undefined;\n/**\n* - Controls the logging level;\n* the constants from {@link VerbosityLevel } should be used.\n*/\nverbosity?: number | undefined;\n};\n/** @type {string} */\nexport const build: string;\nexport let DefaultCanvasFactory: typeof DOMCanvasFactory;\nexport let DefaultCMapReaderFactory: typeof DOMCMapReaderFactory;\nexport let DefaultFilterFactory: typeof DOMFilterFactory;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":25317,"to":25336}}}}],["954",{"pageContent":"let DefaultStandardFontDataFactory: typeof DOMStandardFontDataFactory;\n/**\n* @typedef { Int8Array | Uint8Array | Uint8ClampedArray |\n*            Int16Array | Uint16Array |\n*            Int32Array | Uint32Array | Float32Array |\n*            Float64Array\n* } TypedArray\n*/\n/**\n* @typedef { TypedArray | ArrayBuffer | Array<number> | string } BinaryData\n*/\n/**\n* @typedef {Object} RefProxy\n* @property {number} num\n* @property {number} gen\n*/\n/**\n* Document initialization / loading parameters object.\n*\n* @typedef {Object} DocumentInitParameters\n* @property {string | URL} [url] - The URL of the PDF.\n* @property {BinaryData} [data] - Binary PDF data.\n*   Use TypedArrays (Uint8Array) to improve the memory usage. If PDF data is\n*   BASE64-encoded, use `atob()` to convert it to a binary string first.\n*\n*   NOTE: If TypedArrays are used they will generally be transferred to the\n*   worker-thread. This will help reduce main-thread memory usage, however\n*   it will take ownership of the TypedArrays.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":26725,"to":26752}}}}],["955",{"pageContent":"* @property {Object} [httpHeaders] - Basic authentication headers.\n* @property {boolean} [withCredentials] - Indicates whether or not\n*   cross-site Access-Control requests should be made using credentials such\n*   as cookies or authorization headers. The default is `false`.\n* @property {string} [password] - For decrypting password-protected PDFs.\n* @property {number} [length] - The PDF file length. It's used for progress\n*   reports and range requests operations.\n* @property {PDFDataRangeTransport} [range] - Allows for using a custom range\n*   transport implementation.\n* @property {number} [rangeChunkSize] - Specify maximum number of bytes fetched\n*   per range request. The default value is {@link DEFAULT_RANGE_CHUNK_SIZE}.\n* @property {PDFWorker} [worker] - The worker that will be used for loading and\n*   parsing the PDF data.\n* @property {number} [verbosity] - Controls the logging level; the constants\n*   from {@link VerbosityLevel} should be used.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":28126,"to":28140}}}}],["956",{"pageContent":"* @property {string} [docBaseUrl] - The base URL of the document, used when\n*   attempting to recover valid absolute URLs for annotations, and outline\n*   items, that (incorrectly) only specify relative URLs.\n* @property {string} [cMapUrl] - The URL where the predefined Adobe CMaps are\n*   located. Include the trailing slash.\n* @property {boolean} [cMapPacked] - Specifies if the Adobe CMaps are binary\n*   packed or not. The default value is `true`.\n* @property {Object} [CMapReaderFactory] - The factory that will be used when\n*   reading built-in CMap files. Providing a custom factory is useful for\n*   environments without Fetch API or `XMLHttpRequest` support, such as\n*   Node.js. The default value is {DOMCMapReaderFactory}.\n* @property {boolean} [useSystemFonts] - When `true`, fonts that aren't\n*   embedded in the PDF document will fallback to a system font.\n*   The default value is `true` in web environments and `false` in Node.js;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":29514,"to":29527}}}}],["957",{"pageContent":"*   unless `disableFontFace === true` in which case this defaults to `false`\n*   regardless of the environment (to prevent completely broken fonts).\n* @property {string} [standardFontDataUrl] - The URL where the standard font\n*   files are located. Include the trailing slash.\n* @property {Object} [StandardFontDataFactory] - The factory that will be used\n*   when reading the standard font files. Providing a custom factory is useful\n*   for environments without Fetch API or `XMLHttpRequest` support, such as\n*   Node.js. The default value is {DOMStandardFontDataFactory}.\n* @property {boolean} [useWorkerFetch] - Enable using the Fetch API in the\n*   worker-thread when reading CMap and standard font files. When `true`,\n*   the `CMapReaderFactory` and `StandardFontDataFactory` options are ignored.\n*   The default value is `true` in web environments and `false` in Node.js.\n* @property {boolean} [stopAtErrors] - Reject certain promises, e.g.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":30901,"to":30913}}}}],["958",{"pageContent":"*   `getOperatorList`, `getTextContent`, and `RenderTask`, when the associated\n*   PDF data cannot be successfully parsed, instead of attempting to recover\n*   whatever possible of the data. The default value is `false`.\n* @property {number} [maxImageSize] - The maximum allowed image size in total\n*   pixels, i.e. width * height. Images above this value will not be rendered.\n*   Use -1 for no limit, which is also the default value.\n* @property {boolean} [isEvalSupported] - Determines if we can evaluate strings\n*   as JavaScript. Primarily used to improve performance of font rendering, and\n*   when parsing PDF functions. The default value is `true`.\n* @property {boolean} [isOffscreenCanvasSupported] - Determines if we can use\n*   `OffscreenCanvas` in the worker. Primarily used to improve performance of\n*   image conversion/rendering.\n*   The default value is `true` in web environments and `false` in Node.js.\n* @property {boolean} [canvasMaxAreaInBytes] - The integer value is used to","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":32287,"to":32300}}}}],["959",{"pageContent":"*   know when an image must be resized (uses `OffscreenCanvas` in the worker).\n*   If it's -1 then a possibly slow algorithm is used to guess the max value.\n* @property {boolean} [disableFontFace] - By default fonts are converted to\n*   OpenType fonts and loaded via the Font Loading API or `@font-face` rules.\n*   If disabled, fonts will be rendered using a built-in font renderer that\n*   constructs the glyphs with primitive path commands.\n*   The default value is `false` in web environments and `true` in Node.js.\n* @property {boolean} [fontExtraProperties] - Include additional properties,\n*   which are unused during rendering of PDF documents, when exporting the\n*   parsed font data from the worker-thread. This may be useful for debugging\n*   purposes (and backwards compatibility), but note that it will lead to\n*   increased memory usage. The default value is `false`.\n* @property {boolean} [enableXfa] - Render Xfa forms if any.\n*   The default value is `false`.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":33674,"to":33687}}}}],["960",{"pageContent":"* @property {HTMLDocument} [ownerDocument] - Specify an explicit document\n*   context to create elements with and to load resources, such as fonts,\n*   into. Defaults to the current document.\n* @property {boolean} [disableRange] - Disable range request loading of PDF\n*   files. When enabled, and if the server supports partial content requests,\n*   then the PDF will be fetched in chunks. The default value is `false`.\n* @property {boolean} [disableStream] - Disable streaming of PDF file data.\n*   By default PDF.js attempts to load PDF files in chunks. The default value\n*   is `false`.\n* @property {boolean} [disableAutoFetch] - Disable pre-fetching of PDF file\n*   data. When range requests are enabled PDF.js will automatically keep\n*   fetching more data even if it isn't needed to display the current page.\n*   The default value is `false`.\n*\n*   NOTE: It is also necessary to disable streaming, see above, in order for\n*   disabling of pre-fetching to work correctly.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":35061,"to":35076}}}}],["961",{"pageContent":"* @property {boolean} [pdfBug] - Enables special hooks for debugging PDF.js\n*   (see `web/debugger.js`). The default value is `false`.\n* @property {Object} [canvasFactory] - The factory instance that will be used\n*   when creating canvases. The default value is {new DOMCanvasFactory()}.\n* @property {Object} [filterFactory] - A factory instance that will be used\n*   to create SVG filters when rendering some images on the main canvas.\n*/\n/**\n* This is the main entry point for loading a PDF and interacting with it.\n*\n* NOTE: If a URL is used to fetch the PDF data a standard Fetch API call (or\n* XHR as fallback) is used, which means it must follow same origin rules,\n* e.g. no cross-domain requests without CORS.\n*\n* @param {string | URL | TypedArray | ArrayBuffer | DocumentInitParameters}\n*   src - Can be a URL where a PDF file is located, a typed array (Uint8Array)\n*         already populated with data, or a parameter object.\n* @returns {PDFDocumentLoadingTask}\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":36450,"to":36468}}}}],["962",{"pageContent":"function getDocument(\nsrc: string | URL | TypedArray | ArrayBuffer | DocumentInitParameters\n): PDFDocumentLoadingTask;\nexport class LoopbackPort {\npostMessage(obj: any, transfer: any): void;\naddEventListener(name: any, listener: any): void;\nremoveEventListener(name: any, listener: any): void;\nterminate(): void;\n#private;\n}\n/**\n* @typedef {Object} OnProgressParameters\n* @property {number} loaded - Currently loaded number of bytes.\n* @property {number} total - Total number of bytes in the PDF file.\n*/\n/**\n* The loading task controls the operations required to load a PDF document\n* (such as network requests) and provides a way to listen for completion,\n* after which individual pages can be rendered.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":37842,"to":37861}}}}],["963",{"pageContent":"class PDFDocumentLoadingTask {\nstatic \"__#16@#docId\": number;\n_capability: import(\"../shared/util.js\").PromiseCapability;\n_transport: any;\n_worker: any;\n/**\n* Unique identifier for the document loading task.\n* @type {string}\n*/\ndocId: string;\n/**\n* Whether the loading task is destroyed or not.\n* @type {boolean}\n*/\ndestroyed: boolean;\n/**\n* Callback to request a password if a wrong or no password was provided.\n* The callback receives two parameters: a function that should be called\n* with the new password, and a reason (see {@link PasswordResponses}).\n* @type {function}\n*/\nonPassword: Function;\n/**\n* Callback to be able to monitor the loading progress of the PDF file\n* (necessary to implement e.g. a loading bar).\n* The callback receives an {@link OnProgressParameters} argument.\n* @type {function}\n*/\nonProgress: Function;\n/**\n* Promise for document loading task completion.\n* @type {Promise<PDFDocumentProxy>}\n*/\nget promise(): Promise<PDFDocumentProxy>;\n/**\n* Abort all network requests and destroy the worker.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":39241,"to":39276}}}}],["964",{"pageContent":"* @returns {Promise<void>} A promise that is resolved when destruction is\n*   completed.\n*/\ndestroy(): Promise<void>;\n}\n/**\n* Proxy to a `PDFDocument` in the worker thread.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":40649,"to":40656}}}}],["965",{"pageContent":"class PDFDocumentProxy {\nconstructor(pdfInfo: any, transport: any);\n_pdfInfo: any;\n_transport: any;\n/**\n* @type {AnnotationStorage} Storage for annotation data in forms.\n*/\nget annotationStorage(): AnnotationStorage;\n/**\n* @type {Object} The filter factory instance.\n*/\nget filterFactory(): Object;\n/**\n* @type {number} Total number of pages in the PDF file.\n*/\nget numPages(): number;\n/**\n* @type {Array<string, string|null>} A (not guaranteed to be) unique ID to\n*   identify the PDF document.\n*   NOTE: The first element will always be defined for all PDF documents,\n*   whereas the second element is only defined for *modified* PDF documents.\n*/\nget fingerprints(): string[];\n/**\n* @type {boolean} True if only XFA form.\n*/\nget isPureXfa(): boolean;\n/**\n* NOTE: This is (mostly) intended to support printing of XFA forms.\n*\n* @type {Object | null} An object representing a HTML tree structure\n*   to render the XFA, or `null` when no XFA form exists.\n*/\nget allXfaHtml(): Object | null;\n/**","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":42059,"to":42093}}}}],["966",{"pageContent":"* @param {number} pageNumber - The page number to get. The first page is 1.\n* @returns {Promise<PDFPageProxy>} A promise that is resolved with\n*   a {@link PDFPageProxy} object.\n*/\ngetPage(pageNumber: number): Promise<PDFPageProxy>;\n/**\n* @param {RefProxy} ref - The page reference.\n* @returns {Promise<number>} A promise that is resolved with the page index,\n*   starting from zero, that is associated with the reference.\n*/\ngetPageIndex(ref: RefProxy): Promise<number>;\n/**\n* @returns {Promise<Object<string, Array<any>>>} A promise that is resolved\n*   with a mapping from named destinations to references.\n*\n* This can be slow for large documents. Use `getDestination` instead.\n*/\ngetDestinations(): Promise<{\n[x: string]: Array<any>;\n}>;\n/**\n* @param {string} id - The named destination to get.\n* @returns {Promise<Array<any> | null>} A promise that is resolved with all\n*   information of the given named destination, or `null` when the named\n*   destination is not present in the PDF file.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":43467,"to":43492}}}}],["967",{"pageContent":"getDestination(id: string): Promise<Array<any> | null>;\n/**\n* @returns {Promise<Array<string> | null>} A promise that is resolved with\n*   an {Array} containing the page labels that correspond to the page\n*   indexes, or `null` when no page labels are present in the PDF file.\n*/\ngetPageLabels(): Promise<Array<string> | null>;\n/**\n* @returns {Promise<string>} A promise that is resolved with a {string}\n*   containing the page layout name.\n*/\ngetPageLayout(): Promise<string>;\n/**\n* @returns {Promise<string>} A promise that is resolved with a {string}\n*   containing the page mode name.\n*/\ngetPageMode(): Promise<string>;\n/**\n* @returns {Promise<Object | null>} A promise that is resolved with an\n*   {Object} containing the viewer preferences, or `null` when no viewer\n*   preferences are present in the PDF file.\n*/\ngetViewerPreferences(): Promise<Object | null>;\n/**\n* @returns {Promise<any | null>} A promise that is resolved with an {Array}\n*   containing the destination, or `null` when no open action is present","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":44866,"to":44891}}}}],["968",{"pageContent":"*   in the PDF.\n*/\ngetOpenAction(): Promise<any | null>;\n/**\n* @returns {Promise<any>} A promise that is resolved with a lookup table\n*   for mapping named attachments to their content.\n*/\ngetAttachments(): Promise<any>;\n/**\n* @returns {Promise<Array<string> | null>} A promise that is resolved with\n*   an {Array} of all the JavaScript strings in the name tree, or `null`\n*   if no JavaScript exists.\n*/\ngetJavaScript(): Promise<Array<string> | null>;\n/**\n* @returns {Promise<Object | null>} A promise that is resolved with\n*   an {Object} with the JavaScript actions:\n*     - from the name tree (like getJavaScript);\n*     - from A or AA entries in the catalog dictionary.\n*   , or `null` if no JavaScript exists.\n*/\ngetJSActions(): Promise<Object | null>;\n/**\n* @typedef {Object} OutlineNode\n* @property {string} title\n* @property {boolean} bold\n* @property {boolean} italic\n* @property {Uint8ClampedArray} color - The color in RGB format to use for\n*   display purposes.\n* @property {string | Array<any> | null} dest","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":46264,"to":46293}}}}],["969",{"pageContent":"* @property {string | null} url\n* @property {string | undefined} unsafeUrl\n* @property {boolean | undefined} newWindow\n* @property {number | undefined} count\n* @property {Array<OutlineNode>} items\n*/\n/**\n* @returns {Promise<Array<OutlineNode>>} A promise that is resolved with an\n*   {Array} that is a tree outline (if it has one) of the PDF file.\n*/\ngetOutline(): Promise<\n{\ntitle: string;\nbold: boolean;\nitalic: boolean;\n/**\n* - The color in RGB format to use for\n* display purposes.\n*/\ncolor: Uint8ClampedArray;\ndest: string | Array<any> | null;\nurl: string | null;\nunsafeUrl: string | undefined;\nnewWindow: boolean | undefined;\ncount: number | undefined;\nitems: any[];\n}[]\n>;\n/**\n* @returns {Promise<OptionalContentConfig>} A promise that is resolved with\n*   an {@link OptionalContentConfig} that contains all the optional content\n*   groups (assuming that the document has any).\n*/\ngetOptionalContentConfig(): Promise<OptionalContentConfig>;\n/**\n* @returns {Promise<Array<number> | null>} A promise that is resolved with","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":47666,"to":47701}}}}],["970",{"pageContent":"*   an {Array} that contains the permission flags for the PDF document, or\n*   `null` when no permissions are present in the PDF file.\n*/\ngetPermissions(): Promise<Array<number> | null>;\n/**\n* @returns {Promise<{ info: Object, metadata: Metadata }>} A promise that is\n*   resolved with an {Object} that has `info` and `metadata` properties.\n*   `info` is an {Object} filled with anything available in the information\n*   dictionary and similarly `metadata` is a {Metadata} object with\n*   information from the metadata section of the PDF.\n*/\ngetMetadata(): Promise<{\ninfo: Object;\nmetadata: Metadata;\n}>;\n/**\n* @typedef {Object} MarkInfo\n* Properties correspond to Table 321 of the PDF 32000-1:2008 spec.\n* @property {boolean} Marked\n* @property {boolean} UserProperties\n* @property {boolean} Suspects\n*/\n/**\n* @returns {Promise<MarkInfo | null>} A promise that is resolved with\n*   a {MarkInfo} object that contains the MarkInfo flags for the PDF\n*   document, or `null` when no MarkInfo values are present in the PDF file.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":49074,"to":49099}}}}],["971",{"pageContent":"*/\ngetMarkInfo(): Promise<{\nMarked: boolean;\nUserProperties: boolean;\nSuspects: boolean;\n} | null>;\n/**\n* @returns {Promise<Uint8Array>} A promise that is resolved with a\n*   {Uint8Array} containing the raw data of the PDF document.\n*/\ngetData(): Promise<Uint8Array>;\n/**\n* @returns {Promise<Uint8Array>} A promise that is resolved with a\n*   {Uint8Array} containing the full data of the saved document.\n*/\nsaveDocument(): Promise<Uint8Array>;\n/**\n* @returns {Promise<{ length: number }>} A promise that is resolved when the\n*   document's data is loaded. It is resolved with an {Object} that contains\n*   the `length` property that indicates size of the PDF data in bytes.\n*/\ngetDownloadInfo(): Promise<{\nlength: number;\n}>;\n/**\n* Cleans up resources allocated by the document on both the main and worker\n* threads.\n*\n* NOTE: Do not, under any circumstances, call this method when rendering is\n* currently ongoing since that may lead to rendering errors.\n*","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":50472,"to":50502}}}}],["972",{"pageContent":"* @param {boolean} [keepLoadedFonts] - Let fonts remain attached to the DOM.\n*   NOTE: This will increase persistent memory usage, hence don't use this\n*   option unless absolutely necessary. The default value is `false`.\n* @returns {Promise} A promise that is resolved when clean-up has finished.\n*/\ncleanup(keepLoadedFonts?: boolean | undefined): Promise<any>;\n/**\n* Destroys the current document instance and terminates the worker.\n*/\ndestroy(): Promise<void>;\n/**\n* @type {DocumentInitParameters} A subset of the current\n*   {DocumentInitParameters}, which are needed in the viewer.\n*/\nget loadingParams(): DocumentInitParameters;\n/**\n* @type {PDFDocumentLoadingTask} The loadingTask for the current document.\n*/\nget loadingTask(): PDFDocumentLoadingTask;\n/**\n* @returns {Promise<Object<string, Array<Object>> | null>} A promise that is\n*   resolved with an {Object} containing /AcroForm field data for the JS\n*   sandbox, or `null` when no field data is present in the PDF file.\n*/\ngetFieldObjects(): Promise<{","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":51876,"to":51900}}}}],["973",{"pageContent":"[x: string]: Array<Object>;\n} | null>;\n/**\n* @returns {Promise<boolean>} A promise that is resolved with `true`\n*   if some /AcroForm fields have JavaScript actions.\n*/\nhasJSActions(): Promise<boolean>;\n/**\n* @returns {Promise<Array<string> | null>} A promise that is resolved with an\n*   {Array<string>} containing IDs of annotations that have a calculation\n*   action, or `null` when no such annotations are present in the PDF file.\n*/\ngetCalculationOrderIds(): Promise<Array<string> | null>;\n}\n/**\n* Page getViewport parameters.\n*\n* @typedef {Object} GetViewportParameters\n* @property {number} scale - The desired scale of the viewport.\n* @property {number} [rotation] - The desired rotation, in degrees, of\n*   the viewport. If omitted it defaults to the page rotation.\n* @property {number} [offsetX] - The horizontal, i.e. x-axis, offset.\n*   The default value is `0`.\n* @property {number} [offsetY] - The vertical, i.e. y-axis, offset.\n*   The default value is `0`.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":53273,"to":53297}}}}],["974",{"pageContent":"* @property {boolean} [dontFlip] - If true, the y-axis will not be\n*   flipped. The default value is `false`.\n*/\n/**\n* Page getTextContent parameters.\n*\n* @typedef {Object} getTextContentParameters\n* @property {boolean} [includeMarkedContent] - When true include marked\n*   content items in the items array of TextContent. The default is `false`.\n*/\n/**\n* Page text content.\n*\n* @typedef {Object} TextContent\n* @property {Array<TextItem | TextMarkedContent>} items - Array of\n*   {@link TextItem} and {@link TextMarkedContent} objects. TextMarkedContent\n*   items are included when includeMarkedContent is true.\n* @property {Object<string, TextStyle>} styles - {@link TextStyle} objects,\n*   indexed by font name.\n*/\n/**\n* Page text content part.\n*\n* @typedef {Object} TextItem\n* @property {string} str - Text content.\n* @property {string} dir - Text direction: 'ttb', 'ltr' or 'rtl'.\n* @property {Array<any>} transform - Transformation matrix.\n* @property {number} width - Width in device space.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":54671,"to":54698}}}}],["975",{"pageContent":"* @property {number} height - Height in device space.\n* @property {string} fontName - Font name used by PDF.js for converted font.\n* @property {boolean} hasEOL - Indicating if the text content is followed by a\n*   line-break.\n*/\n/**\n* Page text marked content part.\n*\n* @typedef {Object} TextMarkedContent\n* @property {string} type - Either 'beginMarkedContent',\n*   'beginMarkedContentProps', or 'endMarkedContent'.\n* @property {string} id - The marked content identifier. Only used for type\n*   'beginMarkedContentProps'.\n*/\n/**\n* Text style.\n*\n* @typedef {Object} TextStyle\n* @property {number} ascent - Font ascent.\n* @property {number} descent - Font descent.\n* @property {boolean} vertical - Whether or not the text is in vertical mode.\n* @property {string} fontFamily - The possible font family.\n*/\n/**\n* Page annotation parameters.\n*\n* @typedef {Object} GetAnnotationsParameters\n* @property {string} [intent] - Determines the annotations that are fetched,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":56072,"to":56099}}}}],["976",{"pageContent":"*   can be 'display' (viewable annotations), 'print' (printable annotations),\n*   or 'any' (all annotations). The default value is 'display'.\n*/\n/**\n* Page render parameters.\n*\n* @typedef {Object} RenderParameters\n* @property {CanvasRenderingContext2D} canvasContext - A 2D context of a DOM\n*   Canvas object.\n* @property {PageViewport} viewport - Rendering viewport obtained by calling\n*   the `PDFPageProxy.getViewport` method.\n* @property {string} [intent] - Rendering intent, can be 'display', 'print',\n*   or 'any'. The default value is 'display'.\n* @property {number} [annotationMode] Controls which annotations are rendered\n*   onto the canvas, for annotations with appearance-data; the values from\n*   {@link AnnotationMode} should be used. The following values are supported:\n*    - `AnnotationMode.DISABLE`, which disables all annotations.\n*    - `AnnotationMode.ENABLE`, which includes all possible annotations (thus\n*      it also depends on the `intent`-option, see above).","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":57473,"to":57491}}}}],["977",{"pageContent":"*    - `AnnotationMode.ENABLE_FORMS`, which excludes annotations that contain\n*      interactive form elements (those will be rendered in the display layer).\n*    - `AnnotationMode.ENABLE_STORAGE`, which includes all possible annotations\n*      (as above) but where interactive form elements are updated with data\n*      from the {@link AnnotationStorage}-instance; useful e.g. for printing.\n*   The default value is `AnnotationMode.ENABLE`.\n* @property {Array<any>} [transform] - Additional transform, applied just\n*   before viewport transform.\n* @property {CanvasGradient | CanvasPattern | string} [background] - Background\n*   to use for the canvas.\n*   Any valid `canvas.fillStyle` can be used: a `DOMString` parsed as CSS\n*   <color> value, a `CanvasGradient` object (a linear or radial gradient) or\n*   a `CanvasPattern` object (a repetitive image). The default value is\n*   'rgb(255,255,255)'.\n*\n*   NOTE: This option may be partially, or completely, ignored when the\n*   `pageColors`-option is used.","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":58865,"to":58881}}}}],["978",{"pageContent":"* @property {Object} [pageColors] - Overwrites background and foreground colors\n*   with user defined ones in order to improve readability in high contrast\n*   mode.\n* @property {Promise<OptionalContentConfig>} [optionalContentConfigPromise] -\n*   A promise that should resolve with an {@link OptionalContentConfig}\n*   created from `PDFDocumentProxy.getOptionalContentConfig`. If `null`,\n*   the configuration will be fetched automatically with the default visibility\n*   states set.\n* @property {Map<string, HTMLCanvasElement>} [annotationCanvasMap] - Map some\n*   annotation ids with canvases used to render them.\n* @property {PrintAnnotationStorage} [printAnnotationStorage]\n*/\n/**\n* Page getOperatorList parameters.\n*\n* @typedef {Object} GetOperatorListParameters\n* @property {string} [intent] - Rendering intent, can be 'display', 'print',\n*   or 'any'. The default value is 'display'.\n* @property {number} [annotationMode] Controls which annotations are included","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":60254,"to":60272}}}}],["979",{"pageContent":"*   in the operatorList, for annotations with appearance-data; the values from\n*   {@link AnnotationMode} should be used. The following values are supported:\n*    - `AnnotationMode.DISABLE`, which disables all annotations.\n*    - `AnnotationMode.ENABLE`, which includes all possible annotations (thus\n*      it also depends on the `intent`-option, see above).\n*    - `AnnotationMode.ENABLE_FORMS`, which excludes annotations that contain\n*      interactive form elements (those will be rendered in the display layer).\n*    - `AnnotationMode.ENABLE_STORAGE`, which includes all possible annotations\n*      (as above) but where interactive form elements are updated with data\n*      from the {@link AnnotationStorage}-instance; useful e.g. for printing.\n*   The default value is `AnnotationMode.ENABLE`.\n* @property {PrintAnnotationStorage} [printAnnotationStorage]\n*/\n/**\n* Structure tree node. The root node will have a role \"Root\".\n*\n* @typedef {Object} StructTreeNode","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":61646,"to":61662}}}}],["980",{"pageContent":"* @property {Array<StructTreeNode | StructTreeContent>} children - Array of\n*   {@link StructTreeNode} and {@link StructTreeContent} objects.\n* @property {string} role - element's role, already mapped if a role map exists\n* in the PDF.\n*/\n/**\n* Structure tree content.\n*\n* @typedef {Object} StructTreeContent\n* @property {string} type - either \"content\" for page and stream structure\n*   elements or \"object\" for object references.\n* @property {string} id - unique id that will map to the text layer.\n*/\n/**\n* PDF page operator list.\n*\n* @typedef {Object} PDFOperatorList\n* @property {Array<number>} fnArray - Array containing the operator functions.\n* @property {Array<any>} argsArray - Array containing the arguments of the\n*   functions.\n*/\n/**\n* Proxy to a `PDFPage` in the worker thread.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":63036,"to":63059}}}}],["981",{"pageContent":"class PDFPageProxy {\nconstructor(\npageIndex: any,\npageInfo: any,\ntransport: any,\npdfBug?: boolean\n);\n_pageIndex: any;\n_pageInfo: any;\n_transport: any;\n_stats: StatTimer | null;\n_pdfBug: boolean;\n/** @type {PDFObjects} */\ncommonObjs: PDFObjects;\nobjs: PDFObjects;\n_maybeCleanupAfterRender: boolean;\n_intentStates: Map<any, any>;\ndestroyed: boolean;\n/**\n* @type {number} Page number of the page. First page is 1.\n*/\nget pageNumber(): number;\n/**\n* @type {number} The number of degrees the page is rotated clockwise.\n*/\nget rotate(): number;\n/**\n* @type {RefProxy | null} The reference that points to this page.\n*/\nget ref(): RefProxy | null;\n/**\n* @type {number} The default size of units in 1/72nds of an inch.\n*/\nget userUnit(): number;\n/**\n* @type {Array<number>} An array of the visible portion of the PDF page in\n*   user space units [x1, y1, x2, y2].\n*/\nget view(): number[];\n/**\n* @param {GetViewportParameters} params - Viewport parameters.\n* @returns {PageViewport} Contains 'width' and 'height' properties","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":64436,"to":64477}}}}],["982",{"pageContent":"*   along with transforms required for rendering.\n*/\ngetViewport({\nscale,\nrotation,\noffsetX,\noffsetY,\ndontFlip,\n}?: GetViewportParameters): PageViewport;\n/**\n* @param {GetAnnotationsParameters} params - Annotation parameters.\n* @returns {Promise<Array<any>>} A promise that is resolved with an\n*   {Array} of the annotation objects.\n*/\ngetAnnotations({ intent }?: GetAnnotationsParameters): Promise<Array<any>>;\n/**\n* @returns {Promise<Object>} A promise that is resolved with an\n*   {Object} with JS actions.\n*/\ngetJSActions(): Promise<Object>;\n/**\n* @type {boolean} True if only XFA form.\n*/\nget isPureXfa(): boolean;\n/**\n* @returns {Promise<Object | null>} A promise that is resolved with\n*   an {Object} with a fake DOM object (a tree structure where elements\n*   are {Object} with a name, attributes (class, style, ...), value and\n*   children, very similar to a HTML DOM tree), or `null` if no XFA exists.\n*/\ngetXfa(): Promise<Object | null>;\n/**\n* Begins the process of rendering a page to the desired context.\n*","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":65850,"to":65883}}}}],["983",{"pageContent":"* @param {RenderParameters} params - Page render parameters.\n* @returns {RenderTask} An object that contains a promise that is\n*   resolved when the page finishes rendering.\n*/\nrender(\n{\ncanvasContext,\nviewport,\nintent,\nannotationMode,\ntransform,\nbackground,\noptionalContentConfigPromise,\nannotationCanvasMap,\npageColors,\nprintAnnotationStorage,\n}: RenderParameters,\n...args: any[]\n): RenderTask;\n/**\n* @param {GetOperatorListParameters} params - Page getOperatorList\n*   parameters.\n* @returns {Promise<PDFOperatorList>} A promise resolved with an\n*   {@link PDFOperatorList} object that represents the page's operator list.\n*/\ngetOperatorList({\nintent,\nannotationMode,\nprintAnnotationStorage,\n}?: GetOperatorListParameters): Promise<PDFOperatorList>;\n/**\n* NOTE: All occurrences of whitespace will be replaced by\n* standard spaces (0x20).\n*\n* @param {getTextContentParameters} params - getTextContent parameters.\n* @returns {ReadableStream} Stream for reading text content chunks.\n*/\nstreamTextContent({\nincludeMarkedContent,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":67256,"to":67294}}}}],["984",{"pageContent":"}?: getTextContentParameters): ReadableStream;\n/**\n* NOTE: All occurrences of whitespace will be replaced by\n* standard spaces (0x20).\n*\n* @param {getTextContentParameters} params - getTextContent parameters.\n* @returns {Promise<TextContent>} A promise that is resolved with a\n*   {@link TextContent} object that represents the page's text content.\n*/\ngetTextContent(params?: getTextContentParameters): Promise<TextContent>;\n/**\n* @returns {Promise<StructTreeNode>} A promise that is resolved with a\n*   {@link StructTreeNode} object that represents the page's structure tree,\n*   or `null` when no structure tree is present for the current page.\n*/\ngetStructTree(): Promise<StructTreeNode>;\n/**\n* Destroys the page object.\n* @private\n*/\nprivate _destroy;\n/**\n* Cleans up resources allocated by the page.\n*\n* @param {boolean} [resetStats] - Reset page stats, if enabled.\n*   The default value is `false`.\n* @returns {boolean} Indicates if clean-up was successfully run.\n*/\ncleanup(resetStats?: boolean | undefined): boolean;\n/**","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":68667,"to":68696}}}}],["985",{"pageContent":"* @private\n*/\nprivate _startRenderPage;\n/**\n* @private\n*/\nprivate _renderPageChunk;\n/**\n* @private\n*/\nprivate _pumpOperatorList;\n/**\n* @private\n*/\nprivate _abortOperatorList;\n/**\n* @type {StatTimer | null} Returns page stats, if enabled; returns `null`\n*   otherwise.\n*/\nget stats(): StatTimer | null;\n#private;\n}\n/**\n* PDF.js web worker abstraction that controls the instantiation of PDF\n* documents. Message handlers are used to pass information from the main\n* thread to the worker thread and vice versa. If the creation of a web\n* worker is not possible, a \"fake\" worker will be used instead.\n*\n* @param {PDFWorkerParameters} params - The worker initialization parameters.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":70069,"to":70098}}}}],["986",{"pageContent":"class PDFWorker {\nstatic \"__#19@#workerPorts\": WeakMap<object, any>;\n/**\n* @param {PDFWorkerParameters} params - The worker initialization parameters.\n*/\nstatic fromPort(params: PDFWorkerParameters): any;\n/**\n* The current `workerSrc`, when it exists.\n* @type {string}\n*/\nstatic get workerSrc(): string;\nstatic get _mainThreadWorkerMessageHandler(): any;\nstatic get _setupFakeWorkerGlobal(): any;\nconstructor({\nname,\nport,\nverbosity,\n}?: {\nname?: null | undefined;\nport?: null | undefined;\nverbosity?: number | undefined;\n});\nname: any;\ndestroyed: boolean;\nverbosity: number;\n_readyCapability: import(\"../shared/util.js\").PromiseCapability;\n_port: any;\n_webWorker: Worker | null;\n_messageHandler: MessageHandler | null;\n/**\n* Promise for worker initialization completion.\n* @type {Promise<void>}\n*/\nget promise(): Promise<void>;\n/**\n* The current `workerPort`, when it exists.\n* @type {Worker}\n*/\nget port(): Worker;\n/**\n* The current MessageHandler-instance.\n* @type {MessageHandler}\n*/\nget messageHandler(): MessageHandler;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":71480,"to":71523}}}}],["987",{"pageContent":"_initializeFromPort(port: any): void;\n_initialize(): void;\n_setupFakeWorker(): void;\n/**\n* Destroys the worker instance.\n*/\ndestroy(): void;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":72896,"to":72903}}}}],["988",{"pageContent":"namespace PDFWorkerUtil {\nconst isWorkerDisabled: boolean;\nconst fallbackWorkerSrc: null;\nconst fakeWorkerId: number;\n}\n/**\n* Allows controlling of the rendering tasks.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":74306,"to":74313}}}}],["989",{"pageContent":"class RenderTask {\nconstructor(internalRenderTask: any);\n/**\n* Callback for incremental rendering -- a function that will be called\n* each time the rendering is paused.  To continue rendering call the\n* function that is the first argument to the callback.\n* @type {function}\n*/\nonContinue: Function;\n/**\n* Promise for rendering task completion.\n* @type {Promise<void>}\n*/\nget promise(): Promise<void>;\n/**\n* Cancels the rendering task. If the task is currently rendering it will\n* not be cancelled until graphics pauses with a timeout. The promise that\n* this object extends will be rejected when cancelled.\n*\n* @param {number} [extraDelay]\n*/\ncancel(extraDelay?: number | undefined): void;\n/**\n* Whether form fields are rendered separately from the main operatorList.\n* @type {boolean}\n*/\nget separateAnnots(): boolean;\n#private;\n}\n/** @type {string} */\nexport const version: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/pdf-parse.d.ts","loc":{"lines":{"from":75716,"to":75747}}}}],["990",{"pageContent":"// Utility for marking only some keys of an interface as optional\n// Compare to Partial<T> which marks all keys as optional\nexport type Optional<T, K extends keyof T> = Omit<T, K> & Partial<Pick<T, K>>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/types/type-utils.ts","loc":{"lines":{"from":1,"to":3}}}}],["991",{"pageContent":"import pRetry from \"p-retry\";\nimport PQueueMod from \"p-queue\";\n\nconst STATUS_NO_RETRY = [\n400, // Bad Request\n401, // Unauthorized\n403, // Forbidden\n404, // Not Found\n405, // Method Not Allowed\n406, // Not Acceptable\n407, // Proxy Authentication Required\n408, // Request Timeout\n409, // Conflict\n];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":1,"to":14}}}}],["992",{"pageContent":"interface AsyncCallerParams {\n/**\n* The maximum number of concurrent calls that can be made.\n* Defaults to `Infinity`, which means no limit.\n*/\nmaxConcurrency?: number;\n/**\n* The maximum number of retries that can be made for a single call,\n* with an exponential backoff between each attempt. Defaults to 6.\n*/\nmaxRetries?: number;\n}\n\n/**\n* A class that can be used to make async calls with concurrency and retry logic.\n*\n* This is useful for making calls to any kind of \"expensive\" external resource,\n* be it because it's rate-limited, subject to network issues, etc.\n*\n* Concurrent calls are limited by the `maxConcurrency` parameter, which defaults\n* to `Infinity`. This means that by default, all calls will be made in parallel.\n*\n* Retries are limited by the `maxRetries` parameter, which defaults to 6. This\n* means that by default, each call will be retried up to 6 times, with an\n* exponential backoff between each attempt.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":109,"to":134}}}}],["993",{"pageContent":"class AsyncCaller {\nprotected maxConcurrency: AsyncCallerParams[\"maxConcurrency\"];\n\nprotected maxRetries: AsyncCallerParams[\"maxRetries\"];\n\nprivate queue: typeof import(\"p-queue\")[\"default\"][\"prototype\"];\n\nconstructor(params: AsyncCallerParams) {\nthis.maxConcurrency = params.maxConcurrency ?? Infinity;\nthis.maxRetries = params.maxRetries ?? 6;\n\nconst PQueue = \"default\" in PQueueMod ? PQueueMod.default : PQueueMod;\nthis.queue = new PQueue({ concurrency: this.maxConcurrency });\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":207,"to":220}}}}],["994",{"pageContent":"// eslint-disable-next-line @typescript-eslint/no-explicit-any\ncall<A extends any[], T extends (...args: A) => Promise<any>>(\ncallable: T,\n...args: Parameters<T>\n): Promise<Awaited<ReturnType<T>>> {\nreturn this.queue.add(\n() =>\npRetry(\n() =>\ncallable(...args).catch((error) => {\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (error instanceof Error) {\nthrow error;\n} else {\nthrow new Error(error);\n}\n}),\n{\nonFailedAttempt(error) {\nif (\nerror.message.startsWith(\"Cancel\") ||\nerror.message.startsWith(\"TimeoutError\") ||\nerror.message.startsWith(\"AbortError\")\n) {\nthrow error;\n}\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nif ((error as any)?.code === \"ECONNABORTED\") {\nthrow error;\n}\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconst status = (error as any)?.response?.status;\nif (status && STATUS_NO_RETRY.includes(+status)) {\nthrow error;\n}\n},\nretries: this.maxRetries,\nrandomize: true,\n// If needed we can change some of the defaults here,\n// but they're quite sensible.\n}\n),","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":307,"to":348}}}}],["995",{"pageContent":"{ throwOnTimeout: true }\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":420,"to":422}}}}],["996",{"pageContent":"fetch(...args: Parameters<typeof fetch>): ReturnType<typeof fetch> {\nreturn this.call(() =>\nfetch(...args).then((res) => (res.ok ? res : Promise.reject(res)))\n);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/async_caller.ts","loc":{"lines":{"from":529,"to":534}}}}],["997",{"pageContent":"// eslint-disable-next-line import/no-extraneous-dependencies\nimport { AxiosRequestConfig, AxiosPromise } from \"axios\";\n\nexport default function fetchAdapter(config: AxiosRequestConfig): AxiosPromise;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/axios-fetch-adapter.d.ts","loc":{"lines":{"from":1,"to":4}}}}],["998",{"pageContent":"import type { AxiosRequestConfig } from \"axios\";\nimport type { EventSourceMessage } from \"./event-source-parse.js\";\n\nexport interface StreamingAxiosRequestConfig extends AxiosRequestConfig {\nresponseType: \"stream\";\n\n/**\n* Called when a message is received. NOTE: Unlike the default browser\n* EventSource.onmessage, this callback is called for _all_ events,\n* even ones with a custom `event` field.\n*/\nonmessage?: (ev: EventSourceMessage) => void;\n}\n\nexport type StreamingAxiosConfiguration =\n| StreamingAxiosRequestConfig\n| AxiosRequestConfig;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/axios-types.ts","loc":{"lines":{"from":1,"to":17}}}}],["999",{"pageContent":"export const chunkArray = <T>(arr: T[], chunkSize: number) =>\narr.reduce((chunks, elem, index) => {\nconst chunkIndex = Math.floor(index / chunkSize);\nconst chunk = chunks[chunkIndex] || [];\n// eslint-disable-next-line no-param-reassign\nchunks[chunkIndex] = chunk.concat([elem]);\nreturn chunks;\n}, [] as T[][]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/chunk.ts","loc":{"lines":{"from":1,"to":8}}}}],["1000",{"pageContent":"import {\nisBrowser,\nisNode,\nisWebWorker,\nisJsDom,\nisDeno,\n} from \"browser-or-node\";\n\nexport const getEnv = () => {\nlet env: string;\nif (isBrowser) {\nenv = \"browser\";\n} else if (isNode) {\nenv = \"node\";\n} else if (isWebWorker) {\nenv = \"webworker\";\n} else if (isJsDom) {\nenv = \"jsdom\";\n} else if (isDeno) {\nenv = \"deno\";\n} else {\nenv = \"other\";\n}\n\nreturn env;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/env.ts","loc":{"lines":{"from":1,"to":26}}}}],["1001",{"pageContent":"/* eslint-disable prefer-template */\n/* eslint-disable default-case */\n/* eslint-disable no-plusplus */\n// Adapted from https://github.com/gfortaine/fetch-event-source/blob/main/src/parse.ts\n// due to a packaging issue in the original.\n// MIT License\n\nexport const EventStreamContentType = \"text/event-stream\";\n\n/**\n* Represents a message sent in an event stream\n* https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":1,"to":13}}}}],["1002",{"pageContent":"interface EventSourceMessage {\n/** The event ID to set the EventSource object's last event ID value. */\nid: string;\n/** A string identifying the type of event described. */\nevent: string;\n/** The event data */\ndata: string;\n/** The reconnection interval (in milliseconds) to wait before retrying the connection */\nretry?: number;\n}\n\n/**\n* Converts a ReadableStream into a callback pattern.\n* @param stream The input ReadableStream.\n* @param onChunk A function that will be called on each new byte chunk in the stream.\n* @returns {Promise<void>} A promise that will be resolved when the stream closes.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":204,"to":220}}}}],["1003",{"pageContent":"async function getBytes(\nstream: ReadableStream<Uint8Array>,\nonChunk: (arr: Uint8Array) => void\n) {\nconst reader = stream.getReader();\nlet result: ReadableStreamReadResult<Uint8Array>;\n// eslint-disable-next-line no-cond-assign\nwhile (!(result = await reader.read()).done) {\nonChunk(result.value);\n}\n}\n\nconst enum ControlChars {\nNewLine = 10,\nCarriageReturn = 13,\nSpace = 32,\nColon = 58,\n}\n\n/**\n* Parses arbitary byte chunks into EventSource line buffers.\n* Each line should be of the format \"field: value\" and ends with \\r, \\n, or \\r\\n.\n* @param onLine A function that will be called on each new EventSource line.\n* @returns A function that should be called for each incoming byte chunk.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":407,"to":431}}}}],["1004",{"pageContent":"function getLines(\nonLine: (line: Uint8Array, fieldLength: number) => void\n) {\nlet buffer: Uint8Array | undefined;\nlet position: number; // current read position\nlet fieldLength: number; // length of the `field` portion of the line\nlet discardTrailingNewline = false;\n\n// return a function that can process each incoming byte chunk:\nreturn function onChunk(arr: Uint8Array) {\nif (buffer === undefined) {\nbuffer = arr;\nposition = 0;\nfieldLength = -1;\n} else {\n// we're still parsing the old line. Append the new bytes into buffer:\nbuffer = concat(buffer, arr);\n}\n\nconst bufLength = buffer.length;\nlet lineStart = 0; // index where the current line starts\nwhile (position < bufLength) {\nif (discardTrailingNewline) {\nif (buffer[position] === ControlChars.NewLine) {\nlineStart = ++position; // skip to next char\n}\n\ndiscardTrailingNewline = false;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":615,"to":643}}}}],["1005",{"pageContent":"// start looking forward till the end of line:\nlet lineEnd = -1; // index of the \\r or \\n char\nfor (; position < bufLength && lineEnd === -1; ++position) {\nswitch (buffer[position]) {\ncase ControlChars.Colon:\nif (fieldLength === -1) {\n// first colon in line\nfieldLength = position - lineStart;\n}\nbreak;\n// eslint-disable-next-line @typescript-eslint/ban-ts-comment\n// @ts-ignore:7029 \\r case below should fallthrough to \\n:\ncase ControlChars.CarriageReturn:\ndiscardTrailingNewline = true;\n// eslint-disable-next-line no-fallthrough\ncase ControlChars.NewLine:\nlineEnd = position;\nbreak;\n}\n}\n\nif (lineEnd === -1) {\n// We reached the end of the buffer but the line hasn't ended.\n// Wait for the next arr and then continue parsing:\nbreak;\n}\n\n// we've reached the line end, send it out:\nonLine(buffer.subarray(lineStart, lineEnd), fieldLength);\nlineStart = position; // we're now on the next line\nfieldLength = -1;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":820,"to":851}}}}],["1006",{"pageContent":"if (lineStart === bufLength) {\nbuffer = undefined; // we've finished reading it\n} else if (lineStart !== 0) {\n// Create a new view into buffer beginning at lineStart so we don't\n// need to copy over the previous lines when we get the new arr:\nbuffer = buffer.subarray(lineStart);\nposition -= lineStart;\n}\n};\n}\n\n/**\n* Parses line buffers into EventSourceMessages.\n* @param onId A function that will be called on each `id` field.\n* @param onRetry A function that will be called on each `retry` field.\n* @param onMessage A function that will be called on each message.\n* @returns A function that should be called for each incoming line buffer.\n*/","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":1027,"to":1044}}}}],["1007",{"pageContent":"function getMessages(\nonMessage?: (msg: EventSourceMessage) => void,\nonId?: (id: string) => void,\nonRetry?: (retry: number) => void\n) {\nlet message = newMessage();\nconst decoder = new TextDecoder();\n\n// return a function that can process each incoming line buffer:\nreturn function onLine(line: Uint8Array, fieldLength: number) {\nif (line.length === 0) {\n// empty line denotes end of message. Trigger the callback and start a new message:\nonMessage?.(message);\nmessage = newMessage();\n} else if (fieldLength > 0) {\n// exclude comments and lines with no values\n// line is of format \"<field>:<value>\" or \"<field>: <value>\"\n// https://html.spec.whatwg.org/multipage/server-sent-events.html#event-stream-interpretation\nconst field = decoder.decode(line.subarray(0, fieldLength));\nconst valueOffset =\nfieldLength + (line[fieldLength + 1] === ControlChars.Space ? 2 : 1);\nconst value = decoder.decode(line.subarray(valueOffset));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":1230,"to":1251}}}}],["1008",{"pageContent":"switch (field) {\ncase \"data\":\n// if this message already has data, append the new value to the old.\n// otherwise, just set to the new value:\nmessage.data = message.data ? message.data + \"\\n\" + value : value; // otherwise,\nbreak;\ncase \"event\":\nmessage.event = value;\nbreak;\ncase \"id\":\nonId?.((message.id = value));\nbreak;\ncase \"retry\": {\nconst retry = parseInt(value, 10);\nif (!Number.isNaN(retry)) {\n// per spec, ignore non-integers\nonRetry?.((message.retry = retry));\n}\nbreak;\n}\n}\n}\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":1427,"to":1450}}}}],["1009",{"pageContent":"concat(a: Uint8Array, b: Uint8Array) {\nconst res = new Uint8Array(a.length + b.length);\nres.set(a);\nres.set(b, a.length);\nreturn res;\n}\n\nfunction newMessage(): EventSourceMessage {\n// data, event, and id must be initialized to empty strings:\n// https://html.spec.whatwg.org/multipage/server-sent-events.html#event-stream-interpretation\n// retry should be initialized to undefined so we return a consistent shape\n// to the js engine all the time: https://mathiasbynens.be/notes/shapes-ics#takeaways\nreturn {\ndata: \"\",\nevent: \"\",\nid: \"\",\nretry: undefined,\n};\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/event-source-parse.ts","loc":{"lines":{"from":1639,"to":1657}}}}],["1010",{"pageContent":"export const extname = (path: string) => `.${path.split(\".\").pop()}`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/extname.ts","loc":{"lines":{"from":1,"to":1}}}}],["1011",{"pageContent":"// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport const flattenObject = (obj: Record<string, any>, deep = true) => {\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconst flattenedObject: Record<string, any> = {};\n\nfor (const key in obj) {\nif (typeof obj[key] === \"object\" && !Array.isArray(obj[key])) {\nlet recursiveResult = obj[key];\nif (deep) recursiveResult = flattenObject(recursiveResult);\n\nfor (const deepKey in recursiveResult) {\nif (Object.hasOwn(obj, key))\nflattenedObject[`${key}.${deepKey}`] = recursiveResult[deepKey];\n}\n}\n}\n\nreturn flattenedObject;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/flatten.ts","loc":{"lines":{"from":1,"to":19}}}}],["1012",{"pageContent":"import pRetry from \"p-retry\";\n\nimport { FileLoader, LoadValues } from \"./load.js\";\nimport { extname } from \"./extname.js\";\n\nconst fetchWithTimeout = async (\nurl: string,\ninit: Omit<RequestInit, \"signal\"> & { timeout: number }\n) => {\nconst { timeout, ...rest } = init;\nconst res = await fetch(url, {\n...rest,\nsignal: AbortSignal.timeout(timeout),\n});\nreturn res;\n};\n\nconst HUB_PATH_REGEX = /lc(@[^:]+)?:\\/\\/(.*)/;\n\nconst URL_PATH_SEPARATOR = \"/\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/hub.ts","loc":{"lines":{"from":1,"to":20}}}}],["1013",{"pageContent":"const loadFromHub = async <T>(\nuri: string,\nloader: FileLoader<T>,\nvalidPrefix: string,\nvalidSuffixes: Set<string>,\nvalues: LoadValues = {}\n): Promise<T | undefined> => {\nconst LANGCHAIN_HUB_DEFAULT_REF =\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.LANGCHAIN_HUB_DEFAULT_REF\n: undefined) ?? \"master\";\nconst LANGCHAIN_HUB_URL_BASE =\n(typeof process !== \"undefined\"\n? // eslint-disable-next-line no-process-env\nprocess.env?.LANGCHAIN_HUB_URL_BASE\n: undefined) ??\n\"https://raw.githubusercontent.com/hwchase17/langchain-hub/\";\n\nconst match = uri.match(HUB_PATH_REGEX);\nif (!match) {\nreturn undefined;\n}\nconst [rawRef, remotePath] = match.slice(1);\nconst ref = rawRef ? rawRef.slice(1) : LANGCHAIN_HUB_DEFAULT_REF;\nconst parts = remotePath.split(URL_PATH_SEPARATOR);\nif (parts[0] !== validPrefix) {\nreturn undefined;\n}\n\nif (!validSuffixes.has(extname(remotePath).slice(1))) {\nthrow new Error(\"Unsupported file type.\");\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/hub.ts","loc":{"lines":{"from":65,"to":97}}}}],["1014",{"pageContent":"const url = [LANGCHAIN_HUB_URL_BASE, ref, remotePath].join(\"/\");\nconst res = await pRetry(() => fetchWithTimeout(url, { timeout: 5000 }), {\nretries: 6,\n});\nif (res.status !== 200) {\nthrow new Error(`Could not find file at ${url}`);\n}\n\nreturn loader(await res.text(), remotePath, values);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/hub.ts","loc":{"lines":{"from":126,"to":135}}}}],["1015",{"pageContent":"// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type LoadValues = Record<string, any>;\n\nexport type FileLoader<T> = (\ntext: string,\nfilePath: string,\nvalues: LoadValues\n) => Promise<T>;\n\nexport const loadFromFile = async <T>(\nuri: string,\nloader: FileLoader<T>,\nvalues: LoadValues = {}\n): Promise<T> => {\ntry {\nconst fs = await import(\"node:fs/promises\");\nreturn loader(await fs.readFile(uri, { encoding: \"utf-8\" }), uri, values);\n} catch (e) {\nconsole.error(e);\nthrow new Error(`Could not load file at ${uri}`);\n}\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/load.ts","loc":{"lines":{"from":1,"to":22}}}}],["1016",{"pageContent":"import * as yaml from \"yaml\";\nimport { extname } from \"./extname.js\";\n\nexport const loadFileContents = (contents: string, format: string) => {\nswitch (format) {\ncase \".json\":\nreturn JSON.parse(contents);\ncase \".yml\":\ncase \".yaml\":\nreturn yaml.parse(contents);\ndefault:\nthrow new Error(`Unsupported filetype ${format}`);\n}\n};\n\nexport const parseFileConfig = (\ntext: string,\npath: string,\nsupportedTypes?: string[]\n) => {\nconst suffix = extname(path);\n\nif (\n![\".json\", \".yaml\"].includes(suffix) ||\n(supportedTypes && !supportedTypes.includes(suffix))\n) {\nthrow new Error(`Unsupported filetype ${suffix}`);\n}\n\nreturn loadFileContents(text, suffix);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/parse.ts","loc":{"lines":{"from":1,"to":31}}}}],["1017",{"pageContent":"/**\n* Source: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Set#implementing_basic_set_operations\n*/\n\n/**\n* returns intersection of two sets\n*/\nexport function intersection<T>(setA: Set<T>, setB: Set<T>) {\nconst _intersection = new Set<T>();\nfor (const elem of setB) {\nif (setA.has(elem)) {\n_intersection.add(elem);\n}\n}\nreturn _intersection;\n}\n\n/**\n* returns union of two sets\n*/\nexport function union<T>(setA: Set<T>, setB: Set<T>) {\nconst _union = new Set(setA);\nfor (const elem of setB) {\n_union.add(elem);\n}\nreturn _union;\n}\n\n/**\n* returns difference of two sets\n*/\nexport function difference<T>(setA: Set<T>, setB: Set<T>) {\nconst _difference = new Set(setA);\nfor (const elem of setB) {\n_difference.delete(elem);\n}\nreturn _difference;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/set.ts","loc":{"lines":{"from":1,"to":38}}}}],["1018",{"pageContent":"import type { DataSource, DataSourceOptions } from \"typeorm\";\n\ninterface RawResultTableAndColumn {\ntable_name: string;\ncolumn_name: string;\ndata_type: string | undefined;\nis_nullable: string;\n}\n\nexport interface SqlDatabaseParams {\nincludesTables?: Array<string>;\nignoreTables?: Array<string>;\nsampleRowsInTableInfo?: number;\n}\n\nexport interface SqlDatabaseOptionsParams extends SqlDatabaseParams {\nappDataSourceOptions: DataSourceOptions;\n}\n\nexport interface SqlDatabaseDataSourceParams extends SqlDatabaseParams {\nappDataSource: DataSource;\n}\n\nexport type SerializedSqlDatabase = SqlDatabaseOptionsParams & {\n_type: string;\n};\n\nexport interface SqlTable {\ntableName: string;\ncolumns: SqlColumn[];\n}\n\nexport interface SqlColumn {\ncolumnName: string;\ndataType?: string;\nisNullable?: boolean;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":1,"to":37}}}}],["1019",{"pageContent":"const verifyListTablesExistInDatabase = (\ntablesFromDatabase: Array<SqlTable>,\nlistTables: Array<string>,\nerrorPrefixMsg: string\n): void => {\nconst onlyTableNames: Array<string> = tablesFromDatabase.map(\n(table: SqlTable) => table.tableName\n);\nif (listTables.length > 0) {\nfor (const tableName of listTables) {\nif (!onlyTableNames.includes(tableName)) {\nthrow new Error(\n`${errorPrefixMsg} the table ${tableName} was not found in the database`\n);\n}\n}\n}\n};\n\nexport const verifyIncludeTablesExistInDatabase = (\ntablesFromDatabase: Array<SqlTable>,\nincludeTables: Array<string>\n): void => {\nverifyListTablesExistInDatabase(\ntablesFromDatabase,\nincludeTables,\n\"Include tables not found in database:\"\n);\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":255,"to":283}}}}],["1020",{"pageContent":"const verifyIgnoreTablesExistInDatabase = (\ntablesFromDatabase: Array<SqlTable>,\nignoreTables: Array<string>\n): void => {\nverifyListTablesExistInDatabase(\ntablesFromDatabase,\nignoreTables,\n\"Ignore tables not found in database:\"\n);\n};\n\nconst formatToSqlTable = (\nrawResultsTableAndColumn: Array<RawResultTableAndColumn>\n): Array<SqlTable> => {\nconst sqlTable: Array<SqlTable> = [];\nfor (const oneResult of rawResultsTableAndColumn) {\nconst sqlColumn = {\ncolumnName: oneResult.column_name,\ndataType: oneResult.data_type,\nisNullable: oneResult.is_nullable === \"YES\",\n};\nconst currentTable = sqlTable.find(\n(oneTable) => oneTable.tableName === oneResult.table_name\n);\nif (currentTable) {\ncurrentTable.columns.push(sqlColumn);\n} else {\nconst newTable = {\ntableName: oneResult.table_name,\ncolumns: [sqlColumn],\n};\nsqlTable.push(newTable);\n}\n}\n\nreturn sqlTable;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":506,"to":542}}}}],["1021",{"pageContent":"const getTableAndColumnsName = async (\nappDataSource: DataSource\n): Promise<Array<SqlTable>> => {\nlet sql;\nif (appDataSource.options.type === \"postgres\") {\nconst schema = appDataSource.options?.schema ?? \"public\";\nsql = `SELECT \nt.table_name, \nc.* \nFROM \ninformation_schema.tables t \nJOIN information_schema.columns c \nON t.table_name = c.table_name \nWHERE \nt.table_schema = '${schema}' \nAND c.table_schema = '${schema}' \nORDER BY \nt.table_name,\nc.ordinal_position;`;\nconst rep = await appDataSource.query(sql);\n\nreturn formatToSqlTable(rep);\n}\n\nif (appDataSource.options.type === \"sqlite\") {\nsql =\n\"SELECT \\n\" +\n\"   m.name AS table_name,\\n\" +\n\"   p.name AS column_name,\\n\" +\n\"   p.type AS data_type,\\n\" +\n\"   CASE \\n\" +\n\"      WHEN p.\\\"notnull\\\" = 0 THEN 'YES' \\n\" +\n\"      ELSE 'NO' \\n\" +\n\"   END AS is_nullable \\n\" +\n\"FROM \\n\" +\n\"   sqlite_master m \\n\" +\n\"JOIN \\n\" +\n\"   pragma_table_info(m.name) p \\n\" +\n\"WHERE \\n\" +\n\"   m.type = 'table' AND \\n\" +\n\"   m.name NOT LIKE 'sqlite_%';\\n\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":756,"to":796}}}}],["1022",{"pageContent":"const rep = await appDataSource.query(sql);\n\nreturn formatToSqlTable(rep);\n}\n\nif (appDataSource.options.type === \"mysql\") {\nsql =\n\"SELECT \" +\n\"TABLE_NAME AS table_name, \" +\n\"COLUMN_NAME AS column_name, \" +\n\"DATA_TYPE AS data_type, \" +\n\"IS_NULLABLE AS is_nullable \" +\n\"FROM INFORMATION_SCHEMA.COLUMNS \" +\n`WHERE TABLE_SCHEMA = '${appDataSource.options.database}';`;\n\nconst rep = await appDataSource.query(sql);\n\nreturn formatToSqlTable(rep);\n}\n\nthrow new Error(\"Database type not implemented yet\");\n};\n\nconst formatSqlResponseToSimpleTableString = (rawResult: unknown): string => {\nif (!rawResult || !Array.isArray(rawResult) || rawResult.length === 0) {\nreturn \"\";\n}\n\nlet globalString = \"\";\nfor (const oneRow of rawResult) {\nglobalString += `${Object.values(oneRow).reduce(\n(completeString, columnValue) => `${completeString} ${columnValue}`,\n\"\"\n)}\\n`;\n}\n\nreturn globalString;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":1005,"to":1042}}}}],["1023",{"pageContent":"const generateTableInfoFromTables = async (\ntables: Array<SqlTable> | undefined,\nappDataSource: DataSource,\nnbSampleRow: number\n): Promise<string> => {\nif (!tables) {\nreturn \"\";\n}\n\nlet globalString = \"\";\nfor (const currentTable of tables) {\n// Add the creation of the table in SQL\nconst schema =\nappDataSource.options.type === \"postgres\"\n? appDataSource.options?.schema ?? \"public\"\n: null;\nlet sqlCreateTableQuery = schema\n? `CREATE TABLE \"${schema}\".\"${currentTable.tableName}\" (\\n`\n: `CREATE TABLE ${currentTable.tableName} (\\n`;\nfor (const [key, currentColumn] of currentTable.columns.entries()) {\nif (key > 0) {\nsqlCreateTableQuery += \", \";\n}\nsqlCreateTableQuery += `${currentColumn.columnName} ${\ncurrentColumn.dataType\n} ${currentColumn.isNullable ? \"\" : \"NOT NULL\"}`;\n}\nsqlCreateTableQuery += \") \\n\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":1255,"to":1282}}}}],["1024",{"pageContent":"let sqlSelectInfoQuery;\nif (appDataSource.options.type === \"mysql\") {\n// We use backticks to quote the table names and thus allow for example spaces in table names\nsqlSelectInfoQuery = `SELECT * FROM \\`${currentTable.tableName}\\` LIMIT ${nbSampleRow};\\n`;\n} else if (appDataSource.options.type === \"postgres\") {\nconst schema = appDataSource.options?.schema ?? \"public\";\nsqlSelectInfoQuery = `SELECT * FROM \"${schema}\".\"${currentTable.tableName}\" LIMIT ${nbSampleRow};\\n`;\n} else {\nsqlSelectInfoQuery = `SELECT * FROM \"${currentTable.tableName}\" LIMIT ${nbSampleRow};\\n`;\n}\n\nconst columnNamesConcatString = `${currentTable.columns.reduce(\n(completeString, column) => `${completeString} ${column.columnName}`,\n\"\"\n)}\\n`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":1499,"to":1513}}}}],["1025",{"pageContent":"let sample = \"\";\ntry {\nconst infoObjectResult = nbSampleRow\n? await appDataSource.query(sqlSelectInfoQuery)\n: null;\nsample = formatSqlResponseToSimpleTableString(infoObjectResult);\n} catch (error) {\n// If the request fails we catch it and only display a log message\nconsole.log(error);\n}\n\nglobalString = globalString.concat(\nsqlCreateTableQuery +\nsqlSelectInfoQuery +\ncolumnNamesConcatString +\nsample\n);\n}\n\nreturn globalString;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/sql_utils.ts","loc":{"lines":{"from":1736,"to":1756}}}}],["1026",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { AsyncCaller } from \"../async_caller.js\";\nimport { OpenAI } from \"../../llms/openai.js\";\n\ntest(\"AsyncCaller.call passes on arguments and returns return value\", async () => {\nconst caller = new AsyncCaller({});\nconst callable = () => fetch(\"https://httpstat.us/200\");\n\nconst resultDirect = await callable();\nconst resultWrapped = await caller.call(callable);\n\nexpect(resultDirect.status).toEqual(200);\nexpect(resultWrapped.status).toEqual(200);\n});\n\ntest(\"AsyncCaller doesn't retry on axios error 401\", async () => {\nconst llm = new OpenAI({ openAIApiKey: \"invalid\" });\n\nawait expect(() => llm.call(\"test\")).rejects.toThrowError(\n\"Request failed with status code 401\"\n);\n}, 5000);\n\ntest(\"AsyncCaller doesn't retry on timeout\", async () => {\nconst caller = new AsyncCaller({});\nconst callable = () =>\nfetch(\"https://httpstat.us/200?sleep=1000\", {\nsignal: AbortSignal.timeout(10),\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/async_caller.int.test.ts","loc":{"lines":{"from":1,"to":29}}}}],["1027",{"pageContent":"await expect(() => caller.call(callable)).rejects.toThrowError(\n\"TimeoutError: The operation was aborted due to timeout\"\n);\n}, 5000);\n\ntest(\"AsyncCaller doesn't retry on signal abort\", async () => {\nconst controller = new AbortController();\nconst caller = new AsyncCaller({});\nconst callable = () => {\nconst ret = fetch(\"https://httpstat.us/200?sleep=1000\", {\nsignal: controller.signal,\n});\n\ncontroller.abort();\n\nreturn ret;\n};\n\nawait expect(() => caller.call(callable)).rejects.toThrowError(\n\"AbortError: This operation was aborted\"\n);\n}, 5000);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/async_caller.int.test.ts","loc":{"lines":{"from":53,"to":74}}}}],["1028",{"pageContent":"import { test, expect, jest } from \"@jest/globals\";\nimport { AsyncCaller } from \"../async_caller.js\";\n\ntest(\"AsyncCaller passes on arguments and returns return value\", async () => {\nconst caller = new AsyncCaller({});\nconst callable = jest.fn((arg1, arg2) => Promise.resolve([arg2, arg1]));\n\nconst resultDirect = await callable(1, 2);\nconst resultWrapped = await caller.call(callable, 1, 2);\n\nexpect(resultDirect).toEqual([2, 1]);\nexpect(resultWrapped).toEqual([2, 1]);\n});\n\ntest(\"AsyncCaller retries on failure\", async () => {\nconst caller = new AsyncCaller({});\n\n// A direct call throws an error.\nlet callable = jest\n.fn<() => Promise<number[]>>()\n.mockRejectedValueOnce(\"error\")\n.mockResolvedValueOnce([2, 1]);\n\nawait expect(() => callable()).rejects.toEqual(\"error\");\n\n// A wrapped call retries and succeeds.\ncallable = jest\n.fn<() => Promise<number[]>>()\n.mockRejectedValueOnce(\"error\")\n.mockResolvedValueOnce([2, 1]);\n\nconst resultWrapped = await caller.call(callable);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/async_caller.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["1029",{"pageContent":"expect(resultWrapped).toEqual([2, 1]);\nexpect(callable.mock.calls).toHaveLength(2);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/async_caller.test.ts","loc":{"lines":{"from":36,"to":38}}}}],["1030",{"pageContent":"import { describe, it, expect, jest } from \"@jest/globals\";\nimport { AxiosResponse } from \"axios\";\nimport fetchAdapter from \"../axios-fetch-adapter.js\";\n\nconst mockFetchForOpenAIStream = async ({\nchunks,\nstatus,\ncontentType,\n}: {\nchunks: Array<string>;\nstatus: number;\ncontentType: string;\n}) => {\n// Mock stream response chunks.\nconst stream = new ReadableStream({\nasync start(controller) {\nchunks.forEach((chunk) => {\ncontroller.enqueue(new TextEncoder().encode(chunk));\n});\ncontroller.close();\n},\n});\n\n// Mock Fetch API call.\njest.spyOn(global, \"fetch\").mockImplementation(\nasync () =>\nnew Response(stream, {\nstatus,\nheaders: {\n\"Content-Type\": contentType,\n},\n})\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/openai-stream.test.ts","loc":{"lines":{"from":1,"to":33}}}}],["1031",{"pageContent":"let error: Error | null = null;\nconst receivedChunks: Array<string> = [];\nconst resp = await fetchAdapter({\nurl: \"https://example.com\",\nmethod: \"POST\",\nresponseType: \"stream\",\nonmessage: (strChunk: { data: string }) => {\nreceivedChunks.push(\nJSON.parse(strChunk.data).choices[0].delta.content ?? \"\"\n);\n},\n} as unknown as never).catch((err) => {\nerror = err;\nreturn null;\n});\n\nreturn { resp, receivedChunks, error } as {\nresp: AxiosResponse | null;\nreceivedChunks: Array<string>;\nerror: Error | null;\n};\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/openai-stream.test.ts","loc":{"lines":{"from":139,"to":160}}}}],["1032",{"pageContent":"describe(\"OpenAI Stream Tests\", () => {\nit(\"should return a 200 response chunk by chunk\", async () => {\n// When stream mode enabled, OpenAI responds with a stream of `data: {...}\\n\\n` chunks.\nconst { resp, receivedChunks, error } = await mockFetchForOpenAIStream({\nstatus: 200,\ncontentType: \"text/event-stream\",\nchunks: [\n'data: {\"choices\":[{\"delta\":{\"role\":\"assistant\"},\"index\":0,\"finish_reason\":null}]}\\n\\n',\n'data: {\"choices\":[{\"delta\":{\"content\":\"Hello\"},\"index\":0,\"finish_reason\":null}]}\\n\\n',\n'data: {\"choices\":[{\"delta\":{\"content\":\" World\"},\"index\":0,\"finish_reason\":null}]}\\n\\n',\n'data: {\"choices\":[{\"delta\":{\"content\":\"!\"},\"index\":0,\"finish_reason\":null}]}\\n\\n',\n'data: {\"choices\":[{\"delta\":{},\"index\":0,\"finish_reason\":\"stop\"}]}\\n\\n',\n],\n});\n\nexpect(error).toEqual(null);\nexpect(resp?.status).toEqual(200);\nexpect(receivedChunks).toEqual([\"\", \"Hello\", \" World\", \"!\", \"\"]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/openai-stream.test.ts","loc":{"lines":{"from":275,"to":293}}}}],["1033",{"pageContent":"it(\"should handle OpenAI 400 json error\", async () => {\n// OpenAI returns errors with application/json content type.\n// Even if stream mode is enabled, the error is returned as a normal JSON body.\n// Error information is in the `error` field.\nconst { resp, receivedChunks, error } = await mockFetchForOpenAIStream({\nstatus: 400,\ncontentType: \"application/json\",\nchunks: [\nJSON.stringify({\nerror: {},\n}),\n],\n});\n\nexpect(error).toEqual(null);\nexpect(resp?.status).toEqual(400);\nexpect(resp?.data).toEqual({ error: {} });\nexpect(receivedChunks).toEqual([]);\n});\n\nit(\"should handle 500 non-json error\", async () => {\nconst { resp, receivedChunks, error } = await mockFetchForOpenAIStream({\nstatus: 500,\ncontentType: \"text/plain\",\nchunks: [\"Some error message...\"],\n});\nexpect(error).toEqual(null);\nexpect(resp?.status).toEqual(500);\nexpect(resp?.data).toEqual(\"Some error message...\");\nexpect(receivedChunks).toEqual([]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/openai-stream.test.ts","loc":{"lines":{"from":389,"to":419}}}}],["1034",{"pageContent":"it(\"should throw on 500 non-json body with json content type\", async () => {\nconst { resp, receivedChunks, error } = await mockFetchForOpenAIStream({\nstatus: 500,\ncontentType: \"application/json\",\nchunks: [\"a non-json error body\"],\n});\nexpect(resp).toEqual(null);\nexpect(error?.message).toContain(\"Unexpected token\");\nexpect(receivedChunks).toEqual([]);\n});\n\nit(\"should throw the generic error if non-stream content is detected\", async () => {\nconst { resp, receivedChunks, error } = await mockFetchForOpenAIStream({\nstatus: 200,\ncontentType: \"text/plain\",\nchunks: [\"a non-stream body\"],\n});\nexpect(resp).toEqual(null);\nexpect(error?.message).toBe(\n\"Expected content-type to be text/event-stream, Actual: text/plain\"\n);\nexpect(receivedChunks).toEqual([]);\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/openai-stream.test.ts","loc":{"lines":{"from":514,"to":537}}}}],["1035",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { difference, intersection, union } from \"../set.js\";\n\ntest(\"difference\", () => {\nconst set1 = new Set([\"a\", \"b\"]);\nconst set2 = new Set([\"b\", \"c\"]);\n\nconst resultSet = difference(set1, set2);\nexpect(resultSet).toMatchInlineSnapshot(`\nSet {\n\"a\",\n}\n`);\n});\n\ntest(\"intersection\", () => {\nconst set1 = new Set([\"a\", \"b\", \"c\", \"d\"]);\nconst set2 = new Set([\"b\", \"c\", \"e\"]);\n\nconst resultSet = intersection(set1, set2);\nexpect(resultSet).toMatchInlineSnapshot(`\nSet {\n\"b\",\n\"c\",\n}\n`);\n});\n\ntest(\"union\", () => {\nconst set1 = new Set([\"a\", \"b\"]);\nconst set2 = new Set([\"c\", \"d\"]);\n\nconst resultSet = union(set1, set2);\nexpect(resultSet).toMatchInlineSnapshot(`\nSet {\n\"a\",\n\"b\",\n\"c\",\n\"d\",\n}\n`);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/set.test.ts","loc":{"lines":{"from":1,"to":42}}}}],["1036",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport {\nverifyIgnoreTablesExistInDatabase,\nverifyIncludeTablesExistInDatabase,\n} from \"../sql_utils.js\";\n\ntest(\"Find include tables when there are there\", () => {\nconst includeTables = [\"user\", \"score\"];\nconst allTables = [\n{ tableName: \"plop\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"score\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"user\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"log\", columns: [{ columnName: \"id\" }] },\n];\n\nexpect(() =>\nverifyIncludeTablesExistInDatabase(allTables, includeTables)\n).not.toThrow();\n});\n\ntest(\"Throw Error when include tables are not there\", () => {\nconst includeTables = [\"user\", \"score\"];\nconst allTables = [\n{ tableName: \"plop\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"score\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"log\", columns: [{ columnName: \"id\" }] },\n];\n\nexpect(() =>\nverifyIncludeTablesExistInDatabase(allTables, includeTables)\n).toThrow();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/sql_utils.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["1037",{"pageContent":"test(\"Find include tables when there are there\", () => {\nconst includeTables = [\"user\", \"score\"];\nconst allTables = [\n{ tableName: \"user\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"plop\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"score\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"log\", columns: [{ columnName: \"id\" }] },\n];\n\nexpect(() =>\nverifyIgnoreTablesExistInDatabase(allTables, includeTables)\n).not.toThrow();\n});\n\ntest(\"Throw Error when include tables are not there\", () => {\nconst includeTables = [\"user\", \"score\"];\nconst allTables = [\n{ tableName: \"plop\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"score\", columns: [{ columnName: \"id\" }] },\n{ tableName: \"log\", columns: [{ columnName: \"id\" }] },\n];\n\nexpect(() =>\nverifyIgnoreTablesExistInDatabase(allTables, includeTables)\n).toThrow();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/util/tests/sql_utils.test.ts","loc":{"lines":{"from":61,"to":86}}}}],["1038",{"pageContent":"import { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\nimport { BaseRetriever } from \"../schema/index.js\";\n\nexport interface VectorStoreRetrieverInput<V extends VectorStore> {\nvectorStore: V;\nk?: number;\nfilter?: V[\"FilterType\"];\n}\n\nexport class VectorStoreRetriever<\nV extends VectorStore = VectorStore\n> extends BaseRetriever {\nvectorStore: V;\n\nk = 4;\n\nfilter?: V[\"FilterType\"];\n\nconstructor(fields: VectorStoreRetrieverInput<V>) {\nsuper();\nthis.vectorStore = fields.vectorStore;\nthis.k = fields.k ?? this.k;\nthis.filter = fields.filter;\n}\n\nasync getRelevantDocuments(query: string): Promise<Document[]> {\nconst results = await this.vectorStore.similaritySearch(\nquery,\nthis.k,\nthis.filter\n);\nreturn results;\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nawait this.vectorStore.addDocuments(documents);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/base.ts","loc":{"lines":{"from":1,"to":39}}}}],["1039",{"pageContent":"abstract class VectorStore {\ndeclare FilterType: object;\n\nembeddings: Embeddings;\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconstructor(embeddings: Embeddings, _dbConfig: Record<string, any>) {\nthis.embeddings = embeddings;\n}\n\nabstract addVectors(\nvectors: number[][],\ndocuments: Document[]\n): Promise<void>;\n\nabstract addDocuments(documents: Document[]): Promise<void>;\n\nabstract similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: this[\"FilterType\"]\n): Promise<[Document, number][]>;\n\nasync similaritySearch(\nquery: string,\nk = 4,\nfilter: this[\"FilterType\"] | undefined = undefined\n): Promise<Document[]> {\nconst results = await this.similaritySearchVectorWithScore(\nawait this.embeddings.embedQuery(query),\nk,\nfilter\n);\n\nreturn results.map((result) => result[0]);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/base.ts","loc":{"lines":{"from":133,"to":168}}}}],["1040",{"pageContent":"async similaritySearchWithScore(\nquery: string,\nk = 4,\nfilter: this[\"FilterType\"] | undefined = undefined\n): Promise<[Document, number][]> {\nreturn this.similaritySearchVectorWithScore(\nawait this.embeddings.embedQuery(query),\nk,\nfilter\n);\n}\n\nstatic fromTexts(\n_texts: string[],\n_metadatas: object[] | object,\n_embeddings: Embeddings,\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_dbConfig: Record<string, any>\n): Promise<VectorStore> {\nthrow new Error(\n\"the Langchain vectorstore implementation you are using forgot to override this, please report a bug\"\n);\n}\n\nstatic fromDocuments(\n_docs: Document[],\n_embeddings: Embeddings,\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n_dbConfig: Record<string, any>\n): Promise<VectorStore> {\nthrow new Error(\n\"the Langchain vectorstore implementation you are using forgot to override this, please report a bug\"\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/base.ts","loc":{"lines":{"from":265,"to":298}}}}],["1041",{"pageContent":"asRetriever(\nk?: number,\nfilter?: this[\"FilterType\"]\n): VectorStoreRetriever<this> {\nreturn new VectorStoreRetriever({ vectorStore: this, k, filter });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/base.ts","loc":{"lines":{"from":391,"to":397}}}}],["1042",{"pageContent":"abstract class SaveableVectorStore extends VectorStore {\nabstract save(directory: string): Promise<void>;\n\nstatic load(\n_directory: string,\n_embeddings: Embeddings\n): Promise<SaveableVectorStore> {\nthrow new Error(\"Not implemented\");\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/base.ts","loc":{"lines":{"from":522,"to":531}}}}],["1043",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\nimport type { ChromaClient as ChromaClientT, Collection } from \"chromadb\";\n\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { VectorStore } from \"./base.js\";\nimport { Document } from \"../document.js\";\n\nexport type ChromaLibArgs =\n| {\nurl?: string;\nnumDimensions?: number;\ncollectionName?: string;\n}\n| {\nindex?: ChromaClientT;\nnumDimensions?: number;\ncollectionName?: string;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":1,"to":18}}}}],["1044",{"pageContent":"class Chroma extends VectorStore {\nindex?: ChromaClientT;\n\ncollection?: Collection;\n\ncollectionName: string;\n\nnumDimensions?: number;\n\nurl: string;\n\nconstructor(embeddings: Embeddings, args: ChromaLibArgs) {\nsuper(embeddings, args);\nthis.numDimensions = args.numDimensions;\nthis.embeddings = embeddings;\nthis.collectionName = ensureCollectionName(args.collectionName);\nif (\"index\" in args) {\nthis.index = args.index;\n} else if (\"url\" in args) {\nthis.url = args.url || \"http://localhost:8000\";\n}\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nawait this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync ensureCollection(): Promise<Collection> {\nif (!this.collection) {\nif (!this.index) {\nconst { ChromaClient } = await Chroma.imports();\nthis.index = new ChromaClient(this.url);\n}\nthis.collection = await this.index.getOrCreateCollection(\nthis.collectionName\n);\n}\n\nreturn this.collection;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":190,"to":233}}}}],["1045",{"pageContent":"async addVectors(vectors: number[][], documents: Document[]) {\nif (vectors.length === 0) {\nreturn;\n}\nif (this.numDimensions === undefined) {\nthis.numDimensions = vectors[0].length;\n}\nif (vectors.length !== documents.length) {\nthrow new Error(`Vectors and metadatas must have the same length`);\n}\nif (vectors[0].length !== this.numDimensions) {\nthrow new Error(\n`Vectors must have the same length as the number of dimensions (${this.numDimensions})`\n);\n}\n\nconst collection = await this.ensureCollection();\nconst docstoreSize = await collection.count();\nawait collection.add(\nArray.from({ length: vectors.length }, (_, i) =>\n(docstoreSize + i).toString()\n),\nvectors,\ndocuments.map(({ metadata }) => metadata),\ndocuments.map(({ pageContent }) => pageContent)\n);\n}\n\nasync similaritySearchVectorWithScore(query: number[], k: number) {\nconst collection = await this.ensureCollection();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":382,"to":411}}}}],["1046",{"pageContent":"// similaritySearchVectorWithScore supports one query vector at a time\n// chroma supports multiple query vectors at a time\nconst result = await collection.query(query, k);\n\nconst { ids, distances, documents, metadatas } = result;\nif (!ids || !distances || !documents || !metadatas) {\nreturn [];\n}\n// get the result data from the first and only query vector\nconst [firstIds] = ids;\nconst [firstDistances] = distances;\nconst [firstDocuments] = documents;\nconst [firstMetadatas] = metadatas;\n\nconst results: [Document, number][] = [];\nfor (let i = 0; i < firstIds.length; i += 1) {\nresults.push([\nnew Document({\npageContent: firstDocuments[i],\nmetadata: firstMetadatas[i],\n}),\nfirstDistances[i],\n]);\n}\nreturn results;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":564,"to":589}}}}],["1047",{"pageContent":"static async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig: {\ncollectionName?: string;\nurl?: string;\n}\n): Promise<Chroma> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn Chroma.fromDocuments(docs, embeddings, dbConfig);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: {\ncollectionName?: string;\nurl?: string;\n}\n): Promise<Chroma> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nstatic async fromExistingCollection(\nembeddings: Embeddings,\ndbConfig: {\ncollectionName: string;\nurl?: string;\n}\n): Promise<Chroma> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.ensureCollection();\nreturn instance;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":745,"to":789}}}}],["1048",{"pageContent":"static async imports(): Promise<{\nChromaClient: typeof ChromaClientT;\n}> {\ntry {\nconst { ChromaClient } = await import(\"chromadb\");\nreturn { ChromaClient };\n} catch (e) {\nthrow new Error(\n\"Please install chromadb as a dependency with, e.g. `npm install -S chromadb`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":939,"to":951}}}}],["1049",{"pageContent":"ensureCollectionName(collectionName?: string) {\nif (!collectionName) {\nreturn `langchain-${uuidv4()}`;\n}\nreturn collectionName;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/chroma.ts","loc":{"lines":{"from":1131,"to":1136}}}}],["1050",{"pageContent":"import type {\nHierarchicalNSW as HierarchicalNSWT,\nSpaceName,\n} from \"hnswlib-node\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { SaveableVectorStore } from \"./base.js\";\nimport { Document } from \"../document.js\";\nimport { InMemoryDocstore } from \"../docstore/index.js\";\n\nexport interface HNSWLibBase {\nspace: SpaceName;\nnumDimensions?: number;\n}\n\nexport interface HNSWLibArgs extends HNSWLibBase {\ndocstore?: InMemoryDocstore;\nindex?: HierarchicalNSWT;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":1,"to":18}}}}],["1051",{"pageContent":"class HNSWLib extends SaveableVectorStore {\ndeclare FilterType: (doc: Document) => boolean;\n\n_index?: HierarchicalNSWT;\n\ndocstore: InMemoryDocstore;\n\nargs: HNSWLibBase;\n\nconstructor(embeddings: Embeddings, args: HNSWLibArgs) {\nsuper(embeddings, args);\nthis._index = args.index;\nthis.args = args;\nthis.embeddings = embeddings;\nthis.docstore = args?.docstore ?? new InMemoryDocstore();\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nprivate static async getHierarchicalNSW(args: HNSWLibBase) {\nconst { HierarchicalNSW } = await HNSWLib.imports();\nif (!args.space) {\nthrow new Error(\"hnswlib-node requires a space argument\");\n}\nif (args.numDimensions === undefined) {\nthrow new Error(\"hnswlib-node requires a numDimensions argument\");\n}\nreturn new HierarchicalNSW(args.space, args.numDimensions);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":246,"to":280}}}}],["1052",{"pageContent":"private async initIndex(vectors: number[][]) {\nif (!this._index) {\nif (this.args.numDimensions === undefined) {\nthis.args.numDimensions = vectors[0].length;\n}\nthis.index = await HNSWLib.getHierarchicalNSW(this.args);\n}\nif (!this.index.getCurrentCount()) {\nthis.index.initIndex(vectors.length);\n}\n}\n\npublic get index(): HierarchicalNSWT {\nif (!this._index) {\nthrow new Error(\n\"Vector store not initialised yet. Try calling `addTexts` first.\"\n);\n}\nreturn this._index;\n}\n\nprivate set index(index: HierarchicalNSWT) {\nthis._index = index;\n}\n\nasync addVectors(vectors: number[][], documents: Document[]) {\nif (vectors.length === 0) {\nreturn;\n}\nawait this.initIndex(vectors);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":488,"to":517}}}}],["1053",{"pageContent":"// TODO here we could optionally normalise the vectors to unit length\n// so that dot product is equivalent to cosine similarity, like this\n// https://github.com/nmslib/hnswlib/issues/384#issuecomment-1155737730\n// While we only support OpenAI embeddings this isn't necessary\nif (vectors.length !== documents.length) {\nthrow new Error(`Vectors and metadatas must have the same length`);\n}\nif (vectors[0].length !== this.args.numDimensions) {\nthrow new Error(\n`Vectors must have the same length as the number of dimensions (${this.args.numDimensions})`\n);\n}\nconst capacity = this.index.getMaxElements();\nconst needed = this.index.getCurrentCount() + vectors.length;\nif (needed > capacity) {\nthis.index.resizeIndex(needed);\n}\nconst docstoreSize = this.docstore.count;\nfor (let i = 0; i < vectors.length; i += 1) {\nthis.index.addPoint(vectors[i], docstoreSize + i);\nthis.docstore.add({ [docstoreSize + i]: documents[i] });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":734,"to":756}}}}],["1054",{"pageContent":"async similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: this[\"FilterType\"]\n) {\nif (this.args.numDimensions && !this._index) {\nawait this.initIndex([[]]);\n}\nif (query.length !== this.args.numDimensions) {\nthrow new Error(\n`Query vector must have the same length as the number of dimensions (${this.args.numDimensions})`\n);\n}\nif (k > this.index.getCurrentCount()) {\nconst total = this.index.getCurrentCount();\nconsole.warn(\n`k (${k}) is greater than the number of elements in the index (${total}), setting k to ${total}`\n);\n// eslint-disable-next-line no-param-reassign\nk = total;\n}\nconst filterFunction = (label: number): boolean => {\nif (!filter) {\nreturn true;\n}\nconst document = this.docstore.search(String(label));\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (typeof document !== \"string\") {\nreturn filter(document);\n}\nreturn false;\n};\nconst result = this.index.searchKnn(\nquery,\nk,\nfilter ? filterFunction : undefined\n);\nreturn result.neighbors.map(\n(docIndex, resultIndex) =>\n[","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":964,"to":1003}}}}],["1055",{"pageContent":"this.docstore.search(String(docIndex)),\nresult.distances[resultIndex],\n] as [Document, number]\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":1210,"to":1214}}}}],["1056",{"pageContent":"async save(directory: string) {\nconst fs = await import(\"node:fs/promises\");\nconst path = await import(\"node:path\");\nawait fs.mkdir(directory, { recursive: true });\nawait Promise.all([\nthis.index.writeIndex(path.join(directory, \"hnswlib.index\")),\nawait fs.writeFile(\npath.join(directory, \"args.json\"),\nJSON.stringify(this.args)\n),\nawait fs.writeFile(\npath.join(directory, \"docstore.json\"),\nJSON.stringify(Array.from(this.docstore._docs.entries()))\n),\n]);\n}\n\nstatic async load(directory: string, embeddings: Embeddings) {\nconst fs = await import(\"node:fs/promises\");\nconst path = await import(\"node:path\");\nconst args = JSON.parse(\nawait fs.readFile(path.join(directory, \"args.json\"), \"utf8\")\n);\nconst index = await HNSWLib.getHierarchicalNSW(args);\nconst [docstoreFiles] = await Promise.all([\nfs\n.readFile(path.join(directory, \"docstore.json\"), \"utf8\")\n.then(JSON.parse),\nindex.readIndex(path.join(directory, \"hnswlib.index\")),\n]);\nargs.docstore = new InMemoryDocstore(new Map(docstoreFiles));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":1454,"to":1484}}}}],["1057",{"pageContent":"args.index = index;\n\nreturn new HNSWLib(embeddings, args);\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig?: {\ndocstore?: InMemoryDocstore;\n}\n): Promise<HNSWLib> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn HNSWLib.fromDocuments(docs, embeddings, dbConfig);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig?: {\ndocstore?: InMemoryDocstore;\n}\n): Promise<HNSWLib> {\nconst args: HNSWLibArgs = {\ndocstore: dbConfig?.docstore,\nspace: \"cosine\",\n};\nconst instance = new this(embeddings, args);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nstatic async imports(): Promise<{\nHierarchicalNSW: typeof HierarchicalNSWT;\n}> {\ntry {\nconst {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":1691,"to":1736}}}}],["1058",{"pageContent":": { HierarchicalNSW },\n} = await import(\"hnswlib-node\");\n\nreturn { HierarchicalNSW };\n} catch (err) {\nthrow new Error(\n\"Please install hnswlib-node as a dependency with, e.g. `npm install -S hnswlib-node`\"\n);\n}\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/hnswlib.ts","loc":{"lines":{"from":1944,"to":1954}}}}],["1059",{"pageContent":"/* #__PURE__ */ console.error(\n\"[WARN] Importing from 'langchain/vectorstores' is deprecated. Import from eg. 'langchain/vectorstores/pinecone' instead. See https://js.langchain.com/docs/getting-started/install#updating-from-0052 for upgrade instructions.\"\n);\n\nexport { HNSWLib } from \"./hnswlib.js\";\nexport { Chroma } from \"./chroma.js\";\nexport { PineconeStore } from \"./pinecone.js\";\nexport { VectorStore, SaveableVectorStore } from \"./base.js\";\nexport { SupabaseVectorStore } from \"./supabase.js\";\nexport { PrismaVectorStore } from \"./prisma.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/index.ts","loc":{"lines":{"from":1,"to":10}}}}],["1060",{"pageContent":"import { similarity as ml_distance_similarity } from \"ml-distance\";\nimport { VectorStore } from \"./base.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\n\ninterface MemoryVector {\ncontent: string;\nembedding: number[];\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nmetadata: Record<string, any>;\n}\n\nexport interface MemoryVectorStoreArgs {\nsimilarity?: typeof ml_distance_similarity.cosine;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/memory.ts","loc":{"lines":{"from":1,"to":15}}}}],["1061",{"pageContent":"class MemoryVectorStore extends VectorStore {\nmemoryVectors: MemoryVector[] = [];\n\nsimilarity: typeof ml_distance_similarity.cosine;\n\nconstructor(\nembeddings: Embeddings,\n{ similarity, ...rest }: MemoryVectorStoreArgs = {}\n) {\nsuper(embeddings, rest);\n\nthis.similarity = similarity ?? ml_distance_similarity.cosine;\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nconst memoryVectors = vectors.map((embedding, idx) => ({\ncontent: documents[idx].pageContent,\nembedding,\nmetadata: documents[idx].metadata,\n}));\n\nthis.memoryVectors = this.memoryVectors.concat(memoryVectors);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/memory.ts","loc":{"lines":{"from":108,"to":138}}}}],["1062",{"pageContent":"async similaritySearchVectorWithScore(\nquery: number[],\nk: number\n): Promise<[Document, number][]> {\nconst searches = this.memoryVectors\n.map((vector, index) => ({\nsimilarity: this.similarity(query, vector.embedding),\nindex,\n}))\n.sort((a, b) => (a.similarity > b.similarity ? -1 : 0))\n.slice(0, k);\n\nconst result: [Document, number][] = searches.map((search) => [\nnew Document({\nmetadata: this.memoryVectors[search.index].metadata,\npageContent: this.memoryVectors[search.index].content,\n}),\nsearch.similarity,\n]);\n\nreturn result;\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig?: MemoryVectorStoreArgs\n): Promise<MemoryVectorStore> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn MemoryVectorStore.fromDocuments(docs, embeddings, dbConfig);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/memory.ts","loc":{"lines":{"from":217,"to":256}}}}],["1063",{"pageContent":"static async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig?: MemoryVectorStoreArgs\n): Promise<MemoryVectorStore> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nstatic async fromExistingIndex(\nembeddings: Embeddings,\ndbConfig?: MemoryVectorStoreArgs\n): Promise<MemoryVectorStore> {\nconst instance = new this(embeddings, dbConfig);\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/memory.ts","loc":{"lines":{"from":329,"to":346}}}}],["1064",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\nimport { MilvusClient } from \"@zilliz/milvus2-sdk-node\";\nimport {\nDataType,\nDataTypeMap,\n} from \"@zilliz/milvus2-sdk-node/dist/milvus/const/Milvus.js\";\nimport {\nErrorCode,\nFieldType,\n} from \"@zilliz/milvus2-sdk-node/dist/milvus/types.js\";\n\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { VectorStore } from \"./base.js\";\nimport { Document } from \"../document.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":1,"to":14}}}}],["1065",{"pageContent":"interface MilvusLibArgs {\ncollectionName?: string;\nprimaryField?: string;\nvectorField?: string;\ntextField?: string;\nurl?: string; // db address\nssl?: boolean;\nusername?: string;\npassword?: string;\n}\n\ntype IndexType =\n| \"IVF_FLAT\"\n| \"IVF_SQ8\"\n| \"IVF_PQ\"\n| \"HNSW\"\n| \"RHNSW_FLAT\"\n| \"RHNSW_SQ\"\n| \"RHNSW_PQ\"\n| \"IVF_HNSW\"\n| \"ANNOY\";\n\ninterface IndexParam {\nparams: { nprobe?: number; ef?: number; search_k?: number };\n}\n\ninterface InsertRow {\n[x: string]: string | number[];\n}\n\nconst MILVUS_PRIMARY_FIELD_NAME = \"langchain_primaryid\";\nconst MILVUS_VECTOR_FIELD_NAME = \"langchain_vector\";\nconst MILVUS_TEXT_FIELD_NAME = \"langchain_text\";\nconst MILVUS_COLLECTION_NAME_PREFIX = \"langchain_col\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":520,"to":553}}}}],["1066",{"pageContent":"class Milvus extends VectorStore {\ncollectionName: string;\n\nnumDimensions?: number;\n\nautoId?: boolean;\n\nprimaryField: string;\n\nvectorField: string;\n\ntextField: string;\n\nfields: string[];\n\nclient: MilvusClient;\n\ncolMgr: MilvusClient[\"collectionManager\"];\n\nidxMgr: MilvusClient[\"indexManager\"];\n\ndataMgr: MilvusClient[\"dataManager\"];\n\nindexParams: Record<IndexType, IndexParam> = {\nIVF_FLAT: { params: { nprobe: 10 } },\nIVF_SQ8: { params: { nprobe: 10 } },\nIVF_PQ: { params: { nprobe: 10 } },\nHNSW: { params: { ef: 10 } },\nRHNSW_FLAT: { params: { ef: 10 } },\nRHNSW_SQ: { params: { ef: 10 } },\nRHNSW_PQ: { params: { ef: 10 } },\nIVF_HNSW: { params: { nprobe: 10, ef: 10 } },\nANNOY: { params: { search_k: 10 } },\n};\n\nindexCreateParams = {\nindex_type: \"HNSW\",\nmetric_type: \"L2\",\nparams: JSON.stringify({ M: 8, efConstruction: 64 }),\n};\n\nindexSearchParams = JSON.stringify({ ef: 64 });","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":1044,"to":1085}}}}],["1067",{"pageContent":"constructor(embeddings: Embeddings, args: MilvusLibArgs) {\nsuper(embeddings, args);\nthis.embeddings = embeddings;\nthis.collectionName = args.collectionName ?? genCollectionName();\nthis.textField = args.textField ?? MILVUS_TEXT_FIELD_NAME;\n\nthis.autoId = true;\nthis.primaryField = args.primaryField ?? MILVUS_PRIMARY_FIELD_NAME;\nthis.vectorField = args.vectorField ?? MILVUS_VECTOR_FIELD_NAME;\nthis.fields = [];\n\nconst url =\nargs.url ??\n// eslint-disable-next-line no-process-env\n(typeof process !== \"undefined\" ? process.env?.MILVUS_URL : undefined);\nif (!url) {\nthrow new Error(\"Milvus URL address is not provided.\");\n}\nthis.client = new MilvusClient(url, args.ssl, args.username, args.password);\nthis.colMgr = this.client.collectionManager;\nthis.idxMgr = this.client.indexManager;\nthis.dataMgr = this.client.dataManager;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":1565,"to":1587}}}}],["1068",{"pageContent":"async addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nawait this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nif (vectors.length === 0) {\nreturn;\n}\nawait this.ensureCollection(vectors, documents);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":2068,"to":2080}}}}],["1069",{"pageContent":"const insertDatas: InsertRow[] = [];\n// eslint-disable-next-line no-plusplus\nfor (let index = 0; index < vectors.length; index++) {\nconst vec = vectors[index];\nconst doc = documents[index];\nconst data: InsertRow = {\n[this.textField]: doc.pageContent,\n[this.vectorField]: vec,\n};\nthis.fields.forEach((field) => {\nswitch (field) {\ncase this.primaryField:\nif (!this.autoId) {\nif (doc.metadata[this.primaryField] === undefined) {\nthrow new Error(\n`The Collection's primaryField is configured with autoId=false, thus its value must be provided through metadata.`\n);\n}\ndata[field] = doc.metadata[this.primaryField];\n}\nbreak;\ncase this.textField:\ndata[field] = doc.pageContent;\nbreak;\ncase this.vectorField:\ndata[field] = vec;\nbreak;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":2587,"to":2613}}}}],["1070",{"pageContent":": // metadata fields\nif (doc.metadata[field] === undefined) {\nthrow new Error(\n`The field \"${field}\" is not provided in documents[${index}].metadata.`\n);\n} else if (typeof doc.metadata[field] === \"object\") {\ndata[field] = JSON.stringify(doc.metadata[field]);\n} else {\ndata[field] = doc.metadata[field];\n}\nbreak;\n}\n});\n\ninsertDatas.push(data);\n}\n\nconst insertResp = await this.dataMgr.insert({\ncollection_name: this.collectionName,\nfields_data: insertDatas,\n});\nif (insertResp.status.error_code !== ErrorCode.SUCCESS) {\nthrow new Error(`Error inserting data: ${JSON.stringify(insertResp)}`);\n}\nawait this.dataMgr.flushSync({ collection_names: [this.collectionName] });\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":3101,"to":3126}}}}],["1071",{"pageContent":"async similaritySearchVectorWithScore(\nquery: number[],\nk: number\n): Promise<[Document, number][]> {\nconst hasColResp = await this.colMgr.hasCollection({\ncollection_name: this.collectionName,\n});\nif (hasColResp.status.error_code !== ErrorCode.SUCCESS) {\nthrow new Error(`Error checking collection: ${hasColResp}`);\n}\nif (hasColResp.value === false) {\nthrow new Error(\n`Collection not found: ${this.collectionName}, please create collection before search.`\n);\n}\n\nawait this.grabCollectionFields();\n\nconst loadResp = await this.colMgr.loadCollectionSync({\ncollection_name: this.collectionName,\n});\nif (loadResp.error_code !== ErrorCode.SUCCESS) {\nthrow new Error(`Error loading collection: ${loadResp}`);\n}\n\n// clone this.field and remove vectorField\nconst outputFields = this.fields.filter(\n(field) => field !== this.vectorField\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":3618,"to":3646}}}}],["1072",{"pageContent":"const searchResp = await this.dataMgr.search({\ncollection_name: this.collectionName,\nsearch_params: {\nanns_field: this.vectorField,\ntopk: k.toString(),\nmetric_type: this.indexCreateParams.metric_type,\nparams: this.indexSearchParams,\n},\noutput_fields: outputFields,\nvector_type: DataType.FloatVector,\nvectors: [query],\n});\nif (searchResp.status.error_code !== ErrorCode.SUCCESS) {\nthrow new Error(`Error searching data: ${JSON.stringify(searchResp)}`);\n}\nconst results: [Document, number][] = [];\nsearchResp.results.forEach((result) => {\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconst fields = { pageContent: \"\", metadata: {} as Record<string, any> };\nObject.keys(result).forEach((key) => {\nif (key === this.textField) {\nfields.pageContent = result[key];\n} else if (this.fields.includes(key)) {\nif (typeof result[key] === \"string\") {\nconst { isJson, obj } = checkJsonString(result[key]);\nfields.metadata[key] = isJson ? obj : result[key];\n} else {\nfields.metadata[key] = result[key];\n}\n}\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":4127,"to":4157}}}}],["1073",{"pageContent":"results.push([new Document(fields), result.score]);\n});\n// console.log(\"Search result: \" + JSON.stringify(results, null, 2));\nreturn results;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":4630,"to":4634}}}}],["1074",{"pageContent":"async ensureCollection(vectors?: number[][], documents?: Document[]) {\nconst hasColResp = await this.colMgr.hasCollection({\ncollection_name: this.collectionName,\n});\nif (hasColResp.status.error_code !== ErrorCode.SUCCESS) {\nthrow new Error(\n`Error checking collection: ${JSON.stringify(hasColResp, null, 2)}`\n);\n}\n\nif (hasColResp.value === false) {\nif (vectors === undefined || documents === undefined) {\nthrow new Error(\n`Collection not found: ${this.collectionName}, please provide vectors and documents to create collection.`\n);\n}\nawait this.createCollection(vectors, documents);\n} else {\nawait this.grabCollectionFields();\n}\n}\n\nasync createCollection(\nvectors: number[][],\ndocuments: Document[]\n): Promise<void> {\nconst fieldList: FieldType[] = [];\n\nfieldList.push(...createFieldTypeForMetadata(documents));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":5148,"to":5176}}}}],["1075",{"pageContent":"fieldList.push(\n{\nname: this.primaryField,\ndescription: \"Primary key\",\ndata_type: DataType.Int64,\nis_primary_key: true,\nautoID: this.autoId,\n},\n{\nname: this.textField,\ndescription: \"Text field\",\ndata_type: DataType.VarChar,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":5658,"to":5669}}}}],["1076",{"pageContent":"_params: {\nmax_length: getTextFieldMaxLength(documents).toString(),\n},\n},\n{\nname: this.vectorField,\ndescription: \"Vector field\",\ndata_type: DataType.FloatVector,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":6180,"to":6187}}}}],["1077",{"pageContent":"_params: {\ndim: getVectorFieldDim(vectors).toString(),\n},\n}\n);\n\nfieldList.forEach((field) => {\nif (!field.autoID) {\nthis.fields.push(field.name);\n}\n});\n\nconst createRes = await this.colMgr.createCollection({\ncollection_name: this.collectionName,\nfields: fieldList,\n});\n\nif (createRes.error_code !== ErrorCode.SUCCESS) {\nconsole.log(createRes);\nthrow new Error(`Failed to create collection: ${createRes}`);\n}\n\nawait this.idxMgr.createIndex({\ncollection_name: this.collectionName,\nfield_name: this.vectorField,\nextra_params: this.indexCreateParams,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":6701,"to":6728}}}}],["1078",{"pageContent":"async grabCollectionFields(): Promise<void> {\nif (!this.collectionName) {\nthrow new Error(\"Need collection name to grab collection fields\");\n}\nif (\nthis.primaryField &&\nthis.vectorField &&\nthis.textField &&\nthis.fields.length > 0\n) {\nreturn;\n}\nconst desc = await this.colMgr.describeCollection({\ncollection_name: this.collectionName,\n});\ndesc.schema.fields.forEach((field) => {\nthis.fields.push(field.name);\nif (field.autoID) {\nconst index = this.fields.indexOf(field.name);\nif (index !== -1) {\nthis.fields.splice(index, 1);\n}\n}\nif (field.is_primary_key) {\nthis.primaryField = field.name;\n}\nconst dtype = DataTypeMap[field.data_type.toLowerCase()];\nif (dtype === DataType.FloatVector || dtype === DataType.BinaryVector) {\nthis.vectorField = field.name;\n}\n\nif (dtype === DataType.VarChar && field.name === MILVUS_TEXT_FIELD_NAME) {\nthis.textField = field.name;\n}\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":7228,"to":7263}}}}],["1079",{"pageContent":"static async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig?: {\ncollectionName?: string;\nurl?: string;\n}\n): Promise<Milvus> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn Milvus.fromDocuments(docs, embeddings, dbConfig);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig?: MilvusLibArgs\n): Promise<Milvus> {\nconst args: MilvusLibArgs = {\ncollectionName: dbConfig?.collectionName || genCollectionName(),\nurl: dbConfig?.url,\n};\nconst instance = new this(embeddings, args);\nawait instance.addDocuments(docs);\nreturn instance;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":7744,"to":7777}}}}],["1080",{"pageContent":"static async fromExistingCollection(\nembeddings: Embeddings,\ndbConfig: MilvusLibArgs\n): Promise<Milvus> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.ensureCollection();\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":8259,"to":8267}}}}],["1081",{"pageContent":"createFieldTypeForMetadata(documents: Document[]): FieldType[] {\nconst sampleMetadata = documents[0].metadata;\nlet textFieldMaxLength = 0;\nlet jsonFieldMaxLength = 0;\ndocuments.forEach(({ metadata }) => {\n// check all keys name and count in metadata is same as sampleMetadata\nObject.keys(metadata).forEach((key) => {\nif (\n!(key in metadata) ||","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":8779,"to":8787}}}}],["1082",{"pageContent":"of metadata[key] !== typeof sampleMetadata[key]\n) {\nthrow new Error(\n\"All documents must have same metadata keys and datatype\"\n);\n}\n\n// find max length of string field and json field, cache json string value\nif (typeof metadata[key] === \"string\") {\nif (metadata[key].length > textFieldMaxLength) {\ntextFieldMaxLength = metadata[key].length;\n}\n} else if (typeof metadata[key] === \"object\") {\nconst json = JSON.stringify(metadata[key]);\nif (json.length > jsonFieldMaxLength) {\njsonFieldMaxLength = json.length;\n}\n}\n});\n});\n\nconst fields: FieldType[] = [];\nfor (const [key, value] of Object.entries(sampleMetadata)) {\nconst type = typeof value;\nif (type === \"string\") {\nfields.push({\nname: key,\ndescription: `Metadata String field`,\ndata_type: DataType.VarChar,","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":9294,"to":9322}}}}],["1083",{"pageContent":"_params: {\nmax_length: textFieldMaxLength.toString(),\n},\n});\n} else if (type === \"number\") {\nfields.push({\nname: key,\ndescription: `Metadata Number field`,\ndata_type: DataType.Float,\n});\n} else if (type === \"boolean\") {\nfields.push({\nname: key,\ndescription: `Metadata Boolean field`,\ndata_type: DataType.Bool,\n});\n} else if (value === null) {\n// skip\n} else {\n// use json for other types\ntry {\nfields.push({\nname: key,\ndescription: `Metadata JSON field`,\ndata_type: DataType.VarChar,\ntype_params: {\nmax_length: jsonFieldMaxLength.toString(),\n},\n});\n} catch (e) {\nthrow new Error(\"Failed to parse metadata field as JSON\");\n}\n}\n}\nreturn fields;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":9808,"to":9843}}}}],["1084",{"pageContent":"genCollectionName(): string {\nreturn `${MILVUS_COLLECTION_NAME_PREFIX}_${uuidv4().replaceAll(\"-\", \"\")}`;\n}\n\nfunction getTextFieldMaxLength(documents: Document[]) {\nlet textMaxLength = 0;\n// eslint-disable-next-line no-plusplus\nfor (let i = 0; i < documents.length; i++) {\nconst text = documents[i].pageContent;\nif (text.length > textMaxLength) {\ntextMaxLength = text.length;\n}\n}\nreturn textMaxLength;\n}\n\nfunction getVectorFieldDim(vectors: number[][]) {\nif (vectors.length === 0) {\nthrow new Error(\"No vectors found\");\n}\nreturn vectors[0].length;\n}\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nfunction checkJsonString(value: string): { isJson: boolean; obj: any } {\ntry {\nconst result = JSON.parse(value);\nreturn { isJson: true, obj: result };\n} catch (e) {\nreturn { isJson: false, obj: null };\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/milvus.ts","loc":{"lines":{"from":10338,"to":10369}}}}],["1085",{"pageContent":"import type {\nMongoClient,\nCollection,\nDocument as MongoDocument,\n} from \"mongodb\";\nimport { VectorStore } from \"./base.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\n\nexport type MongoLibArgs = {\nclient: MongoClient;\ncollection: Collection<MongoDocument>;\nindexName?: string;\n};\n\nexport type MongoVectorStoreQueryExtension = {\npostQueryPipelineSteps?: MongoDocument[];\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/mongo.ts","loc":{"lines":{"from":1,"to":18}}}}],["1086",{"pageContent":"class MongoVectorStore extends VectorStore {\ndeclare FilterType: MongoVectorStoreQueryExtension;\n\ncollection: Collection<MongoDocument>;\n\nclient: MongoClient;\n\nindexName: string;\n\nconstructor(embeddings: Embeddings, args: MongoLibArgs) {\nsuper(embeddings, args);\nthis.collection = args.collection;\nthis.client = args.client;\nthis.indexName = args.indexName || \"default\";\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nconst items = vectors.map((embedding, idx) => ({\ncontent: documents[idx].pageContent,\nembedding,\nmetadata: documents[idx].metadata,\n}));\n\nawait this.collection.insertMany(items);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/mongo.ts","loc":{"lines":{"from":152,"to":184}}}}],["1087",{"pageContent":"async similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: MongoVectorStoreQueryExtension\n): Promise<[Document, number][]> {\n// Search has to be the first pipeline step (https://www.mongodb.com/docs/atlas/atlas-search/query-syntax/#behavior)\n// We hopefully this changes in the future\nconst pipeline: MongoDocument[] = [\n{\n$search: {\nindex: this.indexName,\nknnBeta: {\npath: \"embedding\",\nvector: query,\nk,\n},\n},\n},\n];\n\n// apply any post-query pipeline steps (idk how useful the option to do this is in practice)\nif (filter?.postQueryPipelineSteps) {\npipeline.push(...filter.postQueryPipelineSteps);\n}\n\npipeline.push({\n$project: {\n_id: 0,\ncontent: 1,\nmetadata: 1,\nsimilarity: {\n$arrayElemAt: [\"$knnBeta.similarity\", 0],\n},\n},\n});\n\nconst results = this.collection.aggregate(pipeline);\n\nconst ret: [Document, number][] = [];\n\nfor await (const result of results) {\nret.push([\nnew Document({\npageContent: result.content,\nmetadata: result.metadata,\n}),\nresult.similarity,\n]);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/mongo.ts","loc":{"lines":{"from":302,"to":350}}}}],["1088",{"pageContent":"// Attempt to warn if it appears that the indexing failed\nif (\nret.length === 0 &&\nk > 0 &&\nfilter?.postQueryPipelineSteps === undefined\n) {\n// check for existence of documents (if nothing is there we should expect no results)\nconst count = await this.collection.countDocuments();\n\nif (count !== 0) {\nconsole.warn(\n\"MongoDB search query returned no results where results were expected:\\n\" +\n\"This may be because the index is improperly configured or because the indexing over recently added documents has not yet completed.\"\n);\n}\n}\n\nreturn ret;\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig: MongoLibArgs\n): Promise<MongoVectorStore> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn MongoVectorStore.fromDocuments(docs, embeddings, dbConfig);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/mongo.ts","loc":{"lines":{"from":463,"to":499}}}}],["1089",{"pageContent":"static async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: MongoLibArgs\n): Promise<MongoVectorStore> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/mongo.ts","loc":{"lines":{"from":612,"to":621}}}}],["1090",{"pageContent":"import { Client, RequestParams, errors } from \"@opensearch-project/opensearch\";\nimport { v4 as uuid } from \"uuid\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\nimport { VectorStore } from \"./base.js\";\n\ntype OpenSearchEngine = \"nmslib\" | \"hnsw\";\ntype OpenSearchSpaceType = \"l2\" | \"cosinesimil\" | \"ip\";\n\ninterface VectorSearchOptions {\nreadonly engine?: OpenSearchEngine;\nreadonly spaceType?: OpenSearchSpaceType;\nreadonly m?: number;\nreadonly efConstruction?: number;\nreadonly efSearch?: number;\n}\n\nexport interface OpenSearchClientArgs {\nreadonly client: Client;\nreadonly indexName?: string;\n\nreadonly vectorSearchOptions?: VectorSearchOptions;\n}\n\ntype OpenSearchFilter = object;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":1,"to":25}}}}],["1091",{"pageContent":"class OpenSearchVectorStore extends VectorStore {\ndeclare FilterType: OpenSearchFilter;\n\nprivate readonly client: Client;\n\nprivate readonly indexName: string;\n\nprivate readonly engine: OpenSearchEngine;\n\nprivate readonly spaceType: OpenSearchSpaceType;\n\nprivate readonly efConstruction: number;\n\nprivate readonly efSearch: number;\n\nprivate readonly m: number;\n\nconstructor(embeddings: Embeddings, args: OpenSearchClientArgs) {\nsuper(embeddings, args);\n\nthis.spaceType = args.vectorSearchOptions?.spaceType ?? \"l2\";\nthis.engine = args.vectorSearchOptions?.engine ?? \"nmslib\";\nthis.m = args.vectorSearchOptions?.m ?? 16;\nthis.efConstruction = args.vectorSearchOptions?.efConstruction ?? 512;\nthis.efSearch = args.vectorSearchOptions?.efSearch ?? 512;\n\nthis.client = args.client;\nthis.indexName = args.indexName ?? \"documents\";\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":240,"to":268}}}}],["1092",{"pageContent":"async addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nawait this.ensureIndexExists(\nvectors[0].length,\nthis.engine,\nthis.spaceType,\nthis.efSearch,\nthis.efConstruction,\nthis.m\n);\nconst operations = vectors.flatMap((embedding, idx) => [\n{\nindex: {\n_index: this.indexName,\n_id: uuid(),\n},\n},\n{\nembedding,\nmetadata: documents[idx].metadata,\ntext: documents[idx].pageContent,\n},\n]);\nawait this.client.bulk({ body: operations });\nawait this.client.indices.refresh({ index: this.indexName });\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":480,"to":512}}}}],["1093",{"pageContent":"async similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: OpenSearchFilter | undefined\n): Promise<[Document, number][]> {\nconst search: RequestParams.Search = {\nindex: this.indexName,\nbody: {\nquery: {\nbool: {\nfilter: { bool: { must: this.buildMetadataTerms(filter) } },\nmust: [\n{\nknn: {\nembedding: { vector: query, k },\n},\n},\n],\n},\n},\nsize: k,\n},\n};\n\nconst { body } = await this.client.search(search);\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nreturn body.hits.hits.map((hit: any) => [\nnew Document({\npageContent: hit._source.text,\nmetadata: hit._source.metadata,\n}),\nhit._score,\n]);\n}\n\nstatic fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\nargs: OpenSearchClientArgs\n): Promise<OpenSearchVectorStore> {\nconst documents = texts.map((text, idx) => {\nconst metadata = Array.isArray(metadatas) ? metadatas[idx] : metadatas;\nreturn new Document({ pageContent: text, metadata });\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":727,"to":772}}}}],["1094",{"pageContent":"return OpenSearchVectorStore.fromDocuments(documents, embeddings, args);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: OpenSearchClientArgs\n): Promise<OpenSearchVectorStore> {\nconst store = new OpenSearchVectorStore(embeddings, dbConfig);\nawait store.addDocuments(docs).then(() => store);\nreturn store;\n}\n\nstatic async fromExistingIndex(\nembeddings: Embeddings,\ndbConfig: OpenSearchClientArgs\n): Promise<OpenSearchVectorStore> {\nconst store = new OpenSearchVectorStore(embeddings, dbConfig);\nawait store.client.cat.indices({ index: store.indexName });\nreturn store;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":978,"to":998}}}}],["1095",{"pageContent":"private async ensureIndexExists(\ndimension: number,\nengine = \"nmslib\",\nspaceType = \"l2\",\nefSearch = 512,\nefConstruction = 512,\nm = 16\n): Promise<void> {\nconst body = {\nsettings: {\nindex: {\nnumber_of_shards: 5,\nnumber_of_replicas: 1,\nknn: true,\n\"knn.algo_param.ef_search\": efSearch,\n},\n},\nmappings: {\ndynamic_templates: [\n{\n// map all metadata properties to be keyword\n\"metadata.*\": {\nmatch_mapping_type: \"*\",\nmapping: { type: \"keyword\" },\n},\n},\n],\nproperties: {\ntext: { type: \"text\" },\nmetadata: { type: \"object\" },\nembedding: {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":1219,"to":1249}}}}],["1096",{"pageContent":": \"knn_vector\",\ndimension,\nmethod: {\nname: \"hnsw\",\nengine,\nspace_type: spaceType,\nparameters: { ef_construction: efConstruction, m },\n},\n},\n},\n},\n};\n\nconst indexExists = await this.doesIndexExist();\nif (indexExists) return;\n\nawait this.client.indices.create({ index: this.indexName, body });\n}\n\nprivate buildMetadataTerms(\nfilter?: OpenSearchFilter\n): { term: Record<string, unknown> }[] {\nif (filter == null) return [];\nconst result = [];\nfor (const [key, value] of Object.entries(filter)) {\nresult.push({ term: { [`metadata.${key}`]: value } });\n}\nreturn result;\n}\n\nasync doesIndexExist(): Promise<boolean> {\ntry {\nawait this.client.cat.indices({ index: this.indexName });\nreturn true;\n} catch (err: unknown) {\n// eslint-disable-next-line no-instanceof/no-instanceof\nif (err instanceof errors.ResponseError && err.statusCode === 404) {\nreturn false;\n}\nthrow err;\n}\n}\n\nasync deleteIfExists(): Promise<void> {\nconst indexExists = await this.doesIndexExist();\nif (!indexExists) return;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":1474,"to":1519}}}}],["1097",{"pageContent":"await this.client.indices.delete({ index: this.indexName });\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/opensearch.ts","loc":{"lines":{"from":1723,"to":1725}}}}],["1098",{"pageContent":"import { v4 as uuidv4 } from \"uuid\";\nimport flatten from \"flat\";\n\nimport { VectorStore } from \"./base.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\n\n// eslint-disable-next-line @typescript-eslint/ban-types, @typescript-eslint/no-explicit-any\ntype PineconeMetadata = Record<string, any>;\n\ntype VectorOperationsApi = ReturnType<\nimport(\"@pinecone-database/pinecone\").PineconeClient[\"Index\"]\n>;\n\nexport interface PineconeLibArgs {\npineconeIndex: VectorOperationsApi;\ntextKey?: string;\nnamespace?: string;\nfilter?: PineconeMetadata;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":1,"to":20}}}}],["1099",{"pageContent":"class PineconeStore extends VectorStore {\ndeclare FilterType: PineconeMetadata;\n\ntextKey: string;\n\nnamespace?: string;\n\npineconeIndex: VectorOperationsApi;\n\nfilter?: PineconeMetadata;\n\nconstructor(embeddings: Embeddings, args: PineconeLibArgs) {\nsuper(embeddings, args);\n\nthis.embeddings = embeddings;\nthis.namespace = args.namespace;\nthis.pineconeIndex = args.pineconeIndex;\nthis.textKey = args.textKey ?? \"text\";\nthis.filter = args.filter;\n}\n\nasync addDocuments(documents: Document[], ids?: string[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments,\nids\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":188,"to":216}}}}],["1100",{"pageContent":"async addVectors(\nvectors: number[][],\ndocuments: Document[],\nids?: string[]\n): Promise<void> {\nconst documentIds = ids == null ? documents.map(() => uuidv4()) : ids;\nconst pineconeVectors = vectors.map((values, idx) => {\n// Pinecone doesn't support nested objects, so we flatten them\nconst metadata: {\n[key: string]: string | number | boolean | null;\n} = flatten({\n...documents[idx].metadata,\n[this.textKey]: documents[idx].pageContent,\n});\n// Pinecone doesn't support null values, so we remove them\nfor (const key of Object.keys(metadata)) {\nif (metadata[key] == null) {\ndelete metadata[key];\n} else if (","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":380,"to":398}}}}],["1101",{"pageContent":"of metadata[key] === \"object\" &&\nObject.keys(metadata[key] as unknown as object).length === 0\n) {\ndelete metadata[key];\n}\n}\nreturn {\nid: documentIds[idx],\nmetadata,\nvalues,\n};\n});\n\n// Pinecone recommends a limit of 100 vectors per upsert request\nconst chunkSize = 50;\nfor (let i = 0; i < pineconeVectors.length; i += chunkSize) {\nconst chunk = pineconeVectors.slice(i, i + chunkSize);\nawait this.pineconeIndex.upsert({\nupsertRequest: {\nvectors: chunk,\nnamespace: this.namespace,\n},\n});\n}\n}\n\nasync similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: PineconeMetadata\n): Promise<[Document, number][]> {\nif (filter && this.filter) {\nthrow new Error(\"cannot provide both `filter` and `this.filter`\");\n}\nconst _filter = filter ?? this.filter;\nconst results = await this.pineconeIndex.query({\nqueryRequest: {\nincludeMetadata: true,\nnamespace: this.namespace,\ntopK: k,\nvector: query,\nfilter: _filter,\n},\n});\n\nconst result: [Document, number][] = [];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":563,"to":608}}}}],["1102",{"pageContent":"if (results.matches) {\nfor (const res of results.matches) {\nconst { [this.textKey]: pageContent, ...metadata } = (res.metadata ??\n{}) as PineconeMetadata;\nif (res.score) {\nresult.push([new Document({ metadata, pageContent }), res.score]);\n}\n}\n}\n\nreturn result;\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig:\n| {\n/**\n* @deprecated Use pineconeIndex instead\n*/\npineconeClient: VectorOperationsApi;\ntextKey?: string;\nnamespace?: string | undefined;\n}\n| PineconeLibArgs\n): Promise<PineconeStore> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":758,"to":794}}}}],["1103",{"pageContent":"const args: PineconeLibArgs = {\npineconeIndex:\n\"pineconeIndex\" in dbConfig\n? dbConfig.pineconeIndex\n: dbConfig.pineconeClient,\ntextKey: dbConfig.textKey,\nnamespace: dbConfig.namespace,\n};\nreturn PineconeStore.fromDocuments(docs, embeddings, args);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: PineconeLibArgs\n): Promise<PineconeStore> {\nconst args = dbConfig;\nargs.textKey = dbConfig.textKey ?? \"text\";\n\nconst instance = new this(embeddings, args);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nstatic async fromExistingIndex(\nembeddings: Embeddings,\ndbConfig: PineconeLibArgs\n): Promise<PineconeStore> {\nconst instance = new this(embeddings, dbConfig);\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/pinecone.ts","loc":{"lines":{"from":950,"to":981}}}}],["1104",{"pageContent":"import { VectorStore } from \"./base.js\";\nimport { Document } from \"../document.js\";\nimport { type Embeddings } from \"../embeddings/base.js\";\n\nconst IdColumnSymbol = Symbol(\"id\");\nconst ContentColumnSymbol = Symbol(\"content\");\n\ntype ColumnSymbol = typeof IdColumnSymbol | typeof ContentColumnSymbol;\n\ndeclare type Value = unknown;\ndeclare type RawValue = Value | Sql;\n\ndeclare class Sql {\nstrings: string[];\n\nconstructor(\nrawStrings: ReadonlyArray<string>,\nrawValues: ReadonlyArray<RawValue>\n);\n}\n\ntype PrismaNamespace = {\nModelName: Record<string, string>;\nSql: typeof Sql;\nraw: (sql: string) => Sql;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":1,"to":26}}}}],["1105",{"pageContent":"PrismaClient = {\n$queryRaw<T = unknown>(\nquery: TemplateStringsArray | Sql,\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n...values: any[]\n): Promise<T>;\n$executeRaw(\nquery: TemplateStringsArray | Sql,\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n...values: any[]\n): // eslint-disable-next-line @typescript-eslint/no-explicit-any\nPromise<any>;\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n$transaction<P extends Promise<any>[]>(arg: [...P]): Promise<any>;\n};\n\ntype ObjectIntersect<A, B> = {\n[P in keyof A & keyof B]: A[P] | B[P];\n};\n\ntype ModelColumns<TModel extends Record<string, unknown>> = {\n[K in keyof TModel]?: true | ColumnSymbol;\n};\n\ntype SimilarityModel<\nTModel extends Record<string, unknown> = Record<string, unknown>,\nTColumns extends ModelColumns<TModel> = ModelColumns<TModel>\n> = Pick<TModel, keyof ObjectIntersect<TModel, TColumns>> & {\n_distance: number | null;\n};","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":320,"to":350}}}}],["1106",{"pageContent":"DefaultPrismaVectorStore = PrismaVectorStore<\nRecord<string, unknown>,\nstring,\nModelColumns<Record<string, unknown>>\n>;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":633,"to":637}}}}],["1107",{"pageContent":"class PrismaVectorStore<\nTModel extends Record<string, unknown>,\nTModelName extends string,\nTSelectModel extends ModelColumns<TModel>\n> extends VectorStore {\ntableSql: Sql;\n\nvectorColumnSql: Sql;\n\nselectSql: Sql;\n\nidColumn: keyof TModel & string;\n\ncontentColumn: keyof TModel & string;\n\nstatic IdColumn: typeof IdColumnSymbol = IdColumnSymbol;\n\nstatic ContentColumn: typeof ContentColumnSymbol = ContentColumnSymbol;\n\nprotected db: PrismaClient;\n\nprotected Prisma: PrismaNamespace;\n\nconstructor(\nembeddings: Embeddings,\nconfig: {\ndb: PrismaClient;\nprisma: PrismaNamespace;\ntableName: TModelName;\nvectorColumnName: string;\ncolumns: TSelectModel;\n}\n) {\nsuper(embeddings, {});\n\nthis.Prisma = config.prisma;\nthis.db = config.db;\n\nconst entries = Object.entries(config.columns);\nconst idColumn = entries.find((i) => i[1] === IdColumnSymbol)?.[0];\nconst contentColumn = entries.find(\n(i) => i[1] === ContentColumnSymbol\n)?.[0];","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":953,"to":995}}}}],["1108",{"pageContent":"if (idColumn == null) throw new Error(\"Missing ID column\");\nif (contentColumn == null) throw new Error(\"Missing content column\");\n\nthis.idColumn = idColumn;\nthis.contentColumn = contentColumn;\n\nthis.tableSql = this.Prisma.raw(`\"${config.tableName}\"`);\nthis.vectorColumnSql = this.Prisma.raw(`\"${config.vectorColumnName}\"`);\n\nthis.selectSql = this.Prisma.raw(\nentries\n.map(([key, alias]) => (alias && key) || null)\n.filter((x): x is string => !!x)\n.map((key) => `\"${key}\"`)\n.join(\", \")\n);\n}\n\nstatic withModel<TModel extends Record<string, unknown>>(db: PrismaClient) {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":1278,"to":1296}}}}],["1109",{"pageContent":"create<\nTPrisma extends PrismaNamespace,\nTColumns extends ModelColumns<TModel>\n>(\nembeddings: Embeddings,\nconfig: {\nprisma: TPrisma;\ntableName: keyof TPrisma[\"ModelName\"] & string;\nvectorColumnName: string;\ncolumns: TColumns;\n}\n) {\ntype ModelName = keyof TPrisma[\"ModelName\"] & string;\nreturn new PrismaVectorStore<TModel, ModelName, TColumns>(embeddings, {\n...config,\ndb,\n});\n}\n\nasync function fromTexts<\nTPrisma extends PrismaNamespace,\nTColumns extends ModelColumns<TModel>\n>(\ntexts: string[],\nmetadatas: TModel[],\nembeddings: Embeddings,\ndbConfig: {\nprisma: TPrisma;\ntableName: keyof TPrisma[\"ModelName\"] & string;\nvectorColumnName: string;\ncolumns: TColumns;\n}\n) {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\n\nreturn PrismaVectorStore.fromDocuments(docs, embeddings, {\n...dbConfig,\ndb,\n});\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":1592,"to":1639}}}}],["1110",{"pageContent":"fromDocuments<\nTPrisma extends PrismaNamespace,\nTColumns extends ModelColumns<TModel>\n>(\ndocs: Document<TModel>[],\nembeddings: Embeddings,\ndbConfig: {\nprisma: TPrisma;\ntableName: keyof TPrisma[\"ModelName\"] & string;\nvectorColumnName: string;\ncolumns: TColumns;\n}\n) {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":1920,"to":1932}}}}],["1111",{"pageContent":"ModelName = keyof TPrisma[\"ModelName\"] & string;\nconst instance = new PrismaVectorStore<TModel, ModelName, TColumns>(\nembeddings,\n{ ...dbConfig, db }\n);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nreturn { create, fromTexts, fromDocuments };\n}\n\nasync addModels(models: TModel[]) {\nreturn this.addDocuments(\nmodels.map((metadata) => {\nconst pageContent = metadata[this.contentColumn];\nif (typeof pageContent !== \"string\")\nthrow new Error(\"Content column must be a string\");\nreturn new Document({ pageContent, metadata });\n})\n);\n}\n\nasync addDocuments(documents: Document<TModel>[]) {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document<TModel>[]) {\nconst idSql = this.Prisma.raw(`\"${this.idColumn}\"`);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":2243,"to":2275}}}}],["1112",{"pageContent":"await this.db.$transaction(\nvectors.map(\n(vector, idx) => this.db.$executeRaw`\nUPDATE ${this.tableSql}\nSET ${this.vectorColumnSql} = ${`[${vector.join(\",\")}]`}::vector\nWHERE ${idSql} = ${documents[idx].metadata[this.idColumn]}\n`\n)\n);\n}\n\nasync similaritySearch(\nquery: string,\nk = 4\n): Promise<Document<SimilarityModel<TModel, TSelectModel>>[]> {\nconst results = await this.similaritySearchVectorWithScore(\nawait this.embeddings.embedQuery(query),\nk\n);\n\nreturn results.map((result) => result[0]);\n}\n\nasync similaritySearchVectorWithScore(\nquery: number[],\nk: number\n): Promise<[Document<SimilarityModel<TModel, TSelectModel>>, number][]> {\nconst vectorQuery = `[${query.join(\",\")}]`;\nconst articles = await this.db.$queryRaw<\nArray<SimilarityModel<TModel, TSelectModel>>\n>`\nSELECT ${this.selectSql}, ${this.vectorColumnSql} <=> ${vectorQuery}::vector as \"_distance\" \nFROM ${this.tableSql}\nORDER BY \"_distance\" ASC\nLIMIT ${k};\n`;","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":2559,"to":2594}}}}],["1113",{"pageContent":"const results: [Document<SimilarityModel<TModel, TSelectModel>>, number][] =\n[];\nfor (const article of articles) {\nif (article._distance != null && article[this.contentColumn] != null) {\nresults.push([\nnew Document({\npageContent: article[this.contentColumn] as string,\nmetadata: article,\n}),\narticle._distance,\n]);\n}\n}\n\nreturn results;\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[],\nembeddings: Embeddings,\ndbConfig: {\ndb: PrismaClient;\nprisma: PrismaNamespace;\ntableName: string;\nvectorColumnName: string;\ncolumns: ModelColumns<Record<string, unknown>>;\n}\n): Promise<DefaultPrismaVectorStore> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\n\nreturn PrismaVectorStore.fromDocuments(docs, embeddings, dbConfig);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":2877,"to":2917}}}}],["1114",{"pageContent":"static async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: {\ndb: PrismaClient;\nprisma: PrismaNamespace;\ntableName: string;\nvectorColumnName: string;\ncolumns: ModelColumns<Record<string, unknown>>;\n}\n): Promise<DefaultPrismaVectorStore> {\nconst instance = new PrismaVectorStore(embeddings, dbConfig);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/prisma.ts","loc":{"lines":{"from":3200,"to":3215}}}}],["1115",{"pageContent":"import type { SupabaseClient } from \"@supabase/supabase-js\";\nimport { VectorStore } from \"./base.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\n\ninterface SearchEmbeddingsParams {\nquery_embedding: number[];\nmatch_count: number; // int\n}\n\ninterface SearchEmbeddingsResponse {\nid: number;\ncontent: string;\nmetadata: object;\nsimilarity: number;\n}\n\nexport interface SupabaseLibArgs {\nclient: SupabaseClient;\ntableName?: string;\nqueryName?: string;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/supabase.ts","loc":{"lines":{"from":1,"to":22}}}}],["1116",{"pageContent":"class SupabaseVectorStore extends VectorStore {\nclient: SupabaseClient;\n\ntableName: string;\n\nqueryName: string;\n\nconstructor(embeddings: Embeddings, args: SupabaseLibArgs) {\nsuper(embeddings, args);\n\nthis.client = args.client;\nthis.tableName = args.tableName || \"documents\";\nthis.queryName = args.queryName || \"match_documents\";\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nconst texts = documents.map(({ pageContent }) => pageContent);\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(texts),\ndocuments\n);\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nconst rows = vectors.map((embedding, idx) => ({\ncontent: documents[idx].pageContent,\nembedding,\nmetadata: documents[idx].metadata,\n}));","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/supabase.ts","loc":{"lines":{"from":138,"to":166}}}}],["1117",{"pageContent":"// upsert returns 500/502/504 (yes really any of them) if given too many rows/characters\n// ~2000 trips it, but my data is probably smaller than average pageContent and metadata\nconst chunkSize = 500;\nfor (let i = 0; i < rows.length; i += chunkSize) {\nconst chunk = rows.slice(i, i + chunkSize);\n\nconst res = await this.client.from(this.tableName).insert(chunk);\nif (res.error) {\nthrow new Error(\n`Error inserting: ${res.error.message} ${res.status} ${res.statusText}`\n);\n}\n}\n}\n\nasync similaritySearchVectorWithScore(\nquery: number[],\nk: number\n): Promise<[Document, number][]> {\nconst matchDocumentsParams: SearchEmbeddingsParams = {\nquery_embedding: query,\nmatch_count: k,\n};\n\nconst { data: searches, error } = await this.client.rpc(\nthis.queryName,\nmatchDocumentsParams\n);\n\nif (error) {\nthrow new Error(\n`Error searching for documents: ${error.code} ${error.message} ${error.details}`\n);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/supabase.ts","loc":{"lines":{"from":269,"to":302}}}}],["1118",{"pageContent":"const result: [Document, number][] = (\nsearches as SearchEmbeddingsResponse[]\n).map((resp) => [\nnew Document({\nmetadata: resp.metadata,\npageContent: resp.content,\n}),\nresp.similarity,\n]);\n\nreturn result;\n}\n\nstatic async fromTexts(\ntexts: string[],\nmetadatas: object[] | object,\nembeddings: Embeddings,\ndbConfig: SupabaseLibArgs\n): Promise<SupabaseVectorStore> {\nconst docs = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn SupabaseVectorStore.fromDocuments(docs, embeddings, dbConfig);\n}\n\nstatic async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\ndbConfig: SupabaseLibArgs\n): Promise<SupabaseVectorStore> {\nconst instance = new this(embeddings, dbConfig);\nawait instance.addDocuments(docs);\nreturn instance;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/supabase.ts","loc":{"lines":{"from":400,"to":439}}}}],["1119",{"pageContent":"static async fromExistingIndex(\nembeddings: Embeddings,\ndbConfig: SupabaseLibArgs\n): Promise<SupabaseVectorStore> {\nconst instance = new this(embeddings, dbConfig);\nreturn instance;\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/supabase.ts","loc":{"lines":{"from":537,"to":544}}}}],["1120",{"pageContent":"import { test, expect } from \"@jest/globals\";\n\nimport { Chroma } from \"../chroma.js\";\n\n// We'd want a much more thorough test here,\n// but sadly Chroma isn't very easy to test locally at the moment.\ntest(\"Chroma imports correctly\", async () => {\nconst { ChromaClient } = await Chroma.imports();\n\nexpect(ChromaClient).toBeDefined();\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/chroma.test.ts","loc":{"lines":{"from":1,"to":11}}}}],["1121",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport * as fs from \"node:fs/promises\";\nimport * as path from \"node:path\";\nimport * as os from \"node:os\";\n\nimport { HNSWLib } from \"../hnswlib.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { Document } from \"../../document.js\";\n\ntest(\"Test HNSWLib.fromTexts\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }],\nnew OpenAIEmbeddings()\n);\nexpect(vectorStore.index?.getCurrentCount()).toBe(3);\n\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconst resultOneMetadatas = resultOne.map(({ metadata }) => metadata);\nexpect(resultOneMetadatas).toEqual([{ id: 2 }]);\n\nconst resultTwo = await vectorStore.similaritySearch(\"hello world\", 3);\nconst resultTwoMetadatas = resultTwo.map(({ metadata }) => metadata);\nexpect(resultTwoMetadatas).toEqual([{ id: 2 }, { id: 3 }, { id: 1 }]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.int.test.ts","loc":{"lines":{"from":1,"to":25}}}}],["1122",{"pageContent":"test(\"Test HNSWLib.fromTexts + addDocuments\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }],\nnew OpenAIEmbeddings()\n);\nexpect(vectorStore.index?.getMaxElements()).toBe(3);\nexpect(vectorStore.index?.getCurrentCount()).toBe(3);\n\nawait vectorStore.addDocuments([\nnew Document({\npageContent: \"hello worldklmslksmn\",\nmetadata: { id: 4 },\n}),\n]);\nexpect(vectorStore.index?.getMaxElements()).toBe(4);\n\nconst resultTwo = await vectorStore.similaritySearch(\"hello world\", 3);\nconst resultTwoMetadatas = resultTwo.map(({ metadata }) => metadata);\nexpect(resultTwoMetadatas).toEqual([{ id: 2 }, { id: 3 }, { id: 4 }]);\n});\n\ntest(\"Test HNSWLib.load and HNSWLib.save\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\", \"Bye bye\", \"hello nice world\"],\n[{ id: 2 }, { id: 1 }, { id: 3 }],\nnew OpenAIEmbeddings()\n);\nexpect(vectorStore.index?.getCurrentCount()).toBe(3);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.int.test.ts","loc":{"lines":{"from":89,"to":117}}}}],["1123",{"pageContent":"const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconst resultOneMetadatas = resultOne.map(({ metadata }) => metadata);\nexpect(resultOneMetadatas).toEqual([{ id: 2 }]);\n\nconst resultTwo = await vectorStore.similaritySearch(\"hello world\", 3);\nconst resultTwoMetadatas = resultTwo.map(({ metadata }) => metadata);\nexpect(resultTwoMetadatas).toEqual([{ id: 2 }, { id: 3 }, { id: 1 }]);\n\nconst tempDirectory = await fs.mkdtemp(path.join(os.tmpdir(), \"lcjs-\"));\n\nconsole.log(tempDirectory);\n\nawait vectorStore.save(tempDirectory);\n\nconst loadedVectorStore = await HNSWLib.load(\ntempDirectory,\nnew OpenAIEmbeddings()\n);\n\nconst resultThree = await loadedVectorStore.similaritySearch(\n\"hello world\",\n1\n);\n\nconst resultThreeMetadatas = resultThree.map(({ metadata }) => metadata);\nexpect(resultThreeMetadatas).toEqual([{ id: 2 }]);\n\nconst resultFour = await loadedVectorStore.similaritySearch(\"hello world\", 3);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.int.test.ts","loc":{"lines":{"from":181,"to":208}}}}],["1124",{"pageContent":"const resultFourMetadatas = resultFour.map(({ metadata }) => metadata);\nexpect(resultFourMetadatas).toEqual([{ id: 2 }, { id: 3 }, { id: 1 }]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.int.test.ts","loc":{"lines":{"from":272,"to":274}}}}],["1125",{"pageContent":"import { test, expect } from \"@jest/globals\";\nimport { HNSWLib } from \"../hnswlib.js\";\nimport { Document } from \"../../document.js\";\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";\n\ntest(\"Test HNSWLib.fromTexts + addVectors\", async () => {\nconst vectorStore = await HNSWLib.fromTexts(\n[\"Hello world\"],\n[{ id: 2 }],\nnew FakeEmbeddings()\n);\nexpect(vectorStore.index?.getMaxElements()).toBe(1);\nexpect(vectorStore.index?.getCurrentCount()).toBe(1);\n\nawait vectorStore.addVectors(\n[\n[0, 1, 0, 0],\n[1, 0, 0, 0],\n[0.5, 0.5, 0.5, 0.5],\n],\n[\nnew Document({\npageContent: \"hello bye\",\nmetadata: { id: 5 },\n}),\nnew Document({\npageContent: \"hello worlddwkldnsk\",\nmetadata: { id: 4 },\n}),\nnew Document({\npageContent: \"hello you\",\nmetadata: { id: 6 },\n}),\n]\n);\nexpect(vectorStore.index?.getMaxElements()).toBe(4);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.test.ts","loc":{"lines":{"from":1,"to":36}}}}],["1126",{"pageContent":"const resultTwo = await vectorStore.similaritySearchVectorWithScore(\n[1, 0, 0, 0],\n3\n);\nconst resultTwoMetadatas = resultTwo.map(([{ metadata }]) => metadata);\nexpect(resultTwoMetadatas).toEqual([{ id: 4 }, { id: 6 }, { id: 2 }]);\n});\n\ntest(\"Test HNSWLib metadata filtering\", async () => {\nconst pageContent = \"Hello world\";\n\nconst vectorStore = await HNSWLib.fromTexts(\n[pageContent, pageContent, pageContent],\n[{ id: 2 }, { id: 3 }, { id: 4 }],\nnew FakeEmbeddings()\n);\n\n// If the filter wasn't working, we'd get all 3 documents back\nconst results = await vectorStore.similaritySearch(\npageContent,\n3,\n(document) => document.metadata.id === 3\n);\n\nexpect(results).toEqual([new Document({ metadata: { id: 3 }, pageContent })]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/hnswlib.test.ts","loc":{"lines":{"from":69,"to":94}}}}],["1127",{"pageContent":"import { test, expect } from \"@jest/globals\";\n\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { Document } from \"../../document.js\";\nimport { MemoryVectorStore } from \"../memory.js\";\n\ntest(\"MemoryVectorStore with external ids\", async () => {\nconst embeddings = new OpenAIEmbeddings();\n\nconst store = new MemoryVectorStore(embeddings);\n\nexpect(store).toBeDefined();\n\nawait store.addDocuments([\n{ pageContent: \"hello\", metadata: { a: 1 } },\n{ pageContent: \"hi\", metadata: { a: 1 } },\n{ pageContent: \"bye\", metadata: { a: 1 } },\n{ pageContent: \"what's this\", metadata: { a: 1 } },\n]);\n\nconst results = await store.similaritySearch(\"hello\", 1);\n\nexpect(results).toHaveLength(1);\n\nexpect(results).toEqual([\nnew Document({ metadata: { a: 1 }, pageContent: \"hello\" }),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/memory.int.test.ts","loc":{"lines":{"from":1,"to":28}}}}],["1128",{"pageContent":"import { test, expect, afterAll, beforeAll } from \"@jest/globals\";\nimport { ErrorCode } from \"@zilliz/milvus2-sdk-node/dist/milvus/types.js\";\nimport { MilvusClient } from \"@zilliz/milvus2-sdk-node/dist/milvus/index.js\";\n\nimport { Milvus } from \"../milvus.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/index.js\";\n\nlet collectionName: string;\nlet embeddings: OpenAIEmbeddings;\n\nbeforeAll(async () => {\nembeddings = new OpenAIEmbeddings();\ncollectionName = `test_collection_${Math.random().toString(36).substring(7)}`;\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/milvus.int.test.ts","loc":{"lines":{"from":1,"to":14}}}}],["1129",{"pageContent":"test.skip(\"Test Milvus.fromtext\", async () => {\nconst texts = [\n`Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little \nHarmonic Labyrinth of the dreaded Majotaur?`,\n\"Achilles: Yiikes! What is that?\",\n`Tortoise: They say-although I person never believed it myself-that an I \nMajotaur has created a tiny labyrinth sits in a pit in the middle of \nit, waiting innocent victims to get lost in its fears complexity. \nThen, when they wander and dazed into the center, he laughs and \nlaughs at them-so hard, that he laughs them to death!`,\n\"Achilles: Oh, no!\",\n\"Tortoise: But it's only a myth. Courage, Achilles.\",\n];\nconst objA = { A: { B: \"some string\" } };\nconst objB = { A: { B: \"some other string\" } };\nconst metadatas: object[] = [\n{ id: 2, other: objA },\n{ id: 1, other: objB },\n{ id: 3, other: objA },\n{ id: 4, other: objB },\n{ id: 5, other: objA },\n];\nconst milvus = await Milvus.fromTexts(texts, metadatas, embeddings, {\ncollectionName,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/milvus.int.test.ts","loc":{"lines":{"from":86,"to":110}}}}],["1130",{"pageContent":"const query = \"who is achilles?\";\nconst result = await milvus.similaritySearch(query, 1);\nconst resultMetadatas = result.map(({ metadata }) => metadata);\nexpect(resultMetadatas).toEqual([{ id: 1, other: objB }]);\n\nconst resultTwo = await milvus.similaritySearch(query, 3);\nconst resultTwoMetadatas = resultTwo.map(({ metadata }) => metadata);\nexpect(resultTwoMetadatas).toEqual([\n{ id: 1, other: objB },\n{ id: 4, other: objB },\n{ id: 5, other: objA },\n]);\n});\n\ntest.skip(\"Test Milvus.fromExistingCollection\", async () => {\nconst milvus = await Milvus.fromExistingCollection(embeddings, {\ncollectionName,\n});\n\nconst query = \"who is achilles?\";\nconst result = await milvus.similaritySearch(query, 1);\nconst resultMetadatas = result.map(({ metadata }) => metadata);\nexpect(resultMetadatas.length).toBe(1);\nexpect(resultMetadatas[0].id).toEqual(1);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/milvus.int.test.ts","loc":{"lines":{"from":172,"to":195}}}}],["1131",{"pageContent":"const resultTwo = await milvus.similaritySearch(query, 3);\nconst resultTwoMetadatas = resultTwo.map(({ metadata }) => metadata);\nexpect(resultTwoMetadatas.length).toBe(3);\nexpect(resultTwoMetadatas[0].id).toEqual(1);\nexpect(resultTwoMetadatas[1].id).toEqual(4);\nexpect(resultTwoMetadatas[2].id).toEqual(5);\n});\n\nafterAll(async () => {\n// eslint-disable-next-line no-process-env\nif (!process.env.MILVUS_URL) return;\n// eslint-disable-next-line no-process-env\nconst client = new MilvusClient(process.env.MILVUS_URL as string);\nconst dropRes = await client.collectionManager.dropCollection({\ncollection_name: collectionName,\n});\n// console.log(\"Drop collection response: \", dropRes)\nexpect(dropRes.error_code).toBe(ErrorCode.SUCCESS);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/milvus.int.test.ts","loc":{"lines":{"from":258,"to":276}}}}],["1132",{"pageContent":"/* eslint-disable no-process-env */\n/* eslint-disable no-promise-executor-return */\n\nimport { test, expect } from \"@jest/globals\";\nimport { MongoClient } from \"mongodb\";\nimport { CohereEmbeddings } from \"../../embeddings/cohere.js\";\nimport { MongoVectorStore, MongoVectorStoreQueryExtension } from \"../mongo.js\";\n\nimport { Document } from \"../../document.js\";\n\n/**\n* The following json can be used to create an index in atlas for cohere embeddings:\n\n{\n\"mappings\": {\n\"fields\": {\n\"embedding\": [\n{\n\"dimensions\": 1024,\n\"similarity\": \"euclidean\",\n\"type\": \"knnVector\"\n}\n]\n}\n}\n}\n\n*/\n\ntest.skip(\"MongoVectorStore with external ids\", async () => {\nexpect(process.env.MONGO_URI).toBeDefined();\n\n// eslint-disable-next-line @typescript-eslint/no-non-null-assertion\nconst client = new MongoClient(process.env.MONGO_URI!);\n\ntry {\nconst collection = client.db(\"langchain\").collection(\"test\");","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/mongo.int.test.ts","loc":{"lines":{"from":1,"to":37}}}}],["1133",{"pageContent":"const vectorStore = new MongoVectorStore(new CohereEmbeddings(), {\nclient,\ncollection,\n// indexName: \"default\", // make sure that this matches the index name in atlas if not using \"default\"\n});\n\nexpect(vectorStore).toBeDefined();\n\n// check if the database is empty\nconst count = await collection.countDocuments();\n\nconst justInserted = count === 0;\nif (justInserted) {\nawait vectorStore.addDocuments([\n{ pageContent: \"Dogs are tough.\", metadata: { a: 1 } },\n{ pageContent: \"Cats have fluff.\", metadata: { b: 1 } },\n{ pageContent: \"What is a sandwich?\", metadata: { c: 1 } },\n{ pageContent: \"That fence is purple.\", metadata: { d: 1, e: 2 } },\n]);\n}\n\n// This test is awkward because the index in atlas takes time to index new documents\n// This means from a fresh insert the query will return nothing\nlet triesLeft = 4;\n\nlet results: Document[] = [];\nwhile (triesLeft > 0) {\nresults = await vectorStore.similaritySearch(\"Sandwich\", 1);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/mongo.int.test.ts","loc":{"lines":{"from":106,"to":133}}}}],["1134",{"pageContent":"if (justInserted && results.length === 0 && triesLeft > 0) {\n// wait and try again in hopes that the indexing has finished\nawait new Promise((resolve) => setTimeout(resolve, 3000));\n}\n\ntriesLeft -= 1;\n}\n\nexpect(results).toEqual([\n{ pageContent: \"What is a sandwich?\", metadata: { c: 1 } },\n]);\n\n// we can filter the search with custom pipeline stages\nconst filter: MongoVectorStoreQueryExtension = {\npostQueryPipelineSteps: [\n{\n$match: {\n\"metadata.e\": { $exists: true },\n},\n},\n],\n};\n\nconst filteredResults = await vectorStore.similaritySearch(\n\"Sandwich\",\n4,\nfilter\n);\n\nexpect(filteredResults).toEqual([\n{ pageContent: \"That fence is purple.\", metadata: { d: 1, e: 2 } },\n]);\n} finally {\nawait client.close();\n}\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/mongo.int.test.ts","loc":{"lines":{"from":199,"to":234}}}}],["1135",{"pageContent":"/* eslint-disable no-process-env */\nimport { test, expect } from \"@jest/globals\";\nimport { Client } from \"@opensearch-project/opensearch\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { OpenSearchVectorStore } from \"../opensearch.js\";\nimport { Document } from \"../../document.js\";\n\ntest.skip(\"OpenSearchVectorStore integration\", async () => {\nif (!process.env.OPENSEARCH_URL) {\nthrow new Error(\"OPENSEARCH_URL not set\");\n}\n\nconst client = new Client({\nnodes: [process.env.OPENSEARCH_URL],\n});\n\nconst indexName = \"test_index\";\n\nconst embeddings = new OpenAIEmbeddings(undefined, {\nbaseOptions: { temperature: 0 },\n});\nconst store = new OpenSearchVectorStore(embeddings, { client, indexName });\nawait store.deleteIfExists();\n\nexpect(store).toBeDefined();\n\nawait store.addDocuments([\n{ pageContent: \"hello\", metadata: { a: 2 } },\n{ pageContent: \"car\", metadata: { a: 1 } },\n{ pageContent: \"adjective\", metadata: { a: 1 } },\n{ pageContent: \"hi\", metadata: { a: 1 } },\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/opensearch.int.test.ts","loc":{"lines":{"from":1,"to":32}}}}],["1136",{"pageContent":"const results1 = await store.similaritySearch(\"hello!\", 1);\n\nexpect(results1).toHaveLength(1);\nexpect(results1).toEqual([\nnew Document({ metadata: { a: 2 }, pageContent: \"hello\" }),\n]);\n\nconst results2 = await store.similaritySearchWithScore(\"hello!\", 1, {\na: 1,\n});\n\nexpect(results2).toHaveLength(1);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/opensearch.int.test.ts","loc":{"lines":{"from":48,"to":60}}}}],["1137",{"pageContent":"/* eslint-disable no-process-env */\n/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport { beforeEach, describe, expect, test } from \"@jest/globals\";\nimport { faker } from \"@faker-js/faker\";\nimport { PineconeClient } from \"@pinecone-database/pinecone\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { Document } from \"../../document.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { PineconeStore } from \"../pinecone.js\";\n\ndescribe(\"PineconeStore\", () => {\nlet pineconeStore: PineconeStore;\n\nbeforeEach(async () => {\nconst client = new PineconeClient();\n\nawait client.init({\nenvironment: process.env.PINECONE_ENVIRONMENT!,\napiKey: process.env.PINECONE_API_KEY!,\n});\n\nconst embeddings = new OpenAIEmbeddings();\nconst pineconeIndex = client.Index(process.env.PINECONE_INDEX!);\npineconeStore = new PineconeStore(embeddings, { pineconeIndex });\n});\n\ntest(\"user-provided ids\", async () => {\nconst documentId = uuidv4();\nconst pageContent = faker.lorem.sentence(5);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/pinecone.int.test.ts","loc":{"lines":{"from":1,"to":29}}}}],["1138",{"pageContent":"await pineconeStore.addDocuments(\n[{ pageContent, metadata: {} }],\n[documentId]\n);\n\nconst results = await pineconeStore.similaritySearch(pageContent, 1);\n\nexpect(results).toEqual([new Document({ metadata: {}, pageContent })]);\n});\n\ntest(\"auto-generated ids\", async () => {\nconst pageContent = faker.lorem.sentence(5);\n\nawait pineconeStore.addDocuments([\n{ pageContent, metadata: { foo: \"bar\" } },\n]);\n\nconst results = await pineconeStore.similaritySearch(pageContent, 1);\n\nexpect(results).toEqual([\nnew Document({ metadata: { foo: \"bar\" }, pageContent }),\n]);\n});\n\ntest(\"metadata filtering\", async () => {\nconst pageContent = faker.lorem.sentence(5);\nconst uuid = uuidv4();\n\nawait pineconeStore.addDocuments([\n{ pageContent, metadata: { foo: \"bar\" } },\n{ pageContent, metadata: { foo: uuid } },\n{ pageContent, metadata: { foo: \"qux\" } },\n]);\n\n// If the filter wasn't working, we'd get all 3 documents back\nconst results = await pineconeStore.similaritySearch(pageContent, 3, {\nfoo: uuid,\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/pinecone.int.test.ts","loc":{"lines":{"from":75,"to":112}}}}],["1139",{"pageContent":"expect(results).toEqual([\nnew Document({ metadata: { foo: uuid }, pageContent }),\n]);\n});\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/pinecone.int.test.ts","loc":{"lines":{"from":158,"to":162}}}}],["1140",{"pageContent":"/* eslint-disable @typescript-eslint/no-explicit-any */\nimport { jest, test, expect } from \"@jest/globals\";\nimport { FakeEmbeddings } from \"../../embeddings/fake.js\";\n\nimport { PineconeStore } from \"../pinecone.js\";\n\ntest(\"PineconeStore with external ids\", async () => {\nconst client = {\nupsert: jest.fn(),\nquery: jest.fn<any>().mockResolvedValue({\nmatches: [],\n}),\n};\nconst embeddings = new FakeEmbeddings();\n\nconst store = new PineconeStore(embeddings, { pineconeIndex: client as any });\n\nexpect(store).toBeDefined();\n\nawait store.addDocuments(\n[\n{\npageContent: \"hello\",\nmetadata: {\na: 1,\nb: { nested: [1, { a: 4 }] },\n},\n},\n],\n[\"id1\"]\n);\n\nexpect(client.upsert).toHaveBeenCalledTimes(1);\n\nexpect(client.upsert).toHaveBeenCalledWith({\nupsertRequest: {\nnamespace: undefined,\nvectors: [\n{\nid: \"id1\",\nmetadata: { a: 1, \"b.nested.0\": 1, \"b.nested.1.a\": 4, text: \"hello\" },\nvalues: [0.1, 0.2, 0.3, 0.4],\n},\n],\n},\n});\n\nconst results = await store.similaritySearch(\"hello\", 1);\n\nexpect(results).toHaveLength(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/pinecone.test.ts","loc":{"lines":{"from":1,"to":51}}}}],["1141",{"pageContent":"test(\"PineconeStore with generated ids\", async () => {\nconst client = {\nupsert: jest.fn(),\nquery: jest.fn<any>().mockResolvedValue({\nmatches: [],\n}),\n};\nconst embeddings = new FakeEmbeddings();\n\nconst store = new PineconeStore(embeddings, { pineconeIndex: client as any });\n\nexpect(store).toBeDefined();\n\nawait store.addDocuments([{ pageContent: \"hello\", metadata: { a: 1 } }]);\n\nexpect(client.upsert).toHaveBeenCalledTimes(1);\n\nconst results = await store.similaritySearch(\"hello\", 1);\n\nexpect(results).toHaveLength(0);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/pinecone.test.ts","loc":{"lines":{"from":83,"to":103}}}}],["1142",{"pageContent":"/* eslint-disable no-process-env */\n/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport { test, expect } from \"@jest/globals\";\nimport { createClient } from \"@supabase/supabase-js\";\n\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { Document } from \"../../document.js\";\nimport { SupabaseVectorStore } from \"../supabase.js\";\n\ntest(\"SupabaseVectorStore with external ids\", async () => {\nconst client = createClient(\nprocess.env.SUPABASE_URL!,\nprocess.env.SUPABASE_PRIVATE_KEY!\n);\n\nconst embeddings = new OpenAIEmbeddings();\n\nconst store = new SupabaseVectorStore(embeddings, { client });\n\nexpect(store).toBeDefined();\n\nawait store.addDocuments([\n{ pageContent: \"hello\", metadata: { a: 1 } },\n{ pageContent: \"hi\", metadata: { a: 1 } },\n{ pageContent: \"bye\", metadata: { a: 1 } },\n{ pageContent: \"what's this\", metadata: { a: 1 } },\n]);\n\nconst results = await store.similaritySearch(\"hello\", 1);\n\nexpect(results).toHaveLength(1);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/supabase.int.test.ts","loc":{"lines":{"from":1,"to":31}}}}],["1143",{"pageContent":"expect(results).toEqual([\nnew Document({ metadata: { a: 1 }, pageContent: \"hello\" }),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/supabase.int.test.ts","loc":{"lines":{"from":38,"to":41}}}}],["1144",{"pageContent":"/* eslint-disable no-process-env */\nimport { test, expect } from \"@jest/globals\";\nimport weaviate from \"weaviate-ts-client\";\nimport { WeaviateStore } from \"../weaviate.js\";\nimport { OpenAIEmbeddings } from \"../../embeddings/openai.js\";\nimport { Document } from \"../../document.js\";","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/weaviate.int.test.ts","loc":{"lines":{"from":1,"to":6}}}}],["1145",{"pageContent":"test.skip(\"WeaviateStore\", async () => {\n// Something wrong with the weaviate-ts-client types, so we need to disable\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconst client = (weaviate as any).client({\nscheme: process.env.WEAVIATE_SCHEME || \"https\",\nhost: process.env.WEAVIATE_HOST || \"localhost\",\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\napiKey: new (weaviate as any).ApiKey(\nprocess.env.WEAVIATE_API_KEY || \"default\"\n),\n});\nconst store = await WeaviateStore.fromTexts(\n[\"hello world\", \"hi there\", \"how are you\", \"bye now\"],\n[{ foo: \"bar\" }, { foo: \"baz\" }, { foo: \"qux\" }, { foo: \"bar\" }],\nnew OpenAIEmbeddings(),\n{\nclient,\nindexName: \"Test\",\ntextKey: \"text\",\nmetadataKeys: [\"foo\"],\n}\n);\n\nconst results = await store.similaritySearch(\"hello world\", 1);\nexpect(results).toEqual([\nnew Document({ pageContent: \"hello world\", metadata: { foo: \"bar\" } }),\n]);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/weaviate.int.test.ts","loc":{"lines":{"from":89,"to":115}}}}],["1146",{"pageContent":"const results2 = await store.similaritySearch(\"hello world\", 1, {\nwhere: {\noperator: \"Equal\",\npath: [\"foo\"],\nvalueText: \"baz\",\n},\n});\nexpect(results2).toEqual([\nnew Document({ pageContent: \"hi there\", metadata: { foo: \"baz\" } }),\n]);\n\nconst testDocumentWithObjectMetadata = new Document({\npageContent: \"this is the deep document world!\",\nmetadata: {\ndeep: {\nstring: \"deep string\",\ndeepdeep: {\nstring: \"even a deeper string\",\n},\n},\n},\n});\nconst documentStore = await WeaviateStore.fromDocuments(\n[testDocumentWithObjectMetadata],\nnew OpenAIEmbeddings(),\n{\nclient,\nindexName: \"DocumentTest\",\ntextKey: \"text\",\nmetadataKeys: [\"deep.string\", \"deep.deepdeep.string\"],\n}\n);","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/weaviate.int.test.ts","loc":{"lines":{"from":183,"to":214}}}}],["1147",{"pageContent":"const result3 = await documentStore.similaritySearch(\n\"this is the deep document world!\",\n1,\n{\nwhere: {\noperator: \"Equal\",\npath: [\"deep.string\"],\nvalueText: \"deep string\",\n},\n}\n);\nexpect(result3).toEqual([\nnew Document({\npageContent: \"this is the deep document world!\",\nmetadata: {\n\"deep.string\": \"deep string\",\n\"deep.deepdeep.string\": \"even a deeper string\",\n},\n}),\n]);\n});","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/tests/weaviate.int.test.ts","loc":{"lines":{"from":289,"to":309}}}}],["1148",{"pageContent":"import { v4 } from \"uuid\";\nimport type {\nWeaviateObject,\nWeaviateClient,\nWhereFilter,\n} from \"weaviate-ts-client\";\nimport { VectorStore } from \"./base.js\";\nimport { Embeddings } from \"../embeddings/base.js\";\nimport { Document } from \"../document.js\";\nimport { flattenObject } from \"../util/flatten.js\";\n\nexport interface WeaviateLibArgs {\nclient: WeaviateClient;\n/**\n* The name of the class in Weaviate. Must start with a capital letter.\n*/\nindexName: string;\ntextKey?: string;\nmetadataKeys?: string[];\n}\n\ninterface ResultRow {\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n[key: string]: any;\n}\n\nexport interface WeaviateFilter {\ndistance?: number;\nwhere: WhereFilter;\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":1,"to":30}}}}],["1149",{"pageContent":"class WeaviateStore extends VectorStore {\ndeclare FilterType: WeaviateFilter;\n\nprivate client: WeaviateClient;\n\nprivate indexName: string;\n\nprivate textKey: string;\n\nprivate queryAttrs: string[];\n\nconstructor(public embeddings: Embeddings, args: WeaviateLibArgs) {\nsuper(embeddings, args);\n\nthis.client = args.client;\nthis.indexName = args.indexName;\nthis.textKey = args.textKey || \"text\";\nthis.queryAttrs = [this.textKey];\n\nif (args.metadataKeys) {\nthis.queryAttrs = this.queryAttrs.concat(args.metadataKeys);\n}\n}\n\nasync addVectors(vectors: number[][], documents: Document[]): Promise<void> {\nconst batch: WeaviateObject[] = documents.map((document, index) => {\nif (Object.hasOwn(document.metadata, \"id\"))\nthrow new Error(\n\"Document inserted to Weaviate vectorstore should not have `id` in their metadata.\"\n);\n\nconst flattenedMetadata = flattenObject(document.metadata);\nreturn {","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":168,"to":200}}}}],["1150",{"pageContent":": this.indexName,\nid: v4(),\nvector: vectors[index],\nproperties: {\n[this.textKey]: document.pageContent,\n...flattenedMetadata,\n},\n};\n});\n\ntry {\nawait this.client.batch\n.objectsBatcher()\n.withObjects(...batch)\n.do();\n} catch (e) {\nthrow Error(`'Error in addDocuments' ${e}`);\n}\n}\n\nasync addDocuments(documents: Document[]): Promise<void> {\nreturn this.addVectors(\nawait this.embeddings.embedDocuments(documents.map((d) => d.pageContent)),\ndocuments\n);\n}\n\nasync similaritySearchVectorWithScore(\nquery: number[],\nk: number,\nfilter?: WeaviateFilter\n): Promise<[Document, number][]> {\ntry {\nlet builder = await this.client.graphql\n.get()\n.withClassName(this.indexName)\n.withFields(`${this.queryAttrs.join(\" \")} _additional { distance }`)\n.withNearVector({\nvector: query,\ndistance: filter?.distance,\n})\n.withLimit(k);\n\nif (filter?.where) {\nbuilder = builder.withWhere(filter.where);\n}\n\nconst result = await builder.do();","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":327,"to":374}}}}],["1151",{"pageContent":"const documents: [Document, number][] = [];\nfor (const data of result.data.Get[this.indexName]) {\nconst { [this.textKey]: text, _additional, ...rest }: ResultRow = data;\n\ndocuments.push([\nnew Document({\npageContent: text,\nmetadata: rest,\n}),\n_additional.distance,\n]);\n}\nreturn documents;\n} catch (e) {\nthrow Error(`'Error in similaritySearch' ${e}`);\n}\n}\n\nstatic fromTexts(\ntexts: string[],\nmetadatas: object | object[],\nembeddings: Embeddings,\nargs: WeaviateLibArgs\n): Promise<WeaviateStore> {\nconst docs: Document[] = [];\nfor (let i = 0; i < texts.length; i += 1) {\nconst metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\nconst newDoc = new Document({\npageContent: texts[i],\nmetadata,\n});\ndocs.push(newDoc);\n}\nreturn WeaviateStore.fromDocuments(docs, embeddings, args);\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":499,"to":533}}}}],["1152",{"pageContent":"static async fromDocuments(\ndocs: Document[],\nembeddings: Embeddings,\nargs: WeaviateLibArgs\n): Promise<WeaviateStore> {\nconst instance = new this(embeddings, args);\nawait instance.addDocuments(docs);\nreturn instance;\n}\n\nstatic async fromExistingIndex(\nembeddings: Embeddings,\nargs: WeaviateLibArgs\n): Promise<WeaviateStore> {\nreturn new this(embeddings, args);\n}\n}","metadata":{"source":"https://github.com/hwchase17/langchainjs/tree/main/langchain/src/vectorstores/weaviate.ts","loc":{"lines":{"from":666,"to":682}}}}]]